Fast_JJ coordinate_JJ descent_NN methods_NNS with_IN variable_JJ selection_NN for_IN non-negative_JJ matrix_NN factorization_NN
Nonnegative_NNP Matrix_NNP Factorization_NNP -LRB-_-LRB- NMF_NNP -RRB-_-RRB- is_VBZ an_DT effective_JJ dimension_NN reduction_NN method_NN for_IN non-negative_JJ dyadic_JJ data_NNS ,_, and_CC has_VBZ proven_VBN to_TO be_VB useful_JJ in_IN many_JJ areas_NNS ,_, such_JJ as_IN text_NN mining_NN ,_, bioinformatics_NNS and_CC image_NN processing_NN ._.
NMF_NN is_VBZ usually_RB formulated_VBN as_IN a_DT constrained_VBN non-convex_JJ optimization_NN problem_NN ,_, and_CC many_JJ algorithms_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG it_PRP ._.
Recently_RB ,_, a_DT coordinate_JJ descent_NN method_NN ,_, called_VBN FastHals_NNP ,_, has_VBZ been_VBN proposed_VBN to_TO solve_VB least_JJS squares_NNS NMF_NN and_CC is_VBZ regarded_VBN as_IN one_CD of_IN the_DT state-of-the-art_JJ techniques_NNS for_IN the_DT problem_NN ._.
In_IN this_DT paper_NN ,_, we_PRP first_RB show_VBP that_IN FastHals_NNP has_VBZ an_DT inefficiency_NN in_IN that_IN it_PRP uses_VBZ a_DT cyclic_JJ coordinate_JJ descent_NN scheme_NN and_CC thus_RB ,_, performs_VBZ unneeded_JJ descent_NN steps_NNS on_IN unimportant_JJ variables_NNS ._.
We_PRP then_RB present_VBP a_DT variable_JJ selection_NN scheme_NN that_WDT uses_VBZ the_DT gradient_NN of_IN the_DT objective_JJ function_NN to_TO arrive_VB at_IN a_DT new_JJ coordinate_JJ descent_NN method_NN ._.
Our_PRP$ new_JJ method_NN is_VBZ considerably_RB faster_RBR in_IN practice_NN and_CC we_PRP show_VBP that_IN it_PRP has_VBZ theoretical_JJ convergence_NN guarantees_NNS ._.
Moreover_RB when_WRB the_DT solution_NN is_VBZ sparse_JJ ,_, as_IN is_VBZ often_RB the_DT case_NN in_IN real_JJ applications_NNS ,_, our_PRP$ new_JJ method_NN benefits_NNS by_IN selecting_VBG important_JJ variables_NNS to_TO update_VB more_RBR often_RB ,_, thus_RB resulting_VBG in_IN higher_JJR speed_NN ._.
As_IN an_DT example_NN ,_, on_IN a_DT text_NN dataset_NN RCV1_NN ,_, our_PRP$ method_NN is_VBZ 7_CD times_NNS faster_RBR than_IN FastHals_NNP ,_, and_CC more_JJR than_IN 15_CD times_NNS faster_RBR when_WRB the_DT sparsity_NN is_VBZ increased_VBN by_IN adding_VBG an_DT L1_NN penalty_NN ._.
We_PRP also_RB develop_VBP new_JJ coordinate_JJ descent_NN methods_NNS when_WRB error_NN in_IN NMF_NN is_VBZ measured_VBN by_IN KL-divergence_NN by_IN applying_VBG the_DT Newton_NNP method_NN to_TO solve_VB the_DT one-variable_JJ sub-problems_NNS ._.
Experiments_NNS indicate_VBP that_IN our_PRP$ algorithm_NN for_IN minimizing_VBG the_DT KL-divergence_NN is_VBZ faster_RBR than_IN the_DT Lee_NNP &_CC Seung_NNP multiplicative_JJ rule_NN by_IN a_DT factor_NN of_IN 10_CD on_IN the_DT CBCL_NN image_NN dataset_NN ._.
