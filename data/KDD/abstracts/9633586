Primal_JJ sparse_JJ Max-margin_JJ Markov_NNP networks_NNS
Max-margin_JJ Markov_NNP networks_NNS -LRB-_-LRB- M3N_NN -RRB-_-RRB- have_VBP shown_VBN great_JJ promise_NN in_IN structured_JJ prediction_NN and_CC relational_JJ learning_NN ._.
Due_JJ to_TO the_DT KKT_NN conditions_NNS ,_, the_DT M3N_NN enjoys_VBZ dual_JJ sparsity_NN ._.
However_RB ,_, the_DT existing_VBG M3N_NN formulation_NN does_VBZ not_RB enjoy_VB primal_JJ sparsity_NN ,_, which_WDT is_VBZ a_DT desirable_JJ property_NN for_IN selecting_VBG significant_JJ features_NNS and_CC reducing_VBG the_DT risk_NN of_IN over-fitting_NN ._.
In_IN this_DT paper_NN ,_, we_PRP present_VBP an_DT l1-norm_NN regularized_VBD max-margin_JJ Markov_NNP network_NN -LRB-_-LRB- l1-M3N_NN -RRB-_-RRB- ,_, which_WDT enjoys_VBZ dual_JJ and_CC primal_JJ sparsity_NN simultaneously_RB ._.
To_TO learn_VB an_DT l1-M3N_NN ,_, we_PRP present_VBP three_CD methods_NNS including_VBG projected_VBN sub-gradient_JJ ,_, cutting-plane_NN ,_, and_CC a_DT novel_JJ EM-style_JJ algorithm_NN ,_, which_WDT is_VBZ based_VBN on_IN an_DT equivalence_JJ between_IN l1-M3N_NN and_CC an_DT adaptive_JJ M3N_NN ._.
We_PRP perform_VBP extensive_JJ empirical_JJ studies_NNS on_IN both_CC synthetic_JJ and_CC real_JJ data_NNS sets_NNS ._.
Our_PRP$ experimental_JJ results_NNS show_VBP that_IN :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- l1-M3N_NN can_MD effectively_RB select_VB significant_JJ features_NNS ;_: -LRB-_-LRB- 2_LS -RRB-_-RRB- l1-M3N_NN can_MD perform_VB as_RB well_RB as_IN the_DT pseudo-primal_JJ sparse_FW Laplace_FW M3N_NN in_IN prediction_NN accuracy_NN ,_, while_IN consistently_RB outperforms_VBZ other_JJ competing_VBG methods_NNS that_WDT enjoy_VBP either_CC primal_JJ or_CC dual_JJ sparsity_NN ;_: and_CC -LRB-_-LRB- 3_LS -RRB-_-RRB- the_DT EM-algorithm_NN is_VBZ more_RBR robust_JJ than_IN the_DT other_JJ two_CD in_IN pre-diction_JJ accuracy_NN and_CC time_NN efficiency_NN ._.
