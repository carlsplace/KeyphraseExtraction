Smoothing_VBG techniques_NNS for_IN adaptive_JJ online_JJ language_NN models_NNS :_: topic_NN tracking_NN in_IN tweet_NN streams_NNS
We_PRP are_VBP interested_JJ in_IN the_DT problem_NN of_IN tracking_VBG broad_JJ topics_NNS such_JJ as_IN ``_`` baseball_NN ''_'' and_CC ``_`` fashion_NN ''_'' in_IN continuous_JJ streams_NNS of_IN short_JJ texts_NNS ,_, exemplified_VBN by_IN tweets_NNS from_IN the_DT microblogging_NN service_NN Twitter_NNP ._.
The_DT task_NN is_VBZ conceived_VBN as_IN a_DT language_NN modeling_NN problem_NN where_WRB per-topic_JJ models_NNS are_VBP trained_VBN using_VBG hashtags_NNS in_IN the_DT tweet_NN stream_NN ,_, which_WDT serve_VBP as_IN proxies_NNS for_IN topic_NN labels_NNS ._.
Simple_JJ perplexity-based_JJ classifiers_NNS are_VBP then_RB applied_VBN to_TO filter_NN the_DT tweet_NN stream_NN for_IN topics_NNS of_IN interest_NN ._.
Within_IN this_DT framework_NN ,_, we_PRP evaluate_VBP ,_, both_DT intrinsically_RB and_CC extrinsically_RB ,_, smoothing_VBG techniques_NNS for_IN integrating_VBG ``_`` foreground_NN ''_'' models_NNS -LRB-_-LRB- to_TO capture_VB recency_NN -RRB-_-RRB- and_CC ``_`` background_NN ''_'' models_NNS -LRB-_-LRB- to_TO combat_VB sparsity_NN -RRB-_-RRB- ,_, as_RB well_RB as_IN different_JJ techniques_NNS for_IN retaining_VBG history_NN ._.
Experiments_NNS show_VBP that_IN unigram_JJ language_NN models_NNS smoothed_VBD using_VBG a_DT normalized_VBN extension_NN of_IN stupid_JJ backoff_NN and_CC a_DT simple_JJ queue_NN for_IN history_NN retention_NN performs_VBZ well_RB on_IN the_DT task_NN ._.
