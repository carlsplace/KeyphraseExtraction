Magical_JJ thinking_NN in_IN data_NNS mining_NN :_: lessons_NNS from_IN CoIL_NN challenge_NN 2000_CD
CoIL_NN challenge_NN 2000_CD was_VBD a_DT supervised_JJ learning_NN contest_NN that_WDT attracted_VBD 43_CD entries_NNS ._.
The_DT authors_NNS of_IN 29_CD entries_NNS later_RB wrote_VBD explanations_NNS of_IN their_PRP$ work_NN ._.
This_DT paper_NN discusses_VBZ these_DT reports_NNS and_CC reaches_VBZ three_CD main_JJ conclusions_NNS ._.
First_JJ ,_, naive_JJ Bayesian_JJ classifiers_NNS remain_VBP competitive_JJ in_IN practice_NN :_: they_PRP were_VBD used_VBN by_IN both_CC the_DT winning_JJ entry_NN and_CC the_DT next_JJ best_JJS entry_NN ._.
Second_RB ,_, identifying_VBG feature_NN interactions_NNS correctly_RB is_VBZ important_JJ for_IN maximizing_VBG predictive_JJ accuracy_NN :_: this_DT was_VBD the_DT difference_NN between_IN the_DT winning_JJ classifier_NN and_CC all_DT others_NNS ._.
Third_JJ and_CC most_RBS important_JJ ,_, too_RB many_JJ researchers_NNS and_CC practitioners_NNS in_IN data_NNS mining_NN do_VBP not_RB appreciate_VB properly_RB the_DT issue_NN of_IN statistical_JJ significance_NN and_CC the_DT danger_NN of_IN overfitting_NN ._.
Given_VBN a_DT dataset_NN such_JJ as_IN the_DT one_NN for_IN the_DT CoIL_NN contest_NN ,_, it_PRP is_VBZ pointless_JJ to_TO apply_VB a_DT very_RB complicated_JJ learning_NN algorithm_NN ,_, or_CC to_TO perform_VB a_DT very_RB time-consuming_JJ model_NN search_NN ._.
In_IN either_DT ease_NN ,_, one_CD is_VBZ likely_JJ to_TO overfit_VB the_DT training_NN data_NNS and_CC to_TO fool_VB oneself_NN in_IN estimating_VBG predictive_JJ accuracy_NN and_CC in_IN discovering_VBG useful_JJ correlations_NNS ._.
