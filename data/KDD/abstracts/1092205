Model_NNP compression_NN
Often_RB the_DT best_JJS performing_VBG supervised_JJ learning_NN models_NNS are_VBP ensembles_NNS of_IN hundreds_NNS or_CC thousands_NNS of_IN base-level_JJ classifiers_NNS ._.
Unfortunately_RB ,_, the_DT space_NN required_VBN to_TO store_VB this_DT many_JJ classifiers_NNS ,_, and_CC the_DT time_NN required_VBN to_TO execute_VB them_PRP at_IN run-time_NN ,_, prohibits_VBZ their_PRP$ use_NN in_IN applications_NNS where_WRB test_NN sets_NNS are_VBP large_JJ -LRB-_-LRB- e.g._FW Google_NNP -RRB-_-RRB- ,_, where_WRB storage_NN space_NN is_VBZ at_IN a_DT premium_NN -LRB-_-LRB- e.g._FW PDAs_NNS -RRB-_-RRB- ,_, and_CC where_WRB computational_JJ power_NN is_VBZ limited_VBN -LRB-_-LRB- e.g._FW hea-ring_JJ aids_NNS -RRB-_-RRB- ._.
We_PRP present_VBP a_DT method_NN for_IN ``_`` compressing_VBG ''_'' large_JJ ,_, complex_JJ ensembles_NNS into_IN smaller_JJR ,_, faster_JJR models_NNS ,_, usually_RB without_IN significant_JJ loss_NN in_IN performance_NN ._.
