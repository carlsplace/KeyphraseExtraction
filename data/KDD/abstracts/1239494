The_DT distributed_VBN boosting_VBG algorithm_NN
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT general_JJ framework_NN for_IN distributed_VBN boosting_VBG intended_JJ for_IN efficient_JJ integrating_VBG specialized_JJ classifiers_NNS learned_VBD over_IN very_RB large_JJ and_CC distributed_VBN homogeneous_JJ databases_NNS that_WDT can_MD not_RB be_VB merged_VBN at_IN a_DT single_JJ location_NN ._.
Our_PRP$ distributed_VBN boosting_VBG algorithm_NN can_MD also_RB be_VB used_VBN as_IN a_DT parallel_JJ classification_NN technique_NN ,_, where_WRB a_DT massive_JJ database_NN that_WDT can_MD not_RB fit_VB into_IN main_JJ computer_NN memory_NN is_VBZ partitioned_VBN into_IN disjoint_NN subsets_NNS for_IN a_DT more_RBR efficient_JJ analysis_NN ._.
In_IN the_DT proposed_VBN method_NN ,_, at_IN each_DT boosting_VBG round_NN the_DT classifiers_NNS are_VBP first_RB learned_VBN from_IN disjoint_FW datasets_FW and_CC then_RB exchanged_VBD amongst_IN the_DT sites_NNS ._.
Finally_RB the_DT classifiers_NNS are_VBP combined_VBN into_IN a_DT weighted_JJ voting_NN ensemble_NN on_IN each_DT disjoint_JJ data_NN set_NN ._.
The_DT ensemble_NN that_WDT is_VBZ applied_VBN to_TO an_DT unseen_JJ test_NN set_NN represents_VBZ an_DT ensemble_NN of_IN ensembles_NNS built_VBN on_IN all_DT distributed_VBN sites_NNS ._.
In_IN experiments_NNS performed_VBN on_IN four_CD large_JJ data_NNS sets_VBZ the_DT proposed_JJ distributed_VBN boosting_VBG method_NN achieved_VBD classification_NN accuracy_NN comparable_JJ or_CC even_RB slightly_RB better_JJR than_IN the_DT standard_JJ boosting_VBG algorithm_NN while_IN requiring_VBG less_JJR memory_NN and_CC less_RBR computational_JJ time_NN ._.
In_IN addition_NN ,_, the_DT communication_NN overhead_NN of_IN the_DT distributed_VBN boosting_VBG algorithm_NN is_VBZ very_RB small_JJ making_VBG it_PRP a_DT viable_JJ alternative_NN to_TO the_DT standard_NN boosting_VBG for_IN large-scale_JJ databases_NNS ._.
