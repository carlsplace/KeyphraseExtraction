Random_JJ projection_NN in_IN dimensionality_NN reduction_NN :_: applications_NNS to_TO image_NN and_CC text_NN data_NNS
Random_JJ projections_NNS have_VBP recently_RB emerged_VBN as_IN a_DT powerful_JJ method_NN for_IN dimensionality_NN reduction_NN ._.
Theoretical_JJ results_NNS indicate_VBP that_IN the_DT method_NN preserves_VBZ distances_NNS quite_RB nicely_RB ;_: however_RB ,_, empirical_JJ results_NNS are_VBP sparse_JJ ._.
We_PRP present_VBP experimental_JJ results_NNS on_IN using_VBG random_JJ projection_NN as_IN a_DT dimensionality_NN reduction_NN tool_NN in_IN a_DT number_NN of_IN cases_NNS ,_, where_WRB the_DT high_JJ dimensionality_NN of_IN the_DT data_NNS would_MD otherwise_RB lead_VB to_TO burden-some_JJ computations_NNS ._.
Our_PRP$ application_NN areas_NNS are_VBP the_DT processing_NN of_IN both_CC noisy_JJ and_CC noiseless_JJ images_NNS ,_, and_CC information_NN retrieval_NN in_IN text_NN documents_NNS ._.
We_PRP show_VBP that_IN projecting_VBG the_DT data_NNS onto_IN a_DT random_JJ lower-dimensional_JJ subspace_NN yields_NNS results_VBZ comparable_JJ to_TO conventional_JJ dimensionality_NN reduction_NN methods_NNS such_JJ as_IN principal_JJ component_NN analysis_NN :_: the_DT similarity_NN of_IN data_NN vectors_NNS is_VBZ preserved_VBN well_RB under_IN random_JJ projection_NN ._.
However_RB ,_, using_VBG random_JJ projections_NNS is_VBZ computationally_RB significantly_RB less_RBR expensive_JJ than_IN using_VBG ,_, e.g._FW ,_, principal_JJ component_NN analysis_NN ._.
We_PRP also_RB show_VBP experimentally_RB that_IN using_VBG a_DT sparse_JJ random_JJ matrix_NN gives_VBZ additional_JJ computational_JJ savings_NNS in_IN random_JJ projection_NN ._.
ontext_NN of_IN sparse_JJ text_NN document_NN data_NNS ._.
For_IN a_DT sparse_JJ data_NN matrix_NN XdN_NN with_IN about_IN c_NN nonzero_NN entries_NNS per_IN column_NN ,_, the_DT computational_JJ complexity_NN of_IN SVD_NNP is_VBZ of_IN order_NN O_NN -LRB-_-LRB- dcN_NN -RRB-_-RRB- -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
Latent_JJ semantic_JJ indexing_NN -LRB-_-LRB- LSI_NNP -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =_JJ 9_CD ,_, 22_CD -RRB-_-RRB- is_VBZ a_DT -_: =_JJ -_: dimensionality_NN reduction_NN method_NN for_IN text_NN document_NN data_NNS ._.
Using_VBG LSI_NNP ,_, the_DT document_NN data_NNS is_VBZ presented_VBN in_IN a_DT lower-dimensional_JJ \_NN topic_NN ''_'' space_NN :_: the_DT documents_NNS are_VBP characterized_VBN by_IN some_DT underlying_JJ -LRB-_-LRB- latent_JJ
an_DT space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- has_VBZ used_VBN random_JJ projections_NNS in_IN learning_VBG high-dimensional_JJ Gaussian_JJ mixture_NN models_NNS ._.
Other_JJ applications_NNS of_IN random_JJ projection_NN include_VBP e.g._FW =_JJ -_: =[_NN 4_CD ,_, 28_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT problems_NNS of_IN dimensionality_NN reduction_NN and_CC similarity_NN search_NN have_VBP often_RB been_VBN addressed_VBN in_IN the_DT information_NN retrieval_NN literature_NN ,_, and_CC other_JJ approaches_NNS than_IN random_JJ projection_NN have_VBP been_VBN presented_VBN ._.
Ost_NN
g_NN the_DT time_NN series_NN into_IN sections_NNS and_CC indexing_NN only_RB the_DT section_NN means_VBZ ._.
Aggarwal_NNP et_FW al._FW -LRB-_-LRB- 2_LS -RRB-_-RRB- index_NN market_NN basket_NN data_NNS by_IN a_DT specic_JJ signature_NN table_NN ,_, which_WDT easens_VBZ the_DT similarity_NN search_NN ._.
Wavelet_NNP transforms_VBZ -LRB-_-LRB- -LRB-_-LRB- =_JJ -_: =_JJ 12_CD ,_, 27_CD -RRB-_-RRB- -_: =_JJ -_: etc._NN -RRB-_-RRB- are_VBP a_DT common_JJ method_NN of_IN signal_NN compression_NN ._.
2_CD ._.
METHODSFORDIMENSIONALITYREDUCTION_NN 2.1_CD Random_NNP projection_NN In_IN random_JJ projection_NN ,_, the_DT original_JJ d-dimensional_JJ data_NNS is_VBZ projected_VBN to_TO a_DT k-dimensional_NN -LRB-_-LRB- ksd_NN -RRB-_-RRB-
\/_: websom_NN \/_: cause_VB signicant_JJ distortions_NNS in_IN the_DT data_NNS set_VBN if_IN R_NN is_VBZ not_RB orthogonal_JJ ._.
Orthogonalizing_NNP R_NNP is_VBZ unfortunately_RB computationally_RB expensive_JJ ._.
Instead_RB ,_, we_PRP can_MD rely_VB on_IN a_DT result_NN presented_VBN by_IN Hecht-Nielsen_JJ -LRB-_-LRB- =_JJ -_: =_JJ 13_CD -RRB-_-RRB- -_: =_JJ -_: :_: in_IN a_DT high-dimensional_JJ space_NN ,_, there_EX exists_VBZ a_DT much_RB larger_JJR number_NN of_IN almost_RB orthogonal_JJ than_IN orthogonal_JJ directions_NNS ._.
Thus_RB vectors_NNS having_VBG random_JJ directions_NNS might_MD be_VB suciently_RB close_JJ to_TO orthogonal_JJ ,_, and_CC equi_NNS
An_DT image_NN is_VBZ presented_VBN as_IN a_DT matrix_NN of_IN pixel_NN brightness_NN values_NNS ,_, the_DT distribution_NN of_IN which_WDT is_VBZ generally_RB approximately_RB Gaussian_JJ :_: symmetric_JJ and_CC bell-shaped_JJ ._.
Text_NN document_NN data_NNS is_VBZ presented_VBN in_IN vector_NN space_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_JJ -_: ,_, in_IN which_WDT each_DT document_NN forms_VBZ one_CD d-dimensional_NN vector_NN where_WRB d_NN is_VBZ the_DT vocabulary_NN size_NN ._.
The_DT i-th_JJ element_NN of_IN the_DT vector_NN indicates_VBZ -LRB-_-LRB- some_DT function_NN of_IN -RRB-_-RRB- the_DT frequency_NN of_IN the_DT i-th_JJ vocabulary_NN term_NN in_IN the_DT doc_NN
ough_VB the_DT origin_NN is_VBZ p_NN k_NN =d_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ._.
The_DT choice_NN of_IN the_DT random_JJ matrix_NN R_NN is_VBZ one_CD of_IN the_DT key_JJ points_NNS of_IN interest_NN ._.
The_DT elements_NNS r_NN ij_NN of_IN R_NN are_VBP often_RB Gaussian_JJ distributed_VBN ,_, but_CC this_DT need_MD not_RB be_VB the_DT case_NN ._.
Achlioptas_NN =_JJ -_: =[_NN 1_CD -_: =-]_NN has_VBZ recently_RB shown_VBN that_IN the_DT Gaussian_JJ distribution_NN can_MD be_VB replaced_VBN by_IN a_DT much_RB simpler_JJR distribution_NN such_JJ as_IN r_NN ij_NN =_JJ p_NN 3_CD 8_CD -RRB-_-RRB- -RRB-_-RRB- :_: +1_NN with_IN probability_NN 1_CD 6_CD 0_CD with_IN probability_NN 2_CD 3_CD 1_CD with_IN probability_NN 1_CD 6_CD :_: -LRB-_-LRB-
of_IN N_NN d-dimensional_NN observations_NNS ,_, X_NN RP_NN kN_NN =_JJ RkdXdN_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- is_VBZ the_DT projection_NN of_IN the_DT data_NNS onto_IN a_DT lower_JJR k-dimensional_NN subspace_NN ._.
The_DT key_JJ idea_NN of_IN random_JJ mapping_NN arises_VBZ from_IN the_DT Johnson-Lindenstrauss_JJ lemma_NN -LRB-_-LRB- 15_CD =_JJ -_: =]_NN -_: =_JJ -_: :_: if_IN points_NNS in_IN a_DT vector_NN space_NN are_VBP projected_VBN onto_IN a_DT randomly_RB selected_VBN subspace_NN of_IN suitably_RB high_JJ dimension_NN ,_, then_RB the_DT distances_NNS between_IN the_DT points_NNS are_VBP approximately_RB preserved_VBN ._.
For_IN a_DT simple_JJ proof_NN of_IN this_DT
o_NN using_VBG LSI_NNP and_CC SOM_NNP ._.
Kleinberg_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- and_CC Indyk_NNP and_CC Motwani_NNP -LRB-_-LRB- 14_CD -RRB-_-RRB- use_VBP random_JJ projections_NNS in_IN nearest-neighbor_JJ search_NN in_IN a_DT high_JJ dimensional_JJ Euclidean_JJ space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN =_JJ -_: =[_NN 6_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: has_VBZ used_VBN random_JJ projections_NNS in_IN learning_VBG high-dimensional_JJ Gaussian_JJ mixture_NN models_NNS ._.
Other_JJ applications_NNS of_IN random_JJ projection_NN include_VBP e.g._FW -LRB-_-LRB- 4_CD ,_, 28_CD -RRB-_-RRB- ._.
The_DT problems_NNS of_IN dimensionality_NN reduction_NN and_CC similarity_NN
ted_VBN set_NN of_IN documents_NNS ._.
In_IN their_PRP$ approach_NN ,_, the_DT columns_NNS of_IN the_DT random_JJ projection_NN matrix_NN are_VBP assumed_VBN strictly_RB orthogonal_JJ ,_, but_CC actually_RB this_DT need_MD not_RB be_VB the_DT case_NN ,_, as_IN we_PRP shall_MD see_VB in_IN our_PRP$ experiments_NNS ._.
Kaski_NN =_JJ -_: =[_NN 17_CD ,_, 16_CD -RRB-_-RRB- -_: =_SYM -_: has_VBZ presented_VBN experimental_JJ results_NNS in_IN using_VBG the_DT random_JJ mapping_NN in_IN the_DT context_NN of_IN the_DT WEBSOM_NN 1_CD system_NN ._.
Kurimo_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- applies_VBZ random_JJ projection_NN to_TO the_DT indexing_NN of_IN audio_JJ documents_NNS ,_, prior_RB to_TO using_VBG LSI_NNP and_CC
in_IN a_DT vector_NN space_NN are_VBP projected_VBN onto_IN a_DT randomly_RB selected_VBN subspace_NN of_IN suitably_RB high_JJ dimension_NN ,_, then_RB the_DT distances_NNS between_IN the_DT points_NNS are_VBP approximately_RB preserved_VBN ._.
For_IN a_DT simple_JJ proof_NN of_IN this_DT result_NN ,_, see_VBP =_JJ -_: =[_NN 10_CD ,_, 8_CD -_: =-]_CD ._.
Random_JJ projection_NN is_VBZ computationally_RB very_RB simple_JJ :_: forming_VBG the_DT random_JJ matrix_NN R_NN and_CC projecting_VBG the_DT d_NN N_NN data_NNS matrix_NN X_NN into_IN k_NN dimensions_NNS is_VBZ of_IN order_NN O_NN -LRB-_-LRB- dkN_NN -RRB-_-RRB- ,_, and_CC if_IN the_DT data_NNS matrix_NN X_NN is_VBZ sparse_JJ with_IN ab_NN
dimensionality_NN reduction_NN and_CC similarity_NN search_NN have_VBP often_RB been_VBN addressed_VBN in_IN the_DT information_NN retrieval_NN literature_NN ,_, and_CC other_JJ approaches_NNS than_IN random_JJ projection_NN have_VBP been_VBN presented_VBN ._.
Ostrovsky_NNP and_CC Rabani_NNP =_SYM -_: =[_NN 21_CD -RRB-_-RRB- -_: =_JJ -_: give_VB a_DT dimension_NN reduction_NN operation_NN that_WDT is_VBZ suitable_JJ for_IN clustering_NN ._.
Agrawal_NNP et_FW al._FW -LRB-_-LRB- 3_LS -RRB-_-RRB- map_NN time_NN series_NN into_IN frequency_NN domain_NN by_IN the_DT discrete_JJ Fourier_NN transform_VB and_CC only_RB retain_VB thesrst_JJ few_JJ frequencie_NNS
se_FW size_NN is_VBZ dd_NN for_IN d-dimensional_JJ data_NNS -RRB-_-RRB- is_VBZ very_RB expensive_JJ to_TO compute_VB ._.
The_DT computational_JJ complexity_NN of_IN estimating_VBG the_DT PCA_NNP is_VBZ O_NN -LRB-_-LRB- d_NN 2_CD N_NN -RRB-_-RRB- +_CC O_NN -LRB-_-LRB- d_NN 3_CD -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
There_EX exists_VBZ computationally_RB less_RBR expensive_JJ methods_NNS -LRB-_-LRB- =_JJ -_: =_JJ 26_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: forsnding_VBG only_RB a_DT few_JJ eigenvectors_NNS and_CC eigenvalues_NNS of_IN a_DT large_JJ matrix_NN ;_: in_IN our_PRP$ experiments_NNS ,_, we_PRP use_VBP appropriate_JJ Matlab_NNP routines_NNS to_TO realize_VB these_DT ._.
A_DT closely_RB related_JJ method_NN is_VBZ singular_JJ value_NN decomposition_NN
n_NN methods_NNS ._.
Section_NN 3_CD gives_VBZ the_DT experimental_JJ results_NNS of_IN dimensionality_NN reduction_NN on_IN image_NN data_NNS ,_, and_CC Section_NNP 4_CD on_IN text_NN data_NNS ._.
Finally_RB ,_, Section_NN 5_CD gives_VBZ a_DT conclusion_NN ._.
1.1_CD Related_JJ work_NN Papadimitriou_NNP et_FW al._FW =_SYM -_: =[_NN 22_CD -_: =-]_CD use_NN random_JJ projection_NN in_IN the_DT preprocessing_NN of_IN textual_JJ data_NNS ,_, prior_RB to_TO applying_VBG LSI_NNP ._.
They_PRP present_VBP experimental_JJ results_NNS on_IN an_DT articially_RB generated_VBN set_NN of_IN documents_NNS ._.
In_IN their_PRP$ approach_NN ,_, the_DT columns_NNS of_IN th_DT
se_FW size_NN is_VBZ dd_NN for_IN d-dimensional_JJ data_NNS -RRB-_-RRB- is_VBZ very_RB expensive_JJ to_TO compute_VB ._.
The_DT computational_JJ complexity_NN of_IN estimating_VBG the_DT PCA_NNP is_VBZ O_NN -LRB-_-LRB- d_NN 2_CD N_NN -RRB-_-RRB- +_CC O_NN -LRB-_-LRB- d_NN 3_CD -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
There_EX exists_VBZ computationally_RB less_RBR expensive_JJ methods_NNS -LRB-_-LRB- =_JJ -_: =_JJ 26_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: forsnding_VBG only_RB a_DT few_JJ eigenvectors_NNS and_CC eigenvalues_NNS of_IN a_DT large_JJ matrix_NN ;_: in_IN our_PRP$ experiments_NNS ,_, we_PRP use_VBP appropriate_JJ Matlab_NNP routines_NNS to_TO realize_VB these_DT ._.
A_DT closely_RB related_JJ method_NN is_VBZ singular_JJ value_NN decomposition_NN
ng_NN the_DT random_JJ mapping_NN in_IN the_DT context_NN of_IN the_DT WEBSOM_NN 1_CD system_NN ._.
Kurimo_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- applies_VBZ random_JJ projection_NN to_TO the_DT indexing_NN of_IN audio_JJ documents_NNS ,_, prior_RB to_TO using_VBG LSI_NNP and_CC SOM_NNP ._.
Kleinberg_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- and_CC Indyk_NNP and_CC Motwani_NNP =_SYM -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: use_VB random_JJ projections_NNS in_IN nearest-neighbor_JJ search_NN in_IN a_DT high_JJ dimensional_JJ Euclidean_JJ space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- has_VBZ used_VBN random_JJ projections_NNS in_IN learning_VBG high-dimensiona_NNS
X_NN SV_NN D_NN =_JJ U_NN T_NN k_NN X_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- where_WRB Uk_NN is_VBZ of_IN size_NN dk_NN and_CC contains_VBZ these_DT k_NN singular_JJ vectors_NNS ._.
Like_IN PCA_NNP ,_, SVD_NNP is_VBZ also_RB expensive_JJ to_TO compute_VB ._.
There_EX exists_VBZ numerical_JJ routines_NNS such_JJ as_IN the_DT power_NN or_CC the_DT Lanczos_NN method_NN -LRB-_-LRB- =_JJ -_: =_JJ 5_CD -_: =-]_NN that_WDT are_VBP more_RBR ecient_JJ than_IN PCA_NNP for_IN sparse_JJ data_NNS matrices_NNS X_NN ,_, and_CC that_DT is_VBZ why_WRB we_PRP shall_MD use_VB SVD_NNP instead_RB of_IN PCA_NNP in_IN the_DT context_NN of_IN sparse_JJ text_NN document_NN data_NNS ._.
For_IN a_DT sparse_JJ data_NN matrix_NN XdN_NN with_IN about_IN c_NN nonze_NN
o_NN using_VBG LSI_NNP and_CC SOM_NNP ._.
Kleinberg_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- and_CC Indyk_NNP and_CC Motwani_NNP -LRB-_-LRB- 14_CD -RRB-_-RRB- use_VBP random_JJ projections_NNS in_IN nearest-neighbor_JJ search_NN in_IN a_DT high_JJ dimensional_JJ Euclidean_JJ space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN =_JJ -_: =[_NN 6_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: has_VBZ used_VBN random_JJ projections_NNS in_IN learning_VBG high-dimensional_JJ Gaussian_JJ mixture_NN models_NNS ._.
Other_JJ applications_NNS of_IN random_JJ projection_NN include_VBP e.g._FW -LRB-_-LRB- 4_CD ,_, 28_CD -RRB-_-RRB- ._.
The_DT problems_NNS of_IN dimensionality_NN reduction_NN and_CC similarity_NN
but_CC actually_RB this_DT need_MD not_RB be_VB the_DT case_NN ,_, as_IN we_PRP shall_MD see_VB in_IN our_PRP$ experiments_NNS ._.
Kaski_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- has_VBZ presented_VBN experimental_JJ results_NNS in_IN using_VBG the_DT random_JJ mapping_NN in_IN the_DT context_NN of_IN the_DT WEBSOM_NN 1_CD system_NN ._.
Kurimo_NN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: applies_VBZ random_JJ projection_NN to_TO the_DT indexing_NN of_IN audio_JJ documents_NNS ,_, prior_RB to_TO using_VBG LSI_NNP and_CC SOM_NNP ._.
Kleinberg_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- and_CC Indyk_NNP and_CC Motwani_NNP -LRB-_-LRB- 14_CD -RRB-_-RRB- use_VBP random_JJ projections_NNS in_IN nearest-neighbor_JJ search_NN in_IN a_DT high_JJ dimensi_NN
CT_NN is_VBZ also_RB optimal_JJ for_IN human_JJ eye_NN :_: the_DT distortions_NNS introduced_VBN occur_VBP at_IN the_DT highest_JJS frequencies_NNS only_RB ,_, and_CC the_DT human_JJ eye_NN tends_VBZ to_TO neglect_NN these_DT as_IN noise_NN ._.
DCT_NNP can_MD be_VB performed_VBN by_IN simple_JJ matrix_NN operations_NNS =_JJ -_: =[_NN 23_CD ,_, 27_CD -RRB-_-RRB- -_: =_JJ -_: :_: an_DT image_NN is_VBZ transformed_VBN to_TO the_DT DCT_NNP space_NN and_CC dimensionality_NN reduction_NN is_VBZ done_VBN in_IN the_DT inverse_NN transform_VB by_IN discarding_VBG the_DT transform_VB coecients_NNS corresponding_VBG to_TO the_DT highest_JJS frequencies_NNS ._.
Computing_NNP the_DT D_NN
uction_NN operation_NN that_WDT is_VBZ suitable_JJ for_IN clustering_NN ._.
Agrawal_NNP et_FW al._FW -LRB-_-LRB- 3_LS -RRB-_-RRB- map_NN time_NN series_NN into_IN frequency_NN domain_NN by_IN the_DT discrete_JJ Fourier_NN transform_VB and_CC only_RB retain_VB thesrst_JJ few_JJ frequencies_NNS ._.
Keogh_NNP and_CC Pazzani_NNP =_SYM -_: =[_NN 18_CD -_: =-]_CD reduce_VB the_DT dimension_NN of_IN time_NN series_NN data_NNS by_IN segmenting_VBG the_DT time_NN series_NN into_IN sections_NNS and_CC indexing_NN only_RB the_DT section_NN means_VBZ ._.
Aggarwal_NNP et_FW al._FW -LRB-_-LRB- 2_LS -RRB-_-RRB- index_NN market_NN basket_NN data_NNS by_IN a_DT specic_JJ signature_NN table_NN ,_, wh_NN
g_NN the_DT time_NN series_NN into_IN sections_NNS and_CC indexing_NN only_RB the_DT section_NN means_VBZ ._.
Aggarwal_NNP et_FW al._FW -LRB-_-LRB- 2_LS -RRB-_-RRB- index_NN market_NN basket_NN data_NNS by_IN a_DT specic_JJ signature_NN table_NN ,_, which_WDT easens_VBZ the_DT similarity_NN search_NN ._.
Wavelet_NNP transforms_VBZ -LRB-_-LRB- -LRB-_-LRB- =_JJ -_: =_JJ 12_CD ,_, 27_CD -RRB-_-RRB- -_: =_JJ -_: etc._NN -RRB-_-RRB- are_VBP a_DT common_JJ method_NN of_IN signal_NN compression_NN ._.
2_CD ._.
METHODSFORDIMENSIONALITYREDUCTION_NN 2.1_CD Random_NNP projection_NN In_IN random_JJ projection_NN ,_, the_DT original_JJ d-dimensional_JJ data_NNS is_VBZ projected_VBN to_TO a_DT k-dimensional_NN -LRB-_-LRB- ksd_NN -RRB-_-RRB-
n_NN retrieval_NN literature_NN ,_, and_CC other_JJ approaches_NNS than_IN random_JJ projection_NN have_VBP been_VBN presented_VBN ._.
Ostrovsky_NNP and_CC Rabani_NNP -LRB-_-LRB- 21_CD -RRB-_-RRB- give_VBP a_DT dimension_NN reduction_NN operation_NN that_WDT is_VBZ suitable_JJ for_IN clustering_NN ._.
Agrawal_NNP et_FW al._FW =_SYM -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: map_NN time_NN series_NN into_IN frequency_NN domain_NN by_IN the_DT discrete_JJ Fourier_NN transform_VB and_CC only_RB retain_VB thesrst_JJ few_JJ frequencies_NNS ._.
Keogh_NNP and_CC Pazzani_NNP -LRB-_-LRB- 18_CD -RRB-_-RRB- reduce_VB the_DT dimension_NN of_IN time_NN series_NN data_NNS by_IN segmenting_VBG the_DT time_NN
in_IN a_DT vector_NN space_NN are_VBP projected_VBN onto_IN a_DT randomly_RB selected_VBN subspace_NN of_IN suitably_RB high_JJ dimension_NN ,_, then_RB the_DT distances_NNS between_IN the_DT points_NNS are_VBP approximately_RB preserved_VBN ._.
For_IN a_DT simple_JJ proof_NN of_IN this_DT result_NN ,_, see_VBP =_JJ -_: =[_NN 10_CD ,_, 8_CD -_: =-]_CD ._.
Random_JJ projection_NN is_VBZ computationally_RB very_RB simple_JJ :_: forming_VBG the_DT random_JJ matrix_NN R_NN and_CC projecting_VBG the_DT d_NN N_NN data_NNS matrix_NN X_NN into_IN k_NN dimensions_NNS is_VBZ of_IN order_NN O_NN -LRB-_-LRB- dkN_NN -RRB-_-RRB- ,_, and_CC if_IN the_DT data_NNS matrix_NN X_NN is_VBZ sparse_JJ with_IN ab_NN
an_DT space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- has_VBZ used_VBN random_JJ projections_NNS in_IN learning_VBG high-dimensional_JJ Gaussian_JJ mixture_NN models_NNS ._.
Other_JJ applications_NNS of_IN random_JJ projection_NN include_VBP e.g._FW =_JJ -_: =[_NN 4_CD ,_, 28_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT problems_NNS of_IN dimensionality_NN reduction_NN and_CC similarity_NN search_NN have_VBP often_RB been_VBN addressed_VBN in_IN the_DT information_NN retrieval_NN literature_NN ,_, and_CC other_JJ approaches_NNS than_IN random_JJ projection_NN have_VBP been_VBN presented_VBN ._.
Ost_NN
d_NN only_RB retain_VBP thesrst_JJ few_JJ frequencies_NNS ._.
Keogh_NNP and_CC Pazzani_NNP -LRB-_-LRB- 18_CD -RRB-_-RRB- reduce_VB the_DT dimension_NN of_IN time_NN series_NN data_NNS by_IN segmenting_VBG the_DT time_NN series_NN into_IN sections_NNS and_CC indexing_NN only_RB the_DT section_NN means_VBZ ._.
Aggarwal_NNP et_FW al._FW =_SYM -_: =[_NN 2_CD -_: =-]_CD index_NN market_NN basket_NN data_NNS by_IN a_DT specic_JJ signature_NN table_NN ,_, which_WDT easens_VBZ the_DT similarity_NN search_NN ._.
Wavelet_NNP transforms_VBZ -LRB-_-LRB- -LRB-_-LRB- 12_CD ,_, 27_CD -RRB-_-RRB- etc._NN -RRB-_-RRB- are_VBP a_DT common_JJ method_NN of_IN signal_NN compression_NN ._.
2_CD ._.
METHODSFORDIMENSIONALITYRED_NN
experimental_JJ results_NNS in_IN using_VBG the_DT random_JJ mapping_NN in_IN the_DT context_NN of_IN the_DT WEBSOM_NN 1_CD system_NN ._.
Kurimo_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- applies_VBZ random_JJ projection_NN to_TO the_DT indexing_NN of_IN audio_JJ documents_NNS ,_, prior_RB to_TO using_VBG LSI_NNP and_CC SOM_NNP ._.
Kleinberg_NN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Indyk_NNP and_CC Motwani_NNP -LRB-_-LRB- 14_CD -RRB-_-RRB- use_VBP random_JJ projections_NNS in_IN nearest-neighbor_JJ search_NN in_IN a_DT high_JJ dimensional_JJ Euclidean_JJ space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- has_VBZ used_VBN random_JJ projections_NNS
