Extracting_VBG key-substring-group_JJ features_NNS for_IN text_NN classification_NN
In_IN many_JJ text_NN classification_NN applications_NNS ,_, it_PRP is_VBZ appealing_VBG to_TO take_VB every_DT document_NN as_IN a_DT string_NN of_IN characters_NNS rather_RB than_IN a_DT bag_NN of_IN words_NNS ._.
Previous_JJ research_NN studies_NNS in_IN this_DT area_NN mostly_RB focused_VBN on_IN different_JJ variants_NNS of_IN generative_JJ Markov_NNP chain_NN models_NNS ._.
Although_IN discriminative_JJ machine_NN learning_NN methods_NNS like_IN Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- have_VBP been_VBN quite_RB successful_JJ in_IN text_NN classification_NN with_IN word_NN features_NNS ,_, it_PRP is_VBZ neither_CC effective_JJ nor_CC efficient_JJ to_TO apply_VB them_PRP straightforwardly_RB taking_VBG all_DT substrings_NNS in_IN the_DT corpus_NN as_IN features_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP to_TO partition_NN all_DT substrings_NNS into_IN statistical_JJ equivalence_JJ groups_NNS ,_, and_CC then_RB pick_VB those_DT groups_NNS which_WDT are_VBP important_JJ -LRB-_-LRB- in_IN the_DT statistical_JJ sense_NN -RRB-_-RRB- as_IN features_NNS -LRB-_-LRB- named_VBN key-substring-group_JJ features_NNS -RRB-_-RRB- for_IN text_NN classification_NN ._.
In_IN particular_JJ ,_, we_PRP propose_VBP a_DT suffix_NN tree_NN based_JJ algorithm_NN that_WDT can_MD extract_VB such_JJ features_NNS in_IN linear_JJ time_NN -LRB-_-LRB- with_IN respect_NN to_TO the_DT total_JJ number_NN of_IN characters_NNS in_IN the_DT corpus_NN -RRB-_-RRB- ._.
Our_PRP$ experiments_NNS on_IN English_NNP ,_, Chinese_NNP and_CC Greek_JJ datasets_NNS show_VBP that_IN SVM_NN with_IN key-substring-group_JJ features_NNS can_MD achieve_VB outstanding_JJ performance_NN for_IN various_JJ text_NN classification_NN tasks_NNS ._.
would_MD be_VB infeasible_JJ to_TO apply_VB effective_JJ feature_NN selection_NN techniques_NNS -LRB-_-LRB- using_VBG χ_NN 2_CD or_CC information_NN gain_NN ,_, etc._NN -RRB-_-RRB- -LRB-_-LRB- 68_CD ,_, 19_CD -RRB-_-RRB- ,_, feature_NN weighting_NN techniques_NNS -LRB-_-LRB- such_JJ as_IN TF_NN ×_CD IDF_NN -LRB-_-LRB- 60_CD ,_, 2_CD -RRB-_-RRB- -RRB-_-RRB- ,_, or_CC advanced_JJ kernel_NN functions_NNS =_JJ -_: =[_NN 28_CD ,_, 35_CD ,_, 72_CD -RRB-_-RRB- -_: =_SYM -_: ._.
3_LS ._.
KEY-SUBSTRING-GROUP_NN FEATURES_NN A_NN corpus_NN D_NN =_JJ -LCB-_-LRB- d1_NN ,_, d2_NN ,_, ..._: ,_, dm_NN -RCB-_-RRB- of_IN size_NN n_NN -LRB-_-LRB- i.e._FW ,_, m_NN k_NN =_JJ 1_CD |_CD dk_NN |_NN =_JJ n_NN -RRB-_-RRB- contains_VBZ about_IN n_NN -LRB-_-LRB- n_NN +_CC 1_LS -RRB-_-RRB- \/_: 2_CD substrings_NNS ._.
Our_PRP$ most_RBS important_JJ insight_NN comes_VBZ from_IN the_DT fact_NN that_IN these_DT subs_NNS
ls_NNS -LRB-_-LRB- 20_CD ,_, 50_CD -RRB-_-RRB- in_IN the_DT field_NN of_IN natural_JJ language_NN processing_NN ._.
The_DT n-gram_NN language_NN modeling_NN technique_NN has_VBZ been_VBN used_VBN extensively_RB in_IN speech_NN recognition_NN for_IN decades_NNS -LRB-_-LRB- 32_CD -RRB-_-RRB- ,_, and_CC recently_RB in_IN information_NN retrieval_NN =_JJ -_: =[_NN 48_CD ,_, 36_CD ,_, 70_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Most_JJS occurrences_NNS of_IN the_DT term_NN ``_`` n-gram_NN ''_'' in_IN the_DT natural_JJ language_NN processing_NN literature_NN refer_VBP to_TO a_DT continuous_JJ segment_NN of_IN n_NN words_NNS ,_, but_CC in_IN this_DT paper_NN we_PRP focus_VBP on_IN character-level_FW n-grams_FW ,_, i.e._FW ,_, character_NN s_NN
he_PRP more_RBR suitable_JJ to_TO be_VB taken_VBN as_IN a_DT feature_NN ._.
The_DT last_JJ two_CD criteria_NNS -LRB-_-LRB- -_: p_NN and_CC -_: q_NN -RRB-_-RRB- aim_NN at_IN removing_VBG highly_RB redundant_JJ features_NNS ._.
In_IN fact_NN ,_, the_DT probability_NN P_NN r_NN -LRB-_-LRB- SGv_FW |_FW SGu_FW -RRB-_-RRB- is_VBZ proportional_JJ to_TO the_DT mutual_JJ information_NN =_JJ -_: =[_NN 12_CD ,_, 46_CD -RRB-_-RRB- -_: =_SYM -_: between_IN SGv_NNP and_CC SGu_NNP ,_, while_IN the_DT probability_NN P_NN r_NN -LRB-_-LRB- SGv_FW |_FW SGs_NNS -LRB-_-LRB- v_LS -RRB-_-RRB- -RRB-_-RRB- is_VBZ proportional_JJ to_TO the_DT mutual_JJ information_NN -LRB-_-LRB- 12_CD ,_, 46_CD -RRB-_-RRB- between_IN SGv_NNS and_CC SGs_NNS -LRB-_-LRB- v_LS -RRB-_-RRB- ._.
If_IN two_CD substring_JJ groups_NNS have_VBP a_DT high_JJ mutual_JJ information_NN ,_, it_PRP should_MD
5_CD %_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- 83.1_CD %_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- 91.5_CD %_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- Language_NN Modeling_NN -LRB-_-LRB- character_NN n-gram_NN -RRB-_-RRB- --_: --_: 80.3_CD %_NN -LRB-_-LRB- 47_CD -RRB-_-RRB- --_: --_: --_: --_: Data_NNP Compression_NNP -LRB-_-LRB- PPMC_NNP Order_NNP 2_CD -RRB-_-RRB- --_: --_: --_: --_: 74.3_CD %_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- --_: --_: DVMM_NN -LRB-_-LRB- VMM_NN with_IN discriminative_JJ feature_NN selection_NN -RRB-_-RRB- --_: --_: --_: --_: --_: --_: 87.0_CD %_NN =_JJ -_: =[_NN 56_CD -RRB-_-RRB- -_: =_JJ -_: string-based_JJ SVM_NN -LRB-_-LRB- character_NN sequence_NN kernel_NN -RRB-_-RRB- 77.3_CD %_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- --_: --_: --_: --_: --_: --_: SVM_NN -LRB-_-LRB- character_NN n-gram_NN ,_, i.e._FW ,_, fixed-length_JJ string_NN kernel_NN -RRB-_-RRB- 80.6_CD %_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- --_: --_: --_: --_: --_: --_: SVM_NN -LRB-_-LRB- linear_JJ kernel_NN ,_, key-substring-group_JJ features_NNS -RRB-_-RRB- 88.3_CD %_NN 9_CD
string_NN kernel_NN utilizes_VBZ all_DT distinctive_JJ substrings_NNS in_IN the_DT corpus_NN as_IN features_NNS ,_, therefore_RB the_DT redundancy_NN of_IN features_NNS is_VBZ quite_RB high_JJ ,_, which_WDT is_VBZ likely_JJ to_TO hurt_VB the_DT performance_NN of_IN SVM_NN in_IN text_NN classification_NN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Furthermore_RB ,_, string_NN kernel_NN weights_NNS different_JJ substring_VBG features_NNS only_RB according_VBG to_TO their_PRP$ lengths_NNS ._.
Since_IN the_DT number_NN of_IN substring_JJ features_NNS used_VBN implicitly_RB in_IN string_NN kernel_NN is_VBZ extremely_RB large_JJ ,_, it_PRP would_MD
chain_NN models_NNS in_IN variable_JJ order_NN adjust_VBP the_DT memory_NN length_NN according_VBG to_TO the_DT context_NN ,_, hence_RB they_PRP are_VBP much_RB more_RBR flexible_JJ and_CC robust_JJ than_IN fixed_JJ order_NN Markov_NNP chain_NN models_NNS ._.
The_DT amnesic_JJ probabilistic_JJ automata_NN =_JJ -_: =[_NN 49_CD -RRB-_-RRB- -_: =_JJ -_: aka_NN PST_NN -LRB-_-LRB- probabilistic_JJ suffix_NN tree_NN or_CC prediction_NN suffix_NN tree_NN -RRB-_-RRB- ,_, text_NN compression_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- methods_NNS including_VBG PPM_NN -LRB-_-LRB- Predication_NN by_IN Partial_JJ Matching_NN -RRB-_-RRB- -LRB-_-LRB- 14_CD -RRB-_-RRB- and_CC PPM_NN \*_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- are_VBP all_DT in_IN the_DT family_NN of_IN variable_JJ order_NN
models_NNS ._.
PST_NN have_VBP been_VBN applied_VBN to_TO gene\/protein_NN sequence_NN clustering_NN -LRB-_-LRB- 66_CD -RRB-_-RRB- and_CC spam_NN filtering_VBG -LRB-_-LRB- 45_CD -RRB-_-RRB- ._.
Researchers_NNS have_VBP tried_VBN to_TO do_VB text_NN classification_NN by_IN using_VBG PPM_NN ,_, PPM_NN \*_NN and_CC other_JJ text_NN compression_NN methods_NNS =_JJ -_: =[_NN 5_CD ,_, 18_CD ,_, 43_CD ,_, 58_CD ,_, 64_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2.2_CD The_DT Discriminative_JJ Approach_NN In_IN contrast_NN to_TO generative_JJ learning_NN methods_NNS ,_, discriminative_JJ learning_NN methods_NNS do_VBP not_RB posit_VB a_DT generative_JJ model_NN ,_, but_CC attempt_NN to_TO find_VB the_DT optimal_JJ classification_NN function_NN d_NN
arkov_NN chain_NN models_NNS ._.
The_DT amnesic_JJ probabilistic_JJ automata_NN -LRB-_-LRB- 49_CD -RRB-_-RRB- aka_NN PST_NN -LRB-_-LRB- probabilistic_JJ suffix_NN tree_NN or_CC prediction_NN suffix_NN tree_NN -RRB-_-RRB- ,_, text_NN compression_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- methods_NNS including_VBG PPM_NN -LRB-_-LRB- Predication_NN by_IN Partial_JJ Matching_NN -RRB-_-RRB- =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: and_CC PPM_NN \*_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- are_VBP all_DT in_IN the_DT family_NN of_IN variable_JJ order_NN Markov_NNP models_NNS ._.
PST_NN have_VBP been_VBN applied_VBN to_TO gene\/protein_NN sequence_NN clustering_NN -LRB-_-LRB- 66_CD -RRB-_-RRB- and_CC spam_NN filtering_VBG -LRB-_-LRB- 45_CD -RRB-_-RRB- ._.
Researchers_NNS have_VBP tried_VBN to_TO do_VB text_NN classifi_NNS
tive_JJ learning_NN methods_NNS are_VBP often_RB inferior_JJ to_TO discriminative_JJ learning_NN methods_NNS ,_, in_IN terms_NNS of_IN classification_NN performance_NN ._.
Although_IN discriminative_JJ machine_NN learning_NN methods_NNS like_IN Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- =_JJ -_: =[_NN 15_CD ,_, 24_CD ,_, 31_CD ,_, 53_CD ,_, 55_CD -RRB-_-RRB- -_: =_JJ -_: and_CC AdaBoost_NNP -LRB-_-LRB- 52_CD ,_, 51_CD -RRB-_-RRB- have_VBP been_VBN quite_RB successful_JJ in_IN text_NN classification_NN with_IN word_NN features_NNS ,_, it_PRP is_VBZ neither_CC effective_JJ nor_CC efficient_JJ to_TO apply_VB them_PRP straightforwardly_RB taking_VBG all_DT substrings_NNS in_IN the_DT corpus_NN a_DT
cantly_RB reduce_VB the_DT potential_JJ dimension_NN of_IN feature_NN space_NN by_IN taking_VBG the_DT substring-groups_NNS rather_RB than_IN individual_JJ substrings_NNS as_IN features_NNS ._.
These_DT distribution_NN of_IN substring-groups_NNS seem_VBP to_TO meet_VB the_DT Zipf_NNP 's_POS Law_NN =_JJ -_: =[_NN 74_CD ,_, 60_CD ,_, 30_CD -RRB-_-RRB- -_: =_SYM -_: like_IN words_NNS in_IN natural_JJ language_NN documents_NNS as_IN shown_VBN in_IN our_PRP$ experiments_NNS -LRB-_-LRB- see_VB section_NN 4_CD -RRB-_-RRB- ,_, which_WDT implies_VBZ that_IN it_PRP is_VBZ plausible_JJ to_TO regard_VB them_PRP as_IN pseudo-words_NNS ._.
3.3_CD Key-Substring-Groups_NNS To_TO further_RBR reduce_VB the_DT
cantly_RB reduce_VB the_DT potential_JJ dimension_NN of_IN feature_NN space_NN by_IN taking_VBG the_DT substring-groups_NNS rather_RB than_IN individual_JJ substrings_NNS as_IN features_NNS ._.
These_DT distribution_NN of_IN substring-groups_NNS seem_VBP to_TO meet_VB the_DT Zipf_NNP 's_POS Law_NN =_JJ -_: =[_NN 74_CD ,_, 60_CD ,_, 30_CD -RRB-_-RRB- -_: =_SYM -_: like_IN words_NNS in_IN natural_JJ language_NN documents_NNS as_IN shown_VBN in_IN our_PRP$ experiments_NNS -LRB-_-LRB- see_VB section_NN 4_CD -RRB-_-RRB- ,_, which_WDT implies_VBZ that_IN it_PRP is_VBZ plausible_JJ to_TO regard_VB them_PRP as_IN pseudo-words_NNS ._.
3.3_CD Key-Substring-Groups_NNS To_TO further_RBR reduce_VB the_DT
riants_NNS -RRB-_-RRB- and_CC super-word_JJ features_NNS -LRB-_-LRB- e.g._FW ,_, phrasal_JJ effects_NNS -RRB-_-RRB- can_MD be_VB exploited_VBN automatically_RB ._.
This_DT is_VBZ particularly_RB helpful_JJ to_TO non-topical_JJ text_NN classification_NN applications_NNS ,_, such_JJ as_IN text_NN genre_NN classification_NN =_JJ -_: =[_NN 33_CD ,_, 38_CD ,_, 47_CD ,_, 57_CD -RRB-_-RRB- -_: =_JJ -_: and_CC text_NN authorship_NN classification_NN -LRB-_-LRB- 25_CD ,_, 47_CD ,_, 57_CD -RRB-_-RRB- ._.
•_VB The_DT messy_JJ and_CC rather_RB artificial_JJ problem_NN of_IN defining_VBG word_NN boundaries_NNS can_MD be_VB avoided_VBN ._.
This_DT is_VBZ particularly_RB attractive_JJ to_TO text_NN classification_NN for_IN orien_NN
dels_NNS ._.
The_DT amnesic_JJ probabilistic_JJ automata_NN -LRB-_-LRB- 49_CD -RRB-_-RRB- aka_NN PST_NN -LRB-_-LRB- probabilistic_JJ suffix_NN tree_NN or_CC prediction_NN suffix_NN tree_NN -RRB-_-RRB- ,_, text_NN compression_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- methods_NNS including_VBG PPM_NN -LRB-_-LRB- Predication_NN by_IN Partial_JJ Matching_NN -RRB-_-RRB- -LRB-_-LRB- 14_CD -RRB-_-RRB- and_CC PPM_NN \*_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP all_DT in_IN the_DT family_NN of_IN variable_JJ order_NN Markov_NNP models_NNS ._.
PST_NN have_VBP been_VBN applied_VBN to_TO gene\/protein_NN sequence_NN clustering_NN -LRB-_-LRB- 66_CD -RRB-_-RRB- and_CC spam_NN filtering_VBG -LRB-_-LRB- 45_CD -RRB-_-RRB- ._.
Researchers_NNS have_VBP tried_VBN to_TO do_VB text_NN classification_NN by_IN usin_NN
v_LS to_TO s_NN -LRB-_-LRB- v_LS -RRB-_-RRB- in_IN constant_JJ time_NN via_IN the_DT suffix_JJ link_NN between_IN them_PRP ._.
If_IN v_LS is_VBZ a_DT leaf_NN ,_, we_PRP can_MD jump_VB from_IN v_LS to_TO s_NN -LRB-_-LRB- v_LS -RRB-_-RRB- in_IN constant_JJ time_NN through_IN the_DT so-called_JJ ``_`` skip\/count_NN ''_'' trick_NN ._.
Theorem_NNP 5_CD ._.
-LRB-_-LRB- 21_CD -RRB-_-RRB- Ukkonen_NNP 's_POS algorithm_NN =_JJ -_: =[_NN 59_CD -RRB-_-RRB- -_: =_SYM -_: can_MD construct_VB the_DT suffix_NN tree_NN T_NN for_IN a_DT string_NN S_NN of_IN length_NN n_NN ,_, along_IN with_IN all_DT its_PRP$ suffix_NN links_NNS ,_, in_IN O_NN -LRB-_-LRB- n_NN -RRB-_-RRB- time_NN ._.
Definition_NN 4_CD ._.
-LRB-_-LRB- 21_CD -RRB-_-RRB- A_DT generalized_JJ suffix_NN tree_NN can_MD be_VB constructed_VBN for_IN a_DT set_NN of_IN strings_NNS ,_, i.e._FW ,_,
tive_JJ learning_NN methods_NNS are_VBP often_RB inferior_JJ to_TO discriminative_JJ learning_NN methods_NNS ,_, in_IN terms_NNS of_IN classification_NN performance_NN ._.
Although_IN discriminative_JJ machine_NN learning_NN methods_NNS like_IN Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- =_JJ -_: =[_NN 15_CD ,_, 24_CD ,_, 31_CD ,_, 53_CD ,_, 55_CD -RRB-_-RRB- -_: =_JJ -_: and_CC AdaBoost_NNP -LRB-_-LRB- 52_CD ,_, 51_CD -RRB-_-RRB- have_VBP been_VBN quite_RB successful_JJ in_IN text_NN classification_NN with_IN word_NN features_NNS ,_, it_PRP is_VBZ neither_CC effective_JJ nor_CC efficient_JJ to_TO apply_VB them_PRP straightforwardly_RB taking_VBG all_DT substrings_NNS in_IN the_DT corpus_NN a_DT
g_NN the_DT Bayes_NNP 's_POS rule_NN and_CC Pr_NN -LRB-_-LRB- d_NN |_NN C_NN -RRB-_-RRB- given_VBN by_IN MC_NNP ._.
Markov_NNP chain_NN models_NNS could_MD be_VB in_IN fixed_JJ order\/memory_NN or_CC variable_JJ order\/memory_NN ._.
Markov_NNP chain_NN models_NNS in_IN fixed_JJ order_NN n_NN are_VBP usually_RB called_VBN n-gram_NN language_NN models_NNS =_JJ -_: =[_NN 20_CD ,_, 50_CD -RRB-_-RRB- -_: =_SYM -_: in_IN the_DT field_NN of_IN natural_JJ language_NN processing_NN ._.
The_DT n-gram_NN language_NN modeling_NN technique_NN has_VBZ been_VBN used_VBN extensively_RB in_IN speech_NN recognition_NN for_IN decades_NNS -LRB-_-LRB- 32_CD -RRB-_-RRB- ,_, and_CC recently_RB in_IN information_NN retrieval_NN -LRB-_-LRB- 48_CD ,_, 36_CD ,_, 70_CD
tive_JJ learning_NN methods_NNS are_VBP often_RB inferior_JJ to_TO discriminative_JJ learning_NN methods_NNS ,_, in_IN terms_NNS of_IN classification_NN performance_NN ._.
Although_IN discriminative_JJ machine_NN learning_NN methods_NNS like_IN Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- =_JJ -_: =[_NN 15_CD ,_, 24_CD ,_, 31_CD ,_, 53_CD ,_, 55_CD -RRB-_-RRB- -_: =_JJ -_: and_CC AdaBoost_NNP -LRB-_-LRB- 52_CD ,_, 51_CD -RRB-_-RRB- have_VBP been_VBN quite_RB successful_JJ in_IN text_NN classification_NN with_IN word_NN features_NNS ,_, it_PRP is_VBZ neither_CC effective_JJ nor_CC efficient_JJ to_TO apply_VB them_PRP straightforwardly_RB taking_VBG all_DT substrings_NNS in_IN the_DT corpus_NN a_DT
ression_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- methods_NNS including_VBG PPM_NN -LRB-_-LRB- Predication_NN by_IN Partial_JJ Matching_NN -RRB-_-RRB- -LRB-_-LRB- 14_CD -RRB-_-RRB- and_CC PPM_NN \*_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- are_VBP all_DT in_IN the_DT family_NN of_IN variable_JJ order_NN Markov_NNP models_NNS ._.
PST_NN have_VBP been_VBN applied_VBN to_TO gene\/protein_NN sequence_NN clustering_NN =_JJ -_: =[_NN 66_CD -RRB-_-RRB- -_: =_JJ -_: and_CC spam_NN filtering_VBG -LRB-_-LRB- 45_CD -RRB-_-RRB- ._.
Researchers_NNS have_VBP tried_VBN to_TO do_VB text_NN classification_NN by_IN using_VBG PPM_NN ,_, PPM_NN \*_NN and_CC other_JJ text_NN compression_NN methods_NNS -LRB-_-LRB- 5_CD ,_, 18_CD ,_, 43_CD ,_, 58_CD ,_, 64_CD -RRB-_-RRB- ._.
2.2_CD The_DT Discriminative_JJ Approach_NN In_IN contrast_NN to_TO gen_NN
tive_JJ learning_NN methods_NNS are_VBP often_RB inferior_JJ to_TO discriminative_JJ learning_NN methods_NNS ,_, in_IN terms_NNS of_IN classification_NN performance_NN ._.
Although_IN discriminative_JJ machine_NN learning_NN methods_NNS like_IN Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- =_JJ -_: =[_NN 15_CD ,_, 24_CD ,_, 31_CD ,_, 53_CD ,_, 55_CD -RRB-_-RRB- -_: =_JJ -_: and_CC AdaBoost_NNP -LRB-_-LRB- 52_CD ,_, 51_CD -RRB-_-RRB- have_VBP been_VBN quite_RB successful_JJ in_IN text_NN classification_NN with_IN word_NN features_NNS ,_, it_PRP is_VBZ neither_CC effective_JJ nor_CC efficient_JJ to_TO apply_VB them_PRP straightforwardly_RB taking_VBG all_DT substrings_NNS in_IN the_DT corpus_NN a_DT
he_PRP more_RBR suitable_JJ to_TO be_VB taken_VBN as_IN a_DT feature_NN ._.
The_DT last_JJ two_CD criteria_NNS -LRB-_-LRB- -_: p_NN and_CC -_: q_NN -RRB-_-RRB- aim_NN at_IN removing_VBG highly_RB redundant_JJ features_NNS ._.
In_IN fact_NN ,_, the_DT probability_NN P_NN r_NN -LRB-_-LRB- SGv_FW |_FW SGu_FW -RRB-_-RRB- is_VBZ proportional_JJ to_TO the_DT mutual_JJ information_NN =_JJ -_: =[_NN 12_CD ,_, 46_CD -RRB-_-RRB- -_: =_SYM -_: between_IN SGv_NNP and_CC SGu_NNP ,_, while_IN the_DT probability_NN P_NN r_NN -LRB-_-LRB- SGv_FW |_FW SGs_NNS -LRB-_-LRB- v_LS -RRB-_-RRB- -RRB-_-RRB- is_VBZ proportional_JJ to_TO the_DT mutual_JJ information_NN -LRB-_-LRB- 12_CD ,_, 46_CD -RRB-_-RRB- between_IN SGv_NNS and_CC SGs_NNS -LRB-_-LRB- v_LS -RRB-_-RRB- ._.
If_IN two_CD substring_JJ groups_NNS have_VBP a_DT high_JJ mutual_JJ information_NN ,_, it_PRP should_MD
._.
have_VBP tried_VBN character-level_JJ n-gram_NN modeling_NN for_IN various_JJ text_NN classification_NN tasks_NNS -LRB-_-LRB- 47_CD -RRB-_-RRB- ._.
To_TO achieve_VB a_DT decent_JJ performance_NN ,_, one_CD needs_VBZ to_TO choose_VB an_DT appropriate_JJ order_NN n_NN and_CC employ_VB a_DT good_JJ smoothing_NN method_NN =_JJ -_: =[_NN 71_CD ,_, 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Markov_NNP chain_NN models_NNS in_IN variable_JJ order_NN adjust_VBP the_DT memory_NN length_NN according_VBG to_TO the_DT context_NN ,_, hence_RB they_PRP are_VBP much_RB more_RBR flexible_JJ and_CC robust_JJ than_IN fixed_JJ order_NN Markov_NNP chain_NN models_NNS ._.
The_DT amnesic_JJ probabilistic_NN
tly_RB ._.
Previous_JJ studies_NNS have_VBP consistently_RB shown_VBN that_IN discriminative_JJ learning_NN methods_NNS ,_, particularly_RB SVM_NN ,_, can_MD achieve_VB better_JJR performance_NN than_IN generative_JJ learning_NN methods_NNS in_IN word-based_JJ text_NN classification_NN =_JJ -_: =[_NN 17_CD ,_, 29_CD ,_, 67_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT discriminative_JJ approach_NN to_TO string-based_JJ text_NN classification_NN attempts_VBZ to_TO utilize_VB the_DT substrings_NNS of_IN a_DT document_NN as_IN its_PRP$ features_NNS ._.
However_RB ,_, a_DT document_NN of_IN length_NN |_CD d_NN |_NN has_VBZ |_VBN d_FW |_FW -LRB-_-LRB- |_CD d_NN |_CD +_CC 1_CD -RRB-_-RRB- \/_: 2_CD substrings_NNS in_IN t_NN
._.
Wee_NNP Sun_NNP Lee_NNP Department_NNP of_IN Computer_NNP Science_NNP and_CC Singapore-MIT_NNP Alliance_NNP National_NNP University_NNP of_IN Singapore_NNP Singapore_NNP 117543_CD leews@comp.nus.edu.sg_NN 1_CD ._.
INTRODUCTION_NN Text_NN classification_NN -LRB-_-LRB- or_CC categorization_NN -RRB-_-RRB- =_JJ -_: =[_NN 27_CD ,_, 54_CD -RRB-_-RRB- -_: =_SYM -_: via_IN machine_NN learning_NN -LRB-_-LRB- 44_CD -RRB-_-RRB- is_VBZ a_DT fundamental_JJ technique_NN for_IN information_NN organization_NN and_CC management_NN ._.
Traditionally_RB machine_NN learning_VBG methods_NNS for_IN text_NN classification_NN treat_VB every_DT document_NN as_IN a_DT bag_NN of_IN words_NNS
we_PRP omit_VBP the_DT details_NNS here_RB due_JJ to_TO the_DT space_NN limit_NN ._.
Our_PRP$ idea_NN about_IN key-substring-groups_NNS has_VBZ been_VBN partially_RB inspired_VBN by_IN the_DT works_NNS on_IN suffix_NN tree_NN -LRB-_-LRB- or_CC suffix_NN array_NN or_CC PAT_NN tree_NN -RRB-_-RRB- based_VBN key-phrase_JJ extraction_NN =_JJ -_: =[_NN 8_CD ,_, 11_CD ,_, 37_CD -RRB-_-RRB- -_: =_JJ -_: and_CC document_NN clustering_NN -LRB-_-LRB- 69_CD ,_, 73_CD -RRB-_-RRB- ._.
The_DT essential_JJ difference_NN is_VBZ that_IN we_PRP do_VBP not_RB care_VB about_IN whether_IN the_DT extracted_VBN key-substring-groups_NNS are_VBP semantically_RB meaningful_JJ or_CC not_RB ._.
In_IN contrast_NN ,_, our_PRP$ concern_NN is_VBZ the_DT
erformance_NN ._.
After_IN the_DT key-substring-group_JJ features_NNS were_VBD extracted_VBN ,_, we_PRP weighted_VBD every_DT feature_NN value_NN using_VBG the_DT classic_JJ TF_NN ×_NN IDF_NN -LRB-_-LRB- 60_CD ,_, 2_LS -RRB-_-RRB- scheme_NN ,_, and_CC normalized_VBD all_DT feature_NN vectors_NNS to_TO unit_NN length_NN ._.
LibSVM_NN 5_CD =_SYM -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: was_VBD employed_VBN as_IN the_DT implementation_NN of_IN SVM_NN 6_CD ._.
We_PRP chose_VBD the_DT linear_JJ kernel_NN and_CC set_VB all_DT its_PRP$ other_JJ parameters_NNS to_TO their_PRP$ default_NN values_NNS ._.
In_IN all_PDT these_DT experiments_NNS ,_, our_PRP$ proposed_VBN approach_NN worked_VBD very_RB well_RB ._.
4_LS ._.
riants_NNS -RRB-_-RRB- and_CC super-word_JJ features_NNS -LRB-_-LRB- e.g._FW ,_, phrasal_JJ effects_NNS -RRB-_-RRB- can_MD be_VB exploited_VBN automatically_RB ._.
This_DT is_VBZ particularly_RB helpful_JJ to_TO non-topical_JJ text_NN classification_NN applications_NNS ,_, such_JJ as_IN text_NN genre_NN classification_NN =_JJ -_: =[_NN 33_CD ,_, 38_CD ,_, 47_CD ,_, 57_CD -RRB-_-RRB- -_: =_JJ -_: and_CC text_NN authorship_NN classification_NN -LRB-_-LRB- 25_CD ,_, 47_CD ,_, 57_CD -RRB-_-RRB- ._.
•_VB The_DT messy_JJ and_CC rather_RB artificial_JJ problem_NN of_IN defining_VBG word_NN boundaries_NNS can_MD be_VB avoided_VBN ._.
This_DT is_VBZ particularly_RB attractive_JJ to_TO text_NN classification_NN for_IN orien_NN
ferior_JJ to_TO discriminative_JJ learning_NN methods_NNS ,_, in_IN terms_NNS of_IN classification_NN performance_NN ._.
Although_IN discriminative_JJ machine_NN learning_NN methods_NNS like_IN Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- -LRB-_-LRB- 15_CD ,_, 24_CD ,_, 31_CD ,_, 53_CD ,_, 55_CD -RRB-_-RRB- and_CC AdaBoost_NN =_JJ -_: =[_NN 52_CD ,_, 51_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP been_VBN quite_RB successful_JJ in_IN text_NN classification_NN with_IN word_NN features_NNS ,_, it_PRP is_VBZ neither_CC effective_JJ nor_CC efficient_JJ to_TO apply_VB them_PRP straightforwardly_RB taking_VBG all_DT substrings_NNS in_IN the_DT corpus_NN as_IN features_NNS ._.
In_IN this_DT pa_NN
47_CD ,_, 57_CD -RRB-_-RRB- ._.
•_VB The_DT messy_JJ and_CC rather_RB artificial_JJ problem_NN of_IN defining_VBG word_NN boundaries_NNS can_MD be_VB avoided_VBN ._.
This_DT is_VBZ particularly_RB attractive_JJ to_TO text_NN classification_NN for_IN oriental_JJ languages_NNS -LRB-_-LRB- Chinese_NNP ,_, Japanese_NNP ,_, etc._NN -RRB-_-RRB- =_JJ -_: =[_NN 1_CD ,_, 23_CD ,_, 47_CD -RRB-_-RRB- -_: =_JJ -_: ,_, because_IN many_JJ oriental_JJ languages_NNS do_VBP not_RB utilize_VB word_NN delimiters_NNS as_IN whitespace_NN characters_NNS in_IN western_JJ languages_NNS -LRB-_-LRB- English_NNP ,_, French_NNP ,_, etc._NN -RRB-_-RRB- ._.
Although_IN it_PRP is_VBZ possible_JJ to_TO perform_VB automatic_JJ word_NN segmentation_NN t_NN
ls_NNS -LRB-_-LRB- 20_CD ,_, 50_CD -RRB-_-RRB- in_IN the_DT field_NN of_IN natural_JJ language_NN processing_NN ._.
The_DT n-gram_NN language_NN modeling_NN technique_NN has_VBZ been_VBN used_VBN extensively_RB in_IN speech_NN recognition_NN for_IN decades_NNS -LRB-_-LRB- 32_CD -RRB-_-RRB- ,_, and_CC recently_RB in_IN information_NN retrieval_NN =_JJ -_: =[_NN 48_CD ,_, 36_CD ,_, 70_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Most_JJS occurrences_NNS of_IN the_DT term_NN ``_`` n-gram_NN ''_'' in_IN the_DT natural_JJ language_NN processing_NN literature_NN refer_VBP to_TO a_DT continuous_JJ segment_NN of_IN n_NN words_NNS ,_, but_CC in_IN this_DT paper_NN we_PRP focus_VBP on_IN character-level_FW n-grams_FW ,_, i.e._FW ,_, character_NN s_NN
models_NNS ._.
PST_NN have_VBP been_VBN applied_VBN to_TO gene\/protein_NN sequence_NN clustering_NN -LRB-_-LRB- 66_CD -RRB-_-RRB- and_CC spam_NN filtering_VBG -LRB-_-LRB- 45_CD -RRB-_-RRB- ._.
Researchers_NNS have_VBP tried_VBN to_TO do_VB text_NN classification_NN by_IN using_VBG PPM_NN ,_, PPM_NN \*_NN and_CC other_JJ text_NN compression_NN methods_NNS =_JJ -_: =[_NN 5_CD ,_, 18_CD ,_, 43_CD ,_, 58_CD ,_, 64_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2.2_CD The_DT Discriminative_JJ Approach_NN In_IN contrast_NN to_TO generative_JJ learning_NN methods_NNS ,_, discriminative_JJ learning_NN methods_NNS do_VBP not_RB posit_VB a_DT generative_JJ model_NN ,_, but_CC attempt_NN to_TO find_VB the_DT optimal_JJ classification_NN function_NN d_NN
riants_NNS -RRB-_-RRB- and_CC super-word_JJ features_NNS -LRB-_-LRB- e.g._FW ,_, phrasal_JJ effects_NNS -RRB-_-RRB- can_MD be_VB exploited_VBN automatically_RB ._.
This_DT is_VBZ particularly_RB helpful_JJ to_TO non-topical_JJ text_NN classification_NN applications_NNS ,_, such_JJ as_IN text_NN genre_NN classification_NN =_JJ -_: =[_NN 33_CD ,_, 38_CD ,_, 47_CD ,_, 57_CD -RRB-_-RRB- -_: =_JJ -_: and_CC text_NN authorship_NN classification_NN -LRB-_-LRB- 25_CD ,_, 47_CD ,_, 57_CD -RRB-_-RRB- ._.
•_VB The_DT messy_JJ and_CC rather_RB artificial_JJ problem_NN of_IN defining_VBG word_NN boundaries_NNS can_MD be_VB avoided_VBN ._.
This_DT is_VBZ particularly_RB attractive_JJ to_TO text_NN classification_NN for_IN orien_NN
ethods_NNS for_IN text_NN classification_NN treat_VB every_DT document_NN as_IN a_DT bag_NN of_IN words_NNS -LRB-_-LRB- 27_CD ,_, 40_CD ,_, 54_CD -RRB-_-RRB- ._.
However_RB ,_, in_IN many_JJ text_NN classification_NN applications_NNS ,_, it_PRP is_VBZ appealing_VBG to_TO take_VB every_DT document_NN as_IN a_DT string_NN of_IN characters_NNS =_JJ -_: =[_NN 63_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT string-based_JJ approaches_NNS to_TO text_NN classification_NN have_VBP at_IN least_JJS the_DT following_JJ potential_JJ advantages_NNS ._.
•_VB The_DT sub-word_JJ features_NNS -LRB-_-LRB- e.g._FW ,_, morphological_JJ variants_NNS -RRB-_-RRB- and_CC super-word_JJ features_NNS -LRB-_-LRB- e.g._FW ,_, phrasal_JJ eff_NN
would_MD be_VB infeasible_JJ to_TO apply_VB effective_JJ feature_NN selection_NN techniques_NNS -LRB-_-LRB- using_VBG χ_NN 2_CD or_CC information_NN gain_NN ,_, etc._NN -RRB-_-RRB- -LRB-_-LRB- 68_CD ,_, 19_CD -RRB-_-RRB- ,_, feature_NN weighting_NN techniques_NNS -LRB-_-LRB- such_JJ as_IN TF_NN ×_CD IDF_NN -LRB-_-LRB- 60_CD ,_, 2_CD -RRB-_-RRB- -RRB-_-RRB- ,_, or_CC advanced_JJ kernel_NN functions_NNS =_JJ -_: =[_NN 28_CD ,_, 35_CD ,_, 72_CD -RRB-_-RRB- -_: =_SYM -_: ._.
3_LS ._.
KEY-SUBSTRING-GROUP_NN FEATURES_NN A_NN corpus_NN D_NN =_JJ -LCB-_-LRB- d1_NN ,_, d2_NN ,_, ..._: ,_, dm_NN -RCB-_-RRB- of_IN size_NN n_NN -LRB-_-LRB- i.e._FW ,_, m_NN k_NN =_JJ 1_CD |_CD dk_NN |_NN =_JJ n_NN -RRB-_-RRB- contains_VBZ about_IN n_NN -LRB-_-LRB- n_NN +_CC 1_LS -RRB-_-RRB- \/_: 2_CD substrings_NNS ._.
Our_PRP$ most_RBS important_JJ insight_NN comes_VBZ from_IN the_DT fact_NN that_IN these_DT subs_NNS
models_NNS ._.
PST_NN have_VBP been_VBN applied_VBN to_TO gene\/protein_NN sequence_NN clustering_NN -LRB-_-LRB- 66_CD -RRB-_-RRB- and_CC spam_NN filtering_VBG -LRB-_-LRB- 45_CD -RRB-_-RRB- ._.
Researchers_NNS have_VBP tried_VBN to_TO do_VB text_NN classification_NN by_IN using_VBG PPM_NN ,_, PPM_NN \*_NN and_CC other_JJ text_NN compression_NN methods_NNS =_JJ -_: =[_NN 5_CD ,_, 18_CD ,_, 43_CD ,_, 58_CD ,_, 64_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2.2_CD The_DT Discriminative_JJ Approach_NN In_IN contrast_NN to_TO generative_JJ learning_NN methods_NNS ,_, discriminative_JJ learning_NN methods_NNS do_VBP not_RB posit_VB a_DT generative_JJ model_NN ,_, but_CC attempt_NN to_TO find_VB the_DT optimal_JJ classification_NN function_NN d_NN
owledge_NN ,_, SVM_NN with_IN string_NN kernel_NN is_VBZ the_DT only_JJ exist1_NN There_EX are_VBP two_CD kinds_NNS of_IN ``_`` string_NN kernel_NN ''_'' in_IN the_DT machine_NN learning_NN literature_NN ._.
One_CD uses_VBZ all_PDT the_DT continuous_JJ substrings_NNS of_IN documents_NNS as_IN features_NNS -LRB-_-LRB- e.g._FW ,_, in_IN =_JJ -_: =[_NN 62_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, while_IN the_DT other_JJ also_RB uses_VBZ non-continuous_JJ substrings_NNS ,_, aka_NN ,_, sub-sequences_NNS -LRB-_-LRB- e.g._FW ,_, in_IN -LRB-_-LRB- 41_CD -RRB-_-RRB- -RRB-_-RRB- ._.
To_TO avoid_VB confusion_NN ,_, we_PRP would_MD like_VB to_TO call_VB the_DT latter_JJ sequence_NN kernel_NN ._.
In_IN this_DT paper_NN ,_, In_IN this_DT paper_NN we_PRP focus_VBP
tion_NN algorithm_NN and_CC its_PRP$ time_NN complexity_NN analysis_NN -LRB-_-LRB- in_IN section_NN 3.4_CD -RRB-_-RRB- ._.
3.1_CD Data_NN Structure_NN Definition_NN 1_CD ._.
-LRB-_-LRB- 21_CD -RRB-_-RRB- Consider_VB a_DT string_NN S_NN of_IN n_NN characters_NNS :_: S_NN =_JJ c1c2_NN ..._: cn_NN ._.
The_DT suffix_NN tree_NN T_NN for_IN S_NN is_VBZ a_DT compacted_JJ trie_NN =_JJ -_: =[_NN 34_CD -RRB-_-RRB- -_: =_SYM -_: that_IN stores_NNS all_DT suffixes_NNS of_IN S._NNP Specifically_RB ,_, T_NN is_VBZ a_DT rooted_JJ directed_JJ tree_NN with_IN exactly_RB n_NN leaves_NNS numbered_VBD 1_CD to_TO n._RB Let_VB r_NN denote_VB the_DT root_NN of_IN T_NN ._.
Each_DT internal_JJ node_NN other_JJ than_IN r_NN has_VBZ at_IN Figure_NNP 2_CD :_: The_DT suffi_NN
x_NN tree_NN T_NN using_VBG Ukkonen_NNP 's_POS algorithm_NN takes_VBZ O_NN -LRB-_-LRB- n_NN -RRB-_-RRB- time_NN ,_, according_VBG to_TO Theorem_NNP 5_CD ._.
Then_RB ,_, each_DT of_IN the_DT three_CD preparation_NN sub-routines_NNS perform_VBP a_DT 2_CD With_IN some_DT little_JJ trick_NN as_IN in_IN the_DT matching_JJ statistics_NNS algorithm_NN =_JJ -_: =[_NN 9_CD ,_, 62_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT proposed_VBN feature_NN extraction_NN algorithm_NN could_MD also_RB handle_VB the_DT unseen_JJ test_NN documents_NNS ,_, though_IN we_PRP omit_VBP the_DT details_NNS here_RB due_JJ to_TO the_DT space_NN limit_NN ._.
3_CD http:\/\/www.dcs.bbk.ac.uk\/∼dell\/_NN Input_NN :_: a_DT training_NN cor_NN
g_NN the_DT Bayes_NNP 's_POS rule_NN and_CC Pr_NN -LRB-_-LRB- d_NN |_NN C_NN -RRB-_-RRB- given_VBN by_IN MC_NNP ._.
Markov_NNP chain_NN models_NNS could_MD be_VB in_IN fixed_JJ order\/memory_NN or_CC variable_JJ order\/memory_NN ._.
Markov_NNP chain_NN models_NNS in_IN fixed_JJ order_NN n_NN are_VBP usually_RB called_VBN n-gram_NN language_NN models_NNS =_JJ -_: =[_NN 20_CD ,_, 50_CD -RRB-_-RRB- -_: =_SYM -_: in_IN the_DT field_NN of_IN natural_JJ language_NN processing_NN ._.
The_DT n-gram_NN language_NN modeling_NN technique_NN has_VBZ been_VBN used_VBN extensively_RB in_IN speech_NN recognition_NN for_IN decades_NNS -LRB-_-LRB- 32_CD -RRB-_-RRB- ,_, and_CC recently_RB in_IN information_NN retrieval_NN -LRB-_-LRB- 48_CD ,_, 36_CD ,_, 70_CD
ferior_JJ to_TO discriminative_JJ learning_NN methods_NNS ,_, in_IN terms_NNS of_IN classification_NN performance_NN ._.
Although_IN discriminative_JJ machine_NN learning_NN methods_NNS like_IN Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- -LRB-_-LRB- 15_CD ,_, 24_CD ,_, 31_CD ,_, 53_CD ,_, 55_CD -RRB-_-RRB- and_CC AdaBoost_NN =_JJ -_: =[_NN 52_CD ,_, 51_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP been_VBN quite_RB successful_JJ in_IN text_NN classification_NN with_IN word_NN features_NNS ,_, it_PRP is_VBZ neither_CC effective_JJ nor_CC efficient_JJ to_TO apply_VB them_PRP straightforwardly_RB taking_VBG all_DT substrings_NNS in_IN the_DT corpus_NN as_IN features_NNS ._.
In_IN this_DT pa_NN
models_NNS ._.
PST_NN have_VBP been_VBN applied_VBN to_TO gene\/protein_NN sequence_NN clustering_NN -LRB-_-LRB- 66_CD -RRB-_-RRB- and_CC spam_NN filtering_VBG -LRB-_-LRB- 45_CD -RRB-_-RRB- ._.
Researchers_NNS have_VBP tried_VBN to_TO do_VB text_NN classification_NN by_IN using_VBG PPM_NN ,_, PPM_NN \*_NN and_CC other_JJ text_NN compression_NN methods_NNS =_JJ -_: =[_NN 5_CD ,_, 18_CD ,_, 43_CD ,_, 58_CD ,_, 64_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2.2_CD The_DT Discriminative_JJ Approach_NN In_IN contrast_NN to_TO generative_JJ learning_NN methods_NNS ,_, discriminative_JJ learning_NN methods_NNS do_VBP not_RB posit_VB a_DT generative_JJ model_NN ,_, but_CC attempt_NN to_TO find_VB the_DT optimal_JJ classification_NN function_NN d_NN
in_IN the_DT machine_NN learning_NN literature_NN ._.
One_CD uses_VBZ all_PDT the_DT continuous_JJ substrings_NNS of_IN documents_NNS as_IN features_NNS -LRB-_-LRB- e.g._FW ,_, in_IN -LRB-_-LRB- 62_CD -RRB-_-RRB- -RRB-_-RRB- ,_, while_IN the_DT other_JJ also_RB uses_VBZ non-continuous_JJ substrings_NNS ,_, aka_NN ,_, sub-sequences_NNS -LRB-_-LRB- e.g._FW ,_, in_IN =_JJ -_: =[_NN 41_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
To_TO avoid_VB confusion_NN ,_, we_PRP would_MD like_VB to_TO call_VB the_DT latter_JJ sequence_NN kernel_NN ._.
In_IN this_DT paper_NN ,_, In_IN this_DT paper_NN we_PRP focus_VBP on_IN string_NN kernel_NN -LRB-_-LRB- defined_VBN on_IN continuous_JJ substrings_NNS -RRB-_-RRB- because_IN they_PRP are_VBP computationally_RB more_JJR
we_PRP omit_VBP the_DT details_NNS here_RB due_JJ to_TO the_DT space_NN limit_NN ._.
Our_PRP$ idea_NN about_IN key-substring-groups_NNS has_VBZ been_VBN partially_RB inspired_VBN by_IN the_DT works_NNS on_IN suffix_NN tree_NN -LRB-_-LRB- or_CC suffix_NN array_NN or_CC PAT_NN tree_NN -RRB-_-RRB- based_VBN key-phrase_JJ extraction_NN =_JJ -_: =[_NN 8_CD ,_, 11_CD ,_, 37_CD -RRB-_-RRB- -_: =_JJ -_: and_CC document_NN clustering_NN -LRB-_-LRB- 69_CD ,_, 73_CD -RRB-_-RRB- ._.
The_DT essential_JJ difference_NN is_VBZ that_IN we_PRP do_VBP not_RB care_VB about_IN whether_IN the_DT extracted_VBN key-substring-groups_NNS are_VBP semantically_RB meaningful_JJ or_CC not_RB ._.
In_IN contrast_NN ,_, our_PRP$ concern_NN is_VBZ the_DT
via_IN machine_NN learning_NN -LRB-_-LRB- 44_CD -RRB-_-RRB- is_VBZ a_DT fundamental_JJ technique_NN for_IN information_NN organization_NN and_CC management_NN ._.
Traditionally_RB machine_NN learning_VBG methods_NNS for_IN text_NN classification_NN treat_VB every_DT document_NN as_IN a_DT bag_NN of_IN words_NNS =_JJ -_: =[_NN 27_CD ,_, 40_CD ,_, 54_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, in_IN many_JJ text_NN classification_NN applications_NNS ,_, it_PRP is_VBZ appealing_VBG to_TO take_VB every_DT document_NN as_IN a_DT string_NN of_IN characters_NNS -LRB-_-LRB- 63_CD -RRB-_-RRB- ._.
The_DT string-based_JJ approaches_NNS to_TO text_NN classification_NN have_VBP at_IN least_JJS the_DT followi_NN
tly_RB ._.
Previous_JJ studies_NNS have_VBP consistently_RB shown_VBN that_IN discriminative_JJ learning_NN methods_NNS ,_, particularly_RB SVM_NN ,_, can_MD achieve_VB better_JJR performance_NN than_IN generative_JJ learning_NN methods_NNS in_IN word-based_JJ text_NN classification_NN =_JJ -_: =[_NN 17_CD ,_, 29_CD ,_, 67_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT discriminative_JJ approach_NN to_TO string-based_JJ text_NN classification_NN attempts_VBZ to_TO utilize_VB the_DT substrings_NNS of_IN a_DT document_NN as_IN its_PRP$ features_NNS ._.
However_RB ,_, a_DT document_NN of_IN length_NN |_CD d_NN |_NN has_VBZ |_VBN d_FW |_FW -LRB-_-LRB- |_CD d_NN |_CD +_CC 1_CD -RRB-_-RRB- \/_: 2_CD substrings_NNS in_IN t_NN
space_NN limit_NN ._.
Our_PRP$ idea_NN about_IN key-substring-groups_NNS has_VBZ been_VBN partially_RB inspired_VBN by_IN the_DT works_NNS on_IN suffix_NN tree_NN -LRB-_-LRB- or_CC suffix_NN array_NN or_CC PAT_NN tree_NN -RRB-_-RRB- based_VBN key-phrase_JJ extraction_NN -LRB-_-LRB- 8_CD ,_, 11_CD ,_, 37_CD -RRB-_-RRB- and_CC document_NN clustering_NN =_JJ -_: =[_NN 69_CD ,_, 73_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT essential_JJ difference_NN is_VBZ that_IN we_PRP do_VBP not_RB care_VB about_IN whether_IN the_DT extracted_VBN key-substring-groups_NNS are_VBP semantically_RB meaningful_JJ or_CC not_RB ._.
In_IN contrast_NN ,_, our_PRP$ concern_NN is_VBZ the_DT utility_NN of_IN key-substring-groups_NNS a_DT
uding_VBG PPM_NNP -LRB-_-LRB- Predication_NNP by_IN Partial_JJ Matching_NN -RRB-_-RRB- -LRB-_-LRB- 14_CD -RRB-_-RRB- and_CC PPM_NN \*_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- are_VBP all_DT in_IN the_DT family_NN of_IN variable_JJ order_NN Markov_NNP models_NNS ._.
PST_NN have_VBP been_VBN applied_VBN to_TO gene\/protein_NN sequence_NN clustering_NN -LRB-_-LRB- 66_CD -RRB-_-RRB- and_CC spam_NN filtering_VBG =_JJ -_: =[_NN 45_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Researchers_NNS have_VBP tried_VBN to_TO do_VB text_NN classification_NN by_IN using_VBG PPM_NN ,_, PPM_NN \*_NN and_CC other_JJ text_NN compression_NN methods_NNS -LRB-_-LRB- 5_CD ,_, 18_CD ,_, 43_CD ,_, 58_CD ,_, 64_CD -RRB-_-RRB- ._.
2.2_CD The_DT Discriminative_JJ Approach_NN In_IN contrast_NN to_TO generative_JJ learning_NN methods_NNS
we_PRP omit_VBP the_DT details_NNS here_RB due_JJ to_TO the_DT space_NN limit_NN ._.
Our_PRP$ idea_NN about_IN key-substring-groups_NNS has_VBZ been_VBN partially_RB inspired_VBN by_IN the_DT works_NNS on_IN suffix_NN tree_NN -LRB-_-LRB- or_CC suffix_NN array_NN or_CC PAT_NN tree_NN -RRB-_-RRB- based_VBN key-phrase_JJ extraction_NN =_JJ -_: =[_NN 8_CD ,_, 11_CD ,_, 37_CD -RRB-_-RRB- -_: =_JJ -_: and_CC document_NN clustering_NN -LRB-_-LRB- 69_CD ,_, 73_CD -RRB-_-RRB- ._.
The_DT essential_JJ difference_NN is_VBZ that_IN we_PRP do_VBP not_RB care_VB about_IN whether_IN the_DT extracted_VBN key-substring-groups_NNS are_VBP semantically_RB meaningful_JJ or_CC not_RB ._.
In_IN contrast_NN ,_, our_PRP$ concern_NN is_VBZ the_DT
feature_NN selection_NN criterion_NN is_VBZ the_DT document_NN frequency_NN -LRB-_-LRB- DF_NN -RRB-_-RRB- -LRB-_-LRB- 68_CD -RRB-_-RRB- ._.
We_PRP are_VBP able_JJ to_TO count_VB the_DT DF_NN for_IN every_DT substring_NN in_IN linear_JJ time_NN taking_VBG advantage_NN of_IN constant-time_JJ Least_NNP Common_NNP Ancestor_NNP -LRB-_-LRB- LCA_NNP -RRB-_-RRB- algorithm_NN =_JJ -_: =[_NN 4_CD ,_, 21_CD -RRB-_-RRB- -_: =_JJ -_: ,_, as_IN in_IN -LRB-_-LRB- 65_CD -RRB-_-RRB- ,_, though_IN we_PRP omit_VBP the_DT details_NNS here_RB due_JJ to_TO the_DT space_NN limit_NN ._.
Our_PRP$ idea_NN about_IN key-substring-groups_NNS has_VBZ been_VBN partially_RB inspired_VBN by_IN the_DT works_NNS on_IN suffix_NN tree_NN -LRB-_-LRB- or_CC suffix_NN array_NN or_CC PAT_NN tree_NN -RRB-_-RRB- based_VBN ke_IN
periments_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- ,_, -LRB-_-LRB- 3_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 4_LS -RRB-_-RRB- ._.
This_DT trick_NN slightly_RB increased_VBD the_DT program_NN speed_NN without_IN affecting_VBG the_DT performance_NN ._.
5_CD http:\/\/www.csie.ntu.edu.tw\/∼cjlin\/libsvm\/_NN 6_CD LibSVM_NN uses_VBZ the_DT one-vs-one_JJ ensemble_NN method_NN =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_SYM -_: for_IN mutual-exclusive_JJ multi-class_JJ classification_NN tasks_NNS ,_, i.e._FW ,_, in_IN experiments_NNS -LRB-_-LRB- 2_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 3_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 4_LS -RRB-_-RRB- ._.
7_CD http:\/\/www.daviddlewis.com\/resources\/testcollections\/_NN reuters21578_NN \/_: Figure_NNP 4_CD :_: The_DT log_NN -LRB-_-LRB- rank_NN -RRB-_-RRB- ∼_CD log_NN -LRB-_-LRB- frequ_NN
81.5_CD %_NN -LRB-_-LRB- 47_CD -RRB-_-RRB- --_: --_: --_: --_: words-based_JJ SVM_NN -LRB-_-LRB- polynomial_JJ kernel_NN -RRB-_-RRB- --_: --_: --_: --_: --_: --_: 86.0_CD %_NN -LRB-_-LRB- 29_CD -RRB-_-RRB- SVM_NN -LRB-_-LRB- rbf_NN kernel_NN -RRB-_-RRB- --_: --_: --_: --_: --_: --_: 86.4_CD %_NN -LRB-_-LRB- 29_CD -RRB-_-RRB- SVM_NN -LRB-_-LRB- linear_JJ kernel_NN -RRB-_-RRB- 85.3_CD %_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- --_: --_: 87.1_CD %_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- 92.0_CD %_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- SVM_NN -LRB-_-LRB- word_NN sequence_NN kernel_NN -RRB-_-RRB- 80.6_CD %_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: 90.5_CD %_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- 83.1_CD %_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- 91.5_CD %_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- Language_NN Modeling_NN -LRB-_-LRB- character_NN n-gram_NN -RRB-_-RRB- --_: --_: 80.3_CD %_NN -LRB-_-LRB- 47_CD -RRB-_-RRB- --_: --_: --_: --_: Data_NNP Compression_NNP -LRB-_-LRB- PPMC_NNP Order_NNP 2_CD -RRB-_-RRB- --_: --_: --_: --_: 74.3_CD %_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- --_: --_: DVMM_NN -LRB-_-LRB- VMM_NN with_IN discriminative_JJ feature_NN selection_NN -RRB-_-RRB- --_: --_: --_: --_: --_: --_: 87_CD
._.
Wee_NNP Sun_NNP Lee_NNP Department_NNP of_IN Computer_NNP Science_NNP and_CC Singapore-MIT_NNP Alliance_NNP National_NNP University_NNP of_IN Singapore_NNP Singapore_NNP 117543_CD leews@comp.nus.edu.sg_NN 1_CD ._.
INTRODUCTION_NN Text_NN classification_NN -LRB-_-LRB- or_CC categorization_NN -RRB-_-RRB- =_JJ -_: =[_NN 27_CD ,_, 54_CD -RRB-_-RRB- -_: =_SYM -_: via_IN machine_NN learning_NN -LRB-_-LRB- 44_CD -RRB-_-RRB- is_VBZ a_DT fundamental_JJ technique_NN for_IN information_NN organization_NN and_CC management_NN ._.
Traditionally_RB machine_NN learning_VBG methods_NNS for_IN text_NN classification_NN treat_VB every_DT document_NN as_IN a_DT bag_NN of_IN words_NNS
aditional_JJ text_NN classification_NN tasks_NNS that_WDT are_VBP naturally_RB suitable_JJ to_TO string-based_JJ approaches_NNS ,_, e.g._FW ,_, spam_NN filtering_VBG -LRB-_-LRB- 5_CD ,_, 45_CD -RRB-_-RRB- ._.
Obviously_RB key-substring-group_JJ features_NNS could_MD also_RB be_VB used_VBN for_IN text_NN clustering_NN =_JJ -_: =[_NN 16_CD ,_, 69_CD -RRB-_-RRB- -_: =_SYM -_: ._.
It_PRP is_VBZ possible_JJ to_TO go_VB even_RB further_RBR to_TO consider_VB applications_NNS in_IN other_JJ areas_NNS like_IN gene\/protein_NN sequence_NN classification\/clustering_NN -LRB-_-LRB- 39_CD ,_, 66_CD -RRB-_-RRB- ._.
6_CD ._.
ACKNOWLEDGMENTS_NNS We_PRP thank_VBP the_DT anonymous_JJ reviewers_NNS for_IN their_PRP$
tly_RB ._.
Previous_JJ studies_NNS have_VBP consistently_RB shown_VBN that_IN discriminative_JJ learning_NN methods_NNS ,_, particularly_RB SVM_NN ,_, can_MD achieve_VB better_JJR performance_NN than_IN generative_JJ learning_NN methods_NNS in_IN word-based_JJ text_NN classification_NN =_JJ -_: =[_NN 17_CD ,_, 29_CD ,_, 67_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT discriminative_JJ approach_NN to_TO string-based_JJ text_NN classification_NN attempts_VBZ to_TO utilize_VB the_DT substrings_NNS of_IN a_DT document_NN as_IN its_PRP$ features_NNS ._.
However_RB ,_, a_DT document_NN of_IN length_NN |_CD d_NN |_NN has_VBZ |_VBN d_FW |_FW -LRB-_-LRB- |_CD d_NN |_CD +_CC 1_CD -RRB-_-RRB- \/_: 2_CD substrings_NNS in_IN t_NN
tive_JJ learning_NN methods_NNS are_VBP often_RB inferior_JJ to_TO discriminative_JJ learning_NN methods_NNS ,_, in_IN terms_NNS of_IN classification_NN performance_NN ._.
Although_IN discriminative_JJ machine_NN learning_NN methods_NNS like_IN Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- =_JJ -_: =[_NN 15_CD ,_, 24_CD ,_, 31_CD ,_, 53_CD ,_, 55_CD -RRB-_-RRB- -_: =_JJ -_: and_CC AdaBoost_NNP -LRB-_-LRB- 52_CD ,_, 51_CD -RRB-_-RRB- have_VBP been_VBN quite_RB successful_JJ in_IN text_NN classification_NN with_IN word_NN features_NNS ,_, it_PRP is_VBZ neither_CC effective_JJ nor_CC efficient_JJ to_TO apply_VB them_PRP straightforwardly_RB taking_VBG all_DT substrings_NNS in_IN the_DT corpus_NN a_DT
Since_IN the_DT number_NN of_IN substring_JJ features_NNS used_VBN implicitly_RB in_IN string_NN kernel_NN is_VBZ extremely_RB large_JJ ,_, it_PRP would_MD be_VB infeasible_JJ to_TO apply_VB effective_JJ feature_NN selection_NN techniques_NNS -LRB-_-LRB- using_VBG χ_NN 2_CD or_CC information_NN gain_NN ,_, etc._NN -RRB-_-RRB- =_JJ -_: =[_NN 68_CD ,_, 19_CD -RRB-_-RRB- -_: =_JJ -_: ,_, feature_NN weighting_NN techniques_NNS -LRB-_-LRB- such_JJ as_IN TF_NN ×_CD IDF_NN -LRB-_-LRB- 60_CD ,_, 2_CD -RRB-_-RRB- -RRB-_-RRB- ,_, or_CC advanced_JJ kernel_NN functions_NNS -LRB-_-LRB- 28_CD ,_, 35_CD ,_, 72_CD -RRB-_-RRB- ._.
3_LS ._.
KEY-SUBSTRING-GROUP_NN FEATURES_NN A_NN corpus_NN D_NN =_JJ -LCB-_-LRB- d1_NN ,_, d2_NN ,_, ..._: ,_, dm_NN -RCB-_-RRB- of_IN size_NN n_NN -LRB-_-LRB- i.e._FW ,_, m_NN k_NN =_JJ 1_CD |_CD dk_NN |_NN =_JJ n_NN -RRB-_-RRB- co_NN
47_CD ,_, 57_CD -RRB-_-RRB- ._.
•_VB The_DT messy_JJ and_CC rather_RB artificial_JJ problem_NN of_IN defining_VBG word_NN boundaries_NNS can_MD be_VB avoided_VBN ._.
This_DT is_VBZ particularly_RB attractive_JJ to_TO text_NN classification_NN for_IN oriental_JJ languages_NNS -LRB-_-LRB- Chinese_NNP ,_, Japanese_NNP ,_, etc._NN -RRB-_-RRB- =_JJ -_: =[_NN 1_CD ,_, 23_CD ,_, 47_CD -RRB-_-RRB- -_: =_JJ -_: ,_, because_IN many_JJ oriental_JJ languages_NNS do_VBP not_RB utilize_VB word_NN delimiters_NNS as_IN whitespace_NN characters_NNS in_IN western_JJ languages_NNS -LRB-_-LRB- English_NNP ,_, French_NNP ,_, etc._NN -RRB-_-RRB- ._.
Although_IN it_PRP is_VBZ possible_JJ to_TO perform_VB automatic_JJ word_NN segmentation_NN t_NN
er_IN n_NN are_VBP usually_RB called_VBN n-gram_NN language_NN models_NNS -LRB-_-LRB- 20_CD ,_, 50_CD -RRB-_-RRB- in_IN the_DT field_NN of_IN natural_JJ language_NN processing_NN ._.
The_DT n-gram_NN language_NN modeling_NN technique_NN has_VBZ been_VBN used_VBN extensively_RB in_IN speech_NN recognition_NN for_IN decades_NNS =_JJ -_: =[_NN 32_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC recently_RB in_IN information_NN retrieval_NN -LRB-_-LRB- 48_CD ,_, 36_CD ,_, 70_CD -RRB-_-RRB- ._.
Most_JJS occurrences_NNS of_IN the_DT term_NN ``_`` n-gram_NN ''_'' in_IN the_DT natural_JJ language_NN processing_NN literature_NN refer_VBP to_TO a_DT continuous_JJ segment_NN of_IN n_NN words_NNS ,_, but_CC in_IN this_DT paper_NN we_PRP
in_IN string_NN kernel_NN is_VBZ extremely_RB large_JJ ,_, it_PRP would_MD be_VB infeasible_JJ to_TO apply_VB effective_JJ feature_NN selection_NN techniques_NNS -LRB-_-LRB- using_VBG χ_NN 2_CD or_CC information_NN gain_NN ,_, etc._NN -RRB-_-RRB- -LRB-_-LRB- 68_CD ,_, 19_CD -RRB-_-RRB- ,_, feature_NN weighting_NN techniques_NNS -LRB-_-LRB- such_JJ as_IN TF_NN ×_CD IDF_NN =_JJ -_: =[_NN 60_CD ,_, 2_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, or_CC advanced_JJ kernel_NN functions_NNS -LRB-_-LRB- 28_CD ,_, 35_CD ,_, 72_CD -RRB-_-RRB- ._.
3_LS ._.
KEY-SUBSTRING-GROUP_NN FEATURES_NN A_NN corpus_NN D_NN =_JJ -LCB-_-LRB- d1_NN ,_, d2_NN ,_, ..._: ,_, dm_NN -RCB-_-RRB- of_IN size_NN n_NN -LRB-_-LRB- i.e._FW ,_, m_NN k_NN =_JJ 1_CD |_CD dk_NN |_NN =_JJ n_NN -RRB-_-RRB- contains_VBZ about_IN n_NN -LRB-_-LRB- n_NN +_CC 1_LS -RRB-_-RRB- \/_: 2_CD substrings_NNS ._.
Our_PRP$ most_RBS important_JJ
._.
have_VBP tried_VBN character-level_JJ n-gram_NN modeling_NN for_IN various_JJ text_NN classification_NN tasks_NNS -LRB-_-LRB- 47_CD -RRB-_-RRB- ._.
To_TO achieve_VB a_DT decent_JJ performance_NN ,_, one_CD needs_VBZ to_TO choose_VB an_DT appropriate_JJ order_NN n_NN and_CC employ_VB a_DT good_JJ smoothing_NN method_NN =_JJ -_: =[_NN 71_CD ,_, 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Markov_NNP chain_NN models_NNS in_IN variable_JJ order_NN adjust_VBP the_DT memory_NN length_NN according_VBG to_TO the_DT context_NN ,_, hence_RB they_PRP are_VBP much_RB more_RBR flexible_JJ and_CC robust_JJ than_IN fixed_JJ order_NN Markov_NNP chain_NN models_NNS ._.
The_DT amnesic_JJ probabilistic_NN
umber_NN of_IN features_NNS was_VBD controlled_VBN to_TO be_VB relatively_RB small_JJ ._.
4.2_CD Chinese_NNP Text_NNP Topic_NNP Classification_NN We_PRP did_VBD the_DT Chinese_JJ text_NN topic_NN classification_NN experiment_NN on_IN the_DT TREC-5_NN People_NNPS 's_POS Daily_NNP News_NNP dataset_NN ,_, as_IN in_IN =_JJ -_: =[_NN 22_CD ,_, 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT dataset_NN is_VBZ a_DT subset_NN of_IN the_DT Mandarin_NNP News_NNP Corpus_NNP announced_VBD by_IN the_DT Linguistic_NNP Data_NNP Consortium_NNP -LRB-_-LRB- LDC_NNP -RRB-_-RRB- in_IN 1995_CD ._.
There_EX are_VBP six_CD topic_NN categories_NNS :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- Politics_NNPS ,_, Law_NNP and_CC Society_NNP ;_: -LRB-_-LRB- 2_LS -RRB-_-RRB- Literature_NNP and_CC Arts_NNP
ts_NNS -RRB-_-RRB- can_MD be_VB exploited_VBN automatically_RB ._.
This_DT is_VBZ particularly_RB helpful_JJ to_TO non-topical_JJ text_NN classification_NN applications_NNS ,_, such_JJ as_IN text_NN genre_NN classification_NN -LRB-_-LRB- 33_CD ,_, 38_CD ,_, 47_CD ,_, 57_CD -RRB-_-RRB- and_CC text_NN authorship_NN classification_NN =_JJ -_: =[_NN 25_CD ,_, 47_CD ,_, 57_CD -RRB-_-RRB- -_: =_SYM -_: ._.
•_VB The_DT messy_JJ and_CC rather_RB artificial_JJ problem_NN of_IN defining_VBG word_NN boundaries_NNS can_MD be_VB avoided_VBN ._.
This_DT is_VBZ particularly_RB attractive_JJ to_TO text_NN classification_NN for_IN oriental_JJ languages_NNS -LRB-_-LRB- Chinese_NNP ,_, Japanese_NNP ,_, etc._NN -RRB-_-RRB- -LRB-_-LRB- 1_CD ,_, 23_CD ,_,
riants_NNS -RRB-_-RRB- and_CC super-word_JJ features_NNS -LRB-_-LRB- e.g._FW ,_, phrasal_JJ effects_NNS -RRB-_-RRB- can_MD be_VB exploited_VBN automatically_RB ._.
This_DT is_VBZ particularly_RB helpful_JJ to_TO non-topical_JJ text_NN classification_NN applications_NNS ,_, such_JJ as_IN text_NN genre_NN classification_NN =_JJ -_: =[_NN 33_CD ,_, 38_CD ,_, 47_CD ,_, 57_CD -RRB-_-RRB- -_: =_JJ -_: and_CC text_NN authorship_NN classification_NN -LRB-_-LRB- 25_CD ,_, 47_CD ,_, 57_CD -RRB-_-RRB- ._.
•_VB The_DT messy_JJ and_CC rather_RB artificial_JJ problem_NN of_IN defining_VBG word_NN boundaries_NNS can_MD be_VB avoided_VBN ._.
This_DT is_VBZ particularly_RB attractive_JJ to_TO text_NN classification_NN for_IN orien_NN
on_IN criterion_NN is_VBZ the_DT document_NN frequency_NN -LRB-_-LRB- DF_NN -RRB-_-RRB- -LRB-_-LRB- 68_CD -RRB-_-RRB- ._.
We_PRP are_VBP able_JJ to_TO count_VB the_DT DF_NN for_IN every_DT substring_NN in_IN linear_JJ time_NN taking_VBG advantage_NN of_IN constant-time_JJ Least_NNP Common_NNP Ancestor_NNP -LRB-_-LRB- LCA_NNP -RRB-_-RRB- algorithm_NN -LRB-_-LRB- 4_CD ,_, 21_CD -RRB-_-RRB- ,_, as_IN in_IN =_JJ -_: =[_NN 65_CD -RRB-_-RRB- -_: =_JJ -_: ,_, though_IN we_PRP omit_VBP the_DT details_NNS here_RB due_JJ to_TO the_DT space_NN limit_NN ._.
Our_PRP$ idea_NN about_IN key-substring-groups_NNS has_VBZ been_VBN partially_RB inspired_VBN by_IN the_DT works_NNS on_IN suffix_NN tree_NN -LRB-_-LRB- or_CC suffix_NN array_NN or_CC PAT_NN tree_NN -RRB-_-RRB- based_VBN key-phrase_JJ ext_NN
would_MD be_VB infeasible_JJ to_TO apply_VB effective_JJ feature_NN selection_NN techniques_NNS -LRB-_-LRB- using_VBG χ_NN 2_CD or_CC information_NN gain_NN ,_, etc._NN -RRB-_-RRB- -LRB-_-LRB- 68_CD ,_, 19_CD -RRB-_-RRB- ,_, feature_NN weighting_NN techniques_NNS -LRB-_-LRB- such_JJ as_IN TF_NN ×_CD IDF_NN -LRB-_-LRB- 60_CD ,_, 2_CD -RRB-_-RRB- -RRB-_-RRB- ,_, or_CC advanced_JJ kernel_NN functions_NNS =_JJ -_: =[_NN 28_CD ,_, 35_CD ,_, 72_CD -RRB-_-RRB- -_: =_SYM -_: ._.
3_LS ._.
KEY-SUBSTRING-GROUP_NN FEATURES_NN A_NN corpus_NN D_NN =_JJ -LCB-_-LRB- d1_NN ,_, d2_NN ,_, ..._: ,_, dm_NN -RCB-_-RRB- of_IN size_NN n_NN -LRB-_-LRB- i.e._FW ,_, m_NN k_NN =_JJ 1_CD |_CD dk_NN |_NN =_JJ n_NN -RRB-_-RRB- contains_VBZ about_IN n_NN -LRB-_-LRB- n_NN +_CC 1_LS -RRB-_-RRB- \/_: 2_CD substrings_NNS ._.
Our_PRP$ most_RBS important_JJ insight_NN comes_VBZ from_IN the_DT fact_NN that_IN these_DT subs_NNS
space_NN limit_NN ._.
Our_PRP$ idea_NN about_IN key-substring-groups_NNS has_VBZ been_VBN partially_RB inspired_VBN by_IN the_DT works_NNS on_IN suffix_NN tree_NN -LRB-_-LRB- or_CC suffix_NN array_NN or_CC PAT_NN tree_NN -RRB-_-RRB- based_VBN key-phrase_JJ extraction_NN -LRB-_-LRB- 8_CD ,_, 11_CD ,_, 37_CD -RRB-_-RRB- and_CC document_NN clustering_NN =_JJ -_: =[_NN 69_CD ,_, 73_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT essential_JJ difference_NN is_VBZ that_IN we_PRP do_VBP not_RB care_VB about_IN whether_IN the_DT extracted_VBN key-substring-groups_NNS are_VBP semantically_RB meaningful_JJ or_CC not_RB ._.
In_IN contrast_NN ,_, our_PRP$ concern_NN is_VBZ the_DT utility_NN of_IN key-substring-groups_NNS a_DT
