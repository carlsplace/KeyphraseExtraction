Bayesian_JJ networks_NNS for_IN lossless_JJ dataset_NN compression_NN
e_LS that_WDT appears_VBZ in_IN the_DT dictionary_NN ,_, that_DT sequence_NN 's_POS position_NN in_IN the_DT dictionary_NN is_VBZ encoded_VBN rather_RB than_IN the_DT individual_JJ symbols_NNS themselves_PRP ._.
An_DT example_NN of_IN a_DT dictionary-based_JJ algorithm_NN is_VBZ the_DT LZ77_NN algorithm_NN =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- employed_VBN by_IN gzip_NN -RRB-_-RRB- ,_, which_WDT uses_VBZ a_DT sliding_VBG window_NN of_IN previously_RB encoded_VBN symbols_NNS as_IN its_PRP$ dictionary_NN ._.
These_DT algorithms_NNS can_MD be_VB shown_VBN to_TO achieve_VB asympotically_RB optimal_JJ compression_NN rates_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ;_: however_RB ,_, they_PRP m_NN
only_RB needing_VBG a_DT fraction_NN of_IN a_DT bit_NN -RRB-_-RRB- ,_, Huffman_NN coding_NN can_MD be_VB inefficient_JJ ,_, as_IN the_DT code_NN requires_VBZ at_IN least_JJS one_CD bit_NN for_IN each_DT source_NN symbol_NN encoded_VBN ._.
Arithmetic_NNP Coding_NNP Arithmetic_NNP coding_NN -LRB-_-LRB- developed_VBN by_IN Rissanen_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Pasco_NNP -LRB-_-LRB- 11_CD -RRB-_-RRB- ;_: see_VB Witten_NNP ,_, Neal_NNP ,_, and_CC Cleary_NNP -LRB-_-LRB- 16_CD -RRB-_-RRB- for_IN a_DT tutorial_NN -RRB-_-RRB- allows_VBZ sequences_NNS of_IN symbols_NNS to_TO be_VB encoded_VBN nearly_RB optimally_RB -LRB-_-LRB- in_IN the_DT limit_NN as_IN k_NN increases_NNS -RRB-_-RRB- without_IN requiring_VBG the_DT enumeration_NN of_IN all_DT pos_NNS
s_NN for_IN learning_VBG Bayesian_JJ networks_NNS ._.
The_DT first_JJ algorithm_NN uses_VBZ a_DT form_NN of_IN stochastic_JJ hillclimbing_NN over_IN possible_JJ network_NN structures_NNS using_VBG the_DT Bayesian_NNP Information_NNP Criterion_NNP as_IN its_PRP$ scoring_VBG function_NN ._.
ADTrees_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP used_VBN to_TO speed_VB up_RP this_DT search_NN by_IN decreasing_VBG the_DT amount_NN of_IN time_NN necessary_JJ to_TO calculate_VB the_DT dataset_NN statistics_NNS required_VBN for_IN the_DT search_NN ._.
The_DT second_JJ algorithm_NN takes_VBZ two_CD sweeps_NNS through_IN the_DT dataset_NN ._.
In_IN
er_IN the_DT dataset_NN to_TO fill_VB in_IN the_DT probability_NN tables_NNS of_IN the_DT resulting_VBG network_NN ._.
See_VB -LRB-_-LRB- 4_CD -RRB-_-RRB- for_IN further_JJ details_NNS ._.
This_DT algorithm_NN is_VBZ somewhat_RB similar_JJ to_TO an_DT algorithm_NN previously_RB used_VBN by_IN Sahami_NNP for_IN classification_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT special_JJ case_NN where_WRB c_NN is_VBZ 1_CD ,_, this_DT algorithm_NN reduces_VBZ to_TO a_DT penalized_VBN version_NN of_IN Chow_NNP and_CC Liu_NNP 's_POS dependency-tree_JJ algorithm_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC is_VBZ provably_RB optimal_JJ ._.
While_IN the_DT network_NN chosen_VBN by_IN this_DT greedy_JJ algo_NN
th_DT arithmetic_NN coding_VBG on_IN real_JJ datasets_NNS ._.
In_IN conjunction_NN with_IN the_DT Bayesian_JJ network_NN learning_VBG algorithms_NNS discussed_VBN above_RB ,_, we_PRP used_VBD a_DT limited-precision_JJ arithmetic_NN coding_NN library_NN written_VBN by_IN Carpinelli_NNP et_FW al._FW =_SYM -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: based_VBN on_IN a_DT paper_NN by_IN Moffat_NNP et_FW al._FW -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
We_PRP compare_VBP the_DT compression_NN performance_NN of_IN arithmetic_NN coding_VBG with_IN Bayesian_JJ networks_NNS -LRB-_-LRB- using_VBG the_DT best_JJS of_IN the_DT two_CD algorithms_NNS described_VBN above_IN -RRB-_-RRB- and_CC Dynamic_NNP Bayesian_NNP n_NN
fraction_NN of_IN a_DT bit_NN -RRB-_-RRB- ,_, Huffman_NN coding_NN can_MD be_VB inefficient_JJ ,_, as_IN the_DT code_NN requires_VBZ at_IN least_JJS one_CD bit_NN for_IN each_DT source_NN symbol_NN encoded_VBN ._.
Arithmetic_NNP Coding_NNP Arithmetic_NNP coding_NN -LRB-_-LRB- developed_VBN by_IN Rissanen_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC Pasco_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ;_: see_VB Witten_NNP ,_, Neal_NNP ,_, and_CC Cleary_NNP -LRB-_-LRB- 16_CD -RRB-_-RRB- for_IN a_DT tutorial_NN -RRB-_-RRB- allows_VBZ sequences_NNS of_IN symbols_NNS to_TO be_VB encoded_VBN nearly_RB optimally_RB -LRB-_-LRB- in_IN the_DT limit_NN as_IN k_NN increases_NNS -RRB-_-RRB- without_IN requiring_VBG the_DT enumeration_NN of_IN all_DT possible_JJ source_NN co_NN
e_LS LZ77_NN algorithm_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- -LRB-_-LRB- employed_VBN by_IN gzip_NN -RRB-_-RRB- ,_, which_WDT uses_VBZ a_DT sliding_VBG window_NN of_IN previously_RB encoded_VBN symbols_NNS as_IN its_PRP$ dictionary_NN ._.
These_DT algorithms_NNS can_MD be_VB shown_VBN to_TO achieve_VB asympotically_RB optimal_JJ compression_NN rates_NNS =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: ;_: however_RB ,_, they_PRP may_MD require_VB the_DT use_NN of_IN unmanageably_RB large_JJ dictionaries_NNS in_IN order_NN to_TO do_VB so_RB ._.
Huffman_NNP Coding_NNP Given_VBD a_DT small_JJ discrete_JJ set_NN of_IN source_NN symbols_NNS and_CC their_PRP$ associated_VBN probabilities_NNS ,_, a_DT simple_JJ greed_NN
uch_VB a_DT model_NN would_MD typically_RB be_VB useless_JJ for_IN compression_NN ,_, since_IN it_PRP would_MD usually_RB require_VB as_IN much_JJ space_NN as_IN D_NN itself_PRP ._.
What_WDT kinds_NNS of_IN probabilistic_JJ models_NNS might_MD be_VB useful_JJ for_IN compression_NN ?_.
Bayesian_JJ networks_NNS =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, also_RB commonly_RB known_VBN as_IN belief_NN networks_NNS ,_, are_VBP a_DT popular_JJ class_NN of_IN probabilistic_JJ models_NNS that_WDT work_VBP well_RB in_IN conjunction_NN with_IN compression_NN ,_, although_IN they_PRP are_VBP primarily_RB used_VBN for_IN data_NN analysis_NN and_CC decision-ma_NN
unmanageably_RB large_JJ dictionaries_NNS in_IN order_NN to_TO do_VB so_RB ._.
Huffman_NNP Coding_NNP Given_VBD a_DT small_JJ discrete_JJ set_NN of_IN source_NN symbols_NNS and_CC their_PRP$ associated_VBN probabilities_NNS ,_, a_DT simple_JJ greedy_JJ algorithm_NN developed_VBN by_IN David_NNP Huffman_NNP =_SYM -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: can_MD be_VB used_VBN to_TO find_VB an_DT optimal_JJ code_NN with_IN which_WDT to_TO encode_VB these_DT source_NN symbols_NNS on_IN an_DT individual_JJ basis_NN ._.
However_RB ,_, if_IN the_DT probability_NN for_IN one_CD particular_JJ source_NN symbol_NN is_VBZ very_RB high_JJ -LRB-_-LRB- theoretically_RB only_RB nee_NN
l_NN in_IN B_NN 's_POS probability_NN tables_NNS optimally_RB :_: namely_RB ,_, we_PRP simply_RB use_VBP the_DT empirical_JJ distributions_NNS appearing_VBG in_IN D._NNP However_RB ,_, even_RB with_IN complete_JJ data_NNS ,_, the_DT problem_NN of_IN finding_VBG the_DT best_JJS network_NN structure_NN is_VBZ NP-hard_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Learning_VBG a_DT Bayesian_JJ network_NN is_VBZ thus_RB typically_RB done_VBN by_IN using_VBG a_DT search_NN procedure_NN to_TO find_VB a_DT network_NN B_NN that_WDT maximizes_VBZ -LRB-_-LRB- or_CC at_IN least_JJS hopefully_RB comes_VBZ close_JJ to_TO maximizing_VBG -RRB-_-RRB- a_DT scoring_VBG function_NN C_NN -LRB-_-LRB- B_NN ;D_NN -RRB-_-RRB- ._.
A_DT popul_NN
can_MD be_VB inefficient_JJ ,_, as_IN the_DT code_NN requires_VBZ at_IN least_JJS one_CD bit_NN for_IN each_DT source_NN symbol_NN encoded_VBN ._.
Arithmetic_NNP Coding_NNP Arithmetic_NNP coding_NN -LRB-_-LRB- developed_VBN by_IN Rissanen_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC Pasco_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ;_: see_VB Witten_NNP ,_, Neal_NNP ,_, and_CC Cleary_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: for_IN a_DT tutorial_NN -RRB-_-RRB- allows_VBZ sequences_NNS of_IN symbols_NNS to_TO be_VB encoded_VBN nearly_RB optimally_RB -LRB-_-LRB- in_IN the_DT limit_NN as_IN k_NN increases_NNS -RRB-_-RRB- without_IN requiring_VBG the_DT enumeration_NN of_IN all_DT possible_JJ source_NN code_NN sequences_NNS of_IN length_NN k._FW Arithmeti_FW
ish_NN to_TO scan_VB through_IN the_DT dataset_NN sequentially_RB ,_, we_PRP can_MD take_VB advantage_NN of_IN potential_JJ correlations_NNS between_IN the_DT ith_NN record_NN and_CC i_LS +_CC 1th_JJ record_NN to_TO obtain_VB further_JJ compression_NN ._.
We_PRP use_VBP Dynamic_NNP Bayesian_NNP Networks_NNP =_SYM -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: to_TO seek_VB out_RP and_CC learn_VB such_JJ correlations_NNS and_CC exploit_VB them_PRP in_IN a_DT tighter_JJR encoding_NN ._.
These_DT networks_NNS are_VBP learned_VBN with_IN a_DT greedy_JJ algorithm_NN similar_JJ to_TO the_DT greedy_JJ Bayesian_JJ networklearning_NN algorithm_NN described_VBN i_FW
will_MD be_VB examined_VBN in_IN -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
6_CD Concluding_VBG remarks_NNS Related_NNP Work_NNP Automatically-learned_JJ Bayesian_JJ networks_NNS have_VBP been_VBN used_VBN previously_RB in_IN conjunction_NN with_IN arithmetic_NN encoding_VBG in_IN recent_JJ research_NN by_IN Brendan_NNP Frey_NNP =_SYM -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Rather_RB than_IN learning_VBG the_DT structure_NN of_IN the_DT Bayesian_JJ network_NN ,_, Frey_NNP uses_VBZ a_DT fixed_JJ network_NN structure_NN in_IN which_WDT each_DT node_NN has_VBZ many_JJ parents_NNS ;_: the_DT probability_NN of_IN each_DT node_NN given_VBN its_PRP$ parents_NNS is_VBZ paramaterized_VBN u_NN
In_IN conjunction_NN with_IN the_DT Bayesian_JJ network_NN learning_VBG algorithms_NNS discussed_VBN above_RB ,_, we_PRP used_VBD a_DT limited-precision_JJ arithmetic_NN coding_NN library_NN written_VBN by_IN Carpinelli_NNP et_FW al._FW -LRB-_-LRB- 1_LS -RRB-_-RRB- based_VBN on_IN a_DT paper_NN by_IN Moffat_NNP et_FW al._FW =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP compare_VBP the_DT compression_NN performance_NN of_IN arithmetic_NN coding_VBG with_IN Bayesian_JJ networks_NNS -LRB-_-LRB- using_VBG the_DT best_JJS of_IN the_DT two_CD algorithms_NNS described_VBN above_IN -RRB-_-RRB- and_CC Dynamic_NNP Bayesian_NNP networks_NNS with_IN the_DT performance_NN of_IN gzip_NN a_DT
similar_JJ to_TO an_DT algorithm_NN previously_RB used_VBN by_IN Sahami_NNP for_IN classification_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
In_IN the_DT special_JJ case_NN where_WRB c_NN is_VBZ 1_CD ,_, this_DT algorithm_NN reduces_VBZ to_TO a_DT penalized_VBN version_NN of_IN Chow_NNP and_CC Liu_NNP 's_POS dependency-tree_JJ algorithm_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC is_VBZ provably_RB optimal_JJ ._.
While_IN the_DT network_NN chosen_VBN by_IN this_DT greedy_JJ algorithm_NN wo_MD n't_RB generally_RB be_VB as_RB accurate_JJ as_IN one_CD found_VBN via_IN a_DT more_RBR thorough_JJ search_NN ,_, this_DT algorithm_NN has_VBZ the_DT advantage_NN of_IN being_VBG more_JJR comp_NN
hat_NN are_VBP good_JJ for_IN compression_NN ._.
This_DT ``_`` minimum_JJ description_NN length_NN ''_'' -LRB-_-LRB- or_CC MDL_NN -RRB-_-RRB- approach_NN has_VBZ also_RB been_VBN used_VBN for_IN learning_VBG Bayesian_JJ networks_NNS in_IN cases_NNS where_WRB compression_NN is_VBZ not_RB necessarily_RB the_DT primary_NN =_JJ -_: =_JJ objective_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
Bay_NN -_: =_JJ -_: esian_JJ networks_NNS are_VBP straightforward_JJ to_TO use_VB with_IN arithmetic_NN coding_NN ._.
To_TO encode_VB a_DT record_NN I_NN of_IN the_DT dataset_NN with_IN a_DT Bayesian_JJ network_NN B_NN ,_, one_CD treats_VBZ each_DT of_IN the_DT values_NNS in_IN I_NN as_IN an_DT individual_JJ ``_`` source_NN symbol_NN
search_NN procedure_NN to_TO find_VB a_DT network_NN B_NN that_WDT maximizes_VBZ -LRB-_-LRB- or_CC at_IN least_JJS hopefully_RB comes_VBZ close_JJ to_TO maximizing_VBG -RRB-_-RRB- a_DT scoring_VBG function_NN C_NN -LRB-_-LRB- B_NN ;D_NN -RRB-_-RRB- ._.
A_DT popular_JJ scoring_VBG function_NN is_VBZ the_DT Bayesian_NNP Information_NNP Criterion_NNP -LRB-_-LRB- BIC_NNP -RRB-_-RRB- =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, C_NN -LRB-_-LRB- B_NN ;D_NN -RRB-_-RRB- =_JJ log_NN P_NN -LRB-_-LRB- D_NN j_NN B_NN -RRB-_-RRB- \_FW Gamma_FW jBjs0_NN :5_CD log_NN k_NN where_WRB jBj_NN is_VBZ the_DT number_NN of_IN parameters_NNS -LRB-_-LRB- probabilities_NNS -RRB-_-RRB- stored_VBN in_IN the_DT net_NN and_CC k_NN is_VBZ the_DT number_NN of_IN records_NNS in_IN the_DT dataset_NN ._.
4_CD Using_VBG Bayesian_JJ networks_NNS with_IN arit_NN
