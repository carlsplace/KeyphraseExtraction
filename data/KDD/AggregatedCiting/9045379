Structured_VBN metric_JJ learning_NN for_IN high_JJ dimensional_JJ problems_NNS
The_DT success_NN of_IN popular_JJ algorithms_NNS such_JJ as_IN k-means_NN clustering_NN or_CC nearest_JJS neighbor_NN searches_NNS depend_VBP on_IN the_DT assumption_NN that_IN the_DT underlying_VBG distance_NN functions_NNS reflect_VBP domain-specific_JJ notions_NNS of_IN similarity_NN for_IN the_DT problem_NN at_IN hand_NN ._.
The_DT distance_NN metric_JJ learning_NN problem_NN seeks_VBZ to_TO optimize_VB a_DT distance_NN function_NN subject_JJ to_TO constraints_NNS that_WDT arise_VBP from_IN fully-supervised_JJ or_CC semisupervised_JJ information_NN ._.
Several_JJ recent_JJ algorithms_NNS have_VBP been_VBN proposed_VBN to_TO learn_VB such_JJ distance_NN functions_NNS in_IN low_JJ dimensional_JJ settings_NNS ._.
One_CD major_JJ shortcoming_NN of_IN these_DT methods_NNS is_VBZ their_PRP$ failure_NN to_TO scale_VB to_TO high_JJ dimensional_JJ problems_NNS that_WDT are_VBP becoming_VBG increasingly_RB ubiquitous_JJ in_IN modern_JJ data_NNS mining_NN applications_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP present_VBP metric_JJ learning_NN algorithms_NNS that_WDT scale_VBP linearly_RB with_IN dimensionality_NN ,_, permitting_VBG efficient_JJ optimization_NN ,_, storage_NN ,_, and_CC evaluation_NN of_IN the_DT learned_VBN metric_NN ._.
This_DT is_VBZ achieved_VBN through_IN our_PRP$ main_JJ technical_JJ contribution_NN which_WDT provides_VBZ a_DT framework_NN based_VBN on_IN the_DT log-determinant_JJ matrix_NN divergence_NN which_WDT enables_VBZ efficient_JJ optimization_NN of_IN structured_JJ ,_, low-parameter_JJ Mahalanobis_NNP distances_NNS ._.
Experimentally_RB ,_, we_PRP evaluate_VBP our_PRP$ methods_NNS across_IN a_DT variety_NN of_IN high_JJ dimensional_JJ domains_NNS ,_, including_VBG text_NN ,_, statistical_JJ software_NN analysis_NN ,_, and_CC collaborative_JJ filtering_VBG ,_, showing_VBG that_IN our_PRP$ methods_NNS scale_VBP to_TO data_NNS sets_NNS with_IN tens_NNS of_IN thousands_NNS or_CC more_JJR features_NNS ._.
We_PRP show_VBP that_IN our_PRP$ learned_VBN metric_NN can_MD achieve_VB excellent_JJ quality_NN with_IN respect_NN to_TO various_JJ criteria_NNS ._.
For_IN example_NN ,_, in_IN the_DT context_NN of_IN metric_JJ learning_NN for_IN nearest_JJS neighbor_NN classification_NN ,_, we_PRP show_VBP that_IN our_PRP$ methods_NNS achieve_VBP 24_CD %_NN higher_JJR accuracy_NN over_IN the_DT baseline_NN distance_NN ._.
Additionally_RB ,_, our_PRP$ methods_NNS yield_VBP very_RB good_JJ precision_NN while_IN providing_VBG recall_NN measures_NNS up_IN to_TO 20_CD %_NN higher_JJR than_IN other_JJ baseline_NN methods_NNS such_JJ as_IN latent_JJ semantic_JJ analysis_NN ._.
−_FW xj_FW -RRB-_-RRB- -LRB-_-LRB- xi_FW −_FW xj_FW -RRB-_-RRB- T_NN ť_NN -RRB-_-RRB- −_FW ¯_FW cij_FW i_FW ,_, j_FW δijλij_FW where_WRB λij_NN are_VBP dual_JJ variables_NNS with_IN λij_FW ≥_FW 0_CD ,_, δij_NN =_JJ +1_NN for_IN similarity_NN constraints_NNS and_CC δij_NN =_JJ −_NN 1_CD for_IN dissimilarity_NN constraints_NNS ._.
Using_VBG the_DT fact_NN that_IN ∇_NN A_NN log_NN |_NN A_NN |_NN =_JJ A_NN −_NN 1_CD =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT gradient_NN of_IN the_DT Lagrangian_NNP is_VBZ ,_, ∇_JJ AL_NN -LRB-_-LRB- A_NN ,_, λ_NN -RRB-_-RRB- =_JJ I_CD −_NN A_NN −_NN 1_CD +_CC X_NN i_FW ,_, j_NN δijλijUU_NN T_NN -LRB-_-LRB- xi_FW −_FW xj_FW -RRB-_-RRB- -LRB-_-LRB- xi_FW −_FW xj_FW -RRB-_-RRB- T_NN UU_NN T_NN ._.
Setting_VBG the_DT gradient_NN to_TO zero_VB and_CC solving_VBG for_IN A_DT −_NN 1_CD ,_, -LRB-_-LRB- A_DT ∗_NN -RRB-_-RRB- −_NN 1_CD =_JJ I_NN +_CC X_NN δijλijUU_NN T_NN -LRB-_-LRB- xi_FW −_FW xj_FW -RRB-_-RRB- -LRB-_-LRB- xi_FW −_FW xj_FW -RRB-_-RRB- T_NN
the_DT full-rank_JJ ITML_NN formulation_NN -LRB-_-LRB- 2.2_CD -RRB-_-RRB- ,_, we_PRP see_VBP that_IN A0_NN here_RB is_VBZ low-rank_JJ ,_, and_CC an_DT additional_JJ constraint_NN has_VBZ been_VBN added_VBN enforcing_VBG the_DT rank_NN of_IN the_DT optimal_JJ Mahalanobis_NNP matrix_NN A._NN Recent_JJ work_NN by_IN Kulis_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: considers_VBZ a_DT related_JJ problem_NN of_IN learning_VBG low-rank_JJ kernel_NN matrices_NNS subject_JJ to_TO linear_JJ constraints_NNS on_IN the_DT matrix_NN ._.
In_IN -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, the_DT LogDet_NNP divergence_NN was_VBD extended_VBN to_TO the_DT positive_JJ semi-definite_JJ cone_NN ,_, and_CC it_PRP was_VBD
can_MD be_VB constrained_VBN to_TO be_VB similar_JJ if_IN they_PRP share_VBP the_DT same_JJ class_NN label_NN and_CC dissimilar_JJ otherwise_RB ._.
One_CD class_NN of_IN distance_NN functions_NNS that_WDT has_VBZ shown_VBN good_JJ generalization_NN properties_NNS is_VBZ the_DT Mahalanobis_NNP distance_NN =_JJ -_: =[_NN 3_CD ,_, 10_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT Mahalanobis_NNP distance_NN generalizes_VBZ the_DT standard_NN squared_VBD Euclidean_JJ distance_NN commonly_RB used_VBN by_IN algorithms_NNS such_JJ as_IN the_DT k-nearest_NN neighbor_NN classifier_NN ._.
Intuitively_RB ,_, the_DT Mahalanobis_NNP distance_NN works_VBZ by_IN sca_NN
can_MD be_VB constrained_VBN to_TO be_VB similar_JJ if_IN they_PRP share_VBP the_DT same_JJ class_NN label_NN and_CC dissimilar_JJ otherwise_RB ._.
One_CD class_NN of_IN distance_NN functions_NNS that_WDT has_VBZ shown_VBN good_JJ generalization_NN properties_NNS is_VBZ the_DT Mahalanobis_NNP distance_NN =_JJ -_: =[_NN 3_CD ,_, 10_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT Mahalanobis_NNP distance_NN generalizes_VBZ the_DT standard_NN squared_VBD Euclidean_JJ distance_NN commonly_RB used_VBN by_IN algorithms_NNS such_JJ as_IN the_DT k-nearest_NN neighbor_NN classifier_NN ._.
Intuitively_RB ,_, the_DT Mahalanobis_NNP distance_NN works_VBZ by_IN sca_NN
lanobis_FW Distances_FW Term_NN frequency_NN models_NNS represent_VBP text_NN documents_NNS in_IN terms_NNS of_IN individual_JJ words_NNS and_CC their_PRP$ respective_JJ frequencies_NNS and_CC are_VBP standard_JJ representations_NNS used_VBN in_IN many_JJ text_NN analysis_NN applications_NNS =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
These_DT models_NNS typically_RB compute_VBP the_DT distance_NN between_IN two_CD examples_NNS x_NN and_CC y_NN using_VBG the_DT cosine_NN similarity_NN ,_, cos_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- =_JJ xT_NN y_NN ._.
Note_VB x_NN y_NN that_WDT when_WRB x_NN and_CC y_NN are_VBP normalized_VBN to_TO have_VB unit_NN L2_NN norm_NN ,_, the_DT cosine_NN
ubset_NN ,_, the_DT HDILR_NN method_NN outperforms_VBZ HDLR_NN for_IN low_JJ recall_NN values_NNS ,_, yet_CC it_PRP achieves_VBZ slightly_RB worse_JJR precision_NN for_IN higher_JJR recall_NN values_NNS ._.
5.3_CD Software_NNP Analysis_NNP We_PRP now_RB present_VBP results_NNS from_IN the_DT Clarify_NNP system_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: which_WDT attempt_VBP to_TO improve_VB software_NN error_NN messaging_VBG via_IN nearest_JJS neighbor_NN software_NN support_NN ._.
The_DT basis_NN of_IN the_DT Clarify_NNP system_NN lies_VBZ in_IN the_DT fact_NN that_IN modern_JJ software_NN design_NN promotes_VBZ modularity_NN and_CC abstracti_NNS
