Unsupervised_JJ Bayesian_JJ visualization_NN of_IN high-dimensional_JJ data_NNS
and_CC x_NN j_NN in_IN the_DT domain_NN space_NN ,_, and_CC ~_FW d_FW -LRB-_-LRB- ~_CD x_CC i_LS ;_: ~_CD x_NN j_NN -RRB-_-RRB- the_DT distance_NN between_IN the_DT corresponding_JJ vectors_NNS in_IN the_DT visual_JJ space_NN ._.
This_DT approach_NN to_TO the_DT visualization_NN problem_NN is_VBZ known_VBN as_IN Sammon_NNP 's_POS mapping_NN -LRB-_-LRB- see_VB =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Our_PRP$ practical_JJ goal_NN is_VBZ to_TO apply_VB the_DT reduced_VBN data_NNS set_VBD ~_NN X_NN for_IN data_NN visualization_NN purposes_NNS ,_, so_IN the_DT geometric_JJ Euclidean_JJ distance_NN is_VBZ a_DT natural_JJ choice_NN for_IN the_DT distance_NN metric_JJ ~_NN d_NN -LRB-_-LRB- \_CD Delta_NN -RRB-_-RRB- in_IN the_DT visual_JJ sp_NN
;_: Mk_NN -RRB-_-RRB- ._.
Unfortunately_RB ,_, as_IN discussed_VBN in_IN -LRB-_-LRB- 6_CD ,_, 4_CD ,_, 12_CD -RRB-_-RRB- ,_, finding_VBG accurate_JJ Bayesian_JJ network_NN models_NNS for_IN supervised_JJ prediction_NN tasks_NNS is_VBZ a_DT difficult_JJ problem_NN ._.
On_IN the_DT other_JJ hand_NN ,_, as_IN demonstrated_VBN in_IN ,_, for_IN example_NN ,_, =_JJ -_: =[_NN 17_CD ,_, 10_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT structurally_RB simple_JJ Naive_JJ Bayes_NNS classifier_NN performs_VBZ surprisingly_RB well_RB in_IN many_JJ real-world_JJ classification_NN domains_NNS ,_, despite_IN of_IN the_DT fact_NN that_IN the_DT model_NN is_VBZ extremely_RB fast_JJ to_TO construct_NN and_CC use_NN ._.
For_IN th_DT
h-dimensional_NN data_NNS ,_, and_CC give_VB several_JJ examples_NNS of_IN visualizations_NNS obtained_VBN by_IN using_VBG the_DT suggested_JJ scheme_NN with_IN probabilistic_JJ Bayesian_JJ network_NN models_NNS ._.
1_CD ._.
INTRODUCTION_NN Multidimensional_JJ scaling_NN -LRB-_-LRB- see_VB ,_, e.g._FW ,_, =_JJ -_: =[_NN 3_CD ,_, 2_CD -RRB-_-RRB- -_: =--RRB-_NN is_VBZ a_DT data_NN compression_NN or_CC data_NN reduction_NN task_NN where_WRB the_DT goal_NN is_VBZ to_TO replace_VB the_DT original_JJ high-dimensional_JJ data_NN vectors_NNS with_IN much_RB shorter_JJR vectors_NNS ,_, while_IN losing_VBG as_IN little_JJ information_NN as_IN possible_JJ ._.
Intuit_NNP
with_IN certain_JJ technical_JJ assumptions_NNS it_PRP is_VBZ possible_JJ to_TO marginalize_VB -LRB-_-LRB- integrate_VB -RRB-_-RRB- over_IN all_DT parameter_NN instantiations_NNS in_IN order_NN to_TO produce_VB the_DT corresponding_JJ predictive_JJ distribution_NN ._.
As_IN demonstrated_VBN in_IN ,_, e.g._FW ,_, =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: ,_, such_JJ marginalization_NN improves_VBZ the_DT predictive_JJ accuracy_NN of_IN the_DT Bayesian_JJ network_NN model_NN ,_, especially_RB in_IN cases_NNS where_WRB the_DT amount_NN of_IN sample_NN data_NNS is_VBZ small_JJ ._.
Practical_NNP algorithms_NNS for_IN performing_VBG predictive_JJ infe_NN
h-dimensional_NN data_NNS ,_, and_CC give_VB several_JJ examples_NNS of_IN visualizations_NNS obtained_VBN by_IN using_VBG the_DT suggested_JJ scheme_NN with_IN probabilistic_JJ Bayesian_JJ network_NN models_NNS ._.
1_CD ._.
INTRODUCTION_NN Multidimensional_JJ scaling_NN -LRB-_-LRB- see_VB ,_, e.g._FW ,_, =_JJ -_: =[_NN 3_CD ,_, 2_CD -RRB-_-RRB- -_: =--RRB-_NN is_VBZ a_DT data_NN compression_NN or_CC data_NN reduction_NN task_NN where_WRB the_DT goal_NN is_VBZ to_TO replace_VB the_DT original_JJ high-dimensional_JJ data_NN vectors_NNS with_IN much_RB shorter_JJR vectors_NNS ,_, while_IN losing_VBG as_IN little_JJ information_NN as_IN possible_JJ ._.
Intuit_NNP
to_TO similar_JJ predictivesdistributions_NNS ,_, when_WRB the_DT corresponding_JJ attribute-value_JJ pairs_NNS are_VBP given_VBN as_IN input_NN to_TO the_DT same_JJ probabilistic_JJ model_NN ._.
The_DT idea_NN is_VBZ related_JJ to_TO the_DT Bayesian_JJ distance_NN metric_NN suggested_VBN in_IN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: as_IN a_DT method_NN for_IN defining_VBG similarity_NN in_IN the_DT case-based_JJ reasoning_NN framework_NN ._.
The_DT basic_JJ principles_NNS of_IN the_DT suggested_JJ supervised_JJ Bayesian_JJ data_NNS reduction_NN scheme_NN are_VBP reviewed_VBN in_IN Section_NN 3_CD ._.
An_DT obvious_JJ drawba_NN
in_IN various_JJ data_NNS mining_NN tasks_NNS ,_, such_JJ as_IN instance_NN selection_NN and_CC clustering_NN ._.
A_DT formal_JJ description_NN of_IN the_DT visualization_NN problem_NN is_VBZ given_VBN in_IN Section_NN 2_CD ._.
In_IN this_DT paper_NN we_PRP use_VBP probabilistic_JJ Bayesian_JJ networks_NNS =_JJ -_: =[_NN 16_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: as_IN the_DT formal_JJ model_NN family_NN required_VBN in_IN our_PRP$ Bayesian_JJ data_NNS reduction_NN framework_NN ._.
Intuitively_RB speaking_VBG ,_, a_DT Bayesian_JJ -LRB-_-LRB- belief_NN -RRB-_-RRB- network_NN is_VBZ a_DT representation_NN of_IN a_DT probability_NN distribution_NN over_IN a_DT set_NN of_IN -LRB-_-LRB- usuall_NN
tween_NN vectors_NNS x_CC i_LS and_CC x_NN j_NN were_VBD determined_VBN by_IN using_VBG the_DT unsupervised_JJ distance_NN metric_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
This_DT requires_VBZ determining_VBG m_NN predictive_JJ distributions_NNS P_NN -LRB-_-LRB- Xk_NN j_NN x_NN \_FW Gamma_FW i_FW ;_: Mk_NN -RRB-_-RRB- ._.
Unfortunately_RB ,_, as_IN discussed_VBN in_IN =_JJ -_: =[_NN 6_CD ,_, 4_CD ,_, 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, finding_VBG accurate_JJ Bayesian_JJ network_NN models_NNS for_IN supervised_JJ prediction_NN tasks_NNS is_VBZ a_DT difficult_JJ problem_NN ._.
On_IN the_DT other_JJ hand_NN ,_, as_IN demonstrated_VBN in_IN ,_, for_IN example_NN ,_, -LRB-_-LRB- 17_CD ,_, 10_CD -RRB-_-RRB- ,_, the_DT structurally_RB simple_JJ Naive_JJ Bayes_NNS cla_VBP
ayesian_JJ network_NN model_NN ,_, especially_RB in_IN cases_NNS where_WRB the_DT amount_NN of_IN sample_NN data_NNS is_VBZ small_JJ ._.
Practical_NNP algorithms_NNS for_IN performing_VBG predictive_JJ inference_NN in_IN general_JJ Bayesian_JJ networks_NNS are_VBP discussed_VBN for_IN example_NN in_IN =_JJ -_: =[_NN 16_CD ,_, 15_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
325_CD After_IN producing_VBG a_DT compressed_VBN ,_, two_CD -_: or_CC three-dimensional_JJ representation_NN of_IN our_PRP$ data_NNS ,_, an_DT obvious_JJ question_NN concerns_VBZ the_DT quality_NN of_IN the_DT result_NN :_: how_WRB do_VBP we_PRP know_VB whether_IN the_DT compressed_VBN data_NN set_NN represe_NN
ich_RB allow_VB the_DT joint_JJ probability_NN distribution_NN for_IN a_DT data_NN vector_NN to_TO be_VB factorized_VBN as_IN a_DT product_NN of_IN simple_JJ conditional_JJ probabilities_NNS ._.
Techniques_NNS for_IN learning_VBG such_JJ models_NNS from_IN sample_NN data_NNS are_VBP discussed_VBN in_IN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
One_CD of_IN the_DT main_JJ advantages_NNS of_IN the_DT Bayesian_JJ network_NN model_NN is_VBZ the_DT fact_NN that_IN with_IN certain_JJ technical_JJ assumptions_NNS it_PRP is_VBZ possible_JJ to_TO marginalize_VB -LRB-_-LRB- integrate_VB -RRB-_-RRB- over_IN all_DT parameter_NN instantiations_NNS in_IN order_NN to_TO p_NN
change_VB the_DT relevance_NN of_IN the_DT vector_NN ,_, and_CC make_VB it_PRP in_IN some_DT sense_NN a_DT quite_RB different_JJ vector_NN ,_, although_IN geometrically_RB the_DT difference_NN is_VBZ only_RB one_CD bit_NN ._.
This_DT issue_NN is_VBZ discussed_VBN in_IN more_JJR detail_NN in_IN Section_NN 2_CD ._.
In_IN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: we_PRP proposed_VBD and_CC analyzed_VBD a_DT supervised_JJ ,_, probabilistic_JJ Permission_NN to_TO make_VB digital_JJ or_CC hard_JJ copies_NNS of_IN all_DT part_NN of_IN this_DT work_NN for_IN personal_JJ or_CC classroom_NN use_NN is_VBZ granted_VBN without_IN fee_NN provided_VBN that_IN copies_NNS are_VBP n_NN
aling_VBG with_IN a_DT model_NN structure_NN M_NN that_WDT is_VBZ possibly_RB only_RB a_DT poor_JJ model_NN of_IN the_DT ``_`` true_JJ ''_'' joint_JJ domain_NN probability_NN distribution_NN ,_, and_CC hence_RB some_DT of_IN the_DT probabilities_NNS obtained_VBN are_VBP not_RB correct_JJ ._.
As_IN demons_NNS =_JJ -_: =_JJ trated_VBN in_IN -LRB-_-LRB- 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, instead_RB of_IN trying_VBG to_TO find_VB a_DT good_JJ model_NN of_IN the_DT joint_JJ probability_NN distribution_NN ,_, in_IN supervised_JJ classification_NN domains_NNS it_PRP makes_VBZ sense_NN to_TO try_VB to_TO find_VB a_DT model_NN -LRB-_-LRB- or_CC a_DT set_NN of_IN models_NNS -RRB-_-RRB- so_IN that_IN the_DT errors_NNS affec_VBP
in_IN various_JJ data_NNS mining_NN tasks_NNS ,_, such_JJ as_IN instance_NN selection_NN and_CC clustering_NN ._.
A_DT formal_JJ description_NN of_IN the_DT visualization_NN problem_NN is_VBZ given_VBN in_IN Section_NN 2_CD ._.
In_IN this_DT paper_NN we_PRP use_VBP probabilistic_JJ Bayesian_JJ networks_NNS =_JJ -_: =[_NN 16_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: as_IN the_DT formal_JJ model_NN family_NN required_VBN in_IN our_PRP$ Bayesian_JJ data_NNS reduction_NN framework_NN ._.
Intuitively_RB speaking_VBG ,_, a_DT Bayesian_JJ -LRB-_-LRB- belief_NN -RRB-_-RRB- network_NN is_VBZ a_DT representation_NN of_IN a_DT probability_NN distribution_NN over_IN a_DT set_NN of_IN -LRB-_-LRB- usuall_NN
ayesian_JJ network_NN model_NN ,_, especially_RB in_IN cases_NNS where_WRB the_DT amount_NN of_IN sample_NN data_NNS is_VBZ small_JJ ._.
Practical_NNP algorithms_NNS for_IN performing_VBG predictive_JJ inference_NN in_IN general_JJ Bayesian_JJ networks_NNS are_VBP discussed_VBN for_IN example_NN in_IN =_JJ -_: =[_NN 16_CD ,_, 15_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
325_CD After_IN producing_VBG a_DT compressed_VBN ,_, two_CD -_: or_CC three-dimensional_JJ representation_NN of_IN our_PRP$ data_NNS ,_, an_DT obvious_JJ question_NN concerns_VBZ the_DT quality_NN of_IN the_DT result_NN :_: how_WRB do_VBP we_PRP know_VB whether_IN the_DT compressed_VBN data_NN set_NN represe_NN
ginal_JJ likelihood_NN or_CC evidence_NN ._.
The_DT required_JJ conditional_JJ distribution_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- can_MD then_RB be_VB computed_VBN by_IN marginalizing_VBG the_DT joint_JJ probability_NN distribution_NN P_NN -LRB-_-LRB- x_NN i_FW j_FW X_NN ;_: M_NN -RRB-_-RRB- appropriately_RB ._.
However_RB ,_, as_IN noted_VBN in_IN e.g._FW =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, finding_VBG the_DT maximal_JJ evidence_NN model_NN structure_NN is_VBZ not_RB a_DT feasible_JJ task_NN in_IN practice_NN as_IN the_DT number_NN of_IN possible_JJ Bayesian_JJ network_NN structures_NNS is_VBZ superexponential_JJ ._.
This_DT means_VBZ that_IN in_IN practical_JJ situations_NNS we_PRP
;_: Mk_NN -RRB-_-RRB- ._.
Unfortunately_RB ,_, as_IN discussed_VBN in_IN -LRB-_-LRB- 6_CD ,_, 4_CD ,_, 12_CD -RRB-_-RRB- ,_, finding_VBG accurate_JJ Bayesian_JJ network_NN models_NNS for_IN supervised_JJ prediction_NN tasks_NNS is_VBZ a_DT difficult_JJ problem_NN ._.
On_IN the_DT other_JJ hand_NN ,_, as_IN demonstrated_VBN in_IN ,_, for_IN example_NN ,_, =_JJ -_: =[_NN 17_CD ,_, 10_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT structurally_RB simple_JJ Naive_JJ Bayes_NNS classifier_NN performs_VBZ surprisingly_RB well_RB in_IN many_JJ real-world_JJ classification_NN domains_NNS ,_, despite_IN of_IN the_DT fact_NN that_IN the_DT model_NN is_VBZ extremely_RB fast_JJ to_TO construct_NN and_CC use_NN ._.
For_IN th_DT
5.1_CD The_DT setup_NN To_TO illustrate_VB the_DT validity_NN of_IN the_DT suggested_JJ data_NNS reduction_NN scheme_NN ,_, we_PRP performed_VBD a_DT series_NN of_IN experiments_NNS with_IN 20_CD publicly_RB available_JJ classification_NN data_NNS sets_NNS from_IN the_DT UCI_NNP data_NNS repository_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT preprocessing_JJ phase_NN of_IN the_DT experimental_JJ setup_NN ,_, all_DT continuous_JJ attributes_NNS in_IN the_DT data_NNS sets_NNS were_VBD discretized_VBN by_IN using_VBG a_DT straightforward_JJ application_NN of_IN the_DT kmeans_NNS algorithm_NN ._.
Consequently_RB ,_, with_IN r_NN
