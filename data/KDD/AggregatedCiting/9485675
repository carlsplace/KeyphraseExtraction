Knowledge_NN transfer_NN via_IN multiple_JJ model_NN local_JJ structure_NN mapping_NN
The_DT effectiveness_NN of_IN knowledge_NN transfer_NN using_VBG classification_NN algorithms_NNS depends_VBZ on_IN the_DT difference_NN between_IN the_DT distribution_NN that_WDT generates_VBZ the_DT training_NN examples_NNS and_CC the_DT one_NN from_IN which_WDT test_NN examples_NNS are_VBP to_TO be_VB drawn_VBN ._.
The_DT task_NN can_MD be_VB especially_RB difficult_JJ when_WRB the_DT training_NN examples_NNS are_VBP from_IN one_CD or_CC several_JJ domains_NNS different_JJ from_IN the_DT test_NN domain_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT locally_RB weighted_JJ ensemble_NN framework_NN to_TO combine_VB multiple_JJ models_NNS for_IN transfer_NN learning_NN ,_, where_WRB the_DT weights_NNS are_VBP dynamically_RB assigned_VBN according_VBG to_TO a_DT model_NN 's_POS predictive_JJ power_NN on_IN each_DT test_NN example_NN ._.
It_PRP can_MD integrate_VB the_DT advantages_NNS of_IN various_JJ learning_VBG algorithms_NNS and_CC the_DT labeled_JJ information_NN from_IN multiple_JJ training_NN domains_NNS into_IN one_CD unified_JJ classification_NN model_NN ,_, which_WDT can_MD then_RB be_VB applied_VBN on_IN a_DT different_JJ domain_NN ._.
Importantly_RB ,_, different_JJ from_IN many_JJ previously_RB proposed_VBN methods_NNS ,_, none_NN of_IN the_DT base_NN learning_NN method_NN is_VBZ required_VBN to_TO be_VB specifically_RB designed_VBN for_IN transfer_NN learning_NN ._.
We_PRP show_VBP the_DT optimality_NN of_IN a_DT locally_RB weighted_JJ ensemble_NN framework_NN as_IN a_DT general_JJ approach_NN to_TO combine_VB multiple_JJ models_NNS for_IN domain_NN transfer_NN ._.
We_PRP then_RB propose_VBP an_DT implementation_NN of_IN the_DT local_JJ weight_NN assignments_NNS by_IN mapping_VBG the_DT structures_NNS of_IN a_DT model_NN onto_IN the_DT structures_NNS of_IN the_DT test_NN domain_NN ,_, and_CC then_RB weighting_NN each_DT model_NN locally_RB according_VBG to_TO its_PRP$ consistency_NN with_IN the_DT neighborhood_NN structure_NN around_IN the_DT test_NN example_NN ._.
Experimental_JJ results_NNS on_IN text_NN classification_NN ,_, spam_NN filtering_VBG and_CC intrusion_NN detection_NN data_NNS sets_NNS demonstrate_VBP significant_JJ improvements_NNS in_IN classification_NN accuracy_NN gained_VBN by_IN the_DT framework_NN ._.
On_IN a_DT transfer_NN learning_VBG task_NN of_IN newsgroup_NN message_NN categorization_NN ,_, the_DT proposed_VBN locally_RB weighted_JJ ensemble_NN framework_NN achieves_VBZ 97_CD %_NN accuracy_NN when_WRB the_DT best_JJS single_JJ model_NN predicts_VBZ correctly_RB only_RB on_IN 73_CD %_NN of_IN the_DT test_NN examples_NNS ._.
In_IN summary_NN ,_, the_DT improvement_NN in_IN accuracy_NN is_VBZ over_IN 10_CD %_NN and_CC up_IN to_TO 30_CD %_NN across_IN different_JJ problems_NNS ._.
ith_JJ different_JJ learning_NN algorithms_NNS ._.
In_IN particular_JJ ,_, since_IN most_JJS data_NNS sets_NNS are_VBP high-dimensional_JJ ,_, the_DT following_NN commonly_RB used_VBN algorithms_NNS are_VBP appropriate_JJ choices_NNS :_: 1_LS -RRB-_-RRB- Winnow_NN -LRB-_-LRB- WNN_NN -RRB-_-RRB- from_IN learning_VBG package_NN SNoW_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, 2_LS -RRB-_-RRB- Logistic_JJ Regression_NN -LRB-_-LRB- LR_NN -RRB-_-RRB- implemented_VBN in_IN BBR_NN package_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: and_CC 3_LS -RRB-_-RRB- Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- implemented_VBN in_IN LibSVM_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
When_WRB we_PRP only_RB have_VBP a_DT single_JJ source_NN domain_NN in_IN the_DT training_NN ,_, three_CD single_JJ cl_NN
t_NN Process_VB prior_JJ is_VBZ used_VBN to_TO couple_VB the_DT parameters_NNS of_IN several_JJ models_NNS from_IN the_DT same_JJ parameterized_JJ family_NN of_IN dis-tributions_NNS ._.
-LRB-_-LRB- 10_CD -RRB-_-RRB- extends_VBZ the_DT boosting_VBG method_NN to_TO perform_VB transfer_NN learning_NN ._.
Bennett_NNP et_FW al._FW =_SYM -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT methodology_NN for_IN building_VBG a_DT meta-classifier_NN which_WDT combines_VBZ multiple_JJ distinct_JJ classifiers_NNS through_IN the_DT use_NN of_IN reliability_NN indicators_NNS ._.
The_DT proposed_VBN weighted_JJ ensemble_NN provides_VBZ a_DT more_RBR general_JJ fr_NN
arning_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
By_IN combining_VBG decisions_NNS from_IN individual_JJ classifiers_NNS ,_, ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_NNS -LRB-_-LRB- 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX h_NN
ain_NN ._.
Since_IN semi-supervised_JJ learning_NN -LRB-_-LRB- transductive_JJ learning_NN -RRB-_-RRB- is_VBZ closely_RB related_JJ to_TO the_DT problem_NN ,_, we_PRP compare_VBP the_DT proposed_VBN method_NN with_IN Transductive_JJ Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- TSVM_NNP -RRB-_-RRB- implemented_VBN in_IN SVM_NN light_NN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Furthermore_RB ,_, in_IN the_DT proposed_VBN framework_NN ,_, the_DT two_CD main_JJ steps_NNS are_VBP ,_, predicting_VBG labels_NNS using_VBG weighted_JJ classifiers_NNS if_IN the_DT classifiers_NNS are_VBP sufficiently_RB accurate_JJ in_IN terms_NNS of_IN alignment_NN with_IN clustering_NN struct_NN
ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_VBZ =_JJ -_: =[_NN 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN p_NN
ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_VBZ =_JJ -_: =[_NN 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN p_NN
mble_JJ methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN problem_NN ._.
In_IN =_JJ -_: =[_NN 11_CD ,_, 26_CD -RRB-_-RRB- -_: =_JJ -_: ,_, it_PRP is_VBZ assumed_VBN that_IN the_DT training_NN and_CC the_DT test_NN examples_NNS are_VBP generated_VBN from_IN a_DT mixture_NN of_IN different_JJ models_NNS ,_, and_CC the_DT test_NN distribution_NN has_VBZ different_JJ mixture_NN coefficients_NNS than_IN the_DT training_NN distribution_NN ._.
I_PRP
x_NN -RRB-_-RRB- and_CC multiple_JJ output_NN variables_NNS ,_, so_IN the_DT basic_JJ setting_NN is_VBZ different_JJ from_IN our_PRP$ problem_NN ._.
The_DT ``_`` clustering_NN ''_'' assumption_NN in_IN our_PRP$ work_NN is_VBZ exploited_VBN in_IN some_DT transfer_NN learning_NN and_CC semi-supervised_JJ learning_NN works_VBZ =_JJ -_: =[_NN 9_CD ,_, 28_CD -RRB-_-RRB- -_: =_JJ -_: ,_, where_WRB clustering_NN structure_NN is_VBZ utilized_VBN in_IN smoothing_VBG predictions_NNS among_IN neighbors_NNS ._.
Our_PRP$ paper_NN differs_VBZ from_IN these_DT papers_NNS by_IN utilizing_VBG the_DT assumption_NN in_IN weighting_NN different_JJ models_NNS locally_RB to_TO combine_VB all_DT
rithms_NNS are_VBP appropriate_JJ choices_NNS :_: 1_LS -RRB-_-RRB- Winnow_NN -LRB-_-LRB- WNN_NN -RRB-_-RRB- from_IN learning_VBG package_NN SNoW_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, 2_LS -RRB-_-RRB- Logistic_JJ Regression_NN -LRB-_-LRB- LR_NN -RRB-_-RRB- implemented_VBN in_IN BBR_NN package_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: and_CC 3_LS -RRB-_-RRB- Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- implemented_VBN in_IN LibSVM_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
When_WRB we_PRP only_RB have_VBP a_DT single_JJ source_NN domain_NN in_IN the_DT training_NN ,_, three_CD single_JJ classifiers_NNS are_VBP trained_VBN using_VBG the_DT above_JJ learning_NN algorithms_NNS and_CC combined_VBN according_VBG to_TO the_DT proposed_JJ weighted_JJ ensemble_NN framework_NN ._.
iminative_JJ models_NNS ,_, and_CC 2_LS -RRB-_-RRB- the_DT method_NN does_VBZ not_RB depend_VB on_IN specific_JJ applications_NNS and_CC makes_VBZ no_DT assumption_NN about_IN the_DT form_NN of_IN distributions_NNS generating_VBG the_DT training_NN or_CC the_DT test_NN data_NNS ._.
Multi-task_JJ learning_NN -LRB-_-LRB- MTL_NN -RRB-_-RRB- =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT learns_VBZ several_JJ related_JJ tasks_NNS at_IN the_DT same_JJ time_NN with_IN a_DT shared_JJ representation_NN ,_, considers_VBZ single_JJ P_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC multiple_JJ output_NN variables_NNS ,_, so_IN the_DT basic_JJ setting_NN is_VBZ different_JJ from_IN our_PRP$ problem_NN ._.
The_DT ``_`` clust_NN
it_PRP is_VBZ assumed_VBN that_IN the_DT training_NN and_CC the_DT test_NN examples_NNS are_VBP generated_VBN from_IN a_DT mixture_NN of_IN different_JJ models_NNS ,_, and_CC the_DT test_NN distribution_NN has_VBZ different_JJ mixture_NN coefficients_NNS than_IN the_DT training_NN distribution_NN ._.
In_IN =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT Dirichlet_NNP Process_VB prior_JJ is_VBZ used_VBN to_TO couple_VB the_DT parameters_NNS of_IN several_JJ models_NNS from_IN the_DT same_JJ parameterized_JJ family_NN of_IN dis-tributions_NNS ._.
-LRB-_-LRB- 10_CD -RRB-_-RRB- extends_VBZ the_DT boosting_VBG method_NN to_TO perform_VB transfer_NN learning_NN ._.
Ben_NNP
data_NNS sets_NNS are_VBP high-dimensional_JJ ,_, the_DT following_NN commonly_RB used_VBN algorithms_NNS are_VBP appropriate_JJ choices_NNS :_: 1_LS -RRB-_-RRB- Winnow_NN -LRB-_-LRB- WNN_NN -RRB-_-RRB- from_IN learning_VBG package_NN SNoW_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, 2_LS -RRB-_-RRB- Logistic_JJ Regression_NN -LRB-_-LRB- LR_NN -RRB-_-RRB- implemented_VBN in_IN BBR_NN package_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: ;_: and_CC 3_LS -RRB-_-RRB- Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- implemented_VBN in_IN LibSVM_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
When_WRB we_PRP only_RB have_VBP a_DT single_JJ source_NN domain_NN in_IN the_DT training_NN ,_, three_CD single_JJ classifiers_NNS are_VBP trained_VBN using_VBG the_DT above_JJ learning_NN algorithms_NNS and_CC
is_VBZ assumed_VBN that_IN the_DT two_CD distributions_NNS differ_VBP only_RB in_IN P_NN -LRB-_-LRB- x_NN -RRB-_-RRB- but_CC not_RB in_IN P_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- ,_, the_DT problem_NN is_VBZ referred_VBN to_TO as_IN covariate_NN shift_NN -LRB-_-LRB- 25_CD ,_, 18_CD -RRB-_-RRB- or_CC sample_NN selection_NN bias_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
The_DT instance_NN weighting_NN approaches_VBZ =_JJ -_: =[_NN 25_CD ,_, 18_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: try_VB to_TO re-weight_VB each_DT training_NN example_NN with_IN Ptest_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC maximize_VB the_DT re-weighted_JJ log_NN likelihood_NN ._.
Ptrain_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN of_IN work_NN tries_VBZ to_TO change_VB the_DT representation_NN of_IN the_DT observation_NN x_NN hoping_VBG that_IN th_DT
ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_VBZ =_JJ -_: =[_NN 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN p_NN
ngle_JJ source_NN of_IN information_NN and_CC try_VB to_TO learn_VB a_DT global_JJ single_JJ model_NN that_WDT adapts_VBZ well_RB to_TO the_DT test_NN set_NN ._.
Constructing_VBG a_DT good_JJ ensemble_NN of_IN classifiers_NNS has_VBZ been_VBN an_DT active_JJ research_NN area_NN in_IN supervised_JJ learning_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
By_IN combining_VBG decisions_NNS from_IN individual_JJ classifiers_NNS ,_, ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, baggi_NN
mble_JJ methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN problem_NN ._.
In_IN =_JJ -_: =[_NN 11_CD ,_, 26_CD -RRB-_-RRB- -_: =_JJ -_: ,_, it_PRP is_VBZ assumed_VBN that_IN the_DT training_NN and_CC the_DT test_NN examples_NNS are_VBP generated_VBN from_IN a_DT mixture_NN of_IN different_JJ models_NNS ,_, and_CC the_DT test_NN distribution_NN has_VBZ different_JJ mixture_NN coefficients_NNS than_IN the_DT training_NN distribution_NN ._.
I_PRP
d_NN test_NN distributions_NNS started_VBD gaining_VBG much_JJ attention_NN very_RB recently_RB ._.
When_WRB it_PRP is_VBZ assumed_VBN that_IN the_DT two_CD distributions_NNS differ_VBP only_RB in_IN P_NN -LRB-_-LRB- x_NN -RRB-_-RRB- but_CC not_RB in_IN P_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- ,_, the_DT problem_NN is_VBZ referred_VBN to_TO as_RB covariate_VB shift_NN =_JJ -_: =[_NN 25_CD ,_, 18_CD -RRB-_-RRB- -_: =_JJ -_: or_CC sample_NN selection_NN bias_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
The_DT instance_NN weighting_NN approaches_NNS -LRB-_-LRB- 25_CD ,_, 18_CD ,_, 5_CD -RRB-_-RRB- try_VBP to_TO re-weight_VB each_DT training_NN example_NN with_IN Ptest_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC maximize_VB the_DT re-weighted_JJ log_NN likelihood_NN ._.
Ptrain_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN o_NN
n_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN of_IN work_NN tries_VBZ to_TO change_VB the_DT representation_NN of_IN the_DT observation_NN x_NN hoping_VBG that_IN the_DT distributions_NNS of_IN the_DT training_NN and_CC the_DT test_NN examples_NNS will_MD become_VB very_RB similar_JJ after_IN the_DT transformation_NN =_JJ -_: =[_NN 3_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
-LRB-_-LRB- 22_CD -RRB-_-RRB- transforms_VBZ the_DT model_NN learned_VBD from_IN the_DT training_NN examples_NNS into_IN a_DT Bayesian_NNP prior_RB to_TO be_VB applied_VBN to_TO the_DT learning_NN process_NN on_IN the_DT test_NN domain_NN ._.
The_DT major_JJ difference_NN between_IN our_PRP$ work_NN and_CC these_DT studies_NNS i_LS
ng_NN much_JJ attention_NN very_RB recently_RB ._.
When_WRB it_PRP is_VBZ assumed_VBN that_IN the_DT two_CD distributions_NNS differ_VBP only_RB in_IN P_NN -LRB-_-LRB- x_NN -RRB-_-RRB- but_CC not_RB in_IN P_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- ,_, the_DT problem_NN is_VBZ referred_VBN to_TO as_IN covariate_NN shift_NN -LRB-_-LRB- 25_CD ,_, 18_CD -RRB-_-RRB- or_CC sample_NN selection_NN bias_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT instance_NN weighting_NN approaches_NNS -LRB-_-LRB- 25_CD ,_, 18_CD ,_, 5_CD -RRB-_-RRB- try_VBP to_TO re-weight_VB each_DT training_NN example_NN with_IN Ptest_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC maximize_VB the_DT re-weighted_JJ log_NN likelihood_NN ._.
Ptrain_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN of_IN work_NN tries_VBZ to_TO change_VB the_DT rep_NN
n_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN of_IN work_NN tries_VBZ to_TO change_VB the_DT representation_NN of_IN the_DT observation_NN x_NN hoping_VBG that_IN the_DT distributions_NNS of_IN the_DT training_NN and_CC the_DT test_NN examples_NNS will_MD become_VB very_RB similar_JJ after_IN the_DT transformation_NN =_JJ -_: =[_NN 3_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
-LRB-_-LRB- 22_CD -RRB-_-RRB- transforms_VBZ the_DT model_NN learned_VBD from_IN the_DT training_NN examples_NNS into_IN a_DT Bayesian_NNP prior_RB to_TO be_VB applied_VBN to_TO the_DT learning_NN process_NN on_IN the_DT test_NN domain_NN ._.
The_DT major_JJ difference_NN between_IN our_PRP$ work_NN and_CC these_DT studies_NNS i_LS
her_PRP$ line_NN of_IN work_NN tries_VBZ to_TO change_VB the_DT representation_NN of_IN the_DT observation_NN x_NN hoping_VBG that_IN the_DT distributions_NNS of_IN the_DT training_NN and_CC the_DT test_NN examples_NNS will_MD become_VB very_RB similar_JJ after_IN the_DT transformation_NN -LRB-_-LRB- 3_CD ,_, 24_CD -RRB-_-RRB- ._.
=_SYM -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: transforms_VBZ the_DT model_NN learned_VBD from_IN the_DT training_NN examples_NNS into_IN a_DT Bayesian_NNP prior_RB to_TO be_VB applied_VBN to_TO the_DT learning_NN process_NN on_IN the_DT test_NN domain_NN ._.
The_DT major_JJ difference_NN between_IN our_PRP$ work_NN and_CC these_DT studies_NNS is_VBZ that_IN
x_NN where_WRB the_DT ij_NN entry_NN is_VBZ model_NN Mi_NNP 's_POS predicted_VBN P_NN -LRB-_-LRB- y_NN =_JJ j_FW |_FW x_NN ,_, Mi_NN -RRB-_-RRB- ,_, i.e._FW ,_, h_NN i_FW j._FW Then_RB the_DT output_NN of_IN the_DT model_NN averaging_NN framework_NN for_IN x_NN is_VBZ a_DT vector_NN h_NN e_SYM =_JJ Hw_NN ._.
Note_VB that_DT w_NN satisfies_VBZ the_DT constraints_NNS that_WDT wi_VBP ∈_NN =_JJ -_: =[_NN 0_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: and_CC ∑_CD k_NN wi_NN =_JJ 1_CD ,_, and_CC thus_RB the_DT output_NN vector_NN hi_UH i_FW =_JJ 1_CD from_IN a_DT single_JJ model_NN Mi_NNP is_VBZ a_DT special_JJ case_NN of_IN h_NN e_SYM when_WRB wi_NN =_JJ 1_CD and_CC other_JJ weights_NNS are_VBP zero_CD ._.
But_CC we_PRP wish_VBP to_TO find_VB a_DT weight_NN vector_NN w_NN which_WDT minimizes_VBZ the_DT dist_NN
erent_JJ mixture_NN coefficients_NNS than_IN the_DT training_NN distribution_NN ._.
In_IN -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, a_DT Dirichlet_NNP Process_VB prior_JJ is_VBZ used_VBN to_TO couple_VB the_DT parameters_NNS of_IN several_JJ models_NNS from_IN the_DT same_JJ parameterized_JJ family_NN of_IN dis-tributions_NNS ._.
=_SYM -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: extends_VBZ the_DT boosting_VBG method_NN to_TO perform_VB transfer_NN learning_NN ._.
Bennett_NNP et_FW al._FW -LRB-_-LRB- 4_CD -RRB-_-RRB- proposed_VBD a_DT methodology_NN for_IN building_VBG a_DT meta-classifier_NN which_WDT combines_VBZ multiple_JJ distinct_JJ classifiers_NNS through_IN the_DT use_NN of_IN reli_NN
d_NN test_NN distributions_NNS started_VBD gaining_VBG much_JJ attention_NN very_RB recently_RB ._.
When_WRB it_PRP is_VBZ assumed_VBN that_IN the_DT two_CD distributions_NNS differ_VBP only_RB in_IN P_NN -LRB-_-LRB- x_NN -RRB-_-RRB- but_CC not_RB in_IN P_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- ,_, the_DT problem_NN is_VBZ referred_VBN to_TO as_RB covariate_VB shift_NN =_JJ -_: =[_NN 25_CD ,_, 18_CD -RRB-_-RRB- -_: =_JJ -_: or_CC sample_NN selection_NN bias_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
The_DT instance_NN weighting_NN approaches_NNS -LRB-_-LRB- 25_CD ,_, 18_CD ,_, 5_CD -RRB-_-RRB- try_VBP to_TO re-weight_VB each_DT training_NN example_NN with_IN Ptest_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC maximize_VB the_DT re-weighted_JJ log_NN likelihood_NN ._.
Ptrain_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN o_NN
ategy_NN is_VBZ to_TO split_VB the_DT sub-categories_NNS among_IN the_DT training_NN and_CC the_DT test_NN sets_VBZ so_RB that_IN the_DT distributions_NNS of_IN the_DT two_CD sets_NNS are_VBP similar_JJ but_CC not_RB exactly_RB the_DT same_JJ ._.
The_DT tasks_NNS are_VBP generated_VBN in_IN the_DT same_JJ way_NN as_IN in_IN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: and_CC more_JJR details_NNS can_MD be_VB found_VBN there_RB ._.
Intrusion_NN detection_NN ._.
The_DT KDD_NNP cup_NN ’99_CD data_NNS set_NN consists_VBZ of_IN a_DT series_NN of_IN TCP_NNP connection_NN records_NNS for_IN a_DT local_JJ area_NN network_NN ._.
Each_DT example_NN in_IN the_DT data_NNS set_NN corresponds_VBZ to_TO
acy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_NNS -LRB-_-LRB- 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB =_JJ -_: =[_NN 1_CD ,_, 19_CD -RRB-_-RRB- -_: =_JJ -_: ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN problem_NN ._.
In_IN -LRB-_-LRB- 11_CD ,_, 26_CD -RRB-_-RRB- ,_, it_PRP is_VBZ assumed_VBN that_IN the_DT training_NN a_DT
ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_VBZ =_JJ -_: =[_NN 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN p_NN
thods_NNS where_WRB the_DT model_NN weights_NNS are_VBP set_VBN the_DT same_JJ for_IN all_PDT the_DT test_NN examples_NNS ._.
Suppose_VB there_EX are_VBP k_NN models_NNS ,_, then_RB each_DT model_NN will_MD have_VB a_DT weight_NN 1_CD at_IN every_DT test_NN k_NN example_NN ._.
We_PRP use_VBP the_DT clustering_NN package_NN CLUTO_NN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT is_VBZ designed_VBN for_IN high-dimensional_JJ data_NNS clustering_NN ,_, to_TO cluster_VB the_DT test_NN set_NN ._.
Again_RB ,_, other_JJ clustering_NN algorithms_NNS could_MD be_VB used_VBN as_RB long_RB as_IN the_DT ``_`` clustering_NN ''_'' assumption_NN is_VBZ satisfied_VBN ._.
We_PRP compare_VBP with_IN
