A_DT scalable_JJ two-stage_JJ approach_NN for_IN a_DT class_NN of_IN dimensionality_NN reduction_NN techniques_NNS
Dimensionality_NN reduction_NN plays_VBZ an_DT important_JJ role_NN in_IN many_JJ data_NNS mining_NN applications_NNS involving_VBG high-dimensional_JJ data_NNS ._.
Many_JJ existing_VBG dimensionality_NN reduction_NN techniques_NNS can_MD be_VB formulated_VBN as_IN a_DT generalized_JJ eigenvalue_NN problem_NN ,_, which_WDT does_VBZ not_RB scale_VB to_TO large-size_JJ problems_NNS ._.
Prior_JJ work_NN transforms_VBZ the_DT generalized_JJ eigenvalue_NN problem_NN into_IN an_DT equivalent_JJ least_JJS squares_NNS formulation_NN ,_, which_WDT can_MD then_RB be_VB solved_VBN efficiently_RB ._.
However_RB ,_, the_DT equivalence_JJ relationship_NN only_RB holds_VBZ under_IN certain_JJ assumptions_NNS without_IN regularization_NN ,_, which_WDT severely_RB limits_VBZ their_PRP$ applicability_NN in_IN practice_NN ._.
In_IN this_DT paper_NN ,_, an_DT efficient_JJ two-stage_JJ approach_NN is_VBZ proposed_VBN to_TO solve_VB a_DT class_NN of_IN dimensionality_NN reduction_NN techniques_NNS ,_, including_VBG Canonical_JJ Correlation_NN Analysis_NN ,_, Orthonormal_JJ Partial_JJ Least_NN Squares_NNS ,_, linear_JJ Discriminant_NNP Analysis_NNP ,_, and_CC Hypergraph_NNP Spectral_NNP Learning_NNP ._.
The_DT proposed_VBN two-stage_JJ approach_NN scales_NNS linearly_RB in_IN terms_NNS of_IN both_CC the_DT sample_NN size_NN and_CC data_NN dimensionality_NN ._.
The_DT main_JJ contributions_NNS of_IN this_DT paper_NN include_VBP -LRB-_-LRB- 1_LS -RRB-_-RRB- we_PRP rigorously_RB establish_VB the_DT equivalence_JJ relationship_NN between_IN the_DT proposed_VBN two-stage_JJ approach_NN and_CC the_DT original_JJ formulation_NN without_IN any_DT assumption_NN ;_: and_CC -LRB-_-LRB- 2_LS -RRB-_-RRB- we_PRP show_VBP that_IN the_DT equivalence_JJ relationship_NN still_RB holds_VBZ in_IN the_DT regularization_NN setting_NN ._.
We_PRP have_VBP conducted_VBN extensive_JJ experiments_NNS using_VBG both_CC synthetic_JJ and_CC real-world_JJ data_NNS sets_NNS ._.
Our_PRP$ experimental_JJ results_NNS confirm_VBP the_DT equivalence_JJ relationship_NN established_VBN in_IN this_DT paper_NN ._.
Results_NNS also_RB demonstrate_VBP the_DT scalability_NN of_IN the_DT proposed_VBN two-stage_JJ approach_NN ._.
in_IN general_JJ computationally_RB expensive_JJ to_TO solve_VB and_CC hence_RB may_MD not_RB scale_VB to_TO large-size_JJ problems_NNS ._.
There_EX have_VBP been_VBN several_JJ recent_JJ attempts_NNS to_TO improve_VB the_DT scalability_NN of_IN dimensionality_NN reduction_NN techniques_NNS =_JJ -_: =[_NN 8_CD ,_, 9_CD ,_, 16_CD ,_, 24_CD ,_, 25_CD ,_, 26_CD ,_, 32_CD ,_, 33_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT key_JJ idea_NN is_VBZ to_TO transform_VB the_DT generalized_JJ eigenvalue_NN problem_NN into_IN an_DT equivalent_JJ least_JJS squares_NNS formulation_NN ,_, which_WDT can_MD be_VB solved_VBN efficiently_RB using_VBG existing_VBG algorithms_NNS such_JJ as_IN the_DT iterative_JJ conjugat_NN
DUCTION_NN Recent_JJ technological_JJ innovations_NNS have_VBP allowed_VBN us_PRP to_TO collect_VB massive_JJ amounts_NNS of_IN data_NNS with_IN a_DT large_JJ number_NN of_IN features_NNS ._.
One_CD of_IN the_DT key_JJ issues_NNS in_IN such_JJ data_NNS analysis_NN is_VBZ the_DT curse_NN of_IN dimensionality_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, i.e._FW ,_, an_DT enormous_JJ number_NN of_IN samples_NNS is_VBZ required_VBN to_TO perform_VB accurate_JJ prediction_NN on_IN problems_NNS with_IN large_JJ numbers_NNS of_IN features_NNS ._.
Dimensionality_NN reduction_NN ,_, which_WDT extracts_VBZ a_DT small_JJ number_NN of_IN features_NNS by_IN remo_NN
d_NN two_CD benchmark_JJ data_NNS sets_NNS in_IN multi-label_JJ classification_NN -LRB-_-LRB- 7_CD ,_, 10_CD -RRB-_-RRB- are_VBP used_VBN in_IN our_PRP$ experiments_NNS ._.
To_TO investigate_VB the_DT scalability_NN of_IN the_DT two-stage_JJ approach_NN ,_, two_CD large-scale_JJ data_NNS sets_NNS news20_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- and_CC rcv1v2_NN =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP used_VBN ._.
The_DT statistics_NNS of_IN all_DT data_NNS sets_NNS are_VBP summarized_VBN in_IN Table_NNP 1_CD ._.
To_TO distinguish_VB different_JJ techniques_NNS tested_VBN in_IN the_DT experiments_NNS ,_, we_PRP name_VBP the_DT regularized_VBN techniques_NNS using_VBG a_DT prefix_NN ``_`` r_NN ''_'' before_IN the_DT co_NN
ormation_NN is_VBZ encoded_VBN in_IN the_DT matrix_NN Y_NN =_JJ -LRB-_-LRB- y1_NN ,_, y2_NN ,_, ·_FW ·_FW ·_NN ,_, yn_NN -RRB-_-RRB- ∈_NN R_NN k_NN ×_CD n_NN ,_, whereyi_NN -LRB-_-LRB- j_NN -RRB-_-RRB- =_JJ 1ifxi_NN belongs_VBZ to_TO class_NN j_NN and_CC yi_NN -LRB-_-LRB- j_NN -RRB-_-RRB- =_JJ 0otherwise_NN ._.
2.1_CD Canonical_JJ Correlation_NN Analysis_NN In_IN Canonical_JJ Correlation_NN Analysis_NN -LRB-_-LRB- CCA_NN -RRB-_-RRB- =_JJ -_: =[_NN 4_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, two_CD different_JJ representations_NNS ,_, X_NN and_CC Y_NN ,_, of_IN the_DT same_JJ set_NN of_IN objects_NNS are_VBP given_VBN ,_, and_CC a_DT projection_NN is_VBZ computed_VBN for_IN each_DT representation_NN such_JJ that_IN the_DT correlation_NN coefficient_NN w_NN ρ_NN =_JJ T_NN x_NN XY_NN T_NN wy_FW √_FW -LRB-_-LRB- wT_NN x_NN XXT_NN
noisy_JJ information_NN ,_, is_VBZ an_DT effective_JJ way_NN to_TO overcome_VB the_DT curse_NN of_IN dimensionality_NN ._.
Many_JJ dimensionality_NN reduction_NN algorithms_NNS have_VBP been_VBN proposed_VBN in_IN the_DT past_NN ,_, including_VBG Canonical_JJ Correlation_NN Analysis_NN -LRB-_-LRB- CCA_NN -RRB-_-RRB- =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Partial_JJ Least_NNP Squares_NNPS -LRB-_-LRB- PLS_NN -RRB-_-RRB- -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, Linear_JJ Discriminant_JJ Analysis_NN -LRB-_-LRB- LDA_NN -RRB-_-RRB- -LRB-_-LRB- 6_CD -RRB-_-RRB- and_CC Hypergraph_NNP Spectral_NNP Learning_NNP -LRB-_-LRB- HSL_NN -RRB-_-RRB- -LRB-_-LRB- 24_CD -RRB-_-RRB- ._.
A_DT common_JJ characteristic_NN of_IN these_DT algorithms_NNS is_VBZ that_IN they_PRP can_MD be_VB formulated_VBN as_IN a_DT
ralized_FW eigenvalue_FW problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 5_CD -RRB-_-RRB- ._.
4.1_CD The_DT Algorithm_NN To_TO handle_VB the_DT regularization_NN in_IN the_DT generalized_JJ eigenvalue_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, we_PRP solve_VBP a_DT penalized_VBN least_JJS squares_NNS problem_NN ,_, or_CC ridge_NN regression_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: in_IN the_DT first_JJ step_NN ._.
Note_VB that_IN the_DT ``_`` latent_JJ target_NN ''_'' is_VBZ the_DT same_JJ as_IN the_DT one_CD used_VBN in_IN Algorithm_NN 1_CD ;_: the_DT difference_NN is_VBZ the_DT regularization_NN term_NN included_VBN in_IN the_DT least_JJS squares_NNS problem_NN ._.
After_IN projecting_VBG the_DT data_NNS
aximizing_VBG the_DT between-class_JJ variance_NN after_IN the_DT linear_JJ projection_NN ._.
It_PRP has_VBZ been_VBN shown_VBN that_IN the_DT optimal_JJ linear_JJ projection_NN consists_VBZ of_IN the_DT top_JJ eigenvectors_NNS of_IN S_NN †_NN tSb_NN corresponding_VBG to_TO nonzero_JJ eigenvalues_NNS =_JJ -_: =[_NN 11_CD ,_, 31_CD -RRB-_-RRB- -_: =_JJ -_: ,_, where_WRB St_NNP is_VBZ the_DT total_JJ covariance_NN matrix_NN and_CC Sb_NN is_VBZ the_DT between-class_JJ covariance_NN matrix_NN ._.
The_DT matrices_NNS St_NNP and_CC Sb_NNP are_VBP defined_VBN as_IN follows_VBZ :_: St_NNP =_JJ 1_CD n_NN XXT_NN ,_, Sb_NN =_JJ 1_CD n_NN k_NN ∑_CD j_NN =_JJ 1_CD njc_NN -LRB-_-LRB- j_NN -RRB-_-RRB- c_NN -LRB-_-LRB- j_NN -RRB-_-RRB- T_NN ,_, -LRB-_-LRB- 12_CD -RRB-_-RRB- where_WRB c_NN -LRB-_-LRB- j_NN -RRB-_-RRB-
classes_NNS is_VBZ k_NN =_JJ 5_CD ,_, and_CC the_DT labels_NNS are_VBP generated_VBN uniformly_RB with_IN random_JJ ._.
Five_CD real-world_JJ data_NNS sets_NNS from_IN the_DT UCI_NNP machine_NN learning_NN repository_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- and_CC two_CD benchmark_JJ data_NNS sets_NNS in_IN multi-label_JJ classification_NN =_JJ -_: =[_NN 7_CD ,_, 10_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP used_VBN in_IN our_PRP$ experiments_NNS ._.
To_TO investigate_VB the_DT scalability_NN of_IN the_DT two-stage_JJ approach_NN ,_, two_CD large-scale_JJ data_NNS sets_NNS news20_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- and_CC rcv1v2_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- are_VBP used_VBN ._.
The_DT statistics_NNS of_IN all_DT data_NNS sets_NNS are_VBP summarized_VBN i_FW
m_NN in_IN Eq_NN ._.
-LRB-_-LRB- 1_LS -RRB-_-RRB- with_IN S_NN =_JJ Y_NN T_NN Y_NN and_CC H_NN =_JJ Y_NN T_NN ._.
2.3_CD Hypergraph_NNP Spectral_NNP Learning_NNP Hypergraph_NNP Spectral_NNP Learning_NNP -LRB-_-LRB- HSL_NN -RRB-_-RRB- -LRB-_-LRB- 24_CD -RRB-_-RRB- is_VBZ a_DT dimensionality_NN reduction_NN technique_NN for_IN multi-label_JJ classification_NN ._.
A_DT hypergraph_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ a_DT generalization_NN of_IN the_DT traditional_JJ graph_NN in_IN which_WDT the_DT edges_NNS -LRB-_-LRB- a.k.a._NN hyperedges_NNS -RRB-_-RRB- are_VBP arbitrary_JJ nonempty_JJ subsets_NNS of_IN the_DT vertex_NN set_NN ._.
HSL_NN employs_VBZ a_DT hypergraphAlgorithm_NN 1_CD The_DT Two-Stage_JJ Approach_NN witho_NN
rom_VB the_DT standard_JJ Gaussian_JJ distribution_NN N_NN -LRB-_-LRB- 0_CD ,_, 1_CD -RRB-_-RRB- ._.
The_DT number_NN of_IN classes_NNS is_VBZ k_NN =_JJ 5_CD ,_, and_CC the_DT labels_NNS are_VBP generated_VBN uniformly_RB with_IN random_JJ ._.
Five_CD real-world_JJ data_NNS sets_NNS from_IN the_DT UCI_NNP machine_NN learning_NN repository_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: and_CC two_CD benchmark_JJ data_NNS sets_NNS in_IN multi-label_JJ classification_NN -LRB-_-LRB- 7_CD ,_, 10_CD -RRB-_-RRB- are_VBP used_VBN in_IN our_PRP$ experiments_NNS ._.
To_TO investigate_VB the_DT scalability_NN of_IN the_DT two-stage_JJ approach_NN ,_, two_CD large-scale_JJ data_NNS sets_NNS news20_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- and_CC rcv1_NN
epository_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- and_CC two_CD benchmark_JJ data_NNS sets_NNS in_IN multi-label_JJ classification_NN -LRB-_-LRB- 7_CD ,_, 10_CD -RRB-_-RRB- are_VBP used_VBN in_IN our_PRP$ experiments_NNS ._.
To_TO investigate_VB the_DT scalability_NN of_IN the_DT two-stage_JJ approach_NN ,_, two_CD large-scale_JJ data_NNS sets_VBZ news20_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: and_CC rcv1v2_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- are_VBP used_VBN ._.
The_DT statistics_NNS of_IN all_DT data_NNS sets_NNS are_VBP summarized_VBN in_IN Table_NNP 1_CD ._.
To_TO distinguish_VB different_JJ techniques_NNS tested_VBN in_IN the_DT experiments_NNS ,_, we_PRP name_VBP the_DT regularized_VBN techniques_NNS using_VBG a_DT prefix_NN ``_``
classes_NNS is_VBZ k_NN =_JJ 5_CD ,_, and_CC the_DT labels_NNS are_VBP generated_VBN uniformly_RB with_IN random_JJ ._.
Five_CD real-world_JJ data_NNS sets_NNS from_IN the_DT UCI_NNP machine_NN learning_NN repository_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- and_CC two_CD benchmark_JJ data_NNS sets_NNS in_IN multi-label_JJ classification_NN =_JJ -_: =[_NN 7_CD ,_, 10_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP used_VBN in_IN our_PRP$ experiments_NNS ._.
To_TO investigate_VB the_DT scalability_NN of_IN the_DT two-stage_JJ approach_NN ,_, two_CD large-scale_JJ data_NNS sets_NNS news20_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- and_CC rcv1v2_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- are_VBP used_VBN ._.
The_DT statistics_NNS of_IN all_DT data_NNS sets_NNS are_VBP summarized_VBN i_FW
zed_VBN Partial_JJ Least_NNP Squares_NNPS Partial_JJ least_JJS squares_NNS -LRB-_-LRB- PLS_NN -RRB-_-RRB- -LRB-_-LRB- 29_CD -RRB-_-RRB- is_VBZ a_DT family_NN of_IN methods_NNS for_IN modeling_NN relations_NNS between_IN two_CD sets_NNS of_IN variables_NNS ._.
In_IN this_DT paper_NN ,_, the_DT Orthonormalized_JJ Partial_JJ Least_NN Squares_NNS -LRB-_-LRB- OPLS_NNS -RRB-_-RRB- =_JJ -_: =[_NN 30_CD ,_, 2_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT popular_JJ variant_NN of_IN PLS_NN ,_, is_VBZ studied_VBN ._.
In_IN contrast_NN to_TO CCA_NNP ,_, OPLS_NNP computes_VBZ orthogonal_JJ score_NN vectors_NNS by_IN maximizing_VBG the_DT covariance_NN between_IN X_NNP and_CC Y._NNP It_PRP solves_VBZ the_DT following_JJ generalized_JJ eigenvalue_NN problem_NN :_:
in_IN general_JJ computationally_RB expensive_JJ to_TO solve_VB and_CC hence_RB may_MD not_RB scale_VB to_TO large-size_JJ problems_NNS ._.
There_EX have_VBP been_VBN several_JJ recent_JJ attempts_NNS to_TO improve_VB the_DT scalability_NN of_IN dimensionality_NN reduction_NN techniques_NNS =_JJ -_: =[_NN 8_CD ,_, 9_CD ,_, 16_CD ,_, 24_CD ,_, 25_CD ,_, 26_CD ,_, 32_CD ,_, 33_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT key_JJ idea_NN is_VBZ to_TO transform_VB the_DT generalized_JJ eigenvalue_NN problem_NN into_IN an_DT equivalent_JJ least_JJS squares_NNS formulation_NN ,_, which_WDT can_MD be_VB solved_VBN efficiently_RB using_VBG existing_VBG algorithms_NNS such_JJ as_IN the_DT iterative_JJ conjugat_NN
in_IN general_JJ computationally_RB expensive_JJ to_TO solve_VB and_CC hence_RB may_MD not_RB scale_VB to_TO large-size_JJ problems_NNS ._.
There_EX have_VBP been_VBN several_JJ recent_JJ attempts_NNS to_TO improve_VB the_DT scalability_NN of_IN dimensionality_NN reduction_NN techniques_NNS =_JJ -_: =[_NN 8_CD ,_, 9_CD ,_, 16_CD ,_, 24_CD ,_, 25_CD ,_, 26_CD ,_, 32_CD ,_, 33_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT key_JJ idea_NN is_VBZ to_TO transform_VB the_DT generalized_JJ eigenvalue_NN problem_NN into_IN an_DT equivalent_JJ least_JJS squares_NNS formulation_NN ,_, which_WDT can_MD be_VB solved_VBN efficiently_RB using_VBG existing_VBG algorithms_NNS such_JJ as_IN the_DT iterative_JJ conjugat_NN
