A_DT scalable_JJ modular_JJ convex_NN solver_NN for_IN regularized_VBN risk_NN minimization_NN
A_DT wide_JJ variety_NN of_IN machine_NN learning_NN problems_NNS can_MD be_VB described_VBN as_IN minimizing_VBG a_DT regularized_VBN risk_NN functional_JJ ,_, with_IN different_JJ algorithms_NNS using_VBG different_JJ notions_NNS of_IN risk_NN and_CC different_JJ regularizers_NNS ._.
Examples_NNS include_VBP linear_JJ Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- ,_, Logistic_JJ Regression_NN ,_, Conditional_JJ Random_NNP Fields_NNP -LRB-_-LRB- CRFs_NNS -RRB-_-RRB- ,_, and_CC Lasso_NNP amongst_IN others_NNS ._.
This_DT paper_NN describes_VBZ the_DT theory_NN and_CC implementation_NN of_IN a_DT highly_RB scalable_JJ and_CC modular_JJ convex_NN solver_NN which_WDT solves_VBZ all_PDT these_DT estimation_NN problems_NNS ._.
It_PRP can_MD be_VB parallelized_VBN on_IN a_DT cluster_NN of_IN workstations_NNS ,_, allows_VBZ for_IN data-locality_NN ,_, and_CC can_MD deal_VB with_IN regularizers_NNS such_JJ as_IN l1_NN and_CC l2_NN penalties_NNS ._.
At_IN present_NN ,_, our_PRP$ solver_NN implements_VBZ 20_CD different_JJ estimation_NN problems_NNS ,_, can_MD be_VB easily_RB extended_VBN ,_, scales_NNS to_TO millions_NNS of_IN observations_NNS ,_, and_CC is_VBZ up_IN to_TO 10_CD times_NNS faster_RBR than_IN specialized_VBN solvers_NNS for_IN many_JJ applications_NNS ._.
The_DT open_JJ source_NN code_NN is_VBZ freely_RB available_JJ as_IN part_NN of_IN the_DT ELEFANT_NNP toolbox_NN ._.
sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- Quantile_JJ regression_NN -LRB-_-LRB- 27_CD -RRB-_-RRB- max_NN -LRB-_-LRB- τ_NN -LRB-_-LRB- f_FW −_FW y_FW -RRB-_-RRB- ,_, -LRB-_-LRB- 1_CD −_FW τ_FW -RRB-_-RRB- -LRB-_-LRB- y_FW −_FW f_FW -RRB-_-RRB- -RRB-_-RRB- τ_NN if_IN f_LS -RRB-_-RRB- y_NN and_CC τ_FW −_FW 1_CD otherwise_RB ɛ-insensitive_JJ -LRB-_-LRB- 41_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, |_FW f_FW −_FW y_FW |_FW −_FW ɛ_FW -RRB-_-RRB- 0_CD if_IN |_FW f_FW −_FW y_FW |_FW ≤_FW ɛ_NN and_CC sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- otherwise_RB 1_CD Huber_NNP 's_POS robust_JJ loss_NN =_JJ -_: =[_NN 31_CD -RRB-_-RRB- -_: =_SYM -_: 2_CD -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD if_IN |_FW f_FW −_FW y_FW |_FW -LRB-_-LRB- 1_CD ,_, else_RB |_CD f_FW −_FW y_FW |_FW −_FW 1_CD f_FW −_FW y_FW if_IN |_FW f_FW −_FW y_FW |_FW ≤_FW 1_CD ,_, else_JJ sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD Poisson_NN regression_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- exp_NN -LRB-_-LRB- f_LS -RRB-_-RRB- −_FW yf_FW exp_FW -LRB-_-LRB- f_LS -RRB-_-RRB- −_FW y_FW Table_NNP 2_CD :_: Vectorial_JJ loss_NN functions_NNS and_CC their_PRP$ derivatives_NNS ,_, depending_VBG o_NN
ression_NN ._.
Extensions_NNS of_IN these_DT loss_NN functions_NNS allow_VBP us_PRP to_TO handle_VB structure_NN in_IN the_DT output_NN space_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
Changing_VBG the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- to_TO the_DT sparsity_NN inducing_VBG ‖_FW w_FW ‖_NN 1_CD leads_VBZ to_TO Lasso-type_JJ estimation_NN algorithms_NNS =_JJ -_: =[_NN 30_CD ,_, 39_CD ,_, 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT kernel_NN trick_NN is_VBZ widely_RB used_VBN to_TO transform_VB many_JJ of_IN these_DT algorithms_NNS into_IN ones_NNS operating_VBG on_IN a_DT Reproducing_NNP Kernel_NNP Hilbert_NNP Space_NNP -LRB-_-LRB- RKHS_NNP -RRB-_-RRB- ._.
One_CD lifts_VBZ w_NN into_IN an_DT RKHS_NN and_CC replaces_VBZ all_DT inner_JJ product_NN comput_NN
arse_NN features_NNS -LRB-_-LRB- e.g._FW the_DT bag_NN of_IN words_NNS representation_NN of_IN a_DT document_NN -RRB-_-RRB- ._.
Second_JJ ,_, many_JJ kernels_NNS -LRB-_-LRB- e.g._FW kernels_NNS on_IN strings_NNS -LRB-_-LRB- 42_CD -RRB-_-RRB- -RRB-_-RRB- can_MD effectively_RB be_VB linearized_VBN ,_, and_CC third_JJ ,_, efficient_JJ factorization_NN methods_NNS -LRB-_-LRB- e.g._FW =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =--RRB-_NN can_MD be_VB used_VBN for_IN a_DT low_JJ rank_NN representation_NN of_IN the_DT kernel_NN matrix_NN thereby_RB effectively_RB rendering_VBG the_DT problem_NN linear_NN ._.
For_IN each_DT of_IN the_DT above_JJ estimation_NN problems_NNS specialized_VBD solversexist_NN ,_, and_CC the_DT common_JJ a_DT
which_WDT employ_VBP the_DT kernel_NN trick_NN -LRB-_-LRB- but_CC essentially_RB still_RB solve_VB -LRB-_-LRB- 1_LS -RRB-_-RRB- -RRB-_-RRB- include_VBP Support_NN Vector_NNP regression_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_JJ -_: ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponen_NN
ression_NN ._.
Extensions_NNS of_IN these_DT loss_NN functions_NNS allow_VBP us_PRP to_TO handle_VB structure_NN in_IN the_DT output_NN space_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
Changing_VBG the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- to_TO the_DT sparsity_NN inducing_VBG ‖_FW w_FW ‖_NN 1_CD leads_VBZ to_TO Lasso-type_JJ estimation_NN algorithms_NNS =_JJ -_: =[_NN 30_CD ,_, 39_CD ,_, 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT kernel_NN trick_NN is_VBZ widely_RB used_VBN to_TO transform_VB many_JJ of_IN these_DT algorithms_NNS into_IN ones_NNS operating_VBG on_IN a_DT Reproducing_NNP Kernel_NNP Hilbert_NNP Space_NNP -LRB-_-LRB- RKHS_NNP -RRB-_-RRB- ._.
One_CD lifts_VBZ w_NN into_IN an_DT RKHS_NN and_CC replaces_VBZ all_DT inner_JJ product_NN comput_NN
e_LS Support_NN Vector_NNP regression_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_JJ -_: ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_,
tleneck_NN ._.
4.2_CD Off-the-shelf_JJ Methods_NNS Since_IN our_PRP$ architecture_NN is_VBZ modular_JJ -LRB-_-LRB- see_VB figure_NN 2_CD -RRB-_-RRB- ,_, we_PRP show_VBP as_IN a_DT proof_NN of_IN concept_NN that_IN it_PRP can_MD deal_VB with_IN different_JJ types_NNS of_IN solvers_NNS ,_, such_JJ as_IN an_DT implementation_NN of_IN LBFGS_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: from_IN TAO_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ._.
There_EX are_VBP two_CD additional_JJ requirements_NNS :_: First_JJ ,_, weneed_JJ to_TO provide_VB a_DT subdifferential_NN and_CC value_NN of_IN the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- ._.
This_DT is_VBZ easily_RB achieved_VBN via_IN 1_CD ∂_CD w_NN ‖_NNP w_NNP ‖_NNP 2_CD 2_CD 2_CD =_JJ w_NN and_CC ∂_FW w_FW ‖_FW w_FW ‖_FW 1_CD ∋_CD sgn_NN w._NN
nd_IN ordinal_JJ regression_NN ,_, and_CC a_DT particular_JJ regularizer_NN Ω_NN ,_, namely_RB quadratic_JJ regularization_NN ,_, both_DT methods_NNS are_VBP equivalent_JJ ._.
The_DT advantage_NN in_IN our_PRP$ solver_NN is_VBZ the_DT use_NN of_IN efficient_JJ linear_JJ algebra_NN tools_NNS via_IN PETSc_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT modular_JJ structure_NN ,_, the_DT considerably_RB higher_JJR generality_NN in_IN both_CC loss_NN functions_NNS and_CC regularizers_NNS ,_, and_CC the_DT fact_NN that_IN data_NNS may_MD be_VB decentralized_VBN ._.
Moreover_RB ,_, our_PRP$ work_NN is_VBZ related_JJ to_TO -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, where_WRB MapReduce_NNP
deals_NNS with_IN the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- and_CC is_VBZ able_JJ to_TO query_VB the_DT loss_NN function_NN for_IN values_NNS of_IN Remp_NN -LRB-_-LRB- w_NN -RRB-_-RRB- and_CC ∂_FW wRemp_FW -LRB-_-LRB- w_NN -RRB-_-RRB- as_IN needed_VBN ._.
This_DT is_VBZ very_RB similar_JJ to_TO the_DT design_NN of_IN the_DT Toolkit_NNP for_IN Advanced_NNP Optimization_NNP -LRB-_-LRB- TAO_NNP -RRB-_-RRB- =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Depending_VBG on_IN the_DT type_NN of_IN loss_NN function_NN ,_, computing_VBG Remp_NNP can_MD be_VB very_RB costly_JJ ._.
This_DT is_VBZ particularly_RB true_JJ in_IN cases_NNS where_WRB l_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, w_NN -RRB-_-RRB- is_VBZ the_DT log-likelihood_NN of_IN an_DT intractable_JJ conditional_JJ random_JJ fields_NNS or_CC of_IN
-LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN =_JJ -_: =[_NN 38_CD ,_, 40_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN de_IN
d_FW f_FW otherwise_RB 2_CD Soft_JJ Margin_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 1_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Soft_JJ Margin_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 1_CD and_CC f_FW −_FW y_FW otherwise_RB 2_CD Exponential_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- −_CD y_NN exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- Logistic_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- -RRB-_-RRB- −_FW y_FW \/_: -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- yf_NN -RRB-_-RRB- -RRB-_-RRB- Novelty_NN -LRB-_-LRB- 32_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD f_LS -RRB-_-RRB- 0_CD if_IN f_FW ≥_FW 0_CD and_CC −_CD 1_CD otherwise_JJ Least_NNP mean_NN squares_NNS -LRB-_-LRB- 43_CD -RRB-_-RRB- -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD f_FW −_FW y_FW 1_CD 2_CD Least_NN absolute_JJ deviation_NN |_FW f_FW −_FW y_FW |_FW sgn_FW -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- Quantile_JJ regression_NN -LRB-_-LRB- 2_CD
ression_NN ._.
Extensions_NNS of_IN these_DT loss_NN functions_NNS allow_VBP us_PRP to_TO handle_VB structure_NN in_IN the_DT output_NN space_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
Changing_VBG the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- to_TO the_DT sparsity_NN inducing_VBG ‖_FW w_FW ‖_NN 1_CD leads_VBZ to_TO Lasso-type_JJ estimation_NN algorithms_NNS =_JJ -_: =[_NN 30_CD ,_, 39_CD ,_, 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT kernel_NN trick_NN is_VBZ widely_RB used_VBN to_TO transform_VB many_JJ of_IN these_DT algorithms_NNS into_IN ones_NNS operating_VBG on_IN a_DT Reproducing_NNP Kernel_NNP Hilbert_NNP Space_NNP -LRB-_-LRB- RKHS_NNP -RRB-_-RRB- ._.
One_CD lifts_VBZ w_NN into_IN an_DT RKHS_NN and_CC replaces_VBZ all_DT inner_JJ product_NN comput_NN
es_RB ,_, depending_VBG on_IN f_LS :_: =_JJ 〈_CD w_NN ,_, x_NN 〉_NN ,_, and_CC y._NN Loss_NN l_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Derivative_JJ l_NN ′_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Hinge_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 0_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_JJ Hinge_NN -LRB-_-LRB- 26_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 0_CD and_CC f_LS otherwise_RB 2_CD Soft_JJ Margin_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 1_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Soft_JJ Margin_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 1_CD and_CC f_FW −_FW y_FW otherwise_RB 2_CD Exponential_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- −_CD y_NN exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- Logistic_JJ -LRB-_-LRB- 13_CD -RRB-_-RRB- log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- -RRB-_-RRB- −_FW y_FW \/_: -LRB-_-LRB- 1_CD +_CC
ession_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS =_JJ -_: =[_NN 28_CD -RRB-_-RRB- -_: =_JJ -_: ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW -LRB-_-LRB- 9_CD
8_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW =_JJ -_: =[_NN 9_CD ,_, 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
These_DT algorithms_NNS construct_VBP the_DT Lagrange_NNP dual_JJ ,_, and_CC solve_VB for_IN the_DT Lagrange_NNP multipliers_NNS efficiently_RB ._.
Only_RB recently_RB ,_, research_NN focus_NN has_VBZ shifted_VBN back_RB to_TO solving_VBG -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT primal_JJ ,_, e.g._FW -LRB-_-LRB- 10_CD ,_, 25_CD ,_, 36_CD -RRB-_-RRB- ._.
This_DT
link_NN 2_CD ._.
The_DT time_NN reported_VBN for_IN the_DT experiments_NNS are_VBP the_DT CPU_NNP time_NN ._.
One_CD exception_NN is_VBZ for_IN parallel_NN experiments_NNS where_WRB we_PRP report_VBP the_DT CPU_NN and_CC network_NN communication_NN time_NN ._.
5.1_CD Datasets_NNPS We_PRP use_VBP the_DT datasets_NNS in_IN =_JJ -_: =[_NN 25_CD ,_, 36_CD -RRB-_-RRB- -_: =_SYM -_: for_IN classification_NN tasks_NNS ._.
For_IN regression_NN tasks_NNS ,_, we_PRP pick_VBP some_DT of_IN the_DT largest_JJS datasets_NNS in_IN Luís_NNP Torgo_NNP 's_POS website_NN 3_CD ._.
Since_IN some_DT of_IN the_DT regression_NN datasets_VBZ 2_CD http:\/\/nf.apac.edu.au\/facilities\/ac\/hardware.p_NN
ted_VBN ._.
xj_FW iTable_FW 1_CD :_: Scalar_JJ loss_NN functions_NNS and_CC their_PRP$ derivatives_NNS ,_, depending_VBG on_IN f_LS :_: =_JJ 〈_CD w_NN ,_, x_NN 〉_NN ,_, and_CC y._NN Loss_NN l_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Derivative_JJ l_NN ′_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Hinge_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 0_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Hinge_NN =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_JJ -_: max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 0_CD and_CC f_LS otherwise_RB 2_CD Soft_JJ Margin_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 1_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Soft_JJ Margin_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 1_CD and_CC f_FW −_FW y_FW otherwise_RB 2_CD Exponential_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ex_FW
a_DT matrix_NN of_IN the_DT dimensionality_NN of_IN the_DT number_NN of_IN classes_NNS ._.
Let_VB us_PRP discuss_VB the_DT following_JJ two_CD cases_NNS :_: Ontologies_NNS for_IN Structured_NNP Estimation_NNP :_: For_IN hierarchical_JJ labels_NNS ,_, e.g._FW whenever_WRB we_PRP deal_VBP with_IN an_DT ontology_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP can_MD use_VB a_DT decomposition_NN of_IN the_DT coefficient_NN vector_NN along_IN the_DT hierarchy_NN of_IN categories_NNS ._.
Let_VB d_NN denote_VB the_DT depth_NN of_IN the_DT hierarchy_NN tree_NN ,_, and_CC assume_VB that_IN each_DT leaf_NN of_IN this_DT tree_NN corresponds_VBZ to_TO a_DT label_NN ._.
W_NN
he_PRP kernel_NN trick_NN -LRB-_-LRB- but_CC essentially_RB still_RB solve_VB -LRB-_-LRB- 1_LS -RRB-_-RRB- -RRB-_-RRB- include_VBP Support_NN Vector_NNP regression_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS
8_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW =_JJ -_: =[_NN 9_CD ,_, 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
These_DT algorithms_NNS construct_VBP the_DT Lagrange_NNP dual_JJ ,_, and_CC solve_VB for_IN the_DT Lagrange_NNP multipliers_NNS efficiently_RB ._.
Only_RB recently_RB ,_, research_NN focus_NN has_VBZ shifted_VBN back_RB to_TO solving_VBG -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT primal_JJ ,_, e.g._FW -LRB-_-LRB- 10_CD ,_, 25_CD ,_, 36_CD -RRB-_-RRB- ._.
This_DT
large_JJ datasets_NNS -LRB-_-LRB- with_IN the_DT number_NN of_IN data_NNS points_NNS of_IN the_DT order_NN of_IN a_DT million_CD -RRB-_-RRB- and_CC very_RB sparse_JJ features_NNS -LRB-_-LRB- e.g._FW the_DT bag_NN of_IN words_NNS representation_NN of_IN a_DT document_NN -RRB-_-RRB- ._.
Second_JJ ,_, many_JJ kernels_NNS -LRB-_-LRB- e.g._FW kernels_NNS on_IN strings_NNS =_JJ -_: =[_NN 42_CD -RRB-_-RRB- -_: =--RRB-_NN can_MD effectively_RB be_VB linearized_VBN ,_, and_CC third_JJ ,_, efficient_JJ factorization_NN methods_NNS -LRB-_-LRB- e.g._FW -LRB-_-LRB- 18_CD -RRB-_-RRB- -RRB-_-RRB- can_MD be_VB used_VBN for_IN a_DT low_JJ rank_NN representation_NN of_IN the_DT kernel_NN matrix_NN thereby_RB effectively_RB rendering_VBG the_DT problem_NN linear_NN ._.
1_CD 2_CD ‖_CD w_NN ‖_NN 2_CD ._.
Then_RB the_DT bundle_NN method_NN produces_VBZ a_DT duality_NN gap_NN of_IN at_IN most_JJS ɛ_NN after_IN t_NN steps_NNS ,_, where_WRB t_NN ≤_NN log_NN 2_CD λRemp_NN -LRB-_-LRB- 0_CD -RRB-_-RRB- −_NN 2_CD log_NN 2_CD G_NN +_CC 8G2_NN λɛ_FW −_FW 4_CD ._.
-LRB-_-LRB- 13_CD -RRB-_-RRB- Note_VBP that_IN this_DT bound_VBN is_VBZ significantly_RB better_JJR than_IN that_DT of_IN =_JJ -_: =[_NN 40_CD ,_, 35_CD -RRB-_-RRB- -_: =_JJ -_: ,_, since_IN it_PRP only_RB depends_VBZ logarithmically_RB on_IN the_DT value_NN of_IN the_DT loss_NN and_CC offers_VBZ an_DT O_NN -LRB-_-LRB- 1_CD \/_: ɛ_NN -RRB-_-RRB- rate_NN of_IN convergence_NN rather_RB than_IN the_DT O_NN -LRB-_-LRB- 1_CD \/_: ɛ_NN 2_CD -RRB-_-RRB- rate_NN in_IN previous_JJ papers_NNS ._.
This_DT is_VBZ largely_RB due_JJ to_TO an_DT improved_JJ analysis_NN
if_IN yf_FW ≥_FW 1_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Soft_JJ Margin_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 1_CD and_CC f_FW −_FW y_FW otherwise_RB 2_CD Exponential_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- −_CD y_NN exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- Logistic_JJ -LRB-_-LRB- 13_CD -RRB-_-RRB- log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- -RRB-_-RRB- −_FW y_FW \/_: -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- yf_NN -RRB-_-RRB- -RRB-_-RRB- Novelty_NN =_JJ -_: =[_NN 32_CD -RRB-_-RRB- -_: =_JJ -_: max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD f_LS -RRB-_-RRB- 0_CD if_IN f_FW ≥_FW 0_CD and_CC −_CD 1_CD otherwise_JJ Least_NNP mean_NN squares_NNS -LRB-_-LRB- 43_CD -RRB-_-RRB- -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD f_FW −_FW y_FW 1_CD 2_CD Least_NN absolute_JJ deviation_NN |_FW f_FW −_FW y_FW |_FW sgn_FW -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- Quantile_JJ regression_NN -LRB-_-LRB- 27_CD -RRB-_-RRB- max_NN -LRB-_-LRB- τ_NN -LRB-_-LRB- f_FW −_FW y_FW -RRB-_-RRB- ,_, -LRB-_-LRB- 1_CD −_FW τ_FW -RRB-_-RRB- -LRB-_-LRB- y_FW −_FW f_FW -RRB-_-RRB- -RRB-_-RRB- τ_NN if_IN f_LS -RRB-_-RRB- y_NN and_CC
._.
Examples_NNS of_IN algorithms_NNS which_WDT employ_VBP the_DT kernel_NN trick_NN -LRB-_-LRB- but_CC essentially_RB still_RB solve_VB -LRB-_-LRB- 1_LS -RRB-_-RRB- -RRB-_-RRB- include_VBP Support_NN Vector_NNP regression_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN =_JJ -_: =[_NN 37_CD -RRB-_-RRB- -_: =_JJ -_: ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graph_NN
,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW -LRB-_-LRB- 9_CD ,_, 23_CD -RRB-_-RRB- ._.
These_DT algorithms_NNS construct_VBP the_DT Lagrange_NNP du_NNP
a_DT tools_NNS via_IN PETSc_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ,_, the_DT modular_JJ structure_NN ,_, the_DT considerably_RB higher_JJR generality_NN in_IN both_CC loss_NN functions_NNS and_CC regularizers_NNS ,_, and_CC the_DT fact_NN that_IN data_NNS may_MD be_VB decentralized_VBN ._.
Moreover_RB ,_, our_PRP$ work_NN is_VBZ related_JJ to_TO =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, where_WRB MapReduce_NNP is_VBZ used_VBN to_TO accelerate_VB machine_NN learning_NN on_IN parallel_JJ computers_NNS ._.
We_PRP use_VBP similar_JJ parallelization_NN techniques_NNS to_TO distribute_VB the_DT computation_NN of_IN values_NNS and_CC gradients_NNS of_IN the_DT empirical_JJ risk_NN Re_NNP
regularizer_NN but_CC changing_VBG the_DT loss_NN function_NN to_TO l_NN -LRB-_-LRB- xi_NN ,_, yi_NN ,_, w_NN -RRB-_-RRB- =_JJ log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_NN yi_FW 〈_FW w_NN ,_, xi_FW 〉_FW -RRB-_-RRB- -RRB-_-RRB- ,_, yields_VBZ logistic_JJ regression_NN ._.
Extensions_NNS of_IN these_DT loss_NN functions_NNS allow_VBP us_PRP to_TO handle_VB structure_NN in_IN the_DT output_NN space_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Changing_VBG the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- to_TO the_DT sparsity_NN inducing_VBG ‖_FW w_FW ‖_NN 1_CD leads_VBZ to_TO Lasso-type_JJ estimation_NN algorithms_NNS -LRB-_-LRB- 30_CD ,_, 39_CD ,_, 8_CD -RRB-_-RRB- ._.
The_DT kernel_NN trick_NN is_VBZ widely_RB used_VBN to_TO transform_VB many_JJ of_IN these_DT algorithms_NNS into_IN ones_NNS ope_VBP
gression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW -LRB-_-LRB- 9_CD ,_, 23_CD -RRB-_-RRB- ._.
These_DT algorithms_NNS
y_FW ∗_FW is_VBZ the_DT argmax_NN of_IN the_DT loss_NN Softmax_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- log_NN P_NN y_FW ′_FW hP_NN exp_NN -LRB-_-LRB- fy_FW ′_FW -RRB-_-RRB- −_FW fy_FW y_FW ′_FW ey_FW ′_FW exp_FW -LRB-_-LRB- f_FW ′_FW i_FW y_NN -RRB-_-RRB- \/_: P_NN y_NN ′_CD exp_NN -LRB-_-LRB- f_FW ′_FW y_NN -RRB-_-RRB- −_FW ey_FW Multivariate_JJ Regression_NN 1_CD 2_CD -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- ⊤_CD M_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- where_WRB M_NN ≽_NN 0_CD M_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- Document_NNP Ranking_NN =_JJ -_: =[_NN 29_CD -RRB-_-RRB- -_: =_SYM -_: show_VBP that_IN a_DT large_JJ number_NN of_IN ranking_JJ scores_NNS -LRB-_-LRB- normalized_VBN discounted_JJ cumulative_JJ gain_NN ,_, mean_VB reciprocal_JJ rank_NN ,_, expected_VBN rank_NN utility_NN ,_, etc._NN -RRB-_-RRB- can_MD be_VB optimized_VBN directly_RB by_IN minimizing_VBG the_DT following_JJ loss_NN :_: l_NN -LRB-_-LRB- X_NN ,_,
of_IN predicting_VBG binary_JJ valued_VBN labels_NNS y_FW ∈_FW -LCB-_-LRB- ±_NN 1_CD -RCB-_-RRB- ,_, we_PRP may_MD set_VB Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ 1_CD 2_CD ‖_CD w_NN ‖_NN 2_CD ,_, and_CC the_DT loss_NN l_NN -LRB-_-LRB- xi_NN ,_, yi_NN ,_, w_NN -RRB-_-RRB- to_TO be_VB the_DT hinge_NN loss_NN ,_, max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yi_IN 〈_CD w_NN ,_, xi_FW 〉_FW -RRB-_-RRB- ,_, which_WDT recovers_VBZ linear_JJ Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- =_JJ -_: =[_NN 25_CD ,_, 36_CD -RRB-_-RRB- -_: =_SYM -_: ._.
On_IN the_DT other_JJ hand_NN ,_, using_VBG the_DT same_JJ regularizer_NN but_CC changing_VBG the_DT loss_NN function_NN to_TO l_NN -LRB-_-LRB- xi_NN ,_, yi_NN ,_, w_NN -RRB-_-RRB- =_JJ log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_NN yi_FW 〈_FW w_NN ,_, xi_FW 〉_FW -RRB-_-RRB- -RRB-_-RRB- ,_, yields_VBZ logistic_JJ regression_NN ._.
Extensions_NNS of_IN these_DT loss_NN functions_NNS allow_VBP us_PRP to_TO h_NN
3_LS -RRB-_-RRB- log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- -RRB-_-RRB- −_FW y_FW \/_: -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- yf_NN -RRB-_-RRB- -RRB-_-RRB- Novelty_NN -LRB-_-LRB- 32_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD f_LS -RRB-_-RRB- 0_CD if_IN f_FW ≥_FW 0_CD and_CC −_CD 1_CD otherwise_JJ Least_NNP mean_NN squares_NNS -LRB-_-LRB- 43_CD -RRB-_-RRB- -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD f_FW −_FW y_FW 1_CD 2_CD Least_NN absolute_JJ deviation_NN |_FW f_FW −_FW y_FW |_FW sgn_FW -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- Quantile_JJ regression_NN =_JJ -_: =[_NN 27_CD -RRB-_-RRB- -_: =_JJ -_: max_NN -LRB-_-LRB- τ_NN -LRB-_-LRB- f_FW −_FW y_FW -RRB-_-RRB- ,_, -LRB-_-LRB- 1_CD −_FW τ_FW -RRB-_-RRB- -LRB-_-LRB- y_FW −_FW f_FW -RRB-_-RRB- -RRB-_-RRB- τ_NN if_IN f_LS -RRB-_-RRB- y_NN and_CC τ_FW −_FW 1_CD otherwise_RB ɛ-insensitive_JJ -LRB-_-LRB- 41_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, |_FW f_FW −_FW y_FW |_FW −_FW ɛ_FW -RRB-_-RRB- 0_CD if_IN |_FW f_FW −_FW y_FW |_FW ≤_FW ɛ_NN and_CC sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- otherwise_RB 1_CD Huber_NNP 's_POS robust_JJ loss_NN -LRB-_-LRB- 31_CD -RRB-_-RRB- 2_CD -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD if_IN |_FW f_FW −_FW y_FW |_FW -LRB-_-LRB- 1_CD ,_, els_NNS
d_NN gradients_NNS of_IN l_NN can_MD be_VB computed_VBN in_IN linear_JJ time_NN ,_, once_RB f_SYM is_VBZ sorted_VBN ._.
xj_FW iTable_FW 1_CD :_: Scalar_JJ loss_NN functions_NNS and_CC their_PRP$ derivatives_NNS ,_, depending_VBG on_IN f_LS :_: =_JJ 〈_CD w_NN ,_, x_NN 〉_NN ,_, and_CC y._NN Loss_NN l_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Derivative_JJ l_NN ′_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Hinge_NN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_JJ -_: max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 0_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_JJ Hinge_NN -LRB-_-LRB- 26_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 0_CD and_CC f_LS otherwise_RB 2_CD Soft_JJ Margin_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 1_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Soft_JJ Margin_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_,
3_LS -RRB-_-RRB- This_DT gives_VBZ rise_NN to_TO the_DT hope_NN that_IN if_IN we_PRP have_VBP a_DT set_VBN W_NN =_JJ -LCB-_-LRB- w1_NN ,_, ..._: ,_, wn_NN -RCB-_-RRB- of_IN locations_NNS where_WRB we_PRP compute_VBP such_PDT a_DT Taylor_NNP approximation_NN ,_, we_PRP should_MD be_VB able_JJ to_TO obtain_VB an_DT everimproving_JJ approximation_NN of_IN g_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
See_NNP Figure_NNP 2_CD for_IN an_DT illustration_NN ._.
Formally_RB ,_, we_PRP have_VBP g_NN -LRB-_-LRB- w_NN -RRB-_-RRB- ≥_FW max_FW -LRB-_-LRB- g_NN -LRB-_-LRB- ¯_CD w_NN -RRB-_-RRB- +_CC 〈_FW w_FW −_FW ¯_FW w_NN ,_, ∂_FW wg_FW -LRB-_-LRB- ¯_CD w_NN -RRB-_-RRB- 〉_NN -RRB-_-RRB- ,_, -LRB-_-LRB- 4_CD -RRB-_-RRB- ¯_FW w_FW ∈_NN W_NN which_WDT means_VBZ that_IN g_NN -LRB-_-LRB- w_NN -RRB-_-RRB- can_MD be_VB lower-bounded_JJ by_IN a_DT piecewise_JJ linear_JJ function_NN ._.
Moreover_RB ,_, the_DT appro_NN
ve_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, |_FW f_FW −_FW y_FW |_FW −_FW ɛ_FW -RRB-_-RRB- 0_CD if_IN |_FW f_FW −_FW y_FW |_FW ≤_FW ɛ_NN and_CC sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- otherwise_RB 1_CD Huber_NNP 's_POS robust_JJ loss_NN -LRB-_-LRB- 31_CD -RRB-_-RRB- 2_CD -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD if_IN |_FW f_FW −_FW y_FW |_FW -LRB-_-LRB- 1_CD ,_, else_RB |_CD f_FW −_FW y_FW |_FW −_FW 1_CD f_FW −_FW y_FW if_IN |_FW f_FW −_FW y_FW |_FW ≤_FW 1_CD ,_, else_JJ sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD Poisson_NN regression_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: exp_NN -LRB-_-LRB- f_LS -RRB-_-RRB- −_FW yf_FW exp_FW -LRB-_-LRB- f_LS -RRB-_-RRB- −_FW y_FW Table_NNP 2_CD :_: Vectorial_JJ loss_NN functions_NNS and_CC their_PRP$ derivatives_NNS ,_, depending_VBG on_IN the_DT vector_NN f_SYM :_: =_JJ W_NN x_NN and_CC on_IN y._NN Loss_NN Derivative_JJ Soft_JJ Margin_NN -LRB-_-LRB- 38_CD -RRB-_-RRB- maxy_FW ′_FW -LRB-_-LRB- fy_FW ′_FW −_FW fy_FW +_CC ∆_NN -LRB-_-LRB- y_NN ,_, y_FW ′_FW -RRB-_-RRB- -RRB-_-RRB- ey_FW ∗_FW −_FW ey_FW ,_, whe_NN
-LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN =_JJ -_: =[_NN 38_CD ,_, 40_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN de_IN
ormance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW -LRB-_-LRB- 9_CD ,_, 23_CD -RRB-_-RRB- ._.
These_DT algorithms_NNS construct_VBP the_DT Lagrange_NNP dual_JJ ,_, and_CC solve_VB for_IN the_DT Lagrange_NNP multi_NNS
essary_VB that_IN individual_JJ nodes_NNS share_VBP the_DT data_NNS ,_, since_IN all_DT communication_NN revolves_VBZ around_IN sharing_VBG only_JJ values_NNS and_CC gradients_NNS of_IN Remp_NN -LRB-_-LRB- w_NN -RRB-_-RRB- ._.
•_NNP This_NNP has_VBZ the_DT added_VBN benefit_NN of_IN preserving_VBG a_DT large_JJ degree_NN of_IN privacy_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: between_IN the_DT individual_JJ database_NN owners_NNS and_CC the_DT system_NN using_VBG the_DT solver_NN ._.
At_IN every_DT step_NN the_DT data_NNS owner_NN will_MD only_RB return_VB a_DT gradient_NN which_WDT is_VBZ the_DT linear_JJ combination_NN of_IN a_DT set_NN of_IN observations_NNS ._.
Assuming_VBG tha_NN
the_DT past_JJ n_NN gradients_NNS -LRB-_-LRB- n_NN is_VBZ user_NN defined_VBN -RRB-_-RRB- ._.
LBFGS_NN is_VBZ known_VBN to_TO perform_VB well_RB on_IN continuously_RB differentiable_JJ problems_NNS ,_, such_JJ as_IN logistic_JJ regression_NN ,_, least-meansquares_JJ problems_NNS ,_, or_CC conditional_JJ random_JJ fields_NNS =_JJ -_: =[_NN 34_CD -RRB-_-RRB- -_: =_SYM -_: ._.
But_CC ,_, if_IN the_DT functions_NNS are_VBP not_RB continuously_RB differentiable_JJ -LRB-_-LRB- e.g._FW ,_, the_DT hinge_NN loss_NN and_CC its_PRP$ variants_NNS -RRB-_-RRB- then_RB LBFGS_NN may_MD fail_VB ._.
Empirically_RB ,_, we_PRP observe_VBP that_IN LBFGS_NN does_VBZ converge_VB well_RB even_RB for_IN the_DT hinge_NN losses_NNS
dual_JJ ,_, e.g._FW -LRB-_-LRB- 9_CD ,_, 23_CD -RRB-_-RRB- ._.
These_DT algorithms_NNS construct_VBP the_DT Lagrange_NNP dual_JJ ,_, and_CC solve_VB for_IN the_DT Lagrange_NNP multipliers_NNS efficiently_RB ._.
Only_RB recently_RB ,_, research_NN focus_NN has_VBZ shifted_VBN back_RB to_TO solving_VBG -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT primal_JJ ,_, e.g._FW =_JJ -_: =[_NN 10_CD ,_, 25_CD ,_, 36_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT spurt_NN in_IN research_NN interest_NN is_VBZ due_JJ to_TO three_CD main_JJ reasons_NNS :_: First_JJ ,_, many_JJ interesting_JJ problems_NNS in_IN diverse_JJ areas_NNS such_JJ as_IN text_NN classification_NN ,_, word-sense_JJ disambiguation_NN ,_, and_CC drug_NN design_NN already_RB employ_VBP
