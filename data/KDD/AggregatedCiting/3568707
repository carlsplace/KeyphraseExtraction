Efficiently_RB handling_VBG feature_NN redundancy_NN in_IN high-dimensional_JJ data_NNS
High-dimensional_JJ data_NNS poses_VBZ a_DT severe_JJ challenge_NN for_IN data_NN mining_NN ._.
Feature_NN selection_NN is_VBZ a_DT frequently_RB used_VBN technique_NN in_IN pre-processing_JJ high-dimensional_JJ data_NNS for_IN successful_JJ data_NNS mining_NN ._.
Traditionally_RB ,_, feature_NN selection_NN is_VBZ focused_VBN on_IN removing_VBG irrelevant_JJ features_NNS ._.
However_RB ,_, for_IN high-dimensional_JJ data_NNS ,_, removing_VBG redundant_JJ features_NNS is_VBZ equally_RB critical_JJ ._.
In_IN this_DT paper_NN ,_, we_PRP provide_VBP a_DT study_NN of_IN feature_NN redundancy_NN in_IN high-dimensional_JJ data_NNS and_CC propose_VBP a_DT novel_JJ correlation-based_JJ approach_NN to_TO feature_VB selection_NN within_IN the_DT filter_NN model_NN ._.
The_DT extensive_JJ empirical_JJ study_NN using_VBG real-world_JJ data_NNS shows_VBZ that_IN the_DT proposed_VBN approach_NN is_VBZ efficient_JJ and_CC effective_JJ in_IN removing_VBG redundant_JJ and_CC irrelevant_JJ features_NNS ._.
asks_VBZ ,_, improving_VBG mining_NN performance_NN like_IN predictive_JJ accuracy_NN ,_, and_CC enhancing_VBG result_NN comprehensibility_NN -LRB-_-LRB- 3_CD ,_, 5_CD ,_, 9_CD -RRB-_-RRB- ._.
Feature_NN selection_NN algorithms_NNS can_MD broadly_RB fall_VB into_IN the_DT filter_NN model_NN or_CC the_DT wrapper_NN model_NN =_JJ -_: =[_NN 4_CD ,_, 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT filter_NN model_NN relies_VBZ on_IN general_JJ characteristics_NNS of_IN the_DT training_NN data_NNS to_TO select_VB some_DT features_NNS withPermission_NN to_TO make_VB digital_JJ or_CC hard_JJ copies_NNS of_IN all_DT or_CC part_NN of_IN this_DT work_NN for_IN personal_JJ or_CC classroom_NN us_PRP
evidence_NN from_IN feature_NN selection_NN literature_NN shows_VBZ that_IN ,_, along_IN with_IN irrelevant_JJ features_NNS ,_, redundant_JJ features_NNS also_RB affect_VBP the_DT speed_NN and_CC accuracy_NN of_IN mining_NN algorithms_NNS and_CC thus_RB should_MD be_VB eliminated_VBN as_RB well_RB =_JJ -_: =[_NN 7_CD ,_, 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Therefore_RB ,_, in_IN the_DT context_NN of_IN feature_NN selection_NN for_IN high_JJ dimensional_JJ data_NNS where_WRB there_EX may_MD exist_VB many_JJ redundant_JJ features_NNS ,_, pure_JJ relevance-based_JJ feature_NN weighting_NN algorithms_NNS do_VBP not_RB meet_VB the_DT need_NN of_IN feat_NN
has_VBZ been_VBN effective_JJ in_IN removing_VBG irrelevant_JJ and_CC redundant_JJ features_NNS ,_, increasing_VBG efficiency_NN in_IN mining_NN tasks_NNS ,_, improving_VBG mining_NN performance_NN like_IN predictive_JJ accuracy_NN ,_, and_CC enhancing_VBG result_NN comprehensibility_NN =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Feature_NN selection_NN algorithms_NNS can_MD broadly_RB fall_VB into_IN the_DT filter_NN model_NN or_CC the_DT wrapper_NN model_NN -LRB-_-LRB- 4_CD ,_, 9_CD -RRB-_-RRB- ._.
The_DT filter_NN model_NN relies_VBZ on_IN general_JJ characteristics_NNS of_IN the_DT training_NN data_NNS to_TO select_VB some_DT features_NNS withP_NN
ber_NN of_IN subsets_NNS evaluated_VBN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, but_CC experiments_NNS show_VBP that_IN in_IN order_NN to_TO obtain_VB near_IN optimal_JJ results_NNS the_DT required_JJ number_NN of_IN subsets_NNS for_IN evaluation_NN is_VBZ mostly_RB at_IN least_JJS quadratic_JJ to_TO the_DT number_NN of_IN features_NNS N_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Therefore_RB ,_, with_IN at_IN least_JJS quadratic_JJ complexity_NN in_IN terms_NNS of_IN dimensionality_NN ,_, subset_NN search_NN algorithms_NNS do_VBP not_RB have_VB strong_JJ scalability_NN to_TO deal_VB with_IN high_JJ dimensional_JJ data_NNS ._.
To_TO overcome_VB the_DT problems_NNS of_IN algo_NN
has_VBZ been_VBN effective_JJ in_IN removing_VBG irrelevant_JJ and_CC redundant_JJ features_NNS ,_, increasing_VBG efficiency_NN in_IN mining_NN tasks_NNS ,_, improving_VBG mining_NN performance_NN like_IN predictive_JJ accuracy_NN ,_, and_CC enhancing_VBG result_NN comprehensibility_NN =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Feature_NN selection_NN algorithms_NNS can_MD broadly_RB fall_VB into_IN the_DT filter_NN model_NN or_CC the_DT wrapper_NN model_NN -LRB-_-LRB- 4_CD ,_, 9_CD -RRB-_-RRB- ._.
The_DT filter_NN model_NN relies_VBZ on_IN general_JJ characteristics_NNS of_IN the_DT training_NN data_NNS to_TO select_VB some_DT features_NNS withP_NN
ted_VBN using_VBG Weka_NNP 's_POS implementation_NN of_IN all_PDT these_DT existing_VBG algorithms_NNS and_CC FCBF_NN is_VBZ also_RB implemented_VBN in_FW Weka_FW environment_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- ._.
All_DT together_RB 10_CD data_NNS sets_NNS are_VBP selected_VBN from_IN the_DT UCI_NNP Machine_NNP Learning_NNP Repository_NNP =_SYM -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: and_CC the_DT UCI_NNP KDD_NNP Archive_NNP -LRB-_-LRB- 1_LS -RRB-_-RRB- ._.
A_DT summary_NN of_IN data_NNS sets_NNS is_VBZ presented_VBN in_IN Table_NNP 1_CD ._.
For_IN each_DT data_NNS set_VBN ,_, we_PRP run_VBP FCBF_NN ,_, CFS-SF_NN ,_, and_CC ReliefF_NN to_TO obtain_VB a_DT set_NN of_IN selected_VBN features_NNS from_IN each_DT algorithm_NN ,_, and_CC record_NN t_NN
