Cut-and-stitch_JJ :_: efficient_JJ parallel_JJ learning_NN of_IN linear_JJ dynamical_JJ systems_NNS on_IN smps_NNS
Multi-core_JJ processors_NNS with_IN ever_RB increasing_VBG number_NN of_IN cores_NNS per_IN chip_NN are_VBP becoming_VBG prevalent_JJ in_IN modern_JJ parallel_JJ computing_NN ._.
Our_PRP$ goal_NN is_VBZ to_TO make_VB use_NN of_IN the_DT multi-core_NN as_RB well_RB as_IN multi-processor_JJ architectures_NNS to_TO speed_VB up_RP data_NNS mining_NN algorithms_NNS ._.
Specifically_RB ,_, we_PRP present_VBP a_DT parallel_JJ algorithm_NN for_IN approximate_JJ learning_NN of_IN Linear_NNP Dynamical_NNP Systems_NNPS -LRB-_-LRB- LDS_NN -RRB-_-RRB- ,_, also_RB known_VBN as_IN Kalman_NNP Filters_NNP -LRB-_-LRB- KF_NNP -RRB-_-RRB- ._.
LDSs_NNS are_VBP widely_RB used_VBN in_IN time_NN series_NN analysis_NN such_JJ as_IN motion_NN capture_NN modeling_NN ,_, visual_JJ tracking_NN etc._NN ._.
We_PRP propose_VBP Cut-And-Stitch_NNP -LRB-_-LRB- CAS_NNP -RRB-_-RRB- ,_, a_DT novel_JJ method_NN to_TO handle_VB the_DT data_NNS dependencies_NNS from_IN the_DT chain_NN structure_NN of_IN hidden_JJ variables_NNS in_IN LDS_NNP ,_, so_RB as_IN to_TO parallelize_VB the_DT EM-based_JJ parameter_NN learning_NN algorithm_NN ._.
We_PRP implement_VBP the_DT algorithm_NN using_VBG OpenMP_NN on_IN both_CC a_DT supercomputer_NN and_CC a_DT quad-core_JJ commercial_JJ desktop_NN ._.
The_DT experimental_JJ results_NNS show_VBP that_IN parallel_NN algorithms_NNS using_VBG Cut-And-Stitch_NNP achieve_VB comparable_JJ accuracy_NN and_CC almost_RB linear_JJ speedups_NNS over_IN the_DT serial_JJ version_NN ._.
In_IN addition_NN ,_, Cut-And-Stitch_NNP can_MD be_VB generalized_VBN to_TO other_JJ models_NNS with_IN similar_JJ linear_JJ structures_NNS such_JJ as_IN Hidden_NNP Markov_NNP Models_NNS -LRB-_-LRB- HMM_NNS -RRB-_-RRB- and_CC Switching_NN Kalman_NNP Filters_NNP -LRB-_-LRB- SKF_NNP -RRB-_-RRB- ._.
words_NNS :_: Linear_NNP Dynamical_NNP Systems_NNPS ;_: Kalman_NNP Filters_NNP ;_: OpenMP_NN ;_: Expectation_NN Maximization_NN -LRB-_-LRB- EM_NN -RRB-_-RRB- ;_: Optimization_NN ;_: Multi-core_NN ._.
1_CD ._.
INTRODUCTION_NNP Time_NNP series_NN appear_VBP in_IN numerous_JJ applications_NNS ,_, including_VBG motion_NN capture_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, visual_JJ tracking_NN ,_, speech_NN recognition_NN ,_, quantitative_JJ studies_NNS of_IN financial_JJ markets_NNS ,_, network_NN intrusion_NN detection_NN ,_, forecasting_NN ,_, etc._NN ._.
Mining_NN and_CC forecasting_NN are_VBP popular_JJ operations_NNS relevant_JJ to_TO time_NN series_NN a_DT
rning_VBG algorithms_NNS on_IN multicore_NN ?_.
There_EX are_VBP already_RB several_JJ papers_NNS on_IN distributed_VBN computation_NN for_IN data_NN mining_NN operations_NNS ._.
For_IN example_NN ,_, ``_`` cascade_NN SVMs_NNS ''_'' were_VBD proposed_VBN to_TO parallelize_VB Support_NN Vector_NNP Machines_NNP =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Other_JJ articles_NNS use_VBP Google_NNP 's_POS map-reduce_JJ techniques_NNS -LRB-_-LRB- 8_CD -RRB-_-RRB- on_IN multi-core_JJ machines_NNS to_TO design_VB efficient_JJ parallel_JJ learning_NN algorithms_NNS for_IN a_DT set_NN of_IN standard_JJ machine_NN learning_VBG algorithms\/models_NNS such_JJ as_IN na誰ve_JJ B_NN
easing_VBG interest_NN ._.
Parthasarathy_NNP et_FW al._FW -LRB-_-LRB- 2_CD -RRB-_-RRB- develop_VBP parallel_JJ algorithms_NNS for_IN mining_NN terabytes_NNS of_IN data_NNS for_IN frequent_JJ itemsets_NNS ,_, demonstrating_VBG a_DT near-linear_JJ scale-up_NN on_IN up_IN to_TO 48_CD nodes_NNS ._.
Reinhardt_NN and_CC Karypis_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: used_VBN OpenMP_NN to_TO parallelize_VB the_DT discovery_NN of_IN frequent_JJ patterns_NNS in_IN large_JJ graphs_NNS ,_, showing_VBG excellent_JJ speedup_NN of_IN up_IN to_TO 30_CD processors_NNS ._.
Cong_NNP et_FW al._FW -LRB-_-LRB- 7_CD -RRB-_-RRB- develop_VBP the_DT Par-CSP_JJ algorithm_NN that_WDT detects_VBZ closed_JJ sequ_NN
soon_RB we_PRP may_MD be_VB able_JJ to_TO build_VB machines_NNS with_IN even_RB a_DT thousand_CD cores_NNS ,_, e.g._FW an_DT energy_NN efficient_JJ ,_, 80-core_JJ chip_NN not_RB much_RB larger_JJR than_IN the_DT size_NN of_IN a_DT finger_NN nail_NN was_VBD released_VBN by_IN Intel_NNP researchers_NNS in_IN early_JJ 2007_CD =_SYM -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT paper_NN is_VBZ along_IN the_DT line_NN to_TO investigate_VB the_DT following_JJ question_NN :_: how_WRB much_JJ speed_NN up_RP could_MD we_PRP obtain_VB for_IN machine_NN learning_NN algorithms_NNS on_IN multicore_NN ?_.
There_EX are_VBP already_RB several_JJ papers_NNS on_IN distributed_VBN co_NN
uential_JJ patterns_NNS on_IN a_DT distributed_VBN memory_NN system_NN ,_, and_CC report_NN good_JJ scale-up_NN on_IN a_DT 64-node_JJ Linux_NNP cluster_NN ._.
Graf_NNP et_FW al._FW -LRB-_-LRB- 9_CD -RRB-_-RRB- developed_VBD a_DT parallel_JJ algorithm_NN to_TO learn_VB SVM_NN through_IN cascade_NN SVM_NN ._.
Collobert_NNP et_FW al._FW =_SYM -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT method_NN to_TO learn_VB a_DT mixture_NN of_IN SVM_NN in_IN parallel_NN ._.
Both_DT of_IN them_PRP adopted_VBD the_DT idea_NN of_IN splitting_JJ dataset_NN into_IN small_JJ subsets_NNS ,_, training_NN SVM_NN on_IN each_DT ,_, and_CC then_RB combining_VBG those_DT SVMs_NNS ._.
Chang_NNP et_FW al._FW -LRB-_-LRB- 3_LS -RRB-_-RRB- p_NN
l_NN papers_NNS on_IN distributed_VBN computation_NN for_IN data_NN mining_NN operations_NNS ._.
For_IN example_NN ,_, ``_`` cascade_NN SVMs_NNS ''_'' were_VBD proposed_VBN to_TO parallelize_VB Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
Other_JJ articles_NNS use_VBP Google_NNP 's_POS map-reduce_JJ techniques_NNS =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: on_IN multi-core_JJ machines_NNS to_TO design_VB efficient_JJ parallel_JJ learning_NN algorithms_NNS for_IN a_DT set_NN of_IN standard_JJ machine_NN learning_VBG algorithms\/models_NNS such_JJ as_IN na誰ve_JJ Bayes_NNP and_CC PCA_NNP ,_, achieving_VBG almost_RB linear_JJ speedup_NN -LRB-_-LRB- 4_CD ,_, 12_CD -RRB-_-RRB- ._.
m_NN for_IN LDS_NNS running_VBG on_IN a_DT multi-core_JJ computer_NN only_RB takes_VBZ up_RP a_DT single_JJ core_NN with_IN limited_JJ processing_NN power_NN ,_, and_CC the_DT current_JJ state-of-the-art_JJ dynamic_JJ parallelization_NN techniques_NNS such_JJ as_IN speculative_JJ execution_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: benefit_NN little_JJ to_TO the_DT straightforward_JJ EM_NN algorithm_NN due_JJ to_TO the_DT nontrivial_JJ data_NN dependencies_NNS in_IN LDS_NN ._.
As_IN the_DT number_NN of_IN cores_NNS on_IN a_DT single_JJ chip_NN keeps_VBZ increasing_VBG ,_, soon_RB we_PRP may_MD be_VB able_JJ to_TO build_VB machines_NNS with_IN
iques_NNS -LRB-_-LRB- 8_CD -RRB-_-RRB- on_IN multi-core_JJ machines_NNS to_TO design_VB efficient_JJ parallel_JJ learning_NN algorithms_NNS for_IN a_DT set_NN of_IN standard_JJ machine_NN learning_VBG algorithms\/models_NNS such_JJ as_IN na誰ve_JJ Bayes_NNP and_CC PCA_NNP ,_, achieving_VBG almost_RB linear_JJ speedup_NN =_JJ -_: =[_NN 4_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, these_DT methods_NNS do_VBP not_RB apply_VB to_TO HMM_NNP or_CC LDS_NNP directly_RB ._.
In_IN essence_NN ,_, their_PRP$ techniques_NNS are_VBP similar_JJ to_TO dot-product-like_JJ parallelism_NN ,_, by_IN using_VBG divide-andconquer_NN on_IN independent_JJ sub_NN models_NNS ;_: these_DT do_VBP n_NN
ear_NN scale-up_NN on_IN up_IN to_TO 48_CD nodes_NNS ._.
Reinhardt_NNP and_CC Karypis_NNP -LRB-_-LRB- 13_CD -RRB-_-RRB- used_VBD OpenMP_NN to_TO parallelize_VB the_DT discovery_NN of_IN frequent_JJ patterns_NNS in_IN large_JJ graphs_NNS ,_, showing_VBG excellent_JJ speedup_NN of_IN up_IN to_TO 30_CD processors_NNS ._.
Cong_NNP et_FW al._FW =_SYM -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: develop_VB the_DT Par-CSP_NN algorithm_NN that_WDT detects_VBZ closed_JJ sequential_JJ patterns_NNS on_IN a_DT distributed_VBN memory_NN system_NN ,_, and_CC report_NN good_JJ scale-up_NN on_IN a_DT 64-node_JJ Linux_NNP cluster_NN ._.
Graf_NNP et_FW al._FW -LRB-_-LRB- 9_CD -RRB-_-RRB- developed_VBD a_DT parallel_JJ algorit_NN
._.
-LRB-_-LRB- 5_CD -RRB-_-RRB- proposed_VBD a_DT method_NN to_TO learn_VB a_DT mixture_NN of_IN SVM_NN in_IN parallel_NN ._.
Both_DT of_IN them_PRP adopted_VBD the_DT idea_NN of_IN splitting_JJ dataset_NN into_IN small_JJ subsets_NNS ,_, training_NN SVM_NN on_IN each_DT ,_, and_CC then_RB combining_VBG those_DT SVMs_NNS ._.
Chang_NNP et_FW al._FW =_SYM -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD PSVM_NNP to_TO train_VB SVMs_NNS on_IN distributed_VBN computers_NNS through_IN approximate_JJ factorization_NN of_IN the_DT kernel_NN matrix_NN ._.
There_EX is_VBZ an_DT attempt_NN to_TO use_VB Google_NNP 's_POS Map-Reduce_NNP -LRB-_-LRB- 8_CD -RRB-_-RRB- to_TO parallelize_VB a_DT set_NN of_IN learning_VBG algori_NNS
dinates_NNS ._.
Note_VB our_PRP$ reconstruction_NN -LRB-_-LRB- red_JJ lines_NNS -RRB-_-RRB- is_VBZ very_RB close_JJ to_TO the_DT original_JJ signal_NN -LRB-_-LRB- blue_JJ lines_NNS -RRB-_-RRB- ._.
6_CD ._.
RELATED_NNS WORK_VBP Data_NN mining_NN and_CC parallel_JJ programming_NN receives_VBZ increasing_VBG interest_NN ._.
Parthasarathy_NNP et_FW al._FW =_SYM -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: develop_VB parallel_JJ algorithms_NNS for_IN mining_NN terabytes_NNS of_IN data_NNS for_IN frequent_JJ itemsets_NNS ,_, demonstrating_VBG a_DT near-linear_JJ scale-up_NN on_IN up_IN to_TO 48_CD nodes_NNS ._.
Reinhardt_NNP and_CC Karypis_NNP -LRB-_-LRB- 13_CD -RRB-_-RRB- used_VBD OpenMP_NN to_TO parallelize_VB the_DT disco_NN
iques_NNS -LRB-_-LRB- 8_CD -RRB-_-RRB- on_IN multi-core_JJ machines_NNS to_TO design_VB efficient_JJ parallel_JJ learning_NN algorithms_NNS for_IN a_DT set_NN of_IN standard_JJ machine_NN learning_VBG algorithms\/models_NNS such_JJ as_IN na誰ve_JJ Bayes_NNP and_CC PCA_NNP ,_, achieving_VBG almost_RB linear_JJ speedup_NN =_JJ -_: =[_NN 4_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, these_DT methods_NNS do_VBP not_RB apply_VB to_TO HMM_NNP or_CC LDS_NNP directly_RB ._.
In_IN essence_NN ,_, their_PRP$ techniques_NNS are_VBP similar_JJ to_TO dot-product-like_JJ parallelism_NN ,_, by_IN using_VBG divide-andconquer_NN on_IN independent_JJ sub_NN models_NNS ;_: these_DT do_VBP n_NN
