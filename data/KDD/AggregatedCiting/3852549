Mining_NN comparable_JJ bilingual_JJ text_NN corpora_NN for_IN cross-language_JJ information_NN integration_NN
Integrating_VBG information_NN in_IN multiple_JJ natural_JJ languages_NNS is_VBZ a_DT challenging_JJ task_NN that_WDT often_RB requires_VBZ manually_RB created_VBN linguistic_JJ resources_NNS such_JJ as_IN a_DT bilingual_JJ dictionary_NN or_CC examples_NNS of_IN direct_JJ translations_NNS of_IN text_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT general_JJ cross-lingual_JJ text_NN mining_NN method_NN that_WDT does_VBZ not_RB rely_VB on_IN any_DT of_IN these_DT resources_NNS ,_, but_CC can_MD exploit_VB comparable_JJ bilingual_JJ text_NN corpora_NN to_TO discover_VB mappings_NNS between_IN words_NNS and_CC documents_NNS in_IN different_JJ languages_NNS ._.
Comparable_JJ text_NN corpora_NN are_VBP collections_NNS of_IN text_NN documents_NNS in_IN different_JJ languages_NNS that_WDT are_VBP about_IN similar_JJ topics_NNS ;_: such_JJ text_NN corpora_NN are_VBP often_RB naturally_RB available_JJ -LRB-_-LRB- e.g._FW ,_, news_NN articles_NNS in_IN different_JJ languages_NNS published_VBN in_IN the_DT same_JJ time_NN period_NN -RRB-_-RRB- ._.
The_DT main_JJ idea_NN of_IN our_PRP$ method_NN is_VBZ to_TO exploit_VB frequency_NN correlations_NNS of_IN words_NNS in_IN different_JJ languages_NNS in_IN the_DT comparable_JJ corpora_NN and_CC discover_VB mappings_NNS between_IN words_NNS in_IN different_JJ languages_NNS ._.
Such_JJ mappings_NNS can_MD then_RB be_VB used_VBN to_TO further_RB discover_VB mappings_NNS between_IN documents_NNS in_IN different_JJ languages_NNS ,_, achieving_VBG cross-lingual_JJ information_NN integration_NN ._.
Evaluation_NN of_IN the_DT proposed_VBN method_NN on_IN a_DT 120MB_NNP Chinese-English_NNP comparable_JJ news_NN collection_NN shows_VBZ that_IN the_DT proposed_VBN method_NN is_VBZ effective_JJ for_IN mapping_NN words_NNS and_CC documents_NNS in_IN English_NNP and_CC Chinese_NNP ._.
Since_IN our_PRP$ method_NN only_RB relies_VBZ on_IN naturally_RB available_JJ comparable_JJ corpora_NN ,_, it_PRP is_VBZ generally_RB applicable_JJ to_TO any_DT language_NN pairs_NNS as_RB long_RB as_IN we_PRP have_VBP comparable_JJ corpora_NN ._.
s_NN between_IN the_DT two_CD languages_NNS ._.
To_TO evaluate_VB the_DT results_NNS quantitatively_RB ,_, we_PRP select_VBP a_DT sample_NN of_IN representative_JJ topics_NNS from_IN English_NNP by_IN performing_VBG word_NN clustering_NN using_VBG the_DT simple_JJ mixture_NN model_NN presented_VBN in_IN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP generated_VBD 30_CD clusters_NNS in_IN this_DT way_NN ._.
¡_FW sWe_FW then_RB take_VB the_DT top_JJ 5_CD English_JJ documents_NNS from_IN the_DT three_CD randomly_RB chosen_VBN clusters_NNS ,_, to_TO generate_VB 15_CD seed_NN English_JJ documents_NNS ._.
For_IN each_DT English_NNP document_NN ,_, we_PRP use_VBP the_DT
lingual_JJ information_NN integration_NN as_IN one_CD involving_VBG mapping_NN or_CC linking_VBG words_NNS and_CC documents_NNS in_IN different_JJ languages_NNS ._.
While_IN comparable_JJ corpora_NNS have_VBP been_VBN studied_VBN extensively_RB in_IN the_DT existing_VBG literature_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 6_CD ,_, 10_CD ,_, 15_CD ,_, 5_CD ,_, 2_CD ,_, 8_CD ,_, 13_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, almost_RB all_DT existing_VBG work_NN assumes_VBZ some_DT kind_NN of_IN bilingual_JJ dictionary_NN or_CC translation_NN examples_NNS to_TO start_VB with_IN ._.
We_PRP study_VBD how_WRB to_TO map_VB words_NNS and_CC documents_NNS from_IN comparable_JJ bilingual_JJ corpora_NN without_IN requiring_VBG
mation_NN retrieval_NN ,_, many_JJ formulas_NNS have_VBP been_VBN proposed_VBN to_TO heuristically_RB normalize_VB the_DT count_NN of_IN words_NNS to_TO achieve_VB this_DT effect_NN ._.
A_DT popular_JJ and_CC effective_JJ method_NN is_VBZ the_DT BM25_NN term_NN frequency_NN normalization_NN method_NN =_JJ -_: =[_NN 11_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
According_VBG to_TO this_DT formula_NN ,_, the_DT normalized_VBN count_NN of_IN word_NN w_NN in_IN document_NN d_NN is_VBZ given_VBN by_IN k1c_NN -LRB-_-LRB- w_NN ,_, d_NN -RRB-_-RRB- BM25_NN -LRB-_-LRB- w_NN ,_, d_NN -RRB-_-RRB- =_JJ |_FW d_FW |_FW c_NN -LRB-_-LRB- w_NN ,_, d_NN -RRB-_-RRB- +_CC k1_NN -LRB-_-LRB- 1_CD −_NN b_NN +_CC b_NN AvgDocLen_NN -RRB-_-RRB- where_WRB k1_NN and_CC b_NN are_VBP parameters_NNS and_CC AvgDocLen_NNP is_VBZ the_DT a_DT
more_RBR discriminative_JJ word_NN ._.
A_DT commonly_RB used_VBN heuristic_NN in_IN information_NN retrieval_NN is_VBZ to_TO assign_VB an_DT Inverse_NNP Document_NNP Frequency_NN -LRB-_-LRB- IDF_NN -RRB-_-RRB- weight_NN to_TO each_DT word_NN ,_, which_WDT penalizes_VBZ popular_JJ -LRB-_-LRB- thus_RB noninformative_JJ -RRB-_-RRB- words_NNS =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN our_PRP$ case_NN ,_, we_PRP associate_VBP an_DT IDF_NN weight_NN for_IN a_DT matching_JJ pair_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- ,_, which_WDT is_VBZ defined_VBN as_IN IDF_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- =_JJ IDF_NN -LRB-_-LRB- x_NN -RRB-_-RRB- IDF_NN -LRB-_-LRB- y_NN -RRB-_-RRB- ,_, where_WRB IDF_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ log_NN n_NN +1_CD ,_, and_CC df_NN -LRB-_-LRB- w_NN -RRB-_-RRB- is_VBZ the_DT number_NN of_IN docdf_NN -LRB-_-LRB- w_NN -RRB-_-RRB- uments_NNS in_IN a_DT languag_NN
e_LS λ_NN is_VBZ a_DT smoothing_VBG parameter_NN to_TO introduce_VB a_DT background_NN language_NN model_NN p_NN -LRB-_-LRB- x_NN |_NN C_NN -RRB-_-RRB- for_IN modeling_NN the_DT noise_NN -LRB-_-LRB- common_JJ words_NNS -RRB-_-RRB- in_IN d1_NN and_CC p_NN -LRB-_-LRB- y_FW |_FW d2_NN -RRB-_-RRB- is_VBZ the_DT relative_JJ frequency_NN of_IN word_NN y_NN in_IN d2_NN ,_, i.e._FW ,_, p_NN -LRB-_-LRB- y_FW |_FW d2_NN -RRB-_-RRB- =_JJ c_NN -LRB-_-LRB- y_NN ,_, d2_NN -RRB-_-RRB- =_JJ -_: =[_NN 18_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
p_NN -LRB-_-LRB- x_NN |_NN C_NN -RRB-_-RRB- can_MD be_VB estimated_VBN as_IN p_NN -LRB-_-LRB- x_NN |_NN C_NN -RRB-_-RRB- =_JJ CorrTrans_NNS ._.
|_FW d2_FW Pni_NN =_JJ 1_CD c_NN -LRB-_-LRB- x_NN ,_, s_NN P_NN i_LS -RRB-_-RRB- x_NN ′_NN P_NN ni_NN =_JJ 1_CD c_NN -LRB-_-LRB- x_NN ′_NN ,_, si_NN -RRB-_-RRB- ._.
We_PRP refer_VBP this_DT one_CD as_IN 5_CD ._.
EXPERIMENTS_NNS We_PRP use_VBP the_DT data_NNS set_NN described_VBN in_IN Section_NN 2_CD to_TO evaluate_VB the_DT proposed_VBN wo_MD
associations_NNS between_IN the_DT words_NNS in_IN the_DT same_JJ language_NN and_CC then_RB compare_VB association_NN patterns_NNS in_IN two_CD different_JJ languages_NNS ._.
Our_PRP$ idea_NN and_CC the_DT overall_JJ approach_NN appear_VBP to_TO be_VB more_RBR similar_JJ to_TO the_DT method_NN used_VBN in_IN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, but_CC there_RB the_DT task_NN is_VBZ aligning_VBG sentences_NNS in_IN parallel_JJ corpora_NN ._.
With_IN the_DT word_NN mappings_NNS ,_, we_PRP can_MD then_RB try_VB to_TO match_VB documents_NNS in_IN two_CD different_JJ languages_NNS based_VBN on_IN how_WRB well_RB the_DT words_NNS in_IN each_DT document_NN are_VBP c_NN
ula_NN ,_, which_WDT we_PRP refer_VBP to_TO as_IN BM25_NN Correlation_NN -LRB-_-LRB- BM25Corr_NN -RRB-_-RRB- ._.
s_NN -LRB-_-LRB- d1_NN ,_, d2_NN -RRB-_-RRB- =_JJ X_NN x_NN ∈_CD d_NN 1_CD ,_, y_FW ∈_FW d_NN 2_CD IDF_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- r_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- BM25_NN -LRB-_-LRB- x_NN ,_, d1_NN -RRB-_-RRB- BM25_NN -LRB-_-LRB- y_NN ,_, d2_NN -RRB-_-RRB- Finally_RB ,_, motivated_VBN by_IN the_DT language_NN modeling_NN approach_NN to_TO information_NN retrieval_NN =_JJ -_: =[_NN 9_CD ,_, 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP can_MD also_RB use_VB the_DT correlation_NN between_IN words_NNS to_TO estimate_VB a_DT word_NN translation_NN model_NN t_NN -LRB-_-LRB- x_NN |_CD y_NN -RRB-_-RRB- -LRB-_-LRB- 3_CD -RRB-_-RRB- as_IN t_NN -LRB-_-LRB- x_NN |_CD y_NN -RRB-_-RRB- =_JJ r_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- P_NN x_NN ′_CD r_NN -LRB-_-LRB- x_NN ′_NN ._.
With_IN this_DT translation_NN model_NN ,_, we_PRP ,_, y_NN -RRB-_-RRB- can_MD define_VB the_DT similarity_NN between_IN d1_NN
e_LS λ_NN is_VBZ a_DT smoothing_VBG parameter_NN to_TO introduce_VB a_DT background_NN language_NN model_NN p_NN -LRB-_-LRB- x_NN |_NN C_NN -RRB-_-RRB- for_IN modeling_NN the_DT noise_NN -LRB-_-LRB- common_JJ words_NNS -RRB-_-RRB- in_IN d1_NN and_CC p_NN -LRB-_-LRB- y_FW |_FW d2_NN -RRB-_-RRB- is_VBZ the_DT relative_JJ frequency_NN of_IN word_NN y_NN in_IN d2_NN ,_, i.e._FW ,_, p_NN -LRB-_-LRB- y_FW |_FW d2_NN -RRB-_-RRB- =_JJ c_NN -LRB-_-LRB- y_NN ,_, d2_NN -RRB-_-RRB- =_JJ -_: =[_NN 18_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
p_NN -LRB-_-LRB- x_NN |_NN C_NN -RRB-_-RRB- can_MD be_VB estimated_VBN as_IN p_NN -LRB-_-LRB- x_NN |_NN C_NN -RRB-_-RRB- =_JJ CorrTrans_NNS ._.
|_FW d2_FW Pni_NN =_JJ 1_CD c_NN -LRB-_-LRB- x_NN ,_, s_NN P_NN i_LS -RRB-_-RRB- x_NN ′_NN P_NN ni_NN =_JJ 1_CD c_NN -LRB-_-LRB- x_NN ′_NN ,_, si_NN -RRB-_-RRB- ._.
We_PRP refer_VBP this_DT one_CD as_IN 5_CD ._.
EXPERIMENTS_NNS We_PRP use_VBP the_DT data_NNS set_NN described_VBN in_IN Section_NN 2_CD to_TO evaluate_VB the_DT proposed_VBN wo_MD
lingual_JJ information_NN integration_NN as_IN one_CD involving_VBG mapping_NN or_CC linking_VBG words_NNS and_CC documents_NNS in_IN different_JJ languages_NNS ._.
While_IN comparable_JJ corpora_NNS have_VBP been_VBN studied_VBN extensively_RB in_IN the_DT existing_VBG literature_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 6_CD ,_, 10_CD ,_, 15_CD ,_, 5_CD ,_, 2_CD ,_, 8_CD ,_, 13_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, almost_RB all_DT existing_VBG work_NN assumes_VBZ some_DT kind_NN of_IN bilingual_JJ dictionary_NN or_CC translation_NN examples_NNS to_TO start_VB with_IN ._.
We_PRP study_VBD how_WRB to_TO map_VB words_NNS and_CC documents_NNS from_IN comparable_JJ bilingual_JJ corpora_NN without_IN requiring_VBG
Department_NNP of_IN Computer_NNP Science_NNP University_NNP of_IN Illinois_NNP at_IN Urbana_NNP Champaign_NNP the_DT documents_NNS in_IN different_JJ languages_NNS ,_, it_PRP would_MD be_VB very_RB useful_JJ if_IN we_PRP could_MD integrate_VB related_JJ information_NN in_IN multiple_JJ languages_NNS =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Currently_RB ,_, cross-lingual_JJ information_NN integration_NN is_VBZ often_RB achieved_VBN through_IN performing_VBG cross-lingual_JJ information_NN retrieval_NN -LRB-_-LRB- CLIR_NN -RRB-_-RRB- -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, which_WDT allows_VBZ a_DT user_NN to_TO retrieve_VB documents_NNS in_IN language_NN A_NN with_IN a_DT q_NN
BM25_NN -LRB-_-LRB- x_NN ,_, d1_NN -RRB-_-RRB- BM25_NN -LRB-_-LRB- y_NN ,_, d2_NN -RRB-_-RRB- Finally_RB ,_, motivated_VBN by_IN the_DT language_NN modeling_NN approach_NN to_TO information_NN retrieval_NN -LRB-_-LRB- 9_CD ,_, 17_CD -RRB-_-RRB- ,_, we_PRP can_MD also_RB use_VB the_DT correlation_NN between_IN words_NNS to_TO estimate_VB a_DT word_NN translation_NN model_NN t_NN -LRB-_-LRB- x_NN |_CD y_NN -RRB-_-RRB- =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: as_IN t_NN -LRB-_-LRB- x_NN |_CD y_NN -RRB-_-RRB- =_JJ r_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- P_NN x_NN ′_CD r_NN -LRB-_-LRB- x_NN ′_NN ._.
With_IN this_DT translation_NN model_NN ,_, we_PRP ,_, y_NN -RRB-_-RRB- can_MD define_VB the_DT similarity_NN between_IN d1_NN and_CC d2_NN as_IN the_DT likelihood_NN of_IN ``_`` generating_VBG ''_'' d1_NN with_IN a_DT model_NN based_VBN on_IN d2_NN ._.
That_DT is_VBZ ,_, s_NN -LRB-_-LRB- d1_NN ,_, d2_NN -RRB-_-RRB- =_JJ X_NN c_NN
s_NN in_IN language_NN A_NN with_IN a_DT query_NN in_IN language_NN B._NNP Most_NNP CLIR_NNP techniques_NNS rely_VBP on_IN manually_RB created_VBN linguistic_JJ resources_NNS such_JJ as_IN a_DT bilingual_JJ dictionary_NN or_CC examples_NNS of_IN direct_JJ translations_NNS of_IN words_NNS and_CC documents_NNS =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Such_JJ resources_NNS may_MD not_RB always_RB be_VB available_JJ ,_, especially_RB for_IN minority_NN languages_NNS ;_: in_IN such_PDT a_DT case_NN ,_, how_WRB to_TO perform_VB cross-lingual_JJ information_NN integration_NN would_MD be_VB a_DT significant_JJ challenge_NN ._.
In_IN this_DT paper_NN ,_, w_NN
lingual_JJ information_NN integration_NN as_IN one_CD involving_VBG mapping_NN or_CC linking_VBG words_NNS and_CC documents_NNS in_IN different_JJ languages_NNS ._.
While_IN comparable_JJ corpora_NNS have_VBP been_VBN studied_VBN extensively_RB in_IN the_DT existing_VBG literature_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 6_CD ,_, 10_CD ,_, 15_CD ,_, 5_CD ,_, 2_CD ,_, 8_CD ,_, 13_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, almost_RB all_DT existing_VBG work_NN assumes_VBZ some_DT kind_NN of_IN bilingual_JJ dictionary_NN or_CC translation_NN examples_NNS to_TO start_VB with_IN ._.
We_PRP study_VBD how_WRB to_TO map_VB words_NNS and_CC documents_NNS from_IN comparable_JJ bilingual_JJ corpora_NN without_IN requiring_VBG
ul_IN if_IN we_PRP could_MD integrate_VB related_JJ information_NN in_IN multiple_JJ languages_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- ._.
Currently_RB ,_, cross-lingual_JJ information_NN integration_NN is_VBZ often_RB achieved_VBN through_IN performing_VBG cross-lingual_JJ information_NN retrieval_NN -LRB-_-LRB- CLIR_NN -RRB-_-RRB- =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT allows_VBZ a_DT user_NN to_TO retrieve_VB documents_NNS in_IN language_NN A_NN with_IN a_DT query_NN in_IN language_NN B._NNP Most_NNP CLIR_NNP techniques_NNS rely_VBP on_IN manually_RB created_VBN linguistic_JJ resources_NNS such_JJ as_IN a_DT bilingual_JJ dictionary_NN or_CC examples_NNS of_IN d_NN
lingual_JJ information_NN integration_NN as_IN one_CD involving_VBG mapping_NN or_CC linking_VBG words_NNS and_CC documents_NNS in_IN different_JJ languages_NNS ._.
While_IN comparable_JJ corpora_NNS have_VBP been_VBN studied_VBN extensively_RB in_IN the_DT existing_VBG literature_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 6_CD ,_, 10_CD ,_, 15_CD ,_, 5_CD ,_, 2_CD ,_, 8_CD ,_, 13_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, almost_RB all_DT existing_VBG work_NN assumes_VBZ some_DT kind_NN of_IN bilingual_JJ dictionary_NN or_CC translation_NN examples_NNS to_TO start_VB with_IN ._.
We_PRP study_VBD how_WRB to_TO map_VB words_NNS and_CC documents_NNS from_IN comparable_JJ bilingual_JJ corpora_NN without_IN requiring_VBG
mation_NN retrieval_NN ,_, many_JJ formulas_NNS have_VBP been_VBN proposed_VBN to_TO heuristically_RB normalize_VB the_DT count_NN of_IN words_NNS to_TO achieve_VB this_DT effect_NN ._.
A_DT popular_JJ and_CC effective_JJ method_NN is_VBZ the_DT BM25_NN term_NN frequency_NN normalization_NN method_NN =_JJ -_: =[_NN 11_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
According_VBG to_TO this_DT formula_NN ,_, the_DT normalized_VBN count_NN of_IN word_NN w_NN in_IN document_NN d_NN is_VBZ given_VBN by_IN k1c_NN -LRB-_-LRB- w_NN ,_, d_NN -RRB-_-RRB- BM25_NN -LRB-_-LRB- w_NN ,_, d_NN -RRB-_-RRB- =_JJ |_FW d_FW |_FW c_NN -LRB-_-LRB- w_NN ,_, d_NN -RRB-_-RRB- +_CC k1_NN -LRB-_-LRB- 1_CD −_NN b_NN +_CC b_NN AvgDocLen_NN -RRB-_-RRB- where_WRB k1_NN and_CC b_NN are_VBP parameters_NNS and_CC AvgDocLen_NNP is_VBZ the_DT a_DT
atch_NN -LRB-_-LRB- e.g._FW ,_, common_JJ functional_JJ words_NNS are_VBP not_RB interesting_JJ for_IN the_DT purpose_NN of_IN information_NN integration_NN -RRB-_-RRB- ,_, we_PRP use_VBP the_DT following_JJ heuristics_NNS to_TO significantly_RB reduce_VB the_DT space_NN ._.
1_CD ._.
We_PRP first_RB compute_VBP the_DT entropy_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: of_IN a_DT word_NN in_IN each_DT language_NN using_VBG the_DT formula_NN H_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ −_CD P_NN t_NN p_NN -LRB-_-LRB- t_NN |_CD w_NN -RRB-_-RRB- log_NN p_NN -LRB-_-LRB- t_NN |_CD w_NN -RRB-_-RRB- ,_, where_WRB p_NN -LRB-_-LRB- t_NN |_CD w_NN -RRB-_-RRB- is_VBZ the_DT normalized_VBN frequency_NN of_IN word_NN w_NN at_IN time_NN point_NN t_NN -LRB-_-LRB- i.e._FW ,_, a_DT day_NN -RRB-_-RRB- ._.
2_CD ._.
We_PRP then_RB filter_NN out_RP the_DT high_JJ entropy_NN w_NN
lingual_JJ information_NN integration_NN as_IN one_CD involving_VBG mapping_NN or_CC linking_VBG words_NNS and_CC documents_NNS in_IN different_JJ languages_NNS ._.
While_IN comparable_JJ corpora_NNS have_VBP been_VBN studied_VBN extensively_RB in_IN the_DT existing_VBG literature_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 6_CD ,_, 10_CD ,_, 15_CD ,_, 5_CD ,_, 2_CD ,_, 8_CD ,_, 13_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, almost_RB all_DT existing_VBG work_NN assumes_VBZ some_DT kind_NN of_IN bilingual_JJ dictionary_NN or_CC translation_NN examples_NNS to_TO start_VB with_IN ._.
We_PRP study_VBD how_WRB to_TO map_VB words_NNS and_CC documents_NNS from_IN comparable_JJ bilingual_JJ corpora_NN without_IN requiring_VBG
lingual_JJ information_NN integration_NN as_IN one_CD involving_VBG mapping_NN or_CC linking_VBG words_NNS and_CC documents_NNS in_IN different_JJ languages_NNS ._.
While_IN comparable_JJ corpora_NNS have_VBP been_VBN studied_VBN extensively_RB in_IN the_DT existing_VBG literature_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 6_CD ,_, 10_CD ,_, 15_CD ,_, 5_CD ,_, 2_CD ,_, 8_CD ,_, 13_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, almost_RB all_DT existing_VBG work_NN assumes_VBZ some_DT kind_NN of_IN bilingual_JJ dictionary_NN or_CC translation_NN examples_NNS to_TO start_VB with_IN ._.
We_PRP study_VBD how_WRB to_TO map_VB words_NNS and_CC documents_NNS from_IN comparable_JJ bilingual_JJ corpora_NN without_IN requiring_VBG
lingual_JJ information_NN integration_NN as_IN one_CD involving_VBG mapping_NN or_CC linking_VBG words_NNS and_CC documents_NNS in_IN different_JJ languages_NNS ._.
While_IN comparable_JJ corpora_NNS have_VBP been_VBN studied_VBN extensively_RB in_IN the_DT existing_VBG literature_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 6_CD ,_, 10_CD ,_, 15_CD ,_, 5_CD ,_, 2_CD ,_, 8_CD ,_, 13_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, almost_RB all_DT existing_VBG work_NN assumes_VBZ some_DT kind_NN of_IN bilingual_JJ dictionary_NN or_CC translation_NN examples_NNS to_TO start_VB with_IN ._.
We_PRP study_VBD how_WRB to_TO map_VB words_NNS and_CC documents_NNS from_IN comparable_JJ bilingual_JJ corpora_NN without_IN requiring_VBG
