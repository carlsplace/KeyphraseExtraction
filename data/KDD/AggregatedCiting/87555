MetaCost_NNP :_: a_DT general_JJ method_NN for_IN making_VBG classifiers_NNS cost-sensitive_JJ
s_NN into_IN cost-sensitive_JJ ones_NNS ._.
To_TO our_PRP$ knowledge_NN ,_, the_DT only_RB currently_RB available_JJ procedure_NN of_IN this_DT type_NN is_VBZ stratification_NN --_: changing_VBG the_DT frequency_NN of_IN classes_NNS in_IN the_DT training_NN data_NNS in_IN proportion_NN to_TO their_PRP$ cost_NN =_JJ -_: =[_NN 8_CD ,_, 9_CD ,_, 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, this_DT approach_NN has_VBZ severa_NN shortcomings_NNS ._.
It_PRP distorts_VBZ the_DT distribution_NN of_IN examples_NNS ,_, which_WDT may_MD seriously_RB affect_VB the_DT performance_NN of_IN some_DT algorithms_NNS ._.
It_PRP reduces_VBZ the_DT data_NNS available_JJ for_IN learning_NN ,_, i_FW
for_IN its_PRP$ resampling_NN scheme_NN ._.
Unlike_IN MetaCost_NNP ,_, it_PRP requires_VBZ repeating_VBG all_DT runs_VBZ every_DT time_NN the_DT cost_NN matrix_NN changes_NNS ._.
It_PRP has_VBZ only_RB been_VBN tested_VBN in_IN a_DT single_JJ domain_NN -LRB-_-LRB- credit-card_NN fraud_NN detection_NN -RRB-_-RRB- ._.
Ting_NN and_CC Zheng_NN =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP proposed_VBN a_DT cost-sensitive_JJ variant_NN of_IN boosting_VBG -LRB-_-LRB- an_DT ensemble_NN method_NN -RRB-_-RRB- for_IN decision_NN trees_NNS ._.
It_PRP is_VBZ significant_JJ here_RB because_IN it_PRP should_MD be_VB easily_RB adaptable_JJ to_TO other_JJ error-based_JJ learners_NNS ,_, and_CC like_IN MetaCo_NN
be_VB useful_JJ as_IN a_DT method_NN for_IN speeding_VBG up_RP learning_NN ._.
4_CD Related_NNP Work_NNP Cost-sensitive_JJ learning_NN is_VBZ the_DT subject_NN of_IN a_DT burgeoning_VBG literature_NN ,_, which_WDT space_NN does_VBZ not_RB allow_VB us_PRP to_TO review_VB here_RB ._.
We_PRP point_VBP the_DT reader_NN to_TO =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: for_IN a_DT brief_JJ review_NN ,_, and_CC to_TO -LRB-_-LRB- 25_CD -RRB-_-RRB- for_IN an_DT online_JJ bibliography_NN ._.
This_DT section_NN discusses_VBZ those_DT elements_NNS of_IN previous_JJ research_NN that_WDT are_VBP most_RBS closely_RB related_JJ to_TO MetaCost_NNP ._.
Chan_NNP and_CC Stolfo_NNP -LRB-_-LRB- 9_CD -RRB-_-RRB- have_VBP proposed_VBN a_DT v_LS
increase_NN in_IN cost_NN caused_VBN by_IN learning_VBG the_DT class_NN probabilities_NNS on_IN smaller_JJR samples_NNS may_MD be_VB offset_VBN or_CC exceeded_VBN by_IN the_DT reduction_NN obtained_VBN by_IN the_DT use_NN of_IN multiple_JJ models_NNS ._.
Indeed_RB ,_, this_DT idea_NN is_VBZ behind_IN Breiman_NNP 's_POS =_JJ -_: =[_NN 6_CD -RRB-_-RRB- successfu_NN -_: =_JJ -_: l_NN ``_`` pasting_NN ''_'' method_NN for_IN scaling_VBG up_RP learners_NNS ._.
At_IN the_DT same_JJ time_NN ,_, reducing_VBG resample_JJ sizes_NNS will_MD reduce_VB running_VBG time_NN ,_, by_IN a_DT factor_NN that_WDT will_MD be_VB particularly_RB significant_JJ if_IN the_DT errorbased_JJ learner_NN us_PRP
stics_NNS ,_, pattern_NN recognition_NN ,_, neura_NN networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN =_JJ -_: =[_NN 8_CD ,_, 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN
ery_NN poor_NN ._.
For_IN example_NN ,_, most_JJS decision_NN tree_NN and_CC rule_NN learners_NNS work_VBP by_IN attempting_VBG to_TO drive_VB class_NN probabilities_NNS to_TO zero_CD or_CC one_CD within_IN each_DT lea\/or_NN rule_NN ,_, and_CC the_DT resulting_VBG estimates_NNS are_VBP correspondingly_RB off_RB =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Because_IN of_IN this_DT ,_, and_CC because_IN some_DT classifiers_NNS may_MD not_RB produce_VB class_NN probabilities_NNS ,_, MetaCost_NNP allows_VBZ their_PRP$ use_NN ,_, but_CC does_VBZ not_RB require_VB it_PRP ._.
A_DT more_RBR robust_JJ and_CC generally-applicable_JJ method_NN for_IN obtaining_VBG cla_NN
et_CC ,_, like_IN k-nearest_NN neighbor_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- and_CC naive_JJ Bayes_NNS -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
In_IN its_PRP$ present_JJ form_NN ,_, MetaCost_NNP may_MD not_RB be_VB very_RB effective_JJ with_IN these_DT algorithms_NNS ,_, but_CC an_DT alternative_NN is_VBZ readily_RB suggested_VBN by_IN the_DT results_NNS of_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- and_CC =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Their_PRP$ method_NN consists_VBZ of_IN learning_VBG multiple_JJ models_NNS using_VBG different_JJ subsets_NNS of_IN the_DT attributes_NNS ,_, instead_RB of_IN different_JJ subsets_NNS of_IN the_DT examples_NNS ._.
K-nearest_NN neighbor_NN and_CC naive_JJ Bayes_NNS are_VBP unstable_JJ with_IN respec_NN
stics_NNS ,_, pattern_NN recognition_NN ,_, neura_NN networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN =_JJ -_: =[_NN 8_CD ,_, 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN
networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN =_JJ -_: =[_NN 11_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD
esearch_NN in_IN machine_NN learning_NN ,_, statistics_NNS ,_, pattern_NN recognition_NN ,_, neura_NN networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN =_JJ -_: =[_NN 20_CD ,_, 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assi_VB
be_VB useful_JJ as_IN a_DT method_NN for_IN speeding_VBG up_RP learning_NN ._.
4_CD Related_NNP Work_NNP Cost-sensitive_JJ learning_NN is_VBZ the_DT subject_NN of_IN a_DT burgeoning_VBG literature_NN ,_, which_WDT space_NN does_VBZ not_RB allow_VB us_PRP to_TO review_VB here_RB ._.
We_PRP point_VBP the_DT reader_NN to_TO =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: for_IN a_DT brief_JJ review_NN ,_, and_CC to_TO -LRB-_-LRB- 25_CD -RRB-_-RRB- for_IN an_DT online_JJ bibliography_NN ._.
This_DT section_NN discusses_VBZ those_DT elements_NNS of_IN previous_JJ research_NN that_WDT are_VBP most_RBS closely_RB related_JJ to_TO MetaCost_NNP ._.
Chan_NNP and_CC Stolfo_NNP -LRB-_-LRB- 9_CD -RRB-_-RRB- have_VBP proposed_VBN a_DT v_LS
obabilities_NNS for_IN every_DT example_NN ._.
It_PRP would_MD also_RB be_VB interesting_JJ to_TO do_VB an_DT ROC_NN analysis_NN of_IN MetaCost_NNP ,_, by_IN varying_VBG the_DT probability_NN thresholds_NNS at_IN which_WDT an_DT example_NN 's_POS relabeling_VBG changes_NNS from_IN one_CD class_NN to_TO another_DT =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT current_JJ version_NN of_IN MetaCost_NNP is_VBZ based_VBN on_IN bagging_NN ._.
An_DT alternative_JJ method_NN for_IN constructing_VBG model_NN ensembles_NNS is_VBZ boosting_VBG -LRB-_-LRB- 19_CD -RRB-_-RRB- ._.
Boosting_VBG often_RB achieves_VBZ lower_JJR error_NN rates_NNS than_IN bagging_NN ,_, and_CC using_VBG it_PRP in_IN
et_CC ,_, like_IN k-nearest_NN neighbor_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- and_CC naive_JJ Bayes_NNS -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
In_IN its_PRP$ present_JJ form_NN ,_, MetaCost_NNP may_MD not_RB be_VB very_RB effective_JJ with_IN these_DT algorithms_NNS ,_, but_CC an_DT alternative_NN is_VBZ readily_RB suggested_VBN by_IN the_DT results_NNS of_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- and_CC =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Their_PRP$ method_NN consists_VBZ of_IN learning_VBG multiple_JJ models_NNS using_VBG different_JJ subsets_NNS of_IN the_DT attributes_NNS ,_, instead_RB of_IN different_JJ subsets_NNS of_IN the_DT examples_NNS ._.
K-nearest_NN neighbor_NN and_CC naive_JJ Bayes_NNS are_VBP unstable_JJ with_IN respec_NN
fier_NN and_CC the_DT density_NN estimator_NN are_VBP the_DT same_JJ ,_, and_CC a_DT mismatch_NN between_IN probability_NN estimation_NN and_CC classification_NN stages_NNS has_VBZ indeed_RB been_VBN found_VBN to_TO hurt_VB performance_NN in_IN a_DT context_NN similar_JJ to_TO the_DT present_JJ one_CD -LRB-_-LRB- =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: ;_: see_VB oJso_NN related_JJ work_NN section_NN below_IN -RRB-_-RRB- ._.
For_IN example_NN ,_, decision_NN tree_NN and_CC rule_NN inducers_NNS are_VBP some_DT of_IN the_DT most_RBS effective_JJ learners_NNS for_IN very-high-dimensionoJ_NN domains_NNS like_IN those_DT often_RB found_VBN in_IN KDD_NNP ,_, but_CC these_DT
allows_VBZ their_PRP$ use_NN ,_, but_CC does_VBZ not_RB require_VB it_PRP ._.
A_DT more_RBR robust_JJ and_CC generally-applicable_JJ method_NN for_IN obtaining_VBG class_NN probability_NN estimates_NNS from_IN a_DT classifier_NN is_VBZ suggested_VBN by_IN recent_JJ research_NN on_IN model_NN ensembles_NNS =_JJ -_: =[_NN 10_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Many_JJ authors_NNS -LRB-_-LRB- e._NN g_NN ,_, Breiman_NNP -LRB-_-LRB- 5_CD -RRB-_-RRB- -RRB-_-RRB- have_VBP found_VBN that_IN most_JJS modern_JJ learners_NNS are_VBP highly_RB unstable_JJ ,_, in_IN that_IN applying_VBG them_PRP to_TO slightly_RB different_JJ training_NN sets_NNS tends_VBZ to_TO produce_VB very_RB different_JJ models_NNS and_CC corresp_NN
ule_NN rather_RB than_IN the_DT exception_NN has_VBZ led_VBN in_IN recent_JJ years_NNS to_TO an_DT increased_VBN interest_NN in_IN algorithms_NNS for_IN cost-sensitive_JJ classification_NN ._.
-LRB-_-LRB- Some_DT of_IN these_DT will_MD be_VB discussed_VBN in_IN the_DT section_NN on_IN related_JJ work_NN ;_: Turney_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_JJ -_: provides_VBZ an_DT online_JJ bibliography_NN on_IN the_DT topic_NN ._. -RRB-_-RRB-
Substantia_NN work_NN has_VBZ gone_VBN into_IN making_VBG individual_JJ algorithms_NNS cost-sensitive_JJ ._.
Doing_VBG this_DT for_IN ll_NN lgorithms_NNS available_JJ in_IN the_DT literature_NN would_MD be_VB a_DT very_JJ time_NN -_:
ny_IN well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN =_JJ -_: =[_NN 17_CD ,_, 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD of_IN a_DT finite_JJ number_NN of_IN classes_NNS ._.
Most_JJS of_IN the_DT currently-available_JJ
networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN =_JJ -_: =[_NN 11_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD
ny_IN well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN =_JJ -_: =[_NN 17_CD ,_, 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD of_IN a_DT finite_JJ number_NN of_IN classes_NNS ._.
Most_JJS of_IN the_DT currently-available_JJ
aining_VBG set_NN ,_, like_IN k-nearest_NN neighbor_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- and_CC naive_JJ Bayes_NNS -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
In_IN its_PRP$ present_JJ form_NN ,_, MetaCost_NNP may_MD not_RB be_VB very_RB effective_JJ with_IN these_DT algorithms_NNS ,_, but_CC an_DT alternative_NN is_VBZ readily_RB suggested_VBN by_IN the_DT results_NNS of_IN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 26_CD -RRB-_-RRB- ._.
Their_PRP$ method_NN consists_VBZ of_IN learning_VBG multiple_JJ models_NNS using_VBG different_JJ subsets_NNS of_IN the_DT attributes_NNS ,_, instead_RB of_IN different_JJ subsets_NNS of_IN the_DT examples_NNS ._.
K-nearest_NN neighbor_NN and_CC naive_JJ Bayes_NNS are_VBP unstable_JJ wi_NNS
esearch_NN in_IN machine_NN learning_NN ,_, statistics_NNS ,_, pattern_NN recognition_NN ,_, neura_NN networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN =_JJ -_: =[_NN 20_CD ,_, 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assi_VB
s_NN into_IN cost-sensitive_JJ ones_NNS ._.
To_TO our_PRP$ knowledge_NN ,_, the_DT only_RB currently_RB available_JJ procedure_NN of_IN this_DT type_NN is_VBZ stratification_NN --_: changing_VBG the_DT frequency_NN of_IN classes_NNS in_IN the_DT training_NN data_NNS in_IN proportion_NN to_TO their_PRP$ cost_NN =_JJ -_: =[_NN 8_CD ,_, 9_CD ,_, 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, this_DT approach_NN has_VBZ severa_NN shortcomings_NNS ._.
It_PRP distorts_VBZ the_DT distribution_NN of_IN examples_NNS ,_, which_WDT may_MD seriously_RB affect_VB the_DT performance_NN of_IN some_DT algorithms_NNS ._.
It_PRP reduces_VBZ the_DT data_NNS available_JJ for_IN learning_NN ,_, i_FW
sholds_NNS at_IN which_WDT an_DT example_NN 's_POS relabeling_VBG changes_NNS from_IN one_CD class_NN to_TO another_DT -LRB-_-LRB- 21_CD -RRB-_-RRB- ._.
The_DT current_JJ version_NN of_IN MetaCost_NNP is_VBZ based_VBN on_IN bagging_NN ._.
An_DT alternative_JJ method_NN for_IN constructing_VBG model_NN ensembles_NNS is_VBZ boosting_VBG =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Boosting_VBG often_RB achieves_VBZ lower_JJR error_NN rates_NNS than_IN bagging_NN ,_, and_CC using_VBG it_PRP in_IN MetaCost_NNP might_MD produce_VB corresponding_JJ reductions_NNS in_IN cost_NN ._.
6_CD Conclusion_NN KDD_NN applications_NNS have_VBP often_RB been_VBN hindered_VBN by_IN the_DT lack_NN of_IN
ecades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD of_IN a_DT finite_JJ number_NN of_IN classes_NNS ._.
Most_JJS
e_LS it_PRP ._.
A_DT more_RBR robust_JJ and_CC generally-applicable_JJ method_NN for_IN obtaining_VBG class_NN probability_NN estimates_NNS from_IN a_DT classifier_NN is_VBZ suggested_VBN by_IN recent_JJ research_NN on_IN model_NN ensembles_NNS -LRB-_-LRB- 10_CD ,_, 14_CD -RRB-_-RRB- ._.
Many_JJ authors_NNS -LRB-_-LRB- e._NN g_NN ,_, Breiman_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =--RRB-_NN have_VBP found_VBN that_IN most_JJS modern_JJ learners_NNS are_VBP highly_RB unstable_JJ ,_, in_IN that_IN applying_VBG them_PRP to_TO slightly_RB different_JJ training_NN sets_NNS tends_VBZ to_TO produce_VB very_RB different_JJ models_NNS and_CC correspondingly_RB different_JJ predictions_NNS f_SYM
networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN =_JJ -_: =[_NN 11_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD
st_NN and_CC those_DT that_WDT can_MD be_VB varied_VBN without_IN substantial_JJ loss_NN ._.
Experiments_NNS on_IN a_DT larger_JJR database_NN indicate_VBP that_IN MetaCost_NN scales_NNS well_RB ._.
I_NN Introduction_NN Classification_NN is_VBZ one_CD of_IN the_DT primary_JJ tasks_NNS of_IN data_NNS mining_NN =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
It_PRP has_VBZ also_RB been_VBN a_DT subject_NN of_IN research_NN in_IN machine_NN learning_NN ,_, statistics_NNS ,_, pattern_NN recognition_NN ,_, neura_NN networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB
ical_JJ Evaluation_NN The_DT question_NN of_IN whether_IN MetaCost_NNP reduces_VBZ cost_NN compared_VBN to_TO the_DT error-based_JJ classifier_NN and_CC to_TO stratification_NN was_VBD studied_VBN empirically_RB using_VBG 28_CD benchmark_JJ databases_NNS from_IN the_DT UCI_NNP repository_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT C4_NN .5_CD decision_NN tree_NN learner_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- was_VBD used_VBN as_IN the_DT error-based_JJ classifier_NN because_IN of_IN its_PRP$ de_FW facto_FW role_NN as_IN a_DT standard_NN for_IN empirical_JJ comparisons_NNS ._.
The_DT C4_NN .5_CD RULES_NNS post-processor_JJ ,_, which_WDT converts_VBZ C4_NN .5_NN 's_POS d_NN
