Automatic_NNP labeling_NN of_IN multinomial_JJ topic_NN models_NNS
Multinomial_JJ distributions_NNS over_IN words_NNS are_VBP frequently_RB used_VBN to_TO model_VB topics_NNS in_IN text_NN collections_NNS ._.
A_DT common_JJ ,_, major_JJ challenge_NN in_IN applying_VBG all_DT such_JJ topic_NN models_NNS to_TO any_DT text_NN mining_NN problem_NN is_VBZ to_TO label_VB a_DT multinomial_JJ topic_NN model_NN accurately_RB so_IN that_IN a_DT user_NN can_MD interpret_VB the_DT discovered_VBN topic_NN ._.
So_RB far_RB ,_, such_JJ labels_NNS have_VBP been_VBN generated_VBN manually_RB in_IN a_DT subjective_JJ way_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP probabilistic_JJ approaches_NNS to_TO automatically_RB labeling_VBG multinomial_JJ topic_NN models_NNS in_IN an_DT objective_JJ way_NN ._.
We_PRP cast_VBP this_DT labeling_NN problem_NN as_IN an_DT optimization_NN problem_NN involving_VBG minimizing_VBG Kullback-Leibler_NNP divergence_NN between_IN word_NN distributions_NNS and_CC maximizing_VBG mutual_JJ information_NN between_IN a_DT label_NN and_CC a_DT topic_NN model_NN ._.
Experiments_NNS with_IN user_NN study_NN have_VBP been_VBN done_VBN on_IN two_CD text_NN data_NNS sets_NNS with_IN different_JJ genres_NNS ._.
The_DT results_NNS show_VBP that_IN the_DT proposed_VBN labeling_NN methods_NNS are_VBP quite_RB effective_JJ to_TO generate_VB labels_NNS that_WDT are_VBP meaningful_JJ and_CC useful_JJ for_IN interpreting_VBG the_DT discovered_VBN topic_NN models_NNS ._.
Our_PRP$ methods_NNS are_VBP general_JJ and_CC can_MD be_VB applied_VBN to_TO labeling_NN topics_NNS learned_VBD through_IN all_DT kinds_NNS of_IN topic_NN models_NNS such_JJ as_IN PLSA_NNP ,_, LDA_NNP ,_, and_CC their_PRP$ variations_NNS ._.
Keywords_NNS :_: Statistical_JJ topic_NN models_NNS ,_, multinomial_JJ distribution_NN ,_, topic_NN model_NN labeling_NN 1_CD ._.
INTRODUCTION_NN Statistical_JJ topic_NN modeling_NN has_VBZ attracted_VBN much_JJ attention_NN recently_RB in_IN machine_NN learning_NN and_CC text_NN mining_NN =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 16_CD ,_, 18_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: due_JJ to_TO its_PRP$ broad_JJ applications_NNS ,_, including_VBG extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN -LRB-_-LRB- 17_CD ,_, 24_CD -RRB-_-RRB- ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extra_JJ
ppropriate_NN for_IN labeling_VBG a_DT topic_NN ._.
Therefore_RB ,_, given_VBN a_DT reference_NN collection_NN C_NN ,_, the_DT first_JJ task_NN is_VBZ to_TO generate_VB meaningful_JJ phrases_NNS as_IN candidate_NN labels_NNS ._.
Phrase_NN generation_NN has_VBZ been_VBN addressed_VBN in_IN existing_VBG work_NN =_JJ -_: =[_NN 7_CD ,_, 26_CD ,_, 15_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN general_JJ ,_, there_EX are_VBP two_CD basic_JJ approaches_NNS :_: Chunking\/Shallow_NNP Parsing_NNP :_: Chunking_NNP -LRB-_-LRB- Shallow_NNP Parsing_NNP -RRB-_-RRB- is_VBZ a_DT common_JJ technique_NN in_IN natural_JJ language_NN processing_NN ,_, which_WDT aims_VBZ at_IN identifying_VBG short_JJ phrases_NNS ,_, or_CC ``_`` ch_NN
Keywords_NNS :_: Statistical_JJ topic_NN models_NNS ,_, multinomial_JJ distribution_NN ,_, topic_NN model_NN labeling_NN 1_CD ._.
INTRODUCTION_NN Statistical_JJ topic_NN modeling_NN has_VBZ attracted_VBN much_JJ attention_NN recently_RB in_IN machine_NN learning_NN and_CC text_NN mining_NN =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 16_CD ,_, 18_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: due_JJ to_TO its_PRP$ broad_JJ applications_NNS ,_, including_VBG extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN -LRB-_-LRB- 17_CD ,_, 24_CD -RRB-_-RRB- ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extra_JJ
Keywords_NNS :_: Statistical_JJ topic_NN models_NNS ,_, multinomial_JJ distribution_NN ,_, topic_NN model_NN labeling_NN 1_CD ._.
INTRODUCTION_NN Statistical_JJ topic_NN modeling_NN has_VBZ attracted_VBN much_JJ attention_NN recently_RB in_IN machine_NN learning_NN and_CC text_NN mining_NN =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 16_CD ,_, 18_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: due_JJ to_TO its_PRP$ broad_JJ applications_NNS ,_, including_VBG extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN -LRB-_-LRB- 17_CD ,_, 24_CD -RRB-_-RRB- ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extra_JJ
th_DT -LCB-_-LRB- p_NN -LRB-_-LRB- w_NN |_CD Î¸_NN -RRB-_-RRB- -RCB-_-RRB- -RRB-_-RRB- or_CC a_DT candidate_NN label_NN ._.
Each_DT edge_NN between_IN a_DT label_NN and_CC a_DT topical_JJ term_NN is_VBZ then_RB weighted_VBN with_IN the_DT pointwise_JJ mutual_JJ information_NN PMI_NN -LRB-_-LRB- w_NN ,_, l_NN |_NN C_NN -RRB-_-RRB- ,_, which_WDT is_VBZ often_RB used_VBN to_TO measure_VB semantic_JJ associations_NNS =_JJ -_: =[_NN 7_CD ,_, 15_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Thus_RB the_DT weight_NN of_IN each_DT node_NN indicates_VBZ the_DT importance_NN of_IN the_DT term_NN to_TO this_DT topic_NN ,_, while_IN the_DT weight_NN of_IN each_DT edge_NN indicates_VBZ how_WRB strongly_RB the_DT label_NN and_CC the_DT term_NN are_VBP semantically_RB associated_VBN ._.
The_DT scoring_VBG
t_NN collection_NN ._.
Thus_RB it_PRP could_MD be_VB applied_VBN to_TO any_DT text_NN mining_NN problems_NNS ,_, in_IN which_WDT a_DT multinomial_JJ distribution_NN of_IN word_NN is_VBZ involved_VBN ._.
sIn_NN some_DT tasks_NNS ,_, such_JJ as_IN topic_NN modeling_NN and_CC many_JJ information_NN retrieval_NN tasks_NNS =_JJ -_: =[_NN 8_CD ,_, 27_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT multinomial_JJ word_NN distribution_NN is_VBZ explicit_JJ ._.
In_IN other_JJ tasks_NNS ,_, however_RB ,_, a_DT multinomial_JJ word_NN distribution_NN may_MD not_RB be_VB directly_RB available_JJ ;_: in_IN such_JJ tasks_NNS ,_, we_PRP can_MD also_RB apply_VB our_PRP$ method_NN by_IN extracting_VBG a_DT multi_NNS
ppropriate_NN for_IN labeling_VBG a_DT topic_NN ._.
Therefore_RB ,_, given_VBN a_DT reference_NN collection_NN C_NN ,_, the_DT first_JJ task_NN is_VBZ to_TO generate_VB meaningful_JJ phrases_NNS as_IN candidate_NN labels_NNS ._.
Phrase_NN generation_NN has_VBZ been_VBN addressed_VBN in_IN existing_VBG work_NN =_JJ -_: =[_NN 7_CD ,_, 26_CD ,_, 15_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN general_JJ ,_, there_EX are_VBP two_CD basic_JJ approaches_NNS :_: Chunking\/Shallow_NNP Parsing_NNP :_: Chunking_NNP -LRB-_-LRB- Shallow_NNP Parsing_NNP -RRB-_-RRB- is_VBZ a_DT common_JJ technique_NN in_IN natural_JJ language_NN processing_NN ,_, which_WDT aims_VBZ at_IN identifying_VBG short_JJ phrases_NNS ,_, or_CC ``_`` ch_NN
h_NN as_IN possible_JJ ,_, thus_RB we_PRP prefer_VBP ``_`` dimension_NN reduction_NN ''_'' ,_, rather_RB than_IN labels_NNS like_IN ``_`` clustering_NN technique_NN ''_'' ._.
To_TO implement_VB this_DT intuition_NN ,_, we_PRP propose_VBP to_TO select_VB labels_NNS with_IN the_DT Maximal_JJ Marginal_JJ Relevance_NN -LRB-_-LRB- MMR_NN -RRB-_-RRB- =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: criterion_NN ._.
MMR_NN is_VBZ commonly_RB used_VBN in_IN information_NN retrieval_NN tasks_NNS ,_, where_WRB both_DT high_JJ relevance_NN and_CC low_JJ redundancy_NN of_IN retrieval_NN results_NNS are_VBP desired_VBN ._.
Specifically_RB ,_, we_PRP select_VBP labels_NNS one_CD by_IN one_CD ,_, by_IN maximizing_VBG
ttention_NN recently_RB in_IN machine_NN learning_NN and_CC text_NN mining_NN -LRB-_-LRB- 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 16_CD ,_, 18_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- due_JJ to_TO its_PRP$ broad_JJ applications_NNS ,_, including_VBG extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN =_JJ -_: =[_NN 17_CD ,_, 24_CD -RRB-_-RRB- -_: =_JJ -_: ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extraction_NN -LRB-_-LRB- 28_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC information_NN retrieval_NN -LRB-_-LRB- 11_CD ,_, 27_CD ,_, 25_CD -RRB-_-RRB- ._.
Common_JJ to_TO most_JJS of_IN this_DT work_NN is_VBZ the_DT idea_NN of_IN using_VBG a_DT multinomia_NN
extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN -LRB-_-LRB- 17_CD ,_, 24_CD -RRB-_-RRB- ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extraction_NN -LRB-_-LRB- 28_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC information_NN retrieval_NN =_JJ -_: =[_NN 11_CD ,_, 27_CD ,_, 25_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Common_JJ to_TO most_JJS of_IN this_DT work_NN is_VBZ the_DT idea_NN of_IN using_VBG a_DT multinomial_JJ word_NN distribution_NN -LRB-_-LRB- also_RB called_VBN a_DT unigram_JJ language_NN model_NN -RRB-_-RRB- to_TO model_VB a_DT topic_NN in_IN text_NN ._.
For_IN example_NN ,_, the_DT multinomial_JJ distribution_NN shown_VBN on_IN t_NN
Keywords_NNS :_: Statistical_JJ topic_NN models_NNS ,_, multinomial_JJ distribution_NN ,_, topic_NN model_NN labeling_NN 1_CD ._.
INTRODUCTION_NN Statistical_JJ topic_NN modeling_NN has_VBZ attracted_VBN much_JJ attention_NN recently_RB in_IN machine_NN learning_NN and_CC text_NN mining_NN =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 16_CD ,_, 18_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: due_JJ to_TO its_PRP$ broad_JJ applications_NNS ,_, including_VBG extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN -LRB-_-LRB- 17_CD ,_, 24_CD -RRB-_-RRB- ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extra_JJ
ppropriate_NN for_IN labeling_VBG a_DT topic_NN ._.
Therefore_RB ,_, given_VBN a_DT reference_NN collection_NN C_NN ,_, the_DT first_JJ task_NN is_VBZ to_TO generate_VB meaningful_JJ phrases_NNS as_IN candidate_NN labels_NNS ._.
Phrase_NN generation_NN has_VBZ been_VBN addressed_VBN in_IN existing_VBG work_NN =_JJ -_: =[_NN 7_CD ,_, 26_CD ,_, 15_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN general_JJ ,_, there_EX are_VBP two_CD basic_JJ approaches_NNS :_: Chunking\/Shallow_NNP Parsing_NNP :_: Chunking_NNP -LRB-_-LRB- Shallow_NNP Parsing_NNP -RRB-_-RRB- is_VBZ a_DT common_JJ technique_NN in_IN natural_JJ language_NN processing_NN ,_, which_WDT aims_VBZ at_IN identifying_VBG short_JJ phrases_NNS ,_, or_CC ``_`` ch_NN
these_DT topics_NNS models_NNS extracted_VBN from_IN text_NN data_NNS ._.
As_IN we_PRP use_VBP phrases_NNS as_IN candidate_NN labels_NNS ,_, our_PRP$ work_NN is_VBZ related_JJ to_TO phrase_NN extraction_NN ,_, including_VBG shallow_JJ parsing\/chunking_NN in_IN natural_JJ language_NN processing_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 15_CD ,_, 10_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, and_CC N-gram_NN phrase_NN extraction_NN with_IN statistical_JJ approaches_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 7_CD ,_, 26_CD ,_, 1_CD ,_, 6_CD -RRB-_-RRB- -RRB-_-RRB- ._.
A_DT better_JJR phrase_NN extraction_NN method_NN could_MD benefit_VB topic_NN labeling_NN as_IN a_DT better_JJR preprocessing_NN procedure_NN ._.
Text_NN summarizatio_NN
Keywords_NNS :_: Statistical_JJ topic_NN models_NNS ,_, multinomial_JJ distribution_NN ,_, topic_NN model_NN labeling_NN 1_CD ._.
INTRODUCTION_NN Statistical_JJ topic_NN modeling_NN has_VBZ attracted_VBN much_JJ attention_NN recently_RB in_IN machine_NN learning_NN and_CC text_NN mining_NN =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 16_CD ,_, 18_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: due_JJ to_TO its_PRP$ broad_JJ applications_NNS ,_, including_VBG extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN -LRB-_-LRB- 17_CD ,_, 24_CD -RRB-_-RRB- ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extra_JJ
extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN -LRB-_-LRB- 17_CD ,_, 24_CD -RRB-_-RRB- ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extraction_NN -LRB-_-LRB- 28_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC information_NN retrieval_NN =_JJ -_: =[_NN 11_CD ,_, 27_CD ,_, 25_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Common_JJ to_TO most_JJS of_IN this_DT work_NN is_VBZ the_DT idea_NN of_IN using_VBG a_DT multinomial_JJ word_NN distribution_NN -LRB-_-LRB- also_RB called_VBN a_DT unigram_JJ language_NN model_NN -RRB-_-RRB- to_TO model_VB a_DT topic_NN in_IN text_NN ._.
For_IN example_NN ,_, the_DT multinomial_JJ distribution_NN shown_VBN on_IN t_NN
that_IN if_IN the_DT words_NNS in_IN an_DT ngram_NN tend_VB to_TO co-occur_JJ with_IN each_DT other_JJ ,_, the_DT ngram_NN is_VBZ more_RBR likely_JJ to_TO be_VB an_DT n-word_NN phrase_NN ._.
There_EX are_VBP many_JJ methods_NNS for_IN testing_NN whether_IN an_DT ngram_NN is_VBZ a_DT meaningful_JJ collocation\/phrase_NN =_JJ -_: =[_NN 7_CD ,_, 26_CD ,_, 1_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Some_DT methods_NNS rely_VBP on_IN statistical_JJ measures_NNS such_JJ as_IN mutual_JJ information_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
Others_NNS rely_VBP on_IN hypothesis_NN testing_NN techniques_NNS ._.
The_DT null_JJ hypothesis_NN usually_RB assumes_VBZ that_IN ``_`` the_DT words_NNS in_IN an_DT ngram_NN are_VBP independen_VBN
WORK_VB To_TO the_DT best_JJS of_IN our_PRP$ knowledge_NN ,_, no_DT existing_VBG work_NN has_VBZ formally_RB studied_VBN the_DT problem_NN of_IN automatic_JJ labeling_NN of_IN multinomial_JJ topic_NN models_NNS ._.
There_EX has_VBZ been_VBN a_DT large_JJ body_NN of_IN work_NN on_IN statistical_JJ topic_NN models_NNS =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 3_CD ,_, 16_CD ,_, 18_CD ,_, 19_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_JJ -_: ,_, most_JJS of_IN which_WDT uses_VBZ a_DT multinomial_JJ word_NN distribution_NN to_TO represent_VB a_DT topic_NN ._.
In_IN some_DT recent_JJ work_NN ,_, -LRB-_-LRB- 23_CD -RRB-_-RRB- generalized_VBD the_DT representation_NN of_IN a_DT topic_NN model_NN as_IN a_DT multinomial_JJ distribution_NN over_IN ngrams_NNS ._.
Such_JJ top_NN
better_JJR phrase_NN extraction_NN method_NN could_MD benefit_VB topic_NN labeling_NN as_IN a_DT better_JJR preprocessing_NN procedure_NN ._.
Text_NN summarization_NN aims_VBZ at_IN extracting\/generating_JJ sentence_NN summaries_NNS for_IN one\/multiple_JJ documents_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
The_DT summary_NN can_MD be_VB as_RB short_JJ as_IN titles_NNS -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
However_RB ,_, no_DT existing_VBG work_NN has_VBZ been_VBN done_VBN for_IN summarizing_VBG a_DT multinomial_JJ distribution_NN of_IN words_NNS ,_, or_CC a_DT statistical_JJ topic_NN model_NN ._.
Since_IN most_JJS topic_NN models_NNS assume_VBP
ppropriate_NN for_IN labeling_VBG a_DT topic_NN ._.
Therefore_RB ,_, given_VBN a_DT reference_NN collection_NN C_NN ,_, the_DT first_JJ task_NN is_VBZ to_TO generate_VB meaningful_JJ phrases_NNS as_IN candidate_NN labels_NNS ._.
Phrase_NN generation_NN has_VBZ been_VBN addressed_VBN in_IN existing_VBG work_NN =_JJ -_: =[_NN 7_CD ,_, 26_CD ,_, 15_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN general_JJ ,_, there_EX are_VBP two_CD basic_JJ approaches_NNS :_: Chunking\/Shallow_NNP Parsing_NNP :_: Chunking_NNP -LRB-_-LRB- Shallow_NNP Parsing_NNP -RRB-_-RRB- is_VBZ a_DT common_JJ technique_NN in_IN natural_JJ language_NN processing_NN ,_, which_WDT aims_VBZ at_IN identifying_VBG short_JJ phrases_NNS ,_, or_CC ``_`` ch_NN
WORK_VB To_TO the_DT best_JJS of_IN our_PRP$ knowledge_NN ,_, no_DT existing_VBG work_NN has_VBZ formally_RB studied_VBN the_DT problem_NN of_IN automatic_JJ labeling_NN of_IN multinomial_JJ topic_NN models_NNS ._.
There_EX has_VBZ been_VBN a_DT large_JJ body_NN of_IN work_NN on_IN statistical_JJ topic_NN models_NNS =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 3_CD ,_, 16_CD ,_, 18_CD ,_, 19_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_JJ -_: ,_, most_JJS of_IN which_WDT uses_VBZ a_DT multinomial_JJ word_NN distribution_NN to_TO represent_VB a_DT topic_NN ._.
In_IN some_DT recent_JJ work_NN ,_, -LRB-_-LRB- 23_CD -RRB-_-RRB- generalized_VBD the_DT representation_NN of_IN a_DT topic_NN model_NN as_IN a_DT multinomial_JJ distribution_NN over_IN ngrams_NNS ._.
Such_JJ top_NN
Keywords_NNS :_: Statistical_JJ topic_NN models_NNS ,_, multinomial_JJ distribution_NN ,_, topic_NN model_NN labeling_NN 1_CD ._.
INTRODUCTION_NN Statistical_JJ topic_NN modeling_NN has_VBZ attracted_VBN much_JJ attention_NN recently_RB in_IN machine_NN learning_NN and_CC text_NN mining_NN =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 16_CD ,_, 18_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: due_JJ to_TO its_PRP$ broad_JJ applications_NNS ,_, including_VBG extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN -LRB-_-LRB- 17_CD ,_, 24_CD -RRB-_-RRB- ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extra_JJ
Keywords_NNS :_: Statistical_JJ topic_NN models_NNS ,_, multinomial_JJ distribution_NN ,_, topic_NN model_NN labeling_NN 1_CD ._.
INTRODUCTION_NN Statistical_JJ topic_NN modeling_NN has_VBZ attracted_VBN much_JJ attention_NN recently_RB in_IN machine_NN learning_NN and_CC text_NN mining_NN =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 16_CD ,_, 18_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: due_JJ to_TO its_PRP$ broad_JJ applications_NNS ,_, including_VBG extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN -LRB-_-LRB- 17_CD ,_, 24_CD -RRB-_-RRB- ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extra_JJ
e_LS has_VBZ been_VBN a_DT large_JJ body_NN of_IN work_NN on_IN statistical_JJ topic_NN models_NNS -LRB-_-LRB- 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 3_CD ,_, 16_CD ,_, 18_CD ,_, 19_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- ,_, most_JJS of_IN which_WDT uses_VBZ a_DT multinomial_JJ word_NN distribution_NN to_TO represent_VB a_DT topic_NN ._.
In_IN some_DT recent_JJ work_NN ,_, =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: generalized_VBD the_DT representation_NN of_IN a_DT topic_NN model_NN as_IN a_DT multinomial_JJ distribution_NN over_IN ngrams_NNS ._.
Such_JJ topics_NNS are_VBP labeled_VBN with_IN either_CC top_JJ words_NNS in_IN the_DT distribution_NN or_CC manually_RB selected_VBN phrases_NNS ._.
The_DT method_NN w_NN
Keywords_NNS :_: Statistical_JJ topic_NN models_NNS ,_, multinomial_JJ distribution_NN ,_, topic_NN model_NN labeling_NN 1_CD ._.
INTRODUCTION_NN Statistical_JJ topic_NN modeling_NN has_VBZ attracted_VBN much_JJ attention_NN recently_RB in_IN machine_NN learning_NN and_CC text_NN mining_NN =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 16_CD ,_, 18_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: due_JJ to_TO its_PRP$ broad_JJ applications_NNS ,_, including_VBG extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN -LRB-_-LRB- 17_CD ,_, 24_CD -RRB-_-RRB- ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extra_JJ
here_RB a_DT multinomial_JJ word_NN distribution_NN is_VBZ involved_VBN ._.
Here_RB we_PRP look_VBP into_IN one_CD such_JJ application_NN --_: labeling_VBG document_NN clusters_NNS ._.
In_IN this_DT experiment_NN ,_, we_PRP cluster_VBP the_DT SIGMOD_NNP abstracts_NNS with_IN the_DT K-Medoids_NNS algorithm_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC try_VB to_TO utilize_VB the_DT topic_NN labeling_NN method_NN to_TO label_VB the_DT clusters_NNS ._.
Specifically_RB ,_, we_PRP estimate_VBP a_DT multinomial_JJ word_NN distribution_NN for_IN each_DT cluster_NN based_VBN on_IN its_PRP$ member_NN documents_NNS using_VBG the_DT maximum_NN likelih_NN
topic_NN labeling_NN as_IN a_DT better_JJR preprocessing_NN procedure_NN ._.
Text_NN summarization_NN aims_VBZ at_IN extracting\/generating_JJ sentence_NN summaries_NNS for_IN one\/multiple_JJ documents_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 21_CD -RRB-_-RRB- -RRB-_-RRB- ._.
The_DT summary_NN can_MD be_VB as_RB short_JJ as_IN titles_NNS =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, no_DT existing_VBG work_NN has_VBZ been_VBN done_VBN for_IN summarizing_VBG a_DT multinomial_JJ distribution_NN of_IN words_NNS ,_, or_CC a_DT statistical_JJ topic_NN model_NN ._.
Since_IN most_JJS topic_NN models_NNS assume_VBP that_IN a_DT document_NN covers_VBZ multiple_JJ topics_NNS ,_, it_PRP i_FW
WORK_VB To_TO the_DT best_JJS of_IN our_PRP$ knowledge_NN ,_, no_DT existing_VBG work_NN has_VBZ formally_RB studied_VBN the_DT problem_NN of_IN automatic_JJ labeling_NN of_IN multinomial_JJ topic_NN models_NNS ._.
There_EX has_VBZ been_VBN a_DT large_JJ body_NN of_IN work_NN on_IN statistical_JJ topic_NN models_NNS =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 3_CD ,_, 16_CD ,_, 18_CD ,_, 19_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_JJ -_: ,_, most_JJS of_IN which_WDT uses_VBZ a_DT multinomial_JJ word_NN distribution_NN to_TO represent_VB a_DT topic_NN ._.
In_IN some_DT recent_JJ work_NN ,_, -LRB-_-LRB- 23_CD -RRB-_-RRB- generalized_VBD the_DT representation_NN of_IN a_DT topic_NN model_NN as_IN a_DT multinomial_JJ distribution_NN over_IN ngrams_NNS ._.
Such_JJ top_NN
