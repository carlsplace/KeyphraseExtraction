Information_NNP theoretic_JJ regularization_NN for_IN semi-supervised_JJ boosting_VBG
We_PRP present_VBP novel_JJ semi-supervised_JJ boosting_VBG algorithms_NNS that_WDT incrementally_RB build_VBP linear_JJ combinations_NNS of_IN weak_JJ classifiers_NNS through_IN generic_JJ functional_JJ gradient_NN descent_NN using_VBG both_CC labeled_JJ and_CC unlabeled_JJ training_NN data_NNS ._.
Our_PRP$ approach_NN is_VBZ based_VBN on_IN extending_VBG information_NN regularization_NN framework_NN to_TO boosting_VBG ,_, bearing_VBG loss_NN functions_NNS that_WDT combine_VBP log_NN loss_NN on_IN labeled_JJ data_NNS with_IN the_DT information-theoretic_JJ measures_NNS to_TO encode_VB unlabeled_JJ data_NNS ._.
Even_RB though_IN the_DT information-theoretic_JJ regularization_NN terms_NNS make_VBP the_DT optimization_NN non-convex_JJ ,_, we_PRP propose_VBP simple_JJ sequential_JJ gradient_NN descent_NN optimization_NN algorithms_NNS ,_, and_CC obtain_VB impressively_RB improved_VBN results_NNS on_IN synthetic_JJ ,_, benchmark_JJ and_CC real_JJ world_NN tasks_NNS over_IN supervised_JJ boosting_VBG algorithms_NNS which_WDT use_VBP the_DT labeled_JJ data_NNS alone_RB and_CC a_DT state-of-the-art_JJ semi-supervised_JJ boosting_VBG algorithm_NN ._.
