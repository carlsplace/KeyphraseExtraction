Fast_JJ collapsed_JJ gibbs_NNS sampling_NN for_IN latent_JJ dirichlet_NN allocation_NN
In_IN this_DT paper_NN we_PRP introduce_VBP a_DT novel_JJ collapsed_JJ Gibbs_NNP sampling_NN method_NN for_IN the_DT widely_RB used_VBN latent_JJ Dirichlet_NNP allocation_NN -LRB-_-LRB- LDA_NN -RRB-_-RRB- model_NN ._.
Our_PRP$ new_JJ method_NN results_VBZ in_IN significant_JJ speedups_NNS on_IN real_JJ world_NN text_NN corpora_NN ._.
Conventional_JJ Gibbs_NNP sampling_NN schemes_NNS for_IN LDA_NNP require_VB O_NN -LRB-_-LRB- K_NN -RRB-_-RRB- operations_NNS per_IN sample_NN where_WRB K_NN is_VBZ the_DT number_NN of_IN topics_NNS in_IN the_DT model_NN ._.
Our_PRP$ proposed_VBN method_NN draws_VBZ equivalent_JJ samples_NNS but_CC requires_VBZ on_IN average_NN significantly_RB less_RBR then_RB K_NN operations_NNS per_IN sample_NN ._.
On_IN real-word_JJ corpora_NN FastLDA_NN can_MD be_VB as_RB much_JJ as_IN 8_CD times_NNS faster_RBR than_IN the_DT standard_JJ collapsed_JJ Gibbs_NNP sampler_NN for_IN LDA_NNP ._.
No_DT approximations_NNS are_VBP necessary_JJ ,_, and_CC we_PRP show_VBP that_IN our_PRP$ fast_JJ sampling_NN scheme_NN produces_VBZ exactly_RB the_DT same_JJ results_NNS as_IN the_DT standard_JJ -LRB-_-LRB- but_CC slower_JJR -RRB-_-RRB- sampling_NN scheme_NN ._.
Experiments_NNS on_IN four_CD real_JJ world_NN data_NNS sets_NNS demonstrate_VBP speedups_NNS for_IN a_DT wide_JJ range_NN of_IN collection_NN sizes_NNS ._.
For_IN the_DT PubMed_NNP collection_NN of_IN over_IN 8_CD million_CD documents_NNS with_IN a_DT required_JJ computation_NN time_NN of_IN 6_CD CPU_NNP months_NNS for_IN LDA_NNP ,_, our_PRP$ speedup_NN of_IN 5.7_CD can_MD save_VB 5_CD CPU_NNP months_NNS of_IN computation_NN ._.
the_DT data_NNS structure_NN -RRB-_-RRB- ._.
Accelerated_VBN algorithms_NNS of_IN this_DT type_NN exist_VBP for_IN many_JJ common_JJ probabilistic_JJ models_NNS ._.
In_IN some_DT cases_NNS ,_, such_JJ as_IN k-means_NN ,_, it_PRP is_VBZ possible_JJ to_TO accelerate_VB the_DT computation_NN of_IN an_DT exact_JJ solution_NN =_JJ -_: =[_NN 1_CD ,_, 16_CD ,_, 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN other_JJ algorithms_NNS ,_, such_JJ as_IN expectation_NN --_: maximization_NN for_IN Gaussian_JJ mixtures_NNS ,_, the_DT evaluations_NNS are_VBP only_RB approximate_JJ but_CC can_MD be_VB controlled_VBN by_IN tuning_NN a_DT quality_NN parameter_NN -LRB-_-LRB- 13_CD ,_, 10_CD ,_, 9_CD -RRB-_-RRB- ._.
In_IN -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, a_DT similar_JJ
j_NN 1_CD +_CC Wβ_NN -RRB-_-RRB- ,_, ._. ._.
,_, 1_CD \/_: -LRB-_-LRB- N_NN ¬_FW ij_FW K_NN +_CC Wβ_NN -RRB-_-RRB- -RRB-_-RRB- Then_RB ,_, the_DT normalization_NN constant_NN is_VBZ given_VBN by_IN Z_NN =_JJ X_NN ak_FW bk_FW ck_FW k_NN To_TO construct_VB an_DT initial_JJ upper_JJ bound_VBN Z0_NN on_IN Z_NN ,_, we_PRP turn_VBP to_TO the_DT generalized_VBN version_NN of_IN Hölder_NNP 's_POS inequality_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT states_VBZ Z0_NN =_JJ ‖_FW a_FW ‖_FW p_NN ‖_CD b_NN ‖_FW q_FW ‖_FW c_NN ‖_CD r_NN ≥_NN Z_NN where_WRB 1\/p_NN +_CC 1\/q_NN +_CC 1\/r_NN =_JJ 1_CD Notice_NNP that_IN ,_, as_IN we_PRP examine_VBP topics_NNS in_IN order_NN ,_, we_PRP learn_VBP the_DT actual_JJ value_NN of_IN the_DT product_NN ak_FW bk_FW ck_FW ._.
We_PRP can_MD use_VB these_DT calculations_NNS to_TO i_FW
supervised_VBN learning_NN methods_NNS ._.
Blei_NNP et_NNP al_NNP -LRB-_-LRB- 3_CD -RRB-_-RRB- introduced_VBD the_DT LDA_NNP model_NN within_IN a_DT general_JJ Bayesian_JJ framework_NN and_CC developed_VBD a_DT variational_JJ algorithm_NN for_IN learning_VBG the_DT model_NN from_IN data_NNS ._.
Griffiths_NNP and_CC Steyvers_NNP =_SYM -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: subsequently_RB proposed_VBN a_DT learning_NN algorithm_NN based_VBN on_IN collapsed_JJ Gibbs_NNP sampling_NN ._.
Both_CC the_DT variational_JJ and_CC Gibbs_NNP sampling_NN approaches_NNS have_VBP their_PRP$ advantages_NNS :_: the_DT variational_JJ approach_NN is_VBZ arguably_RB faster_JJR com_NN
ine_NN learning_NN and_CC data_NN mining_NN ,_, particularly_RB in_IN text_NN analysis_NN and_CC computer_NN vision_NN ,_, with_IN the_DT Gibbs_NNP sampling_NN algorithm_NN in_IN common_JJ use_NN ._.
For_IN example_NN ,_, Wei_NNP and_CC Croft_NNP -LRB-_-LRB- 19_CD -RRB-_-RRB- and_CC Chemudugunta_NNP ,_, Smyth_NNP ,_, and_CC Steyvers_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP successfully_RB applied_VBN the_DT LDA_NNP model_NN to_TO information_NN retrieval_NN and_CC shown_VBN that_IN it_PRP can_MD significantly_RB outperform_VB --_: in_IN terms_NNS of_IN precision-recall_JJ --_: alternative_JJ methods_NNS such_JJ as_IN latent_JJ semantic_JJ analysis_NN ._.
outperform_JJ --_: in_IN terms_NNS of_IN precision-recall_JJ --_: alternative_JJ methods_NNS such_JJ as_IN latent_JJ semantic_JJ analysis_NN ._.
LDA_NN models_NNS have_VBP also_RB been_VBN increasingly_RB applied_VBN to_TO problems_NNS involving_VBG very_RB large_JJ text_NN corpora_NN :_: Buntine_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Mimno_NNP and_CC McCallum_NNP -LRB-_-LRB- 12_CD -RRB-_-RRB- and_CC Newman_NNP et_FW al_FW -LRB-_-LRB- 15_CD -RRB-_-RRB- have_VBP all_DT used_VBN the_DT LDA_NNP model_NN to_TO automatically_RB generate_VB topic_NN models_NNS for_IN millions_NNS of_IN documents_NNS and_CC used_VBD these_DT models_NNS as_IN the_DT basis_NN for_IN automated_JJ indexing_NN a_DT
ty_NN distribution_NN over_IN words_NNS ._.
The_DT mixing_VBG coefficients_NNS for_IN each_DT document_NN and_CC the_DT wordtopic_JJ distributions_NNS are_VBP unobserved_JJ -LRB-_-LRB- hidden_JJ -RRB-_-RRB- and_CC are_VBP learned_VBN from_IN data_NNS using_VBG unsupervised_JJ learning_NN methods_NNS ._.
Blei_NNP et_NNP al_NNP =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: introduced_VBD the_DT LDA_NNP model_NN within_IN a_DT general_JJ Bayesian_JJ framework_NN and_CC developed_VBD a_DT variational_JJ algorithm_NN for_IN learning_VBG the_DT model_NN from_IN data_NNS ._.
Griffiths_NNP and_CC Steyvers_NNP -LRB-_-LRB- 6_CD -RRB-_-RRB- subsequently_RB proposed_VBD a_DT learning_NN algori_NN
