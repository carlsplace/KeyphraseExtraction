Using_VBG ghost_NN edges_NNS for_IN classification_NN in_IN sparsely_RB labeled_VBN networks_NNS
We_PRP address_VBP the_DT problem_NN of_IN classification_NN in_IN partially_RB labeled_VBN networks_NNS -LRB-_-LRB- a.k.a._FW within-network_JJ classification_NN -RRB-_-RRB- where_WRB observed_VBN class_NN labels_NNS are_VBP sparse_JJ ._.
Techniques_NNS for_IN statistical_JJ relational_JJ learning_NN have_VBP been_VBN shown_VBN to_TO perform_VB well_RB on_IN network_NN classification_NN tasks_NNS by_IN exploiting_VBG dependencies_NNS between_IN class_NN labels_NNS of_IN neighboring_JJ nodes_NNS ._.
However_RB ,_, relational_JJ classifiers_NNS can_MD fail_VB when_WRB unlabeled_JJ nodes_NNS have_VBP too_RB few_JJ labeled_JJ neighbors_NNS to_TO support_VB learning_NN -LRB-_-LRB- during_IN training_NN phase_NN -RRB-_-RRB- and\/or_CC inference_NN -LRB-_-LRB- during_IN testing_NN phase_NN -RRB-_-RRB- ._.
This_DT situation_NN arises_VBZ in_IN real-world_JJ problems_NNS when_WRB observed_VBN labels_NNS are_VBP sparse_JJ ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT novel_JJ approach_NN to_TO within-network_JJ classification_NN that_WDT combines_VBZ aspects_NNS of_IN statistical_JJ relational_JJ learning_NN and_CC semi-supervised_JJ learning_NN to_TO improve_VB classification_NN performance_NN in_IN sparse_JJ networks_NNS ._.
Our_PRP$ approach_NN works_VBZ by_IN adding_VBG ``_`` ghost_NN edges_NNS ''_'' to_TO a_DT network_NN ,_, which_WDT enable_VBP the_DT flow_NN of_IN information_NN from_IN labeled_VBN to_TO unlabeled_JJ nodes_NNS ._.
Through_IN experiments_NNS on_IN real-world_JJ data_NNS sets_NNS ,_, we_PRP demonstrate_VBP that_IN our_PRP$ approach_NN performs_VBZ well_RB across_IN a_DT range_NN of_IN conditions_NNS where_WRB existing_VBG approaches_NNS ,_, such_JJ as_IN collective_JJ classification_NN and_CC semi-supervised_JJ learning_NN ,_, fail_VBP ._.
On_IN all_DT tasks_NNS ,_, our_PRP$ approach_NN improves_VBZ area_NN under_IN the_DT ROC_NN curve_NN -LRB-_-LRB- AUC_NN -RRB-_-RRB- by_IN up_IN to_TO 15_CD points_NNS over_IN existing_VBG approaches_NNS ._.
Furthermore_RB ,_, we_PRP demonstrate_VBP that_IN our_PRP$ approach_NN runs_VBZ in_IN time_NN proportional_JJ to_TO L_NNP •_NNP E_NNP ,_, where_WRB L_NN is_VBZ the_DT number_NN of_IN labeled_JJ nodes_NNS and_CC E_NN is_VBZ the_DT number_NN of_IN edges_NNS ._.
ing_NN estimates_NNS of_IN neighboring_VBG labels_NNS to_TO influence_VB one_CD another_DT ._.
Representative_JJ work_NN in_IN this_DT line_NN started_VBD with_IN the_DT seminal_JJ paper_NN of_IN Chakrabarti_NNP et_FW al._FW on_IN using_VBG hyperlinks_NNS to_TO for_IN hypertext_NN classification_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Sen_NNP et_FW al._FW -LRB-_-LRB- 17_CD -RRB-_-RRB- provide_VBP a_DT careful_JJ empirical_JJ study_NN of_IN the_DT various_JJ procedures_NNS for_IN collective_JJ inference_NN ._.
Macskassy_NNP and_CC Provost_NNP -LRB-_-LRB- 12_CD -RRB-_-RRB- provide_VBP a_DT nice_JJ case-study_NN of_IN previous_JJ work_NN in_IN learning_VBG attributes_NNS of_IN
eight_CD features_NNS based_VBN on_IN their_PRP$ predictiveness_NN ._.
We_PRP achieved_VBD better_JJR results_NNS using_VBG an_DT ensemble_NN of_IN LR_NN models_NNS we_PRP refer_VBP to_TO as_IN logForest_NN ._.
The_DT logForest_NN model_NN is_VBZ inspired_VBN by_IN Breiman_NNP 's_POS Random_NNP Forest_NNP classifier_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP use_VBP a_DT bag_NN of_IN LR_NN classifiers_NNS ,_, where_WRB each_DT is_VBZ given_VBN a_DT subset_NN of_IN log_NN -LRB-_-LRB- M_NN -RRB-_-RRB- +_CC 1_CD of_IN the_DT M_NN total_JJ features_NNS ._.
For_IN this_DT study_NN ,_, our_PRP$ logForest_NN model_NN uses_VBZ 500_CD LR_NN classifiers_NNS ._.
GhostEdgeL_NN divides_VBZ ghost_NN edges_NNS into_IN s_NN
assification_NN To_TO perform_VB collective_JJ classification_NN ,_, we_PRP use_VBP both_CC the_DT iterative_JJ classification_NN algorithm_NN -LRB-_-LRB- ICA_NN -RRB-_-RRB- and_CC relaxation_NN labeling_NN -LRB-_-LRB- RL_NN -RRB-_-RRB- -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
We_PRP also_RB ran_VBD preliminary_JJ experiments_NNS using_VBG Gibbs_NNP sampling_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT yielded_VBD results_NNS comparable_JJ to_TO ICA_NNP ._.
This_DT is_VBZ consistent_JJ with_IN findings_NNS of_IN other_JJ researchers_NNS -LRB-_-LRB- 12_CD ,_, 17_CD -RRB-_-RRB- ._.
In_IN our_PRP$ experiments_NNS ,_, the_DT logForest_NN classifier_NN performed_VBD better_JJR overall_NN using_VBG ICA_NNP and_CC the_DT wvRN_NN
is_VBZ defined_VBN as_IN :_: ri_NN -LRB-_-LRB- t_NN +_CC 1_LS -RRB-_-RRB- =_JJ cA_NN 2_CD ri_NN -LRB-_-LRB- t_NN -RRB-_-RRB- +_CC -LRB-_-LRB- 1_CD −_NN c_NN -RRB-_-RRB- ei_FW where_WRB ei_FW is_VBZ the_DT starting_VBG vector_NN for_IN the_DT node_NN i_FW ,_, c_NN is_VBZ the_DT fly-out_JJ probability_NN ,_, t_NN is_VBZ the_DT iteration_NN number_NN ,_, and_CC A_DT is_VBZ the_DT normalized_VBN graph_NN Laplacian_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2_CD We_PRP can_MD use_VB the_DT following_JJ iterative_JJ strategy_NN :_: For_IN t_NN =_JJ 2_CD ,_, 3_CD ,_, ..._: ,_, do_VBP the_DT following_JJ two_CD steps_NNS :_: ri_FW -LRB-_-LRB- 1_LS -RRB-_-RRB- =_JJ ei_FW -LRB-_-LRB- 1_LS -RRB-_-RRB- ri_NN -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ A_NN ri_NN -LRB-_-LRB- t_NN −_NN 1_CD -RRB-_-RRB- -LRB-_-LRB- 2_LS -RRB-_-RRB- ri_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ←_FW cA_FW ri_FW -LRB-_-LRB- t_NN -RRB-_-RRB- +_CC -LRB-_-LRB- 1_CD −_NN c_NN -RRB-_-RRB- ei_FW -LRB-_-LRB- 3_LS -RRB-_-RRB- The_DT complexity_NN for_IN each_DT ste_NN
l_NN label_NN consistency_NN -LRB-_-LRB- i.e._FW ,_, homophily_RB -RRB-_-RRB- and_CC lack_NN of_IN it_PRP ?_.
4.1_CD Data_NN Sets_VBZ We_PRP present_VBP results_NNS on_IN four_CD real-world_JJ data_NNS sets_NNS :_: political_JJ book_NN co-purchases_NNS -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, Enron_NNP emails_NNS -LRB-_-LRB- 4_CD -RRB-_-RRB- ,_, Reality_NN Mining_NN cell-phone_NN calls_VBZ =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC high-energy_JJ physics_NN citations_NNS from_IN arXiv_NN -LRB-_-LRB- a.k.a._NN HEP-TH_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Our_PRP$ tasks_NNS are_VBP to_TO identify_VB neutral_JJ political_JJ books_NNS ,_, Enron_NNP executives_NNS ,_, Reality_NN Mining_NN study_NN participants_NNS ,_, and_CC HEP-TH_NN papers_NNS with_IN the_DT t_NN
etition_NN ?_.
•_NN What_WP is_VBZ the_DT impact_NN of_IN local_JJ label_NN consistency_NN -LRB-_-LRB- i.e._FW ,_, homophily_RB -RRB-_-RRB- and_CC lack_NN of_IN it_PRP ?_.
4.1_CD Data_NN Sets_VBZ We_PRP present_VBP results_NNS on_IN four_CD real-world_JJ data_NNS sets_NNS :_: political_JJ book_NN co-purchases_NNS -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, Enron_NNP emails_NNS =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Reality_NN Mining_NN cell-phone_NN calls_NNS -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, and_CC high-energy_JJ physics_NN citations_NNS from_IN arXiv_NN -LRB-_-LRB- a.k.a._NN HEP-TH_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Our_PRP$ tasks_NNS are_VBP to_TO identify_VB neutral_JJ political_JJ books_NNS ,_, Enron_NNP executives_NNS ,_, Reality_NN Mining_NN study_NN parti_NNS
tently_RB well_RB regardless_RB of_IN the_DT degree_NN of_IN local_JJ consistency_NN in_IN the_DT data_NN set_NN ._.
2_CD ._.
RELATED_NNS WORK_VBP In_IN recent_JJ years_NNS ,_, there_EX has_VBZ been_VBN a_DT great_JJ deal_NN of_IN work_NN on_IN models_NNS for_IN learning_NN and_CC inference_NN in_IN relational_JJ data_NN =_JJ -_: =[_NN 7_CD ,_, 10_CD ,_, 11_CD ,_, 16_CD ,_, 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN within-network_JJ classification_NN tasks_NNS where_WRB we_PRP have_VBP sparse_JJ labels_NNS ,_, we_PRP categorize_VBP the_DT previous_JJ work_NN into_IN two_CD main_JJ groups_NNS :_: collective_JJ classification_NN and_CC graph-based_JJ semi-supervised_JJ learning_NN ._.
Collect_VB
