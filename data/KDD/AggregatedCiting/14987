Accurate_JJ decision_NN trees_NNS for_IN mining_NN high-speed_JJ data_NNS streams_NNS
In_IN this_DT paper_NN we_PRP study_VBD the_DT problem_NN of_IN constructing_VBG accurate_JJ decision_NN tree_NN models_NNS from_IN data_NNS streams_NNS ._.
Data_NN streams_NNS are_VBP incremental_JJ tasks_NNS that_WDT require_VBP incremental_JJ ,_, online_NN ,_, and_CC any-time_JJ learning_NN algorithms_NNS ._.
One_CD of_IN the_DT most_RBS successful_JJ algorithms_NNS for_IN mining_NN data_NNS streams_NNS is_VBZ VFDT_NNP ._.
In_IN this_DT paper_NN we_PRP extend_VBP the_DT VFDT_NN system_NN in_IN two_CD directions_NNS :_: the_DT ability_NN to_TO deal_VB with_IN continuous_JJ data_NNS and_CC the_DT use_NN of_IN more_RBR powerful_JJ classification_NN techniques_NNS at_IN tree_NN leaves_NNS ._.
The_DT proposed_VBN system_NN ,_, VFDTc_NN ,_, can_MD incorporate_VB and_CC classify_VB new_JJ information_NN online_NN ,_, with_IN a_DT single_JJ scan_VB of_IN the_DT data_NNS ,_, in_IN time_NN constant_JJ per_IN example_NN ._.
The_DT most_RBS relevant_JJ property_NN of_IN our_PRP$ system_NN is_VBZ the_DT ability_NN to_TO obtain_VB a_DT performance_NN similar_JJ to_TO a_DT standard_JJ decision_NN tree_NN algorithm_NN even_RB for_IN medium_NN size_NN datasets_NNS ._.
This_DT is_VBZ relevant_JJ due_JJ to_TO the_DT any-time_JJ property_NN ._.
We_PRP study_VBD the_DT behavior_NN of_IN VFDTc_NN in_IN different_JJ problems_NNS and_CC demonstrate_VB its_PRP$ utility_NN in_IN large_JJ and_CC medium_JJ data_NNS sets_NNS ._.
Under_IN a_DT bias-variance_JJ analysis_NN we_PRP observe_VBP that_IN VFDTc_NN in_IN comparison_NN to_TO C4_NN .5_CD is_VBZ able_JJ to_TO reduce_VB the_DT variance_NN component_NN ._.
arch_NN lines_NNS ._.
The_DT first_JJ one_CD ,_, a_DT tree_NN is_VBZ constructed_VBN using_VBG a_DT greedy_JJ search_NN ._.
Incorporation_NN of_IN new_JJ information_NN involves_VBZ the_DT re-structuring_NN the_DT actual_JJ tree_NN ._.
This_DT is_VBZ the_DT case_NN of_IN systems_NNS like_IN ITI_NNP -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, or_CC ID5R_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT second_JJ research_NN line_NN does_VBZ not_RB use_VB the_DT greedy_JJ search_NN of_IN standard_JJ tree_NN induction_NN ._.
It_PRP maintains_VBZ a_DT set_NN of_IN sufficient_JJ statistics_NNS at_IN each_DT decision_NN node_NN and_CC only_RB make_VB a_DT decision_NN ,_, i._NNP e._NNP ,_, install_VBP a_DT split_NN
two_CD main_JJ research_NN lines_NNS ._.
The_DT first_JJ one_CD ,_, a_DT tree_NN is_VBZ constructed_VBN using_VBG a_DT greedy_JJ search_NN ._.
Incorporation_NN of_IN new_JJ information_NN involves_VBZ the_DT re-structuring_NN the_DT actual_JJ tree_NN ._.
This_DT is_VBZ the_DT case_NN of_IN systems_NNS like_IN ITI_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, or_CC ID5R_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
The_DT second_JJ research_NN line_NN does_VBZ not_RB use_VB the_DT greedy_JJ search_NN of_IN standard_JJ tree_NN induction_NN ._.
It_PRP maintains_VBZ a_DT set_NN of_IN sufficient_JJ statistics_NNS at_IN each_DT decision_NN node_NN and_CC only_RB make_VB a_DT decision_NN ,_, i._FW e._FW ,_, ins_NNS
n_NN node_NN a_DT set_NN of_IN sufficient_JJ statistics_NNS and_CC only_RB make_VB a_DT decision_NN -LRB-_-LRB- install_VB a_DT split-test_NN in_IN that_DT node_NN -RRB-_-RRB- ,_, when_WRB there_EX is_VBZ enough_JJ statistical_JJ evidence_NN in_IN favor_NN of_IN a_DT particular_JJ split_NN test_NN ._.
This_DT is_VBZ the_DT case_NN of_IN =_JJ -_: =[_NN 6_CD ,_, 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT paper_NN we_PRP argue_VBP that_IN incremental_JJ tree_NN induction_NN methods_NNS that_WDT only_RB install_VBP a_DT split-test_NN when_WRB there_EX is_VBZ enough_JJ statistical_JJ support_NN ,_, will_MD large_RB benefit_VB of_IN using_VBG more_RBR appropriate_JJ classification_NN st_IN
g_NN ,_, the_DT constant_NN that_WDT minimizes_VBZ the_DT 0-1_CD loss_NN function_NN is_VBZ the_DT mode_NN of_IN the_DT target_NN attribute_NN of_IN the_DT examples_NNS that_WDT fall_VBP at_IN this_DT leaf_NN ._.
Several_JJ authors_NNS have_VBP studied_VBN the_DT use_NN of_IN other_JJ functions_NNS at_IN tree_NN leaves_VBZ =_JJ -_: =[_NN 9_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
One_CD of_IN the_DT earliest_JJS works_NNS is_VBZ the_DT Perceptron_NNP tree_NN algorithm_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- where_WRB leaf_NN nodes_NNS may_MD implement_VB a_DT general_JJ linear_JJ discriminant_JJ function_NN ._.
Also_RB Kohavi_NNP -LRB-_-LRB- 9_CD -RRB-_-RRB- has_VBZ presented_VBN the_DT naive_JJ Bayes_NNP tree_NN that_IN uses_NNS fu_VBP
the_DT overhead_NN is_VBZ more_RBR visible_JJ for_IN reduced_VBN number_NN of_IN training_NN examples_NNS -LRB-_-LRB- Figure_NNP 5_CD -RRB-_-RRB- ._.
From_IN now_RB on_IN we_PRP focus_VBP our_PRP$ analysis_NN in_IN VFDTcNB_NN ._.
C4_NN .5_CD versus_CC VFDTcNB_NN In_IN this_DT subsection_NN ,_, we_PRP compare_VBP VFDTcNB_NN against_IN C4_NN .5_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
VFDTc_NN as_IN VFDT_NN was_VBD designed_VBN for_IN fast_JJ induction_NN of_IN interpretable_JJ ,_, and_CC accurate_JJ models_NNS from_IN large_JJ data_NNS streams_NNS using_VBG one_CD scan_VB of_IN the_DT data_NNS ._.
The_DT motivation_NN for_IN these_DT experiments_NNS is_VBZ the_DT comparison_NN of_IN the_DT
n_NN node_NN a_DT set_NN of_IN sufficient_JJ statistics_NNS and_CC only_RB make_VB a_DT decision_NN -LRB-_-LRB- install_VB a_DT split-test_NN in_IN that_DT node_NN -RRB-_-RRB- ,_, when_WRB there_EX is_VBZ enough_JJ statistical_JJ evidence_NN in_IN favor_NN of_IN a_DT particular_JJ split_NN test_NN ._.
This_DT is_VBZ the_DT case_NN of_IN =_JJ -_: =[_NN 6_CD ,_, 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT paper_NN we_PRP argue_VBP that_IN incremental_JJ tree_NN induction_NN methods_NNS that_WDT only_RB install_VBP a_DT split-test_NN when_WRB there_EX is_VBZ enough_JJ statistical_JJ support_NN ,_, will_MD large_RB benefit_VB of_IN using_VBG more_RBR appropriate_JJ classification_NN st_IN
the_DT target_NN attribute_NN of_IN the_DT examples_NNS that_WDT fall_VBP at_IN this_DT leaf_NN ._.
Several_JJ authors_NNS have_VBP studied_VBN the_DT use_NN of_IN other_JJ functions_NNS at_IN tree_NN leaves_NNS -LRB-_-LRB- 9_CD ,_, 5_CD -RRB-_-RRB- ._.
One_CD of_IN the_DT earliest_JJS works_NNS is_VBZ the_DT Perceptron_NNP tree_NN algorithm_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: where_WRB leaf_NN nodes_NNS may_MD implement_VB a_DT general_JJ linear_JJ discriminant_JJ function_NN ._.
Also_RB Kohavi_NNP -LRB-_-LRB- 9_CD -RRB-_-RRB- has_VBZ presented_VBN the_DT naive_JJ Bayes_NNP tree_NN that_WDT uses_VBZ functional_JJ leaves_NNS ._.
NBtree_NN is_VBZ a_DT hybrid_NN algorithm_NN that_WDT generates_VBZ a_DT re_NN
here_RB is_VBZ a_DT much_RB better_JJR exploitation_NN of_IN the_DT available_JJ information_NN at_IN each_DT leaf_NN ._.
Moreover_RB ,_, naive_JJ Bayes_NNP is_VBZ naturally_RB incremental_JJ ._.
It_PRP deals_VBZ with_IN heterogeneous_JJ data_NNS and_CC missing_VBG values_NNS ._.
It_PRP has_VBZ been_VBN observed_VBN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: that_IN for_IN small_JJ datasets_NNS naive_JJ Bayes_NNP is_VBZ a_DT very_RB competitive_JJ algorithm_NN ._.
Given_VBN the_DT example_NN −_FW →_FW e_LS =_JJ -LRB-_-LRB- x1_NN ,_, ..._: ,_, xj_NN -RRB-_-RRB- and_CC applying_VBG Bayes_NNP theorem_NN ,_, we_PRP obtain_VBP :_: P_NN -LRB-_-LRB- Ck_NN |_FW −_FW →_FW e_LS -RRB-_-RRB- ∝_NN P_NN -LRB-_-LRB- C_NN k_NN -RRB-_-RRB- P_NN -LRB-_-LRB- −_FW →_FW e_LS -RRB-_-RRB- P_NN -LRB-_-LRB- xj_FW |_FW Ck_NN -RRB-_-RRB- ._.
To_TO c_NN
g_NN ,_, the_DT constant_NN that_WDT minimizes_VBZ the_DT 0-1_CD loss_NN function_NN is_VBZ the_DT mode_NN of_IN the_DT target_NN attribute_NN of_IN the_DT examples_NNS that_WDT fall_VBP at_IN this_DT leaf_NN ._.
Several_JJ authors_NNS have_VBP studied_VBN the_DT use_NN of_IN other_JJ functions_NNS at_IN tree_NN leaves_VBZ =_JJ -_: =[_NN 9_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
One_CD of_IN the_DT earliest_JJS works_NNS is_VBZ the_DT Perceptron_NNP tree_NN algorithm_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- where_WRB leaf_NN nodes_NNS may_MD implement_VB a_DT general_JJ linear_JJ discriminant_JJ function_NN ._.
Also_RB Kohavi_NNP -LRB-_-LRB- 9_CD -RRB-_-RRB- has_VBZ presented_VBN the_DT naive_JJ Bayes_NNP tree_NN that_IN uses_NNS fu_VBP
the_DT application_NN phase_NN is_VBZ the_DT most_RBS important_JJ factor_NN ._.
Bias-Variance_NNP Decomposition_NNP of_IN the_DT Error_NNP An_NNP interesting_JJ analysis_NN of_IN the_DT classification_NN error_NN is_VBZ given_VBN by_IN the_DT so-called_JJ Bias-Variance_NN decomposition_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Several_JJ authors_NNS refer_VBP that_IN there_EX is_VBZ a_DT trade-off_NN between_IN the_DT systematic_JJ errors_NNS due_JJ to_TO the_DT representational_JJ language_NN used_VBN by_IN an_DT algorithm_NN -LRB-_-LRB- the_DT bias_NN -RRB-_-RRB- and_CC the_DT variance_NN due_JJ to_TO the_DT dependence_NN of_IN the_DT model_NN
The_DT bias-variance_JJ analysis_NN shows_VBZ that_IN VFDTcNB_NN generates_VBZ very_RB stable_JJ predictive_JJ models_NNS concerning_VBG variations_NNS of_IN the_DT training_NN set_NN ._.
In_IN this_DT paper_NN ,_, we_PRP do_VBP not_RB discuss_VB the_DT problem_NN of_IN timechanging_JJ concepts_NNS =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Nevertheless_RB ,_, our_PRP$ extensions_NNS could_MD be_VB applied_VBN to_TO any_DT strategy_NN that_WDT takes_VBZ into_IN account_NN concept_NN drift_NN ._.
Acknowledgments_NNS :_: The_DT authors_NNS reveal_VBP its_PRP$ gratitude_NN to_TO the_DT financial_JJ support_NN given_VBN by_IN the_DT FEDER_NN -LRB-_-LRB- Pl_NN
