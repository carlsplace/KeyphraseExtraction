Formulating_VBG distance_NN functions_NNS via_IN the_DT kernel_NN trick_NN
Tasks_NNS of_IN data_NNS mining_NN and_CC information_NN retrieval_NN depend_VBP on_IN a_DT good_JJ distance_NN function_NN for_IN measuring_VBG similarity_NN between_IN data_NNS instances_NNS ._.
The_DT most_RBS effective_JJ distance_NN function_NN must_MD be_VB formulated_VBN in_IN a_DT context-dependent_JJ -LRB-_-LRB- also_RB application_NN -_: ,_, data_NNS -_: ,_, and_CC user-dependent_JJ -RRB-_-RRB- way_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP to_TO learn_VB a_DT distance_NN function_NN by_IN capturing_VBG the_DT nonlinear_JJ relationships_NNS among_IN contextual_JJ information_NN provided_VBN by_IN the_DT application_NN ,_, data_NNS ,_, or_CC user_NN ._.
We_PRP show_VBP that_IN through_IN a_DT process_NN called_VBN the_DT ``_`` kernel_NN trick_NN ,_, ''_'' such_JJ nonlinear_JJ relationships_NNS can_MD be_VB learned_VBN efficiently_RB in_IN a_DT projected_JJ space_NN ._.
Theoretically_RB ,_, we_PRP substantiate_VBP that_IN our_PRP$ method_NN is_VBZ both_CC sound_JJ and_CC optimal_JJ ._.
Empirically_RB ,_, using_VBG several_JJ datasets_NNS and_CC applications_NNS ,_, we_PRP demonstrate_VBP that_IN our_PRP$ method_NN is_VBZ effective_JJ and_CC useful_JJ ._.
2.2_CD Distance_NNP Metric_NNP Learning_NNP In_IN this_DT subsection_NN ,_, we_PRP show_VBP how_WRB to_TO generalize_VB the_DT model_NN in_IN Eqn_NN ._.
13_CD to_TO unseen_JJ instances_NNS without_IN overfitting_VBG the_DT contextual_JJ information_NN ._.
We_PRP use_VBP the_DT feature-weighting_JJ method_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: by_IN modifying_VBG the_DT inner_JJ product_NN kij_NN =_JJ φ_NN T_NN i_FW φj_FW in_IN P_NN as_IN φ_NN T_NN i_FW AA_FW T_NN φj_NN ._.
Here_RB ,_, Am_VBP ′_FW ×_FW m_NN ′_NN is_VBZ the_DT weighting_NN matrix_NN and_CC m_NN ′_NN is_VBZ the_DT dimension_NN of_IN P._NNP Based_VBN on_IN the_DT idea_NN of_IN feature_NN reduction_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, we_PRP aim_VBP to_TO achi_VB
ely_RB and_CC qualitatively_RB different_JJ contextual_JJ information_NN for_IN learning_NN ._.
2_CD ._.
Learning_NNP effectiveness_NN ._.
We_PRP compared_VBD our_PRP$ DAlign_NN algorithm_NN with_IN the_DT regular_JJ Euclidean_JJ metric_NN ,_, Kwok_NNP et_FW al._FW -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, and_CC Xing_NNP et_FW al._FW 's_POS =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: on_IN classification_NN and_CC clustering_NN applications_NNS ._.
To_TO conduct_VB our_PRP$ experiments_NNS ,_, we_PRP used_VBD five_CD datasets_NNS :_: one_CD toy_NN dataset_NN ,_, and_CC four_CD UCI_NNP benchmark_NN datasets_NNS ._.
One_CD toy_NN dataset_NN The_DT toy_NN dataset_NN was_VBD first_RB introduce_VB
,_, π_NN -RRB-_-RRB- is_VBZ maximal_JJ at_IN π_NN =_JJ 0_CD ._.
Now_RB ,_, the_DT dual_JJ formulation_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- becomes_VBZ a_DT convex_NN quadratic_JJ function_NN w.r.t._NN only_JJ αij_NN ._.
Next_RB ,_, we_PRP examine_VBP the_DT constraints_NNS of_IN dual_JJ formulation_NN on_IN αij_NN ._.
According_VBG to_TO the_DT KKT_NN theorem_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP have_VBP the_DT following_JJ conditions_NNS h_NN αij_FW β2d_FW 2_CD ij_FW −_FW ˜_FW d_NN 2_CD i_FW ij_FW =_JJ 0_CD ,_, -LRB-_-LRB- xi_NN ,_, xj_NN -RRB-_-RRB- ∈_NN S_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- h_NN ˜d_NN 2_CD ij_FW −_FW β1d_NN 2_CD i_FW ij_FW −_NN 2_CD -LRB-_-LRB- 1_CD −_NN β1_NN -RRB-_-RRB- =_JJ 0_CD ,_, -LRB-_-LRB- xi_NN ,_, xj_NN -RRB-_-RRB- ∈_NN D_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- αij_FW γ_FW -LRB-_-LRB- β2_FW −_FW β1_NN -RRB-_-RRB- =_JJ 0_CD ,_, -LRB-_-LRB- 23_CD -RRB-_-RRB- µβ1_NN =_JJ 0_CD ,_, -LRB-_-LRB- 24_CD -RRB-_-RRB- π_NN -LRB-_-LRB- 1_CD −_NN β2_NN -RRB-_-RRB- =_JJ 0_CD ._.
-LRB-_-LRB- 25_CD -RRB-_-RRB-
data_NNS ,_, and_CC user_NN -LRB-_-LRB- hereafter_RB we_PRP refer_VBP to_TO these_DT factors_NNS as_IN contextual_JJ information_NN -RRB-_-RRB- ._.
The_DT quality_NN of_IN the_DT distance_NN function_NN significantly_RB affects_VBZ the_DT success_NN in_IN organizing_VBG data_NNS or_CC finding_VBG meaningful_JJ results_NNS =_JJ -_: =[_NN 1_CD ,_, 2_CD ,_, 5_CD ,_, 9_CD ,_, 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
How_WRB do_VBP we_PRP consider_VB contextual_JJ information_NN in_IN formulating_VBG a_DT good_JJ distance_NN function_NN ?_.
One_CD extension_NN of_IN the_DT popular_JJ Euclidean_JJ Permission_NN to_TO make_VB digital_JJ or_CC hard_JJ copies_NNS of_IN all_DT or_CC part_NN of_IN this_DT work_NN for_IN pe_NN
Science_NNP University_NNP of_IN California_NNP Santa_NNP Barbara_NNP ,_, CA_NNP panda@cs.ucsb.edu_NN distance_NN -LRB-_-LRB- or_CC more_RBR generally_RB ,_, the_DT Lp-norm_NN -RRB-_-RRB- is_VBZ to_TO weight_VB the_DT data_NNS attributes_NNS -LRB-_-LRB- features_NNS -RRB-_-RRB- based_VBN on_IN their_PRP$ importance_NN for_IN a_DT target_NN task_NN =_JJ -_: =[_NN 2_CD ,_, 9_CD ,_, 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN example_NN ,_, for_IN answering_VBG an_DT ocean_NN image-query_NN ,_, color_NN features_NNS should_MD be_VB weighted_VBN higher_JJR ._.
For_IN answering_VBG an_DT architecture_NN image-query_NN ,_, shape_NN and_CC texture_NN features_NNS may_MD be_VB more_RBR important_JJ ._.
Weighting_NN thes_NNS
datasets_NNS ._.
For_IN example_NN ,_, it_PRP has_VBZ been_VBN widely_RB acknowledged_VBN in_IN the_DT image\/video_JJ retrieval_NN domain_NN that_IN a_DT query_NN concept_NN is_VBZ typically_RB a_DT nonlinear_JJ combination_NN of_IN perceptual_JJ features_NNS -LRB-_-LRB- color_NN ,_, texture_NN ,_, and_CC shape_NN -RRB-_-RRB- =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT paper_NN we_PRP propose_VBP performing_VBG a_DT nonlinear_JJ transformation_NN on_IN the_DT feature_NN space_NN to_TO gain_VB greater_JJR flexibility_NN for_IN mapping_NN features_NNS to_TO semantics_NNS ._.
We_PRP name_VBP our_PRP$ method_NN distance-function_NN alignment_NN -LRB-_-LRB- DAl_NN
orresponds_NNS to_TO the_DT kernel_NN matrix_NN ˜_NN K_NN ,_, associated_VBN with_IN the_DT training_NN set_NN X_NN ,_, in_IN Eqn_NNP ._.
3_LS ._.
If_IN the_DT prior_JJ K_NN is_VBZ positive_JJ -LRB-_-LRB- semi_JJ -_: -RRB-_-RRB- definite_JJ ,_, using_VBG the_DT fact_NN that_IN the_DT ideal_JJ kernel_NN K_NNP ∗_NNP is_VBZ positive_JJ -LRB-_-LRB- semi_JJ -_: -RRB-_-RRB- definite_JJ =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP can_MD derive_VB that_IN ˜_NNP K_NNP in_IN Eqn_NNP ._.
4_CD is_VBZ also_RB positive_JJ -LRB-_-LRB- semi_JJ -_: -RRB-_-RRB- definite_JJ if_IN 0_CD ≤_FW β1_FW ≤_FW β2_FW ≤_NN 1_CD ._.
Here_RB ,_, we_PRP use_VBP the_DT closure_NN properties_NNS of_IN kernels_NNS ,_, namely_RB that_IN the_DT product_NN and_CC summation_NN of_IN valid_JJ kernels_NNS also_RB giv_VBP
data_NNS ,_, and_CC user_NN -LRB-_-LRB- hereafter_RB we_PRP refer_VBP to_TO these_DT factors_NNS as_IN contextual_JJ information_NN -RRB-_-RRB- ._.
The_DT quality_NN of_IN the_DT distance_NN function_NN significantly_RB affects_VBZ the_DT success_NN in_IN organizing_VBG data_NNS or_CC finding_VBG meaningful_JJ results_NNS =_JJ -_: =[_NN 1_CD ,_, 2_CD ,_, 5_CD ,_, 9_CD ,_, 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
How_WRB do_VBP we_PRP consider_VB contextual_JJ information_NN in_IN formulating_VBG a_DT good_JJ distance_NN function_NN ?_.
One_CD extension_NN of_IN the_DT popular_JJ Euclidean_JJ Permission_NN to_TO make_VB digital_JJ or_CC hard_JJ copies_NNS of_IN all_DT or_CC part_NN of_IN this_DT work_NN for_IN pe_NN
nduces_VBZ an_DT NP-Hard_JJ problem_NN by_IN directly_RB minimizing_VBG rank_NN -LRB-_-LRB- A_NN -RRB-_-RRB- ,_, the_DT so-called_JJ zero-norm_NN problem_NN in_IN -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
Since_IN minimizing_VBG the_DT trace_NN of_IN a_DT matrix_NN tends_VBZ to_TO give_VB a_DT low-rank_JJ solution_NN when_WRB the_DT matrix_NN is_VBZ symmetric_JJ =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP approximate_JJ the_DT rank_NN of_IN a_DT matrix_NN by_IN its_PRP$ trace_NN ._.
Moreover_RB ,_, since_IN rank_NN -LRB-_-LRB- AA_NN T_NN -RRB-_-RRB- =_JJ rank_NN -LRB-_-LRB- AA_NN T_NN AA_NN T_NN -RRB-_-RRB- ≈_NN trace_NN -LRB-_-LRB- AA_NN T_NN AA_NN T_NN -RRB-_-RRB- =_JJ AA_NN T_NN 2_CD F_NN ,_, we_PRP approximate_JJ the_DT problem_NN of_IN minimizing_VBG the_DT rank_NN of_IN A_NN by_IN minimizi_NN
ontrary_JJ ,_, β1_NN =_JJ β2_NN =_JJ 0_CD would_MD overfit_VB the_DT training_NN dataset_NN ._.
We_PRP hence_RB add_VBP two_CD penalty_NN terms_NNS CDβ1_NN and_CC CSβ2_NN to_TO control_VB the_DT alignment_NN degree_NN ._.
This_DT strategy_NN is_VBZ similar_JJ to_TO that_DT used_VBN in_IN Support_NN Vector_NNP Machines_NNP =_SYM -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT limits_VBZ the_DT length_NN of_IN weight_NN vector_NN w_NN 2_CD in_IN projected_VBN space_NN P_NN to_TO combat_VB the_DT overfitting_NN problem_NN ._.
The_DT constrained_VBN optimization_NN problem_NN above_IN can_MD be_VB solved_VBN by_IN considering_VBG the_DT corresponding_JJ Lagra_NNP
algorithm_NN -LRB-_-LRB- DAlign_NN -RRB-_-RRB- ,_, the_DT Euclidean_JJ distance_NN function_NN -LRB-_-LRB- Euclidean_JJ -RRB-_-RRB- ,_, the_DT method_NN developed_VBN by_IN Kwok_NNP et_FW al._FW -LRB-_-LRB- 14_CD -RRB-_-RRB- -LRB-_-LRB- Kwok_NN -RRB-_-RRB- ,_, and_CC the_DT method_NN developed_VBN by_IN Xing_NNP et_FW al._FW -LRB-_-LRB- 20_CD -RRB-_-RRB- ._.
The_DT latter_JJ two_CD methods_NNS are_VBP presented_VBN in_IN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP chose_VBD the_DT methods_NNS of_IN Kwok_NNP et_FW al._FW and_CC Xing_NNP et_FW al._FW for_IN two_CD reasons_NNS ._.
First_RB ,_, both_DT are_VBP based_VBN on_IN contextual_JJ information_NN to_TO learn_VB a_DT distance_NN function_NN ,_, as_IN is_VBZ used_VBN in_IN DAlign_NN ._.
Second_RB ,_, the_DT method_NN of_IN Xing_FW et_FW
imization_NN problem_NN whose_WP$ objective_JJ function_NN is_VBZ to_TO minimize_VB the_DT rank_NN of_IN the_DT weighting_NN matrix_NN A._NN However_RB ,_, it_PRP induces_VBZ an_DT NP-Hard_JJ problem_NN by_IN directly_RB minimizing_VBG rank_NN -LRB-_-LRB- A_NN -RRB-_-RRB- ,_, the_DT so-called_JJ zero-norm_NN problem_NN in_IN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Since_IN minimizing_VBG the_DT trace_NN of_IN a_DT matrix_NN tends_VBZ to_TO give_VB a_DT low-rank_JJ solution_NN when_WRB the_DT matrix_NN is_VBZ symmetric_JJ -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, we_PRP approximate_JJ the_DT rank_NN of_IN a_DT matrix_NN by_IN its_PRP$ trace_NN ._.
Moreover_RB ,_, since_IN rank_NN -LRB-_-LRB- AA_NN T_NN -RRB-_-RRB- =_JJ rank_NN -LRB-_-LRB- AA_NN T_NN
data_NNS ,_, and_CC user_NN -LRB-_-LRB- hereafter_RB we_PRP refer_VBP to_TO these_DT factors_NNS as_IN contextual_JJ information_NN -RRB-_-RRB- ._.
The_DT quality_NN of_IN the_DT distance_NN function_NN significantly_RB affects_VBZ the_DT success_NN in_IN organizing_VBG data_NNS or_CC finding_VBG meaningful_JJ results_NNS =_JJ -_: =[_NN 1_CD ,_, 2_CD ,_, 5_CD ,_, 9_CD ,_, 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
How_WRB do_VBP we_PRP consider_VB contextual_JJ information_NN in_IN formulating_VBG a_DT good_JJ distance_NN function_NN ?_.
One_CD extension_NN of_IN the_DT popular_JJ Euclidean_JJ Permission_NN to_TO make_VB digital_JJ or_CC hard_JJ copies_NNS of_IN all_DT or_CC part_NN of_IN this_DT work_NN for_IN pe_NN
kernel_NN ˜K_NN obtains_VBZ a_DT better_JJR alignment_NN than_IN the_DT prior_JJ kernel_NN matrix_NN K_NN to_TO the_DT ideal_JJ kernel_NN matrix_NN K_NNP ∗_NNP ,_, if_IN 0_CD ≤_FW β1_FW ≤_FW β2_FW ≤_NN 1_CD ._.
Moreover_RB ,_, a_DT smaller_JJR β1_NN or_CC β2_NN would_MD induce_VB a_DT higher_JJR alignment_NN score_NN ._.
PROOF_NN ._.
In_IN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, it_PRP has_VBZ been_VBN proven_VBN that_IN a_DT kernel_NN with_IN the_DT following_JJ form_NN has_VBZ a_DT higher_JJR alignment_NN score_NN with_IN the_DT ideal_JJ kernel_NN K_NNP ∗_NNP than_IN the_DT original_JJ kernel_NN K_NN ,_, K_NN =_JJ K_NN +_CC γK_NN ∗_NN ,_, γ_NN -RRB-_-RRB- 0_CD ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- where_WRB we_PRP use_VBP K_NN to_TO distinguish_VB wi_NN
data_NNS ,_, and_CC user_NN -LRB-_-LRB- hereafter_RB we_PRP refer_VBP to_TO these_DT factors_NNS as_IN contextual_JJ information_NN -RRB-_-RRB- ._.
The_DT quality_NN of_IN the_DT distance_NN function_NN significantly_RB affects_VBZ the_DT success_NN in_IN organizing_VBG data_NNS or_CC finding_VBG meaningful_JJ results_NNS =_JJ -_: =[_NN 1_CD ,_, 2_CD ,_, 5_CD ,_, 9_CD ,_, 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
How_WRB do_VBP we_PRP consider_VB contextual_JJ information_NN in_IN formulating_VBG a_DT good_JJ distance_NN function_NN ?_.
One_CD extension_NN of_IN the_DT popular_JJ Euclidean_JJ Permission_NN to_TO make_VB digital_JJ or_CC hard_JJ copies_NNS of_IN all_DT or_CC part_NN of_IN this_DT work_NN for_IN pe_NN
ser_NN ._.
At_IN first_JJ it_PRP might_MD seem_VB that_IN capturing_VBG nonlinear_JJ relationships_NNS among_IN contextual_JJ information_NN can_MD suffer_VB from_IN high_JJ computational_JJ complexity_NN ._.
DAlign_NN avoids_VBZ this_DT concern_NN by_IN employing_VBG the_DT kernel_NN trick_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT kernel_NN trick_NN lets_VBZ us_PRP generalize_VB distance-based_JJ algorithms_NNS to_TO operate_VB in_IN the_DT projected_VBN space_NN -LRB-_-LRB- defined_VBN next_JJ -RRB-_-RRB- ,_, usually_RB nonlinearly_RB related_JJ to_TO the_DT input_NN space_NN ._.
The_DT input_NN space_NN -LRB-_-LRB- denoted_VBN as_IN I_PRP -RRB-_-RRB- is_VBZ the_DT
data_NNS ,_, and_CC user_NN -LRB-_-LRB- hereafter_RB we_PRP refer_VBP to_TO these_DT factors_NNS as_IN contextual_JJ information_NN -RRB-_-RRB- ._.
The_DT quality_NN of_IN the_DT distance_NN function_NN significantly_RB affects_VBZ the_DT success_NN in_IN organizing_VBG data_NNS or_CC finding_VBG meaningful_JJ results_NNS =_JJ -_: =[_NN 1_CD ,_, 2_CD ,_, 5_CD ,_, 9_CD ,_, 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
How_WRB do_VBP we_PRP consider_VB contextual_JJ information_NN in_IN formulating_VBG a_DT good_JJ distance_NN function_NN ?_.
One_CD extension_NN of_IN the_DT popular_JJ Euclidean_JJ Permission_NN to_TO make_VB digital_JJ or_CC hard_JJ copies_NNS of_IN all_DT or_CC part_NN of_IN this_DT work_NN for_IN pe_NN
