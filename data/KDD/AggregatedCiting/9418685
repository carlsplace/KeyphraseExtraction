Training_VBG structural_JJ svms_NNS with_IN kernels_NNS using_VBG sampled_VBN cuts_NNS
Discriminative_JJ training_NN for_IN structured_JJ outputs_NNS has_VBZ found_VBN increasing_VBG applications_NNS in_IN areas_NNS such_JJ as_IN natural_JJ language_NN processing_NN ,_, bioinformatics_NNS ,_, information_NN retrieval_NN ,_, and_CC computer_NN vision_NN ._.
Focusing_VBG on_IN large-margin_JJ methods_NNS ,_, the_DT most_RBS general_JJ -LRB-_-LRB- in_IN terms_NNS of_IN loss_NN function_NN and_CC model_NN structure_NN -RRB-_-RRB- training_NN algorithms_NNS known_VBN to_TO date_NN are_VBP based_VBN on_IN cutting-plane_JJ approaches_NNS ._.
While_IN these_DT algorithms_NNS are_VBP very_RB efficient_JJ for_IN linear_JJ models_NNS ,_, their_PRP$ training_NN complexity_NN becomes_VBZ quadratic_JJ in_IN the_DT number_NN of_IN examples_NNS when_WRB kernels_NNS are_VBP used_VBN ._.
To_TO overcome_VB this_DT bottleneck_NN ,_, we_PRP propose_VBP new_JJ training_NN algorithms_NNS that_WDT use_VBP approximate_JJ cutting_VBG planes_NNS and_CC random_JJ sampling_NN to_TO enable_VB efficient_JJ training_NN with_IN kernels_NNS ._.
We_PRP prove_VBP that_IN these_DT algorithms_NNS have_VBP improved_VBN time_NN complexity_NN while_IN providing_VBG approximation_NN guarantees_NNS ._.
In_IN empirical_JJ evaluations_NNS ,_, our_PRP$ algorithms_NNS produced_VBD solutions_NNS with_IN training_NN and_CC test_NN error_NN rates_NNS close_RB to_TO those_DT of_IN exact_JJ solvers_NNS ._.
Even_RB on_IN binary_JJ classification_NN problems_NNS where_WRB highly_RB optimized_VBN conventional_JJ training_NN methods_NNS exist_VBP -LRB-_-LRB- e.g._FW SVM-light_JJ -RRB-_-RRB- ,_, our_PRP$ methods_NNS are_VBP about_IN an_DT order_NN of_IN magnitude_NN faster_RBR than_IN conventional_JJ training_NN methods_NNS on_IN large_JJ datasets_NNS ,_, while_IN remaining_VBG competitive_JJ in_IN speed_NN on_IN datasets_NNS of_IN medium_NN size_NN ._.
ls_NNS is_VBZ a_DT well-studied_JJ problem_NN ,_, and_CC there_EX are_VBP stable_JJ SVM_NN solvers_NNS that_WDT are_VBP suitable_JJ for_IN comparisons_NNS ._.
Moreover_RB ,_, scaling_VBG up_RP SVM_NN with_IN kernels_NNS to_TO large_JJ datasets_NNS is_VBZ an_DT interesting_JJ research_NN problem_NN on_IN its_PRP$ own_JJ =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN binary_JJ classification_NN the_DT loss_NN function_NN ∆_NN is_VBZ just_RB the_DT zeroone_NN loss_NN ._.
The_DT feature_NN map_NN Ψ_NN is_VBZ defined_VBN by_IN Ψ_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- =_JJ yφ_NN -LRB-_-LRB- x_NN -RRB-_-RRB- ,_, where_WRB y_FW ∈_FW -LCB-_-LRB- 1_CD ,_, −_NN 1_CD -RCB-_-RRB- and_CC φ_NN is_VBZ the_DT nonlinear_JJ feature_NN map_NN induced_VBN from_IN a_DT Mercer_NNP kerne_NN
he_PRP Nyström_NNP method_NN has_VBZ been_VBN proposed_VBN in_IN -LRB-_-LRB- 18_CD -RRB-_-RRB- to_TO approximate_JJ the_DT kernel_NN matrix_NN used_VBN for_IN Gaussian_NNP Process_VB classification_NN ._.
Low-rank_JJ approximation_NN has_VBZ been_VBN exploited_VBN to_TO speed_VB up_RP the_DT training_NN of_IN kernel_NN SVMs_NNS =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
A_DT greedy_JJ basis-pursuit-style_JJ algorithm_NN is_VBZ also_RB proposed_VBN in_IN -LRB-_-LRB- 7_CD -RRB-_-RRB- to_TO build_VB sparse_JJ kernel_NN SVMs_NNS to_TO speed_VB up_RP both_CC training_NN and_CC classification_NN ._.
4_LS ._.
THE_DT CUT_NNP SUBSAMPLING_NNP ALGORITHMS_NNPS Our_PRP$ main_JJ idea_NN is_VBZ to_TO speed_VB u_NN
g_NN and_CC learning_VBG ranking_JJ functions_NNS ._.
Second_RB ,_, they_PRP allow_VBP optimizing_VBG directly_RB to_TO non-standard_JJ loss_NN functions_NNS that_WDT do_VBP not_RB necessarily_RB have_VB to_TO decompose_VB linearly_RB -LRB-_-LRB- e.g._FW Average_NNP Precision_NNP ,_, ROC-Area_NN ,_, F1-score_NN -RRB-_-RRB- =_JJ -_: =[_NN 4_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
And_CC third_JJ ,_, their_PRP$ runtime_NN provably_RB scales_NNS linearly_RB with_IN the_DT number_NN of_IN training_NN examples_NNS for_IN linear_JJ models_NNS ._.
This_DT makes_VBZ cutting-plane_JJ methods_NNS not_RB only_RB attractive_JJ for_IN training_NN structured_JJ prediction_NN mode_NN
This_DT makes_VBZ cutting-plane_JJ methods_NNS not_RB only_RB attractive_JJ for_IN training_NN structured_JJ prediction_NN models_NNS ,_, but_CC they_PRP are_VBP also_RB orders_NNS of_IN magnitude_NN faster_RBR than_IN conventional_JJ methods_NNS for_IN training_NN binary_JJ classifiers_NNS =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Unfortunately_RB ,_, the_DT computational_JJ efficiency_NN of_IN cutting-plane_JJ methods_NNS becomes_VBZ substantially_RB worse_JJR for_IN non-linear_JJ models_NNS that_WDT involve_VBP kernels_NNS ._.
While_IN it_PRP is_VBZ possible_JJ to_TO train_VB kernel_NN models_NNS ,_, the_DT computat_NN
the_DT popular_JJ shrinking_NN heuristic_NN used_VBN in_IN training_NN SVMs_NNS ._.
On_IN the_DT other_JJ hand_NN ,_, one_CD major_JJ bottleneck_NN in_IN the_DT speed_NN of_IN the_DT current_JJ algorithm_NN is_VBZ the_DT large_JJ number_NN of_IN cuts_NNS required_VBN before_IN convergence_NN ._.
Recently_RB =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: proposes_VBZ a_DT stabilized_VBN cutting_NN plane_NN algorithm_NN for_IN linear_JJ SVMs_NNS with_IN much_JJ improved_JJ convergence_NN ,_, and_CC it_PRP will_MD be_VB interesting_JJ to_TO extend_VB their_PRP$ techniques_NNS to_TO improve_VB the_DT speed_NN of_IN our_PRP$ sampled-cut_JJ algorithm_NN f_SYM
