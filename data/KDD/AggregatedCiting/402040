Magical_JJ thinking_NN in_IN data_NNS mining_NN :_: lessons_NNS from_IN CoIL_NN challenge_NN 2000_CD
CoIL_NN challenge_NN 2000_CD was_VBD a_DT supervised_JJ learning_NN contest_NN that_WDT attracted_VBD 43_CD entries_NNS ._.
The_DT authors_NNS of_IN 29_CD entries_NNS later_RB wrote_VBD explanations_NNS of_IN their_PRP$ work_NN ._.
This_DT paper_NN discusses_VBZ these_DT reports_NNS and_CC reaches_VBZ three_CD main_JJ conclusions_NNS ._.
First_JJ ,_, naive_JJ Bayesian_JJ classifiers_NNS remain_VBP competitive_JJ in_IN practice_NN :_: they_PRP were_VBD used_VBN by_IN both_CC the_DT winning_JJ entry_NN and_CC the_DT next_JJ best_JJS entry_NN ._.
Second_RB ,_, identifying_VBG feature_NN interactions_NNS correctly_RB is_VBZ important_JJ for_IN maximizing_VBG predictive_JJ accuracy_NN :_: this_DT was_VBD the_DT difference_NN between_IN the_DT winning_JJ classifier_NN and_CC all_DT others_NNS ._.
Third_JJ and_CC most_RBS important_JJ ,_, too_RB many_JJ researchers_NNS and_CC practitioners_NNS in_IN data_NNS mining_NN do_VBP not_RB appreciate_VB properly_RB the_DT issue_NN of_IN statistical_JJ significance_NN and_CC the_DT danger_NN of_IN overfitting_NN ._.
Given_VBN a_DT dataset_NN such_JJ as_IN the_DT one_NN for_IN the_DT CoIL_NN contest_NN ,_, it_PRP is_VBZ pointless_JJ to_TO apply_VB a_DT very_RB complicated_JJ learning_NN algorithm_NN ,_, or_CC to_TO perform_VB a_DT very_RB time-consuming_JJ model_NN search_NN ._.
In_IN either_DT ease_NN ,_, one_CD is_VBZ likely_JJ to_TO overfit_VB the_DT training_NN data_NNS and_CC to_TO fool_VB oneself_NN in_IN estimating_VBG predictive_JJ accuracy_NN and_CC in_IN discovering_VBG useful_JJ correlations_NNS ._.
itioner_NN of_IN magic_NN does_VBZ not_RB unlearn_VB his_PRP$ magical_JJ view_NN of_IN events_NNS when_WRB the_DT magic_NN does_VBZ not_RB work_VB ._.
In_IN fact_NN ,_, the_DT propositions_NNS which_WDT govern_VBP punctuation_NN have_VBP the_DT general_JJ characteristic_NN of_IN being_VBG self-validating_JJ &_CC q_NN =_JJ -_: =_JJ uot_NN ;_: -LRB-_-LRB- 1_LS -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN any_DT culture_NN ,_, humans_NNS have_VBP a_DT certain_JJ set_NN of_IN expectations_NNS that_IN they_PRP use_VBP to_TO explain_VB the_DT results_NNS of_IN their_PRP$ actions_NNS ._.
When_WRB something_NN surprising_JJ happens_VBZ ,_, rather_RB than_IN question_VB the_DT expectations_NNS ,_, people_NNS typi_VBP
ater_NN wrote_VBD reports_NNS explaining_VBG their_PRP$ methods_NNS and_CC results_NNS ._.
The_DT authors_NNS of_IN these_DT reports_NNS appear_VBP to_TO be_VB data_NN mining_NN practitioners_NNS or_CC researchers_NNS ,_, as_IN opposed_VBN to_TO students_NNS ._.
The_DT reports_NNS have_VBP been_VBN published_VBN by_IN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT CoIL_NN contest_NN was_VBD quite_RB similar_JJ to_TO the_DT competitions_NNS organized_VBN in_IN conjunction_NN with_IN the_DT KDD_NNP conference_NN in_IN recent_JJ years_NNS ,_, and_CC to_TO other_JJ data_NNS mining_NN competitions_NNS ._.
The_DT contest_NN task_NN was_VBD to_TO learn_VB a_DT classi_NN
1_CD yields_NNS an_DT improved_JJ estimate_NN of_IN the_DT true_JJ standard_JJ deviation_NN of_IN the_DT parent_NN population_NN that_WDT will_MD not_RB systematically_RB be_VB too_RB small_JJ ._.
Technically_NNP ,_, using_VBG 1_CD n_NN 1_CD gives_VBZ the_DT minimum_JJ variance_NN unbiased_JJ estimator_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- ._.
Man_NN -_: =_JJ -_: y_NN contest_NN submissions_NNS reveal_VBP basic_JJ misunderstandings_NNS about_IN the_DT issue_NN of_IN overfitting_NN ._.
For_IN example_NN ,_, one_CD team_NN wrote_VBD that_IN they_PRP used_VBD ''_'' :_: :_: :_: evolutionary_JJ search_NN for_IN choosing_VBG the_DT predictive_JJ features_NNS ._.
T_NN
ustering_NN methods_NNS are_VBP false_JJ ._.
The_DT k-means_NN algorithm_NN ,_, for_IN example_NN ,_, can_MD handle_VB datasets_NNS with_IN millions_NNS of_IN records_NNS and_CC hundreds_NNS of_IN dimensions_NNS ,_, where_WRB no_DT two_CD records_NNS are_VBP identical_JJ ,_, in_IN effectively_RB linear_JJ time_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Of_IN course_NN the_DT k-means_NN algorithm_NN is_VBZ not_RB a_DT panacea_NN :_: it_PRP assumes_VBZ that_IN all_DT features_NNS are_VBP numerical_JJ and_CC a_DT Euclidean_JJ distance_NN metric_NN ,_, and_CC no_DT universally_RB good_JJ method_NN is_VBZ known_VBN for_IN relaxing_VBG these_DT assumptions_NNS ._.
to_TO use_VB to_TO organize_VB our_PRP$ perceptions_NNS ._.
Whatever_WDT these_DT principles_NNS are_VBP ,_, if_IN we_PRP have_VBP learned_VBN them_PRP ,_, it_PRP is_VBZ because_IN they_PRP appeared_VBD to_TO be_VB useful_JJ in_IN the_DT past_NN ._.
As_IN pointed_VBN out_RP in_IN a_DT different_JJ context_NN by_IN Thomas_NNP Kuhn_NNP =_SYM -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, practitioners_NNS of_IN science_NN do_VBP not_RB unlearn_VB their_PRP$ scientific_JJ worldview_NN when_WRB science_NN can_MD not_RB explain_VB a_DT certain_JJ phenomenon_NN ._.
Instead_RB ,_, they_PRP either_RB ignore_VBP the_DT phenomenon_NN ,_, or_CC they_PRP redouble_VBP their_PRP$ efforts_NNS to_TO und_VB
is_VBZ sensitive_JJ to_TO unbalanced_JJ data_NNS ,_, and_CC probability_NN estimates_NNS at_IN leaves_NNS are_VBP smoothed_VBN ,_, then_RB decision_NN trees_NNS can_MD be_VB fully_RB competitive_JJ with_IN naive_JJ Bayesian_JJ classifiers_NNS on_IN commercial_JJ response_NN prediction_NN tasks_NNS =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
9_CD ._.
CONCLUSIONS_NNS In_IN summary_NN ,_, there_EX are_VBP three_CD main_JJ lessons_NNS to_TO be_VB learned_VBN from_IN the_DT CoIL_NN data_NNS mining_NN contest_NN ._.
The_DT first_JJ two_CD lessons_NNS are_VBP technical_JJ ,_, one_CD positive_JJ and_CC one_CD negative_JJ ._.
The_DT positive_JJ lesson_NN is_VBZ th_DT
possible_JJ to_TO compare_VB two_CD different_JJ learning_VBG methods_NNS with_IN the_DT same_JJ training_NN and_CC test_NN datasets_NNS in_IN a_DT way_NN that_WDT is_VBZ more_RBR sensitive_JJ than_IN the_DT simple_JJ binomial_JJ calculation_NN above_IN ,_, using_VBG McNemar_NNP 's_POS hypothesis_NN test_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Let_VB A_NN and_CC B_NN designate_VBP two_CD learning_VBG algorithms_NNS and_CC let_VB n_NN 10_CD be_VB the_DT number_NN of_IN test_NN examples_NNS classified_VBN correctly_RB by_IN A_NN but_CC incorrectly_RB by_IN B._NNP Similarly_RB ,_, let_VB n_NN 01_CD be_VB the_DT number_NN classified_VBN incorrectly_RB by_IN
eatures_NNS were_VBD discretized_VBN in_IN advance_NN ,_, the_DT CoIL_NN competition_NN could_MD not_RB serve_VB as_IN a_DT test_NN of_IN discretization_NN methods_NNS ._.
The_DT predictive_JJ accuracy_NN of_IN a_DT naive_JJ Bayesian_JJ classifier_NN can_MD often_RB be_VB improved_VBN by_IN boosting_VBG =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC by_IN adding_VBG new_JJ attributes_NNS derived_VBN from_IN combinations_NNS of_IN existing_VBG attributes_NNS ._.
Both_DT boosting_VBG and_CC derived_VBN attributes_NNS are_VBP ways_NNS of_IN relaxing_VBG the_DT conditional_JJ independence_NN assumptions_NNS that_WDT constitute_VBP the_DT
making_VBG the_DT offer_NN ._.
Therefore_RB ,_, the_DT aim_NN of_IN data_NNS mining_NN should_MD be_VB to_TO estimate_VB the_DT probability_NN that_IN a_DT customer_NN would_MD accept_VB an_DT offer_NN ,_, and_CC also_RB the_DT costs_NNS and_CC benefits_NNS of_IN the_DT customer_NN accepting_VBG or_CC declining_VBG =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Second_RB ,_, a_DT customer_NN should_MD not_RB be_VB offered_VBN an_DT insurance_NN policy_NN just_RB because_IN he_PRP or_CC she_PRP resembles_VBZ other_JJ customers_NNS who_WP have_VBP the_DT same_JJ type_NN of_IN policy_NN ._.
The_DT characteristics_NNS that_WDT predict_VBP who_WP is_VBZ most_RBS likely_JJ to_TO
also_RB methodologies_NNS for_IN data_NN mining_NN ._.
It_PRP is_VBZ noteworthy_JJ that_IN none_NN of_IN the_DT reports_NNS written_VBN by_IN CoIL_NN contest_NN participants_NNS mention_VBP using_VBG any_DT part_NN of_IN the_DT CRISP-DM_NN European_JJ standard_JJ methodology_NN for_IN data_NNS mining_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
8_CD ._.
CHOICE_NN OF_IN LEARNING_VBG METHOD_NN Although_IN it_PRP is_VBZ difficult_JJ to_TO say_VB with_IN certainty_NN that_IN one_CD learning_NN method_NN gives_VBZ more_RBR accurate_JJ classifiers_NNS than_IN another_DT ,_, it_PRP is_VBZ possible_JJ to_TO say_VB with_IN certainty_NN that_IN for_IN pract_NN
fic_JJ thinking_NN ._.
Being_VBG primed_VBN to_TO see_VB patterns_NNS in_IN small_JJ datasets_NNS is_VBZ an_DT innate_JJ characteristic_NN of_IN humans_NNS and_CC perhaps_RB other_JJ animals_NNS also_RB ,_, and_CC this_DT characteristic_NN is_VBZ often_RB useful_JJ for_IN success_NN in_IN everyday_JJ life_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Moreover_RB ,_, the_DT starting_VBG point_NN of_IN scientific_JJ thinking_NN is_VBZ often_RB a_DT type_NN of_IN magical_JJ thinking_NN :_: scientists_NNS commonly_RB posit_VBP hypotheses_NNS based_VBN on_IN a_DT low_JJ number_NN of_IN observations_NNS ._.
These_DT hypotheses_NNS are_VBP frequently_RB u_NN
