Efficient_JJ progressive_JJ sampling_NN
._.
Locality_NN is_VBZ defined_VBN within_IN a_DT particular_JJ progressive_JJ sampling_NN procedure_NN ._.
Not_RB all_DT learning_VBG curves_NNS are_VBP well_RB behaved_VBN ._.
For_IN example_NN ,_, theoretical_JJ analyses_NNS of_IN learning_VBG curves_NNS based_VBN on_IN statistical_JJ mechanics_NNS =_JJ -_: =[_NN 7_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP shown_VBN that_IN sudden_JJ increases_NNS in_IN accuracy_NN are_VBP possible_JJ ,_, particularly_RB on_IN small_JJ samples_NNS ._.
However_RB ,_, empirical_JJ studies_NNS of_IN the_DT application_NN of_IN standard_JJ induction_NN algorithms_NNS to_TO large_JJ data_NNS sets_NNS --_: those_DT of_IN
t._FW In_FW their_PRP$ paper_NN on_IN arithmetic_NN sampling_NN ,_, John_NNP and_CC Langley_NNP -LRB-_-LRB- 8_CD -RRB-_-RRB- model_NN the_DT learning_VBG curve_NN as_IN sampling_NN progresses_VBZ ._.
They_PRP determine_VBP convergence_NN using_VBG a_DT stopping_VBG criterion_NN modeled_VBD after_IN the_DT work_NN of_IN Valiant_NNP =_SYM -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Specifically_RB ,_, convergence_NN is_VBZ reached_VBN when_WRB P_NN r_NN -LRB-_-LRB- -LRB-_-LRB- acc_NN -LRB-_-LRB- N_NN -RRB-_-RRB- \_FW Gamma_FW acc_NN -LRB-_-LRB- n_NN i_LS -RRB-_-RRB- -RRB-_-RRB- ?_.
ffl_NN -RRB-_-RRB- sffi_NN ,_, where_WRB acc_NN -LRB-_-LRB- x_NN -RRB-_-RRB- is_VBZ the_DT accuracy_NN of_IN the_DT model_NN that_IN an_DT algorithm_NN produces_VBZ after_IN seeing_VBG x_NN instances_NNS ,_, ffl_NN refers_VBZ to_TO the_DT m_NN
m_NN on_IN subsets_NNS of_IN size_NN 2_CD We_PRP follow_VBP the_DT reasoning_NN of_IN Korf_NNP ,_, who_WP shows_VBZ that_IN progressive_JJ deepening_VBG is_VBZ an_DT optimal_JJ schedule_NN for_IN conducting_VBG depth-first_JJ search_NN when_WRB the_DT smallest_JJS sufficient_JJ depth_NN is_VBZ unknown_JJ -LRB-_-LRB- 9_CD -RRB-_-RRB- =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
a_DT i_FW \_FW Delta_NN n_NN 0_CD for_IN i_LS =_JJ 0_CD ;_: 1_CD ;_: :_: :_: :_: ;_: b_NN ,_, where_WRB b_NN +_CC 1_CD is_VBZ the_DT number_NN of_IN samples_NNS processed_VBN before_IN detecting_VBG convergence_NN ._.
Now_RB ,_, we_PRP assume_VBP convergence_NN is_VBZ well_RB detected_VBN ,_, so_IN a_DT b_NN \_NNP Gamma1_NNP \_NNP Delta_NNP n_NN 0_CD !_.
nminsa_NN b_NN
un-time_JJ complexity_NN -LRB-_-LRB- in_IN n_NN -RRB-_-RRB- of_IN the_DT underlying_VBG induction_NN algorithm_NN ._.
Run-time_JJ complexity_NN models_NNS are_VBP not_RB always_RB easy_JJ to_TO obtain_VB ._.
For_IN example_NN ,_, our_PRP$ empirical_JJ results_NNS below_RB use_VBP the_DT decisiontree_JJ algorithm_NN C4_NN .5_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, for_IN which_WDT reported_VBD time_NN complexity_NN varies_VBZ widely_RB ._.
Moreover_RB ,_, DP_NN sampling_NN requires_VBZ the_DT actual_JJ runtime_NN complexity_NN for_IN the_DT problem_NN in_IN question_NN ,_, rather_RB than_IN a_DT worst-case_JJ complexity_NN ._.
We_PRP obtained_VBD empirical_JJ
died_VBD the_DT general_JJ case_NN ._.
Methods_NNS for_IN active_JJ sampling_NN ,_, choosing_VBG subsequent_JJ samples_NNS based_VBN upon_IN the_DT models_NNS learned_VBD previously_RB ,_, are_VBP of_IN particular_JJ interest_NN ._.
A_DT classic_JJ example_NN of_IN active_JJ sampling_NN is_VBZ windowing_VBG =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, wherein_WRB subsequent_JJ sampling_NN chooses_VBZ instances_NNS for_IN which_WDT the_DT current_JJ model_NN makes_VBZ errors_NNS ._.
Active_JJ sampling_NN changes_VBZ the_DT learning_NN curve_NN ._.
For_IN example_NN ,_, on_IN noisy_JJ data_NNS ,_, windowing_VBG learning_NN curves_NNS are_VBP notoriou_JJ
on_IN small_JJ samples_NNS ._.
However_RB ,_, empirical_JJ studies_NNS of_IN the_DT application_NN of_IN standard_JJ induction_NN algorithms_NNS to_TO large_JJ data_NNS sets_NNS --_: those_DT of_IN relevance_NN to_TO this_DT paper_NN --_: have_VBP shown_VBN learning_VBG curves_NNS to_TO be_VB well_RB behaved_VBD =_JJ -_: =[_NN 3_CD ,_, 4_CD ,_, 6_CD ,_, 12_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN ,_, practical_JJ progressive_JJ sampling_NN demands_NNS only_RB that_IN learning_VBG curves_NNS are_VBP well_RB behaved_VBN at_IN the_DT level_NN Compute_VB schedule_NN S_NN =_JJ fn_NN 0_CD ;_: n_NN 1_CD ;_: n_NN 2_CD ;_: :_: :_: :_: ;_: n_NN k_NN g_NN of_IN sample_NN sizes_NNS n_NN \/_: n_NN 0_CD M_NN \/_: model_NN in_IN
on_IN small_JJ samples_NNS ._.
However_RB ,_, empirical_JJ studies_NNS of_IN the_DT application_NN of_IN standard_JJ induction_NN algorithms_NNS to_TO large_JJ data_NNS sets_NNS --_: those_DT of_IN relevance_NN to_TO this_DT paper_NN --_: have_VBP shown_VBN learning_VBG curves_NNS to_TO be_VB well_RB behaved_VBD =_JJ -_: =[_NN 3_CD ,_, 4_CD ,_, 6_CD ,_, 12_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN ,_, practical_JJ progressive_JJ sampling_NN demands_NNS only_RB that_IN learning_VBG curves_NNS are_VBP well_RB behaved_VBN at_IN the_DT level_NN Compute_VB schedule_NN S_NN =_JJ fn_NN 0_CD ;_: n_NN 1_CD ;_: n_NN 2_CD ;_: :_: :_: :_: ;_: n_NN k_NN g_NN of_IN sample_NN sizes_NNS n_NN \/_: n_NN 0_CD M_NN \/_: model_NN in_IN
can_MD be_VB done_VBN on_IN convergence_NN detection_NN ._.
With_IN very_RB slow_JJ sampling_NN ,_, the_DT efficiency_NN of_IN progressive_JJ sampling_NN will_MD be_VB the_DT same_JJ as_IN if_IN nmin_NN were_VBD known_VBN a_DT priori_FW ._.
7_CD Other_JJ related_JJ work_NN The_DT method_NN of_IN Musick_NNP et_FW al._FW =_SYM -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: for_IN determining_VBG the_DT best_JJS attribute_NN at_IN each_DT decision-tree_JJ node_NN can_MD be_VB seen_VBN as_IN an_DT instance_NN of_IN the_DT generic_JJ progressive_JJ sampling_NN algorithm_NN shown_VBN in_IN figure_NN 2_CD ,_, if_IN we_PRP regard_VBP each_DT node_NN of_IN the_DT decision_NN tree_NN a_DT
5_CD run_NN time_NN is_VBZ often_RB claimed_VBN to_TO be_VB linear_JJ in_IN the_DT number_NN of_IN instances_NNS ,_, for_IN non-numeric_JJ data_NNS sets_NNS ._.
This_DT claim_NN is_VBZ based_VBN on_IN an_DT analysis_NN by_IN Utgoff_NNP ,_, where_WRB he_PRP shows_VBZ the_DT asymptotic_JJ time_NN complexity_NN to_TO be_VB O_NN -LRB-_-LRB- n_NN -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =_JJ Utgoff_NNP ,_, 1989_CD -_: =--RRB-_NN ._.
With_IN numeric_JJ data_NNS ,_, sorting_NN adds_VBZ a_DT log_NN n_NN term_NN at_IN each_DT node_NN ._.
However_RB ,_, C4_NN .5_NN has_VBZ been_VBN observed_VBN to_TO be_VB worse_JJR than_IN O_NN -LRB-_-LRB- n_NN 2_CD -RRB-_-RRB- -LRB-_-LRB- Catlett_NNP ,_, 1991a_CD -RRB-_-RRB- ._.
One_CD explanation_NN for_IN the_DT discrepancy_NN is_VBZ that_IN Utgoff_NNP actually_RB sho_VBP
rithm_NN on_IN subsets_NNS of_IN size_NN 2_CD We_PRP follow_VBP the_DT reasoning_NN of_IN Korf_NNP ,_, who_WP shows_VBZ that_IN progressive_JJ deepening_VBG is_VBZ an_DT optimal_JJ schedule_NN for_IN conducting_VBG depth-first_JJ search_NN when_WRB the_DT smallest_JJS sufficient_JJ depth_NN is_VBZ unknown_JJ =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 15_CD -RRB-_-RRB- ._.
a_DT i_FW \_FW Delta_NN n_NN 0_CD for_IN i_LS =_JJ 0_CD ;_: 1_CD ;_: :_: :_: :_: ;_: b_NN ,_, where_WRB b_NN +_CC 1_CD is_VBZ the_DT number_NN of_IN samples_NNS processed_VBN before_IN detecting_VBG convergence_NN ._.
Now_RB ,_, we_PRP assume_VBP convergence_NN is_VBZ well_RB detected_VBN ,_, so_IN a_DT b_NN \_NNP Gamma1_NNP \_NNP Delta_NNP n_NN 0_CD !_.
nmin_NN
urve_NN ._.
For_IN example_NN ,_, on_IN noisy_JJ data_NNS ,_, windowing_VBG learning_NN curves_NNS are_VBP notoriously_RB ill_RB behaved_VBN :_: subsequent_JJ samples_NNS contain_VBP increasing_VBG amounts_NNS of_IN noise_NN ,_, and_CC performance_NN often_RB decreases_VBZ as_IN sampling_NN progresses_VBZ =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
It_PRP would_MD be_VB interesting_JJ to_TO examine_VB more_RBR closely_RB the_DT use_NN of_IN the_DT techniques_NNS outlined_VBN above_RB in_IN the_DT context_NN of_IN active_JJ sampling_NN ,_, and_CC the_DT potential_JJ synergies_NNS ._.
8_CD Conclusion_NN With_IN this_DT work_NN we_PRP have_VBP made_VBN subs_NNS
ves_NNS typically_RB have_VBP a_DT steeply_RB sloping_VBG portion_NN early_RB in_IN the_DT curve_NN ,_, a_DT more_RBR gently_RB sloping_VBG middle_JJ portion_NN ,_, and_CC a_DT plateau_NN late_RB in_IN the_DT curve_NN ._.
The_DT middle_JJ portion_NN can_MD be_VB extremely_RB large_JJ in_IN some_DT curves_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 2_CD ,_, 3_CD ,_, 6_CD -RRB-_-RRB- -_: =--RRB-_NN and_CC almost_RB entirely_RB missing_VBG in_IN others_NNS ._.
The_DT plateau_NN occurs_VBZ when_WRB adding_VBG additional_JJ data_NNS instances_NNS does_VBZ not_RB improve_VB accuracy_NN ._.
The_DT plateau_NN ,_, and_CC even_RB the_DT entire_JJ middle_JJ portion_NN ,_, can_MD be_VB missing_VBG from_IN curves_NNS
._.
For_IN each_DT value_NN of_IN n_NN ,_, from_IN 1_CD to_TO N_NN ,_, a_DT model_NN can_MD either_RB be_VB built_VBN or_CC not_RB ,_, leading_VBG to_TO 2_CD N_NN possible_JJ schedules_NNS ._.
However_RB ,_, identification_NN of_IN the_DT optimal_JJ schedule_NN can_MD be_VB cast_VBN in_IN terms_NNS of_IN dynamic_JJ programming_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, yielding_VBG an_DT algorithm_NN that_WDT requires_VBZ O_NN -LRB-_-LRB- N_NN 2_CD -RRB-_-RRB- space_NN and_CC O_NN -LRB-_-LRB- N_NN 3_CD -RRB-_-RRB- time_NN ._.
Let_VB f_LS -LRB-_-LRB- n_NN -RRB-_-RRB- be_VB the_DT cost_NN of_IN building_VBG a_DT model_NN with_IN n_NN instances_NNS and_CC determining_VBG whether_IN accuracy_NN has_VBZ converged_VBN ._.
As_IN described_VBN above_IN ,_, let_VB
use_NN for_IN comparison_NN the_DT schedule_NN composed_VBN of_IN a_DT single_JJ data_NN set_VBN with_IN all_DT instances_NNS ,_, SN_NN =_JJ fNg_NN ._.
We_PRP also_RB will_MD consider_VB the_DT simple_JJ schedule_NN generated_VBN by_IN an_DT omniscient_JJ oracle_NN ,_, SO_NN =_JJ fnmin_FW g._FW John_NNP and_CC Langley_NNP =_SYM -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: define_VB a_DT progressive_JJ sampling_NN approach_NN we_PRP call_VBP arithmetic_NN sampling_NN using_VBG the_DT schedule_NN S_NN a_DT =_JJ n_NN 0_CD +_CC -LRB-_-LRB- i_FW \_FW Delta_NN n_NN ffi_NN -RRB-_-RRB- =_JJ fn_NN 0_CD ;_: n_NN 0_CD +_CC n_NN ffi_NN ;_: n_NN 0_CD +_CC 2n_FW ffi_FW ;_: :_: :_: :_: ;_: n_NN 0_CD +_CC k_NN \_CD Delta_NNP n_NN ffi_NN g._NN 1_CD An_DT example_NN
on_IN small_JJ samples_NNS ._.
However_RB ,_, empirical_JJ studies_NNS of_IN the_DT application_NN of_IN standard_JJ induction_NN algorithms_NNS to_TO large_JJ data_NNS sets_NNS --_: those_DT of_IN relevance_NN to_TO this_DT paper_NN --_: have_VBP shown_VBN learning_VBG curves_NNS to_TO be_VB well_RB behaved_VBD =_JJ -_: =[_NN 3_CD ,_, 4_CD ,_, 6_CD ,_, 12_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN ,_, practical_JJ progressive_JJ sampling_NN demands_NNS only_RB that_IN learning_VBG curves_NNS are_VBP well_RB behaved_VBN at_IN the_DT level_NN Compute_VB schedule_NN S_NN =_JJ fn_NN 0_CD ;_: n_NN 1_CD ;_: n_NN 2_CD ;_: :_: :_: :_: ;_: n_NN k_NN g_NN of_IN sample_NN sizes_NNS n_NN \/_: n_NN 0_CD M_NN \/_: model_NN in_IN
ves_NNS typically_RB have_VBP a_DT steeply_RB sloping_VBG portion_NN early_RB in_IN the_DT curve_NN ,_, a_DT more_RBR gently_RB sloping_VBG middle_JJ portion_NN ,_, and_CC a_DT plateau_NN late_RB in_IN the_DT curve_NN ._.
The_DT middle_JJ portion_NN can_MD be_VB extremely_RB large_JJ in_IN some_DT curves_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 2_CD ,_, 3_CD ,_, 6_CD -RRB-_-RRB- -_: =--RRB-_NN and_CC almost_RB entirely_RB missing_VBG in_IN others_NNS ._.
The_DT plateau_NN occurs_VBZ when_WRB adding_VBG additional_JJ data_NNS instances_NNS does_VBZ not_RB improve_VB accuracy_NN ._.
The_DT plateau_NN ,_, and_CC even_RB the_DT entire_JJ middle_JJ portion_NN ,_, can_MD be_VB missing_VBG from_IN curves_NNS
ves_NNS typically_RB have_VBP a_DT steeply_RB sloping_VBG portion_NN early_RB in_IN the_DT curve_NN ,_, a_DT more_RBR gently_RB sloping_VBG middle_JJ portion_NN ,_, and_CC a_DT plateau_NN late_RB in_IN the_DT curve_NN ._.
The_DT middle_JJ portion_NN can_MD be_VB extremely_RB large_JJ in_IN some_DT curves_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 2_CD ,_, 3_CD ,_, 6_CD -RRB-_-RRB- -_: =--RRB-_NN and_CC almost_RB entirely_RB missing_VBG in_IN others_NNS ._.
The_DT plateau_NN occurs_VBZ when_WRB adding_VBG additional_JJ data_NNS instances_NNS does_VBZ not_RB improve_VB accuracy_NN ._.
The_DT plateau_NN ,_, and_CC even_RB the_DT entire_JJ middle_JJ portion_NN ,_, can_MD be_VB missing_VBG from_IN curves_NNS
n_NN ,_, and_CC low_JJ values_NNS for_IN very_RB large_JJ n._NN For_IN example_NN ,_, data_NNS from_IN Oates_NNP and_CC Jensen_NNP -LRB-_-LRB- 12_CD ,_, 13_CD -RRB-_-RRB- show_VBP that_IN the_DT distribution_NN of_IN the_DT number_NN of_IN instances_NNS needed_VBN for_IN convergence_NN over_IN a_DT large_JJ set_NN of_IN the_DT UCI_NNP databases_NNS =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ roughly_RB log-normal_JJ ._.
Given_VBN such_JJ expectations_NNS about_IN nmin_NN ,_, is_VBZ it_PRP possible_JJ to_TO construct_VB the_DT schedule_NN with_IN the_DT minimum_NN expected_VBD cost_NN of_IN convergence_NN ?_.
This_DT seems_VBZ a_DT daunting_JJ task_NN ._.
For_IN each_DT value_NN of_IN n_NN ,_, fr_NN
._.
Locality_NN is_VBZ defined_VBN within_IN a_DT particular_JJ progressive_JJ sampling_NN procedure_NN ._.
Not_RB all_DT learning_VBG curves_NNS are_VBP well_RB behaved_VBN ._.
For_IN example_NN ,_, theoretical_JJ analyses_NNS of_IN learning_VBG curves_NNS based_VBN on_IN statistical_JJ mechanics_NNS =_JJ -_: =[_NN 7_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP shown_VBN that_IN sudden_JJ increases_NNS in_IN accuracy_NN are_VBP possible_JJ ,_, particularly_RB on_IN small_JJ samples_NNS ._.
However_RB ,_, empirical_JJ studies_NNS of_IN the_DT application_NN of_IN standard_JJ induction_NN algorithms_NNS to_TO large_JJ data_NNS sets_NNS --_: those_DT of_IN
can_MD overshoot_VB nmin_NN greatly_RB ,_, they_PRP then_RB calculate_VBP n_NN ffi_NN for_IN an_DT efficient_JJ arithmetic_NN schedule_NN ,_, and_CC revise_VB the_DT estimate_NN after_IN executing_VBG each_DT schedule_NN point_NN ._.
Other_JJ sequential_JJ multi-sample_JJ learning_NN methods_NNS =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP degenerate_JJ instances_NNS of_IN progressive_JJ sampling_NN ,_, typically_RB using_VBG fixed_JJ arithmetic_NN schedules_NNS and_CC treating_VBG convergence_NN detection_NN simplistically_RB ,_, if_IN at_IN all_DT ._.
For_IN this_DT paper_NN ,_, we_PRP have_VBP considered_VBN only_JJ draw_NN
