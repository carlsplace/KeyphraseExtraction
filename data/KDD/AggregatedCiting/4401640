Learning_VBG sparse_JJ metrics_NNS via_IN linear_JJ programming_NN
Calculation_NN of_IN object_NN similarity_NN ,_, for_IN example_NN through_IN a_DT distance_NN function_NN ,_, is_VBZ a_DT common_JJ part_NN of_IN data_NNS mining_NN and_CC machine_NN learning_NN algorithms_NNS ._.
This_DT calculation_NN is_VBZ crucial_JJ for_IN efficiency_NN since_IN distances_NNS are_VBP usually_RB evaluated_VBN a_DT large_JJ number_NN of_IN times_NNS ,_, the_DT classical_JJ example_NN being_VBG query-by-example_JJ -LRB-_-LRB- find_VB objects_NNS that_WDT are_VBP similar_JJ to_TO a_DT given_VBN query_NN object_NN -RRB-_-RRB- ._.
Moreover_RB ,_, the_DT performance_NN of_IN these_DT algorithms_NNS depends_VBZ critically_RB on_IN choosing_VBG a_DT good_JJ distance_NN function_NN ._.
However_RB ,_, it_PRP is_VBZ often_RB the_DT case_NN that_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- the_DT correct_JJ distance_NN is_VBZ unknown_JJ or_CC chosen_VBN by_IN hand_NN ,_, and_CC -LRB-_-LRB- 2_LS -RRB-_-RRB- its_PRP$ calculation_NN is_VBZ computationally_RB expensive_JJ -LRB-_-LRB- e.g._FW ,_, such_JJ as_IN for_IN large_JJ dimensional_JJ objects_NNS -RRB-_-RRB- ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT method_NN for_IN constructing_VBG relative-distance_JJ preserving_NN low-dimensional_JJ mapping_NN -LRB-_-LRB- sparse_JJ mappings_NNS -RRB-_-RRB- ._.
This_DT method_NN allows_VBZ learning_VBG unknown_JJ distance_NN functions_NNS -LRB-_-LRB- or_CC approximating_VBG known_JJ functions_NNS -RRB-_-RRB- with_IN the_DT additional_JJ property_NN of_IN reducing_VBG distance_NN computation_NN time_NN ._.
We_PRP present_VBP an_DT algorithm_NN that_IN given_VBN examples_NNS of_IN proximity_NN comparisons_NNS among_IN triples_NNS of_IN objects_NNS -LRB-_-LRB- object_NN i_FW is_VBZ more_JJR like_IN object_NN j_NN than_IN object_NN k_NN -RRB-_-RRB- ,_, learns_VBZ a_DT distance_NN function_NN ,_, in_IN as_IN few_JJ dimensions_NNS as_IN possible_JJ ,_, that_WDT preserves_VBZ these_DT distance_NN relationships_NNS ._.
The_DT formulation_NN is_VBZ based_VBN on_IN solving_VBG a_DT linear_JJ programming_NN optimization_NN problem_NN that_WDT finds_VBZ an_DT optimal_JJ mapping_NN for_IN the_DT given_VBN dataset_NN and_CC distance_NN relationships_NNS ._.
Unlike_IN other_JJ popular_JJ embedding_VBG algorithms_NNS ,_, this_DT method_NN can_MD easily_RB generalize_VB to_TO new_JJ points_NNS ,_, does_VBZ not_RB have_VB local_JJ minima_NN ,_, and_CC explicitly_RB models_NNS computational_JJ efficiency_NN by_IN finding_VBG a_DT mapping_NN that_WDT is_VBZ sparse_JJ ,_, i.e._FW one_CD that_WDT depends_VBZ on_IN a_DT small_JJ subset_NN of_IN features_NNS or_CC dimensions_NNS ._.
Experimental_JJ evaluation_NN shows_VBZ that_IN the_DT proposed_VBN formulation_NN compares_VBZ favorably_RB with_IN a_DT state-of-the_JJ art_NN method_NN in_IN several_JJ publicly_RB available_JJ datasets_NNS ._.
ods_NNS that_WDT have_VBP approached_VBN the_DT problem_NN of_IN finding_VBG low_JJ dimensional_JJ representations_NNS of_IN the_DT data_NNS ._.
Some_DT of_IN these_DT approaches_NNS attempt_VBP to_TO capture_VB the_DT variance_NN of_IN the_DT data_NNS such_JJ as_IN Principal_NN Components_NNP Analysis_NNP =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, while_IN others_NNS build_VBP low-dimensional_JJ embeddings_NNS ,_, that_DT is_VBZ ,_, by_IN transforming_VBG a_DT set_NN of_IN data_NNS points_NNS into_IN a_DT lower_JJR dimensional_JJ one_CD such_JJ that_IN -LRB-_-LRB- some_DT or_CC all_DT -RRB-_-RRB- distances_NNS are_VBP preserved_JJ ._.
Examples_NNS of_IN these_DT approache_NN
arly_RB not_RB equivalent_JJ to_TO the_DT base_NN formulation_NN ;_: however_RB ,_, the_DT 1-norm_NN tends_VBZ to_TO suppress_VB terms_NNS and_CC to_TO produce_VB sparse_JJ solutions_NNS ._.
This_DT has_VBZ been_VBN empirically_RB validated_VBN ,_, in_IN particular_JJ in_IN the_DT SVM_NN framework_NN e.g._FW ,_, =_JJ -_: =[_NN 4_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Hence_RB ,_, the_DT expression_NN PD_NN |_CD |_NNP Am_NNP |_NNP |_NNP 1_CD in_IN m_NN =_JJ 1_CD formulation_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- will_MD be_VB used_VBN to_TO replace_VB the_DT ideal_JJ expression_NN −_NN P_NN D_NN m_NN =_JJ 1_CD 1_CD -LRB-_-LRB- Am_VBP =_JJ 0_CD -RRB-_-RRB- ._.
The_DT later_RB would_MD lead_VB to_TO a_DT 0-1_CD Mixed_JJ Integer_NN Programming_NN -LRB-_-LRB- MIP_NN -RRB-_-RRB- problem_NN ,_, know_VBP
A_DT :_: ℜ_NN D_NN →_NN ℜ_NN d_NN that_WDT relates_VBZ any_DT point_NN in_IN the_DT original_JJ space_NN to_TO its_PRP$ low_JJ dimensional_JJ counterpart_NN ._.
When_WRB A_NN is_VBZ a_DT linear_JJ transformation_NN ,_, this_DT can_MD be_VB thought_VBN of_IN as_IN learning_VBG a_DT Mahalanobis_NNP distance_NN -LRB-_-LRB- e.g._FW ,_, see_VBP =_JJ -_: =[_NN 18_CD ,_, 12_CD ,_, 16_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Sec_NN ._.
1.2_CD describes_VBZ the_DT connection_NN and_CC differences_NNS between_IN the_DT formulation_NN introduced_VBN in_IN this_DT paper_NN ,_, the_DT above_JJ ,_, and_CC other_JJ related_JJ methods_NNS ._.
1.1_CD Specifying_VBG distance_NN relationships_NNS as_IN side_JJ information_NN I_PRP
B_NN =_JJ B_NN ⊤_NN B_NN 0_CD ,_, where_WRB the_DT set_NN of_IN constraints_NNS involving_VBG B_NN follow_VBP from_IN Eqs_NNP ._.
5_CD This_DT is_VBZ also_RB a_DT semi-definite_JJ programming_NN -LRB-_-LRB- SDP_NN -RRB-_-RRB- problem_NN which_WDT is_VBZ convex_NN and_CC can_MD be_VB solved_VBN using_VBG specialized_JJ SDP_NN solvers_NNS like_IN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, we_PRP will_MD further_RB develop_VB the_DT formulation_NN presented_VBN above_RB by_IN restricting_VBG our_PRP$ solution_NN space_NN to_TO a_DT subfamily_NN of_IN the_DT PSD_NNP matrices_NNS :_: the_DT set_NN of_IN diagonal_JJ dominant_JJ matrices_NNS ._.
4.2.1_CD Imposing_VBG diagonal_JJ
to_TO a_DT lower_JJR dimensional_JJ one_CD such_JJ that_IN -LRB-_-LRB- some_DT or_CC all_DT -RRB-_-RRB- distances_NNS are_VBP preserved_JJ ._.
Examples_NNS of_IN these_DT approaches_NNS include_VBP algorithms_NNS such_JJ as_IN Multidimensional_JJ Scaling_NN -LRB-_-LRB- MDS_NN -RRB-_-RRB- -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, Locally_RB Linear_JJ Embeddings_NNS -LRB-_-LRB- LLE_NN -RRB-_-RRB- =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, ISOMAP_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, and_CC low-dimensional_JJ embeddings_NNS via_IN SDP_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
These_DT methods_NNS can_MD implicitly_RB reduce_VB the_DT amount_NN of_IN computation_NN regarding_VBG distance_NN calculations_NNS ;_: however_RB due_JJ to_TO their_PRP$ purely_RB unsupervised_JJ natu_NN
features_NNS ._.
This_DT simple_JJ but_CC powerful_JJ change_NN may_MD increase_VB classification_NN performance_NN considerably_RB and_CC it_PRP is_VBZ part_NN of_IN our_PRP$ future_JJ work_NN ._.
Another_DT idea_NN worth_JJ exploring_VBG is_VBZ the_DT application_NN of_IN LPboost_NN algorithms_NNS =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: to_TO allow_VB our_PRP$ method_NN the_DT handling_NN of_IN datasets_NNS in_IN very_RB high_JJ dimensional_JJ spaces_NNS ._.
The_DT approach_NN followed_VBD in_IN this_DT paper_NN have_VBP possible_JJ implications_NNS in_IN other_JJ areas_NNS ._.
Our_PRP$ results_NNS suggest_VBP that_IN the_DT technique_NN app_NN
es_NNS are_VBP preserved_JJ ._.
Examples_NNS of_IN these_DT approaches_NNS include_VBP algorithms_NNS such_JJ as_IN Multidimensional_JJ Scaling_NN -LRB-_-LRB- MDS_NN -RRB-_-RRB- -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, Locally_RB Linear_JJ Embeddings_NNS -LRB-_-LRB- LLE_NN -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, ISOMAP_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, and_CC low-dimensional_JJ embeddings_NNS via_IN SDP_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
These_DT methods_NNS can_MD implicitly_RB reduce_VB the_DT amount_NN of_IN computation_NN regarding_VBG distance_NN calculations_NNS ;_: however_RB due_JJ to_TO their_PRP$ purely_RB unsupervised_JJ nature_NN ,_, they_PRP rely_VBP on_IN a_DT distance_NN function_NN to_TO be_VB given_VBN and_CC canno_NN
of_IN diagonal_JJ dominant_JJ matrices_NNS ._.
4.2.1_CD Imposing_VBG diagonal_JJ dominance_NN on_IN B_NN In_IN order_NN to_TO provide_VB a_DT better_JJR understanding_NN of_IN the_DT motivation_NN for_IN our_PRP$ next_JJ formulation_NN we_PRP present_VBP the_DT following_JJ theorem_NN stated_VBN in_IN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: :_: Theorem_NN 4.1_CD ._.
Diagonal_NNP Dominance_NNP Theorem_NNP Suppose_VB that_IN M_NNP ∈_NNP ℜ_NNP D_NNP ×_NNP D_NNP is_VBZ symmetric_JJ and_CC that_IN for_IN each_DT i_FW =_JJ 1_CD ,_, ..._: ,_, n_NN ,_, we_PRP have_VBP :_: Mii_NNP ≥_NNP X_NNP |_NNP Mij_NNP |_NNP ,_, j_NN =_JJ i_LS then_RB M_NN is_VBZ positive_JJ semi-definite_NN -LRB-_-LRB- PSD_NN -RRB-_-RRB- ._.
Furthermore_RB ,_, if_IN
ality_VB Reduction_NNP and_CC Metric_NNP Learning_NNP -LRB-_-LRB- Y_NN =_JJ Yes_NNP ,_, N_NN =_JJ No_NNP -RRB-_-RRB- Low_NNP dim_FW Generaliz_NNP ._.
Learns_FW Local-minima_FW Approach_NN -LRB-_-LRB- efficient_JJ to_TO unseen_JJ from_IN free_JJ evaluat_NN -RRB-_-RRB- data_NN examples_NNS -LRB-_-LRB- convexity_NN -RRB-_-RRB- Athitsos_NNP et_FW al._FW -LRB-_-LRB- 1_LS -RRB-_-RRB- Y_NN Y_NN Y_NN N_NN FastMap_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: Y_NN Y_NN N_NN N_NN MDS_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- Y_NN N_NN N_NN N_NN Schultz-Joachims_NNS -LRB-_-LRB- 12_CD -RRB-_-RRB- N_NN Y_NN Y_NN Y_NN -LRB-_-LRB- QP_NN -RRB-_-RRB- Wagstaff_NNP et_FW al._FW -LRB-_-LRB- 10_CD -RRB-_-RRB- N_NN Y_NN Y_NN N_NN Weinberger_NNP et_FW al._FW -LRB-_-LRB- 16_CD -RRB-_-RRB- Y_NN Y_NN N_NN Y_NN -LRB-_-LRB- SDP_NN -RRB-_-RRB- Xing_NNP et_FW al._FW -LRB-_-LRB- 18_CD -RRB-_-RRB- N_NN Y_NN Y_NN Y_NN -LRB-_-LRB- Iterat_NN ._.
Proj_NNP ._. -RRB-_-RRB-
This_DT paper_NN Y_NN Y_NN Y_NN Y_NN -LRB-_-LRB- LP_NN -RRB-_-RRB- 2_CD ._.
LEARNIN_NN
transforming_VBG a_DT set_NN of_IN data_NNS points_NNS into_IN a_DT lower_JJR dimensional_JJ one_CD such_JJ that_IN -LRB-_-LRB- some_DT or_CC all_DT -RRB-_-RRB- distances_NNS are_VBP preserved_JJ ._.
Examples_NNS of_IN these_DT approaches_NNS include_VBP algorithms_NNS such_JJ as_IN Multidimensional_JJ Scaling_NN -LRB-_-LRB- MDS_NN -RRB-_-RRB- =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Locally_RB Linear_JJ Embeddings_NNS -LRB-_-LRB- LLE_NN -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, ISOMAP_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, and_CC low-dimensional_JJ embeddings_NNS via_IN SDP_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
These_DT methods_NNS can_MD implicitly_RB reduce_VB the_DT amount_NN of_IN computation_NN regarding_VBG distance_NN calculations_NNS ;_: however_RB
nformation_NN can_MD be_VB obtained_VBN more_RBR automatically_RB ._.
In_IN choosing_VBG this_DT type_NN of_IN relative_JJ relationships_NNS ,_, we_PRP were_VBD inspired_VBN by_IN the_DT work_NN in_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- ;_: however_RB ,_, distance_NN relationships_NNS of_IN this_DT type_NN were_VBD also_RB employed_VBN in_IN =_JJ -_: =[_NN 15_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Note_VB that_IN we_PRP are_VBP not_RB interested_JJ in_IN preserving_VBG absolute_JJ distances_NNS ,_, which_WDT are_VBP in_IN general_JJ much_RB more_RBR difficult_JJ to_TO obtain_VB 3_CD ._.
Another_DT interesting_JJ property_NN of_IN this_DT type_NN of_IN distance_NN relationships_NNS is_VBZ that_IN t_NN
A_DT :_: ℜ_NN D_NN →_NN ℜ_NN d_NN that_WDT relates_VBZ any_DT point_NN in_IN the_DT original_JJ space_NN to_TO its_PRP$ low_JJ dimensional_JJ counterpart_NN ._.
When_WRB A_NN is_VBZ a_DT linear_JJ transformation_NN ,_, this_DT can_MD be_VB thought_VBN of_IN as_IN learning_VBG a_DT Mahalanobis_NNP distance_NN -LRB-_-LRB- e.g._FW ,_, see_VBP =_JJ -_: =[_NN 18_CD ,_, 12_CD ,_, 16_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Sec_NN ._.
1.2_CD describes_VBZ the_DT connection_NN and_CC differences_NNS between_IN the_DT formulation_NN introduced_VBN in_IN this_DT paper_NN ,_, the_DT above_JJ ,_, and_CC other_JJ related_JJ methods_NNS ._.
1.1_CD Specifying_VBG distance_NN relationships_NNS as_IN side_JJ information_NN I_PRP
cal-minima_NN Approach_NN -LRB-_-LRB- efficient_JJ to_TO unseen_JJ from_IN free_JJ evaluat_NN -RRB-_-RRB- data_NN examples_NNS -LRB-_-LRB- convexity_NN -RRB-_-RRB- Athitsos_NNP et_FW al._FW -LRB-_-LRB- 1_LS -RRB-_-RRB- Y_NN Y_NN Y_NN N_NN FastMap_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- Y_NN Y_NN N_NN N_NN MDS_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- Y_NN N_NN N_NN N_NN Schultz-Joachims_NNS -LRB-_-LRB- 12_CD -RRB-_-RRB- N_NN Y_NN Y_NN Y_NN -LRB-_-LRB- QP_NN -RRB-_-RRB- Wagstaff_NNP et_FW al._FW =_SYM -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: N_NN Y_NN Y_NN N_NN Weinberger_NNP et_FW al._FW -LRB-_-LRB- 16_CD -RRB-_-RRB- Y_NN Y_NN N_NN Y_NN -LRB-_-LRB- SDP_NN -RRB-_-RRB- Xing_NNP et_FW al._FW -LRB-_-LRB- 18_CD -RRB-_-RRB- N_NN Y_NN Y_NN Y_NN -LRB-_-LRB- Iterat_NN ._.
Proj_NNP ._. -RRB-_-RRB-
This_DT paper_NN Y_NN Y_NN Y_NN Y_NN -LRB-_-LRB- LP_NN -RRB-_-RRB- 2_CD ._.
LEARNING_NN METRICS_NNS Let_VBP us_PRP say_VB we_PRP are_VBP given_VBN a_DT set_NN of_IN points_NNS xk_FW ∈_FW ℜ_NN D_NN with_IN k_NN =_JJ -LCB-_-LRB- 1_CD ,_, ..._: ,_, N_NN -RCB-_-RRB-
A_DT :_: ℜ_NN D_NN →_NN ℜ_NN d_NN that_WDT relates_VBZ any_DT point_NN in_IN the_DT original_JJ space_NN to_TO its_PRP$ low_JJ dimensional_JJ counterpart_NN ._.
When_WRB A_NN is_VBZ a_DT linear_JJ transformation_NN ,_, this_DT can_MD be_VB thought_VBN of_IN as_IN learning_VBG a_DT Mahalanobis_NNP distance_NN -LRB-_-LRB- e.g._FW ,_, see_VBP =_JJ -_: =[_NN 18_CD ,_, 12_CD ,_, 16_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Sec_NN ._.
1.2_CD describes_VBZ the_DT connection_NN and_CC differences_NNS between_IN the_DT formulation_NN introduced_VBN in_IN this_DT paper_NN ,_, the_DT above_JJ ,_, and_CC other_JJ related_JJ methods_NNS ._.
1.1_CD Specifying_VBG distance_NN relationships_NNS as_IN side_JJ information_NN I_PRP
arly_RB not_RB equivalent_JJ to_TO the_DT base_NN formulation_NN ;_: however_RB ,_, the_DT 1-norm_NN tends_VBZ to_TO suppress_VB terms_NNS and_CC to_TO produce_VB sparse_JJ solutions_NNS ._.
This_DT has_VBZ been_VBN empirically_RB validated_VBN ,_, in_IN particular_JJ in_IN the_DT SVM_NN framework_NN e.g._FW ,_, =_JJ -_: =[_NN 4_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Hence_RB ,_, the_DT expression_NN PD_NN |_CD |_NNP Am_NNP |_NNP |_NNP 1_CD in_IN m_NN =_JJ 1_CD formulation_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- will_MD be_VB used_VBN to_TO replace_VB the_DT ideal_JJ expression_NN −_NN P_NN D_NN m_NN =_JJ 1_CD 1_CD -LRB-_-LRB- Am_VBP =_JJ 0_CD -RRB-_-RRB- ._.
The_DT later_RB would_MD lead_VB to_TO a_DT 0-1_CD Mixed_JJ Integer_NN Programming_NN -LRB-_-LRB- MIP_NN -RRB-_-RRB- problem_NN ,_, know_VBP
can_MD not_RB always_RB be_VB used_VBN because_IN e.g._FW ,_, it_PRP is_VBZ expensive_JJ to_TO evaluate_VB -RRB-_-RRB- ,_, this_DT information_NN can_MD be_VB obtained_VBN more_RBR automatically_RB ._.
In_IN choosing_VBG this_DT type_NN of_IN relative_JJ relationships_NNS ,_, we_PRP were_VBD inspired_VBN by_IN the_DT work_NN in_IN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_JJ -_: ;_: however_RB ,_, distance_NN relationships_NNS of_IN this_DT type_NN were_VBD also_RB employed_VBN in_IN -LRB-_-LRB- 15_CD ,_, 12_CD -RRB-_-RRB- ._.
Note_VB that_IN we_PRP are_VBP not_RB interested_JJ in_IN preserving_VBG absolute_JJ distances_NNS ,_, which_WDT are_VBP in_IN general_JJ much_RB more_RBR difficult_JJ to_TO obtain_VB 3_CD ._.
