Turning_VBG down_RP the_DT noise_NN in_IN the_DT blogosphere_NN
In_IN recent_JJ years_NNS ,_, the_DT blogosphere_NN has_VBZ experienced_VBN a_DT substantial_JJ increase_NN in_IN the_DT number_NN of_IN posts_NNS published_VBN daily_RB ,_, forcing_VBG users_NNS to_TO cope_VB with_IN information_NN overload_NN ._.
The_DT task_NN of_IN guiding_VBG users_NNS through_IN this_DT flood_NN of_IN information_NN has_VBZ thus_RB become_VBN critical_JJ ._.
To_TO address_VB this_DT issue_NN ,_, we_PRP present_VBP a_DT principled_JJ approach_NN for_IN picking_VBG a_DT set_NN of_IN posts_NNS that_WDT best_RB covers_VBZ the_DT important_JJ stories_NNS in_IN the_DT blogosphere_NN ._.
We_PRP define_VBP a_DT simple_JJ and_CC elegant_JJ notion_NN of_IN coverage_NN and_CC formalize_VB it_PRP as_IN a_DT submodular_JJ optimization_NN problem_NN ,_, for_IN which_WDT we_PRP can_MD efficiently_RB compute_VB a_DT near-optimal_JJ solution_NN ._.
In_IN addition_NN ,_, since_IN people_NNS have_VBP varied_VBN interests_NNS ,_, the_DT ideal_JJ coverage_NN algorithm_NN should_MD incorporate_VB user_NN preferences_NNS in_IN order_NN to_TO tailor_VB the_DT selected_VBN posts_NNS to_TO individual_JJ tastes_NNS ._.
We_PRP define_VBP the_DT problem_NN of_IN learning_VBG a_DT personalized_JJ coverage_NN function_NN by_IN providing_VBG an_DT appropriate_JJ user-interaction_JJ model_NN and_CC formalizing_VBG an_DT online_JJ learning_NN framework_NN for_IN this_DT task_NN ._.
We_PRP then_RB provide_VBP a_DT no-regret_JJ algorithm_NN which_WDT can_MD quickly_RB learn_VB a_DT user_NN 's_POS preferences_NNS from_IN limited_JJ feedback_NN ._.
We_PRP evaluate_VBP our_PRP$ coverage_NN and_CC personalization_NN algorithms_NNS extensively_RB over_IN real_JJ blog_NN data_NNS ._.
Results_NNS from_IN a_DT user_NN study_NN show_VBP that_IN our_PRP$ simple_JJ coverage_NN algorithm_NN does_VBZ as_RB well_RB as_IN most_JJS popular_JJ blog_NN aggregation_NN sites_NNS ,_, including_VBG Google_NNP Blog_NNP Search_VB ,_, Yahoo_NNP !_.
Buzz_NNP ,_, and_CC Digg_NNP ._.
Furthermore_RB ,_, we_PRP demonstrate_VBP empirically_RB that_IN our_PRP$ algorithm_NN can_MD successfully_RB adapt_VB to_TO user_NN preferences_NNS ._.
We_PRP believe_VBP that_IN our_PRP$ technique_NN ,_, especially_RB with_IN personalization_NN ,_, can_MD dramatically_RB reduce_VB information_NN overload_NN ._.
next_RB ,_, we_PRP employ_VBP a_DT simple_JJ bipartite_JJ matching_NN algorithm_NN to_TO map_VB personalization_NN weights_NNS across_IN epochs_NNS ._.
Alternatively_RB ,_, one_PRP could_MD use_VB more_RBR recent_JJ topic_NN models_NNS that_WDT are_VBP designed_VBN to_TO work_VB on_IN streaming_NN data_NNS -LRB-_-LRB- =_JJ -_: =_JJ Canini_FW et_FW al._FW ,_, 2009_CD -_: =--RRB-_NN ._.
130_CD 1_CD 2_CD 3_CD 4_CD 5_CD number_NN of_IN liked_VBN posts_NNS 6.5_CD 6_CD 5.5_CD 5_CD 4.5_CD 4_CD 3.5_CD 3_CD epochs_NNS personalized_VBD unpersonalized_JJ F_NN personalized_VBD \/_: F_NN unpersonalized_VBD 1.1_CD 1.08_CD 1.06_CD 1.04_CD 1.02_CD 1_CD F_NN -LRB-_-LRB- FanHouse.com_NN -RRB-_-RRB- F_NN -LRB-_-LRB- Deadspin.com_NN -RRB-_-RRB- F_NN -LRB-_-LRB- Huffingt_NN
also_RB incorporate_VB personalization_NN into_IN our_PRP$ algorithm_NN ,_, which_WDT they_PRP do_VBP not_RB ._.
There_EX has_VBZ also_RB been_VBN extensive_JJ work_NN on_IN building_NN models_NNS and_CC analyzing_VBG the_DT structure_NN of_IN the_DT blogosphere_NN ._.
For_IN example_NN ,_, Finin_NNP et_FW al._FW =_SYM -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: present_VB a_DT model_NN of_IN information_NN flow_NN in_IN the_DT blogosphere_NN ._.
We_PRP could_MD potentially_RB leverage_NN such_JJ analysis_NN in_IN the_DT future_NN in_IN order_NN to_TO extract_VB better_JJR features_NNS for_IN our_PRP$ algorithms_NNS ._.
Blogscope_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- is_VBZ intended_VBN to_TO
rload_NN ._.
∗_NNP School_NNP of_IN Computer_NNP Science_NNP ,_, Carnegie_NNP Mellon_NNP University_NNP ,_, Pittsburgh_NNP ,_, PA_NN ,_, USAKeywords_NNS :_: Blogs_NNS ,_, Personalization1_NN Introduction_NN ``_`` How_WRB many_JJ blogs_NNS does_VBZ the_DT world_NN need_VB ?_. ''_''
asked_VBD TIME_NNP Magazine_NNP in_IN 2008_CD -LRB-_-LRB- =_JJ -_: =_JJ Kinsley_NNP ,_, 2008_CD -_: =--RRB-_NN ,_, claiming_VBG that_IN there_EX are_VBP already_RB too_RB many_JJ ._.
Indeed_RB ,_, the_DT blogosphere_NN has_VBZ experienced_VBN a_DT substantial_JJ increase_NN in_IN the_DT number_NN of_IN posts_NNS published_VBN daily_RB ._.
One_CD immediate_JJ consequence_NN is_VBZ that_IN many_JJ readers_NNS now_RB s_VBZ
be_VB robust_JJ to_TO content_NN extraction_NN problems_NNS ._.
For_IN each_DT post_NN ,_, we_PRP extract_VBP named_VBN entities_NNS and_CC noun_NN phrases_NNS using_VBG the_DT Stanford_NNP Named_VBD Entity_NNP Recognizer_NNP -LRB-_-LRB- Finkel_NNP et_FW al._FW ,_, 2005_CD -RRB-_-RRB- and_CC the_DT LBJ_NNP Part_NNP of_IN Speech_NNP Tagger_NNP -LRB-_-LRB- =_JJ -_: =_JJ Rizzolo_NNP &_CC Roth_NNP ,_, 2007_CD -_: =--RRB-_NN ,_, respectively_RB ._.
We_PRP remove_VBP infrequent_JJ named_VBN entities_NNS and_CC uninformative_JJ noun_NN phrases_NNS -LRB-_-LRB- e.g._FW ,_, common_JJ nouns_NNS such_JJ as_IN ``_`` year_NN ''_'' -RRB-_-RRB- ,_, leaving_VBG us_PRP with_IN a_DT total_JJ collection_NN size_NN of_IN nearly_RB 3,000_CD ._.
-LRB-_-LRB- More_JJR details_NNS can_MD be_VB f_SYM
te_VB in_IN the_DT blogosphere_NN to_TO an_DT extent_NN that_WDT is_VBZ largely_RB uncorrelated_JJ with_IN their_PRP$ true_JJ importance_NN ._.
For_IN example_NN ,_, in_IN the_DT spring_NN of_IN 2007_CD ,_, Politico_NNP broke_VBD a_DT story_NN about_IN John_NNP Edwards_NNP '_POS $_$ 400_CD haircut_NN in_IN a_DT blog_NN post_NN -LRB-_-LRB- =_JJ -_: =_JJ Smith_NNP ,_, 2007_CD -_: =--RRB-_NN ,_, which_WDT was_VBD almost_RB instantly_RB seized_VBN upon_IN by_IN the_DT rest_NN of_IN the_DT blogosphere_NN ._.
Over_IN the_DT next_JJ two_CD weeks_NNS ,_, the_DT haircut_NN story_NN sparked_VBD several_JJ major_JJ online_NN debates_NNS ._.
Avoiding_VBG this_DT story_NN was_VBD difficult_JJ for_IN most_JJS We_PRP
h_NN eight_CD hour_NN segment_NN ._.
Moreover_RB ,_, as_IN described_VBN in_IN section_NN 4_CD ,_, our_PRP$ data_NNS is_VBZ very_RB noisy_JJ ,_, and_CC we_PRP do_VBP not_RB have_VB access_NN to_TO click-through_JJ rates_NNS ._.
Another_DT line_NN of_IN related_JJ research_NN is_VBZ the_DT area_NN of_IN subtopic_JJ retrieval_NN =_JJ -_: =[_NN 27_CD ,_, 13_CD ,_, 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN subtopic_JJ retrieval_NN ,_, the_DT task_NN is_VBZ to_TO retrieve_VB documents_NNS that_WDT cover_VBP many_JJ subtopics_NNS of_IN the_DT given_VBN query_NN ._.
In_IN the_DT traditional_JJ information_NN retrieval_NN setting_NN ,_, it_PRP is_VBZ assumed_VBN that_IN the_DT relevance_NN of_IN each_DT docu_NN
ur_NN algorithm_NN must_MD be_VB robust_JJ to_TO content_NN extraction_NN problems_NNS ._.
For_IN each_DT post_NN ,_, we_PRP extract_VBP named_VBN entities_NNS and_CC noun_NN phrases_NNS using_VBG the_DT Stanford_NNP Named_VBD Entity_NNP Recognizer_NNP -LRB-_-LRB- 17_CD -RRB-_-RRB- and_CC the_DT LBJ_NNP Part_NNP of_IN Speech_NNP Tagger_NNP =_SYM -_: =[_NN 25_CD -RRB-_-RRB- -_: =_JJ -_: ,_, respectively_RB ._.
We_PRP remove_VBP infrequent_JJ named_VBN entities_NNS and_CC uninformative_JJ noun_NN phrases_NNS -LRB-_-LRB- e.g._FW ,_, common_JJ nouns_NNS such_JJ as_IN ``_`` year_NN ''_'' -RRB-_-RRB- ,_, leaving_VBG us_PRP with_IN a_DT total_JJ collection_NN size_NN of_IN nearly_RB 3,000_CD ._.
-LRB-_-LRB- More_JJR details_NNS can_MD be_VB fo_FW
on_IN that_DT reading_NN a_DT post_NN s_NN after_IN reading_VBG a_DT small_JJ set_NN of_IN posts_NNS A_NN provides_VBZ more_JJR coverage_NN than_IN reading_VBG s_NN after_IN having_VBG already_RB read_VB the_DT larger_JJR set_NN B_NN ⊇_FW A._FW Although_IN maximizing_VBG submodular_JJ functions_NNS is_VBZ NP-hard_NN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_JJ -_: ,_, by_IN discovering_VBG this_DT property_NN in_IN our_PRP$ problem_NN ,_, we_PRP can_MD take_VB advantage_NN of_IN several_JJ efficient_JJ approximation_NN algorithms_NNS with_IN theoretical_JJ guarantees_NNS ._.
For_IN example_NN ,_, the_DT classic_JJ result_NN of_IN Nemhauser_NNP et_FW al._FW -LRB-_-LRB- 24_CD -RRB-_-RRB-
However_RB ,_, while_IN subtopic_JJ retrieval_NN is_VBZ query-based_JJ ,_, we_PRP intend_VBP to_TO cover_VB all_PDT the_DT popular_JJ stories_NNS being_VBG discussed_VBN in_IN the_DT blogosphere_NN ._.
Two_CD common_JJ approaches_NNS to_TO personalization_NN are_VBP collaborative_JJ filtering_VBG -LRB-_-LRB- =_JJ -_: =_JJ Linden_NNP et_FW al._FW ,_, 2003_CD -_: =_JJ -_: ;_: Das_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- and_CC content-based_JJ filtering_VBG ._.
In_IN collaborative_JJ filtering_VBG ,_, user_NN preferences_NNS are_VBP learned_VBN in_IN a_DT content-agnostic_JJ manner_NN by_IN correlating_VBG the_DT user_NN 's_POS past_JJ activity_NN with_IN data_NNS from_IN the_DT entire_NN
99_CD -RRB-_-RRB- ,_, by_IN discovering_VBG this_DT property_NN in_IN our_PRP$ problem_NN ,_, we_PRP can_MD take_VB advantage_NN of_IN several_JJ efficient_JJ approximation_NN algorithms_NNS with_IN theoretical_JJ guarantees_NNS ._.
For_IN example_NN ,_, the_DT classic_JJ result_NN of_IN Nemhauser_NNP et_FW al._FW -LRB-_-LRB- =_JJ -_: =_JJ Nemhauser_NNP et_FW al._FW ,_, 1978_CD -_: =--RRB-_NN shows_VBZ that_IN by_IN simply_RB applying_VBG a_DT greedy_JJ algorithm_NN to_TO maximize_VB our_PRP$ objective_JJ function_NN in_IN Eq_NN ._.
3_CD ,_, we_PRP can_MD obtain_VB a_DT -LRB-_-LRB- 1_CD −_NN 1_CD e_LS -RRB-_-RRB- approximation_NN of_IN the_DT optimal_JJ value_NN ._.
Thus_RB ,_, a_DT simple_JJ greedy_JJ optimization_NN can_MD pr_VB
bed_NN in_IN section_NN 4_CD ,_, our_PRP$ data_NNS is_VBZ very_RB noisy_JJ ,_, and_CC we_PRP do_VBP not_RB have_VB access_NN to_TO click-through_JJ rates_NNS ._.
Another_DT line_NN of_IN related_JJ research_NN is_VBZ the_DT area_NN of_IN subtopic_JJ retrieval_NN -LRB-_-LRB- Zhai_NNP et_FW al._FW ,_, 2003_CD ;_: Chen_NNP &_CC Karger_NNP ,_, 2006_CD ;_: =_JJ -_: =_JJ Carbonell_NNP &_CC Goldstein_NNP ,_, 1998_CD -_: =--RRB-_NN ._.
In_IN subtopic_JJ retrieval_NN ,_, the_DT task_NN is_VBZ to_TO retrieve_VB documents_NNS that_WDT cover_VBP many_JJ subtopics_NNS of_IN the_DT given_VBN query_NN ._.
In_IN the_DT traditional_JJ information_NN retrieval_NN setting_NN ,_, it_PRP is_VBZ assumed_VBN that_IN the_DT relevance_NN of_IN each_DT doc_NN
bly_RB noisy_JJ even_RB after_IN cleaning_NN ._.
Thus_RB ,_, our_PRP$ algorithm_NN must_MD be_VB robust_JJ to_TO content_NN extraction_NN problems_NNS ._.
For_IN each_DT post_NN ,_, we_PRP extract_VBP named_VBN entities_NNS and_CC noun_NN phrases_NNS using_VBG the_DT Stanford_NNP Named_VBD Entity_NNP Recognizer_NNP -LRB-_-LRB- =_JJ -_: =_JJ Finkel_NNP et_FW al._FW ,_, 2005_CD -_: =--RRB-_NN and_CC the_DT LBJ_NNP Part_NNP of_IN Speech_NNP Tagger_NNP -LRB-_-LRB- Rizzolo_NNP &_CC Roth_NNP ,_, 2007_CD -RRB-_-RRB- ,_, respectively_RB ._.
We_PRP remove_VBP infrequent_JJ named_VBN entities_NNS and_CC uninformative_JJ noun_NN phrases_NNS -LRB-_-LRB- e.g._FW ,_, common_JJ nouns_NNS such_JJ as_IN ``_`` year_NN ''_'' -RRB-_-RRB- ,_, leaving_VBG us_PRP with_IN a_DT tota_NN
6_CD -RRB-_-RRB- ,_, our_PRP$ approach_NN updates_NNS our_PRP$ estimated_VBN π_NN -LRB-_-LRB- t_NN -RRB-_-RRB- using_VBG a_DT multiplicative_JJ update_VB rule_NN ._.
In_IN particular_JJ ,_, our_PRP$ approach_NN can_MD be_VB viewed_VBN as_IN a_DT special_JJ case_NN of_IN Freund_NNP and_CC Schapire_NNP 's_POS multiplicative_JJ weights_NNS algorithm_NN -LRB-_-LRB- =_JJ -_: =_JJ Freund_NNP &_CC Schapire_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
The_DT algorithm_NN starts_VBZ by_IN choosing_VBG an_DT initial_JJ set_NN of_IN weights_NNS π_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
-LRB-_-LRB- WLOG_NN ,_, we_PRP assume_VBP weights_NNS are_VBP normalized_VBN to_TO sum_VB to_TO 1_CD ,_, since_IN the_DT coverage_NN function_NN is_VBZ insensitive_JJ to_TO scaling_VBG ._. -RRB-_-RRB-
In_IN the_DT absence_NN of_IN pri_NN
n_NN Section_NN 2.2_CD ,_, we_PRP can_MD directly_RB define_VB coverj_NN -LRB-_-LRB- i_LS -RRB-_-RRB- =_JJ P_NN -LRB-_-LRB- ui_FW |_FW post_NN j_NN -RRB-_-RRB- ,_, which_WDT in_IN the_DT setting_NN of_IN topic_NN models_NNS is_VBZ the_DT probability_NN that_WDT post_VBP j_NN is_VBZ about_IN topic_NN i._NN We_PRP use_VBP a_DT Gibbs_NNP sampling_NN implementation_NN of_IN LDA_NN -LRB-_-LRB- =_JJ -_: =_JJ Griffiths_NNP &_CC Steyvers_NNP ,_, 2004_CD -_: =--RRB-_NN with_IN 100_CD topics_NNS and_CC the_DT default_NN parameter_NN settings_NNS ._.
6_CD http:\/\/www.spinn3r.com_NN 8Once_NN we_PRP have_VBP extracted_VBN the_DT named_VBN entities_NNS and_CC noun_NN phrases_NNS ,_, LDA_NNP is_VBZ the_DT slowest_JJS part_NN of_IN running_VBG TDN+LDA_NN ._.
After_IN a_DT 300_CD ite_NN
es_NNS ._.
We_PRP intend_VBP to_TO address_VB this_DT issue_NN in_IN future_JJ work_NN ._.
3.3_CD Learning_NNP a_DT User_NN 's_POS Preferences_NNPS We_PRP now_RB describe_VBP our_PRP$ algorithm_NN for_IN learning_VBG π_FW ∗_FW from_IN repeated_VBN user_NN feedback_NN sessions_NNS ._.
Like_IN many_JJ online_JJ algorithms_NNS -LRB-_-LRB- =_JJ -_: =_JJ Cesa-Bianchi_NNP &_CC Lugosi_NNP ,_, 2006_CD -_: =--RRB-_NN ,_, our_PRP$ approach_NN updates_NNS our_PRP$ estimated_VBN π_NN -LRB-_-LRB- t_NN -RRB-_-RRB- using_VBG a_DT multiplicative_JJ update_VB rule_NN ._.
In_IN particular_JJ ,_, our_PRP$ approach_NN can_MD be_VB viewed_VBN as_IN a_DT special_JJ case_NN of_IN Freund_NNP and_CC Schapire_NNP 's_POS multiplicative_JJ weights_NNS algorithm_NN -LRB-_-LRB- F_NN
eight_CD hour_NN segment_NN ._.
Moreover_RB ,_, as_IN described_VBN in_IN section_NN 4_CD ,_, our_PRP$ data_NNS is_VBZ very_RB noisy_JJ ,_, and_CC we_PRP do_VBP not_RB have_VB access_NN to_TO click-through_JJ rates_NNS ._.
Another_DT line_NN of_IN related_JJ research_NN is_VBZ the_DT area_NN of_IN subtopic_JJ retrieval_NN -LRB-_-LRB- =_JJ -_: =_JJ Zhai_NNP et_FW al._FW ,_, 2003_CD -_: =_JJ -_: ;_: Chen_NNP &_CC Karger_NNP ,_, 2006_CD ;_: Carbonell_NNP &_CC Goldstein_NNP ,_, 1998_CD -RRB-_-RRB- ._.
In_IN subtopic_JJ retrieval_NN ,_, the_DT task_NN is_VBZ to_TO retrieve_VB documents_NNS that_WDT cover_VBP many_JJ subtopics_NNS of_IN the_DT given_VBN query_NN ._.
In_IN the_DT traditional_JJ information_NN retrieval_NN set_VBN
alue_NN ._.
Thus_RB ,_, a_DT simple_JJ greedy_JJ optimization_NN can_MD provide_VB us_PRP with_IN a_DT near-optimal_JJ solution_NN ._.
However_RB ,_, since_IN our_PRP$ set_NN of_IN posts_NNS is_VBZ very_RB large_JJ ,_, a_DT naïve_JJ greedy_JJ approach_NN can_MD be_VB too_RB costly_JJ ._.
Therefore_RB ,_, we_PRP use_VBP CELF_NN -LRB-_-LRB- =_JJ -_: =_JJ Leskovec_NNP et_FW al._FW ,_, 2007_CD -_: =--RRB-_NN ,_, which_WDT provides_VBZ the_DT same_JJ approximation_NN guarantees_NNS ,_, but_CC uses_VBZ lazy_JJ evaluations_NNS ,_, often_RB leading_VBG to_TO dramatic_JJ speedups_NNS ._.
3_CD Personalization_NN Thus_RB far_RB ,_, we_PRP have_VBP defined_VBN a_DT global_JJ notion_NN of_IN coverage_NN for_IN the_DT blog_NN
osts_NNS in_IN the_DT same_JJ cluster_NN cover_VBP the_DT same_JJ features_NNS ._.
Then_RB ,_, given_VBN clusters_NNS ,_, we_PRP can_MD pick_VB a_DT representative_JJ post_NN from_IN each_DT of_IN the_DT k_NN largest_JJS clusters_NNS ._.
Such_JJ clustering_NN approaches_NNS are_VBP common_JJ in_IN the_DT literature_NN -LRB-_-LRB- =_JJ -_: =_JJ Zhang_NNP et_FW al._FW ,_, 2005_CD -_: =--RRB-_NN ._.
However_RB ,_, most_JJS clustering_NN methods_NNS require_VBP us_PRP to_TO compute_VB the_DT distance_NN between_IN every_DT pair_NN of_IN posts_NNS ,_, which_WDT amounts_VBZ to_TO O_NN -LRB-_-LRB- n2_NN -RRB-_-RRB- comparisons_NNS for_IN n_NN posts_NNS ._.
Due_JJ to_TO the_DT sizable_JJ amount_NN of_IN posts_NNS published_VBN daily_RB ,_,
n_NN that_IN reading_VBG a_DT post_NN s_NN after_IN reading_VBG a_DT small_JJ set_NN of_IN posts_NNS A_NN provides_VBZ more_JJR coverage_NN than_IN reading_VBG s_NN after_IN having_VBG already_RB read_VB the_DT larger_JJR set_NN B_NN ⊇_FW A._FW Although_IN maximizing_VBG submodular_JJ functions_NNS is_VBZ NP-hard_JJ -LRB-_-LRB- =_JJ -_: =_JJ Khuller_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN ,_, by_IN discovering_VBG this_DT property_NN in_IN our_PRP$ problem_NN ,_, we_PRP can_MD take_VB advantage_NN of_IN several_JJ efficient_JJ approximation_NN algorithms_NNS with_IN theoretical_JJ guarantees_NNS ._.
For_IN example_NN ,_, the_DT classic_JJ result_NN of_IN Nemhauser_NNP et_FW al._FW -LRB-_-LRB- Ne_NN
evaluate_VB an_DT instantiation_NN of_IN our_PRP$ algorithm_NN with_IN high_JJ level_NN topic_NN model-based_JJ features_NNS ,_, which_WDT we_PRP refer_VBP to_TO as_IN TDN+LDA_NN ._.
We_PRP define_VBP our_PRP$ set_NN of_IN features_NNS as_IN topics_NNS from_IN a_DT latent_JJ Dirichlet_NN allocation_NN -LRB-_-LRB- LDA_NN -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =_JJ Blei_NNP et_FW al._FW ,_, 2003_CD -_: =--RRB-_NN topic_NN model_NN learned_VBD on_IN the_DT noun_NN phrases_NNS and_CC named_VBN entities_NNS described_VBN above_IN ._.
We_PRP take_VBP the_DT weight_NN of_IN each_DT feature_NN to_TO be_VB the_DT fraction_NN of_IN words_NNS in_IN the_DT corpus_NN assigned_VBN to_TO that_DT topic_NN ._.
As_IN described_VBN in_IN Secti_NNP
pic_JJ retrieval_NN is_VBZ query-based_JJ ,_, we_PRP intend_VBP to_TO cover_VB all_PDT the_DT popular_JJ stories_NNS being_VBG discussed_VBN in_IN the_DT blogosphere_NN ._.
Two_CD common_JJ approaches_NNS to_TO personalization_NN are_VBP collaborative_JJ filtering_VBG -LRB-_-LRB- Linden_NNP et_FW al._FW ,_, 2003_CD ;_: =_JJ -_: =_JJ Das_FW et_FW al._FW ,_, 2007_CD -_: =--RRB-_NN and_CC content-based_JJ filtering_VBG ._.
In_IN collaborative_JJ filtering_VBG ,_, user_NN preferences_NNS are_VBP learned_VBN in_IN a_DT content-agnostic_JJ manner_NN by_IN correlating_VBG the_DT user_NN 's_POS past_JJ activity_NN with_IN data_NNS from_IN the_DT entire_JJ user_NN community_NN ._.
I_PRP
hope_NN to_TO incorporate_VB the_DT link_NN structure_NN between_IN posts_NNS into_IN our_PRP$ algorithm_NN ._.
Another_DT key_JJ difference_NN is_VBZ that_IN most_JJS of_IN these_DT websites_NNS lack_VBP the_DT personalization_NN functionality_NN we_PRP provide_VBP ._.
14In_VB a_DT recent_JJ paper_NN -LRB-_-LRB- =_JJ -_: =_JJ Agarwal_NNP et_FW al._FW ,_, 2008_CD -_: =--RRB-_NN ,_, Agarwal_NNP et_NNP ._.
al_FW address_NN a_DT problem_NN similar_JJ to_TO ours_PRP ._.
Their_PRP$ task_NN is_VBZ to_TO select_VB four_CD out_IN of_IN a_DT set_NN of_IN sixteen_CD stories_NNS to_TO be_VB displayed_VBN on_IN the_DT Yahoo_NNP !_.
homepage_NN ._.
The_DT sixteen_CD stories_NNS are_VBP manually_RB picked_VBN by_IN hum_NN
