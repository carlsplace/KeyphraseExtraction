Scalable_JJ look-ahead_JJ linear_JJ regression_NN trees_NNS
Most_JJS decision_NN tree_NN algorithms_NNS base_VBP their_PRP$ splitting_JJ decisions_NNS on_IN a_DT piecewise_JJ constant_JJ model_NN ._.
Often_RB these_DT splitting_JJ algorithms_NNS are_VBP extrapolated_VBN to_TO trees_NNS with_IN non-constant_JJ models_NNS at_IN the_DT leaf_NN nodes_NNS ._.
The_DT motivation_NN behind_IN Look-ahead_JJ Linear_JJ Regression_NN Trees_NNP -LRB-_-LRB- LLRT_NNP -RRB-_-RRB- is_VBZ that_IN out_IN of_IN all_PDT the_DT methods_NNS proposed_VBN to_TO date_NN ,_, there_EX has_VBZ been_VBN no_DT scalable_JJ approach_NN to_TO exhaustively_RB evaluate_VB all_DT possible_JJ models_NNS in_IN the_DT leaf_NN nodes_NNS in_IN order_NN to_TO obtain_VB an_DT optimal_JJ split_NN ._.
Using_VBG several_JJ optimizations_NNS ,_, LLRT_NN is_VBZ able_JJ to_TO generate_VB and_CC evaluate_VB thousands_NNS of_IN linear_JJ regression_NN models_NNS per_IN second_NN ._.
This_DT allows_VBZ for_IN a_DT near-exhaustive_JJ evaluation_NN of_IN all_DT possible_JJ splits_VBZ in_IN a_DT node_NN ,_, based_VBN on_IN the_DT quality_NN of_IN fit_NN of_IN linear_JJ regression_NN models_NNS in_IN the_DT resulting_VBG branches_NNS ._.
We_PRP decompose_VBP the_DT calculation_NN of_IN the_DT Residual_JJ Sum_NN of_IN Squares_NNS in_IN such_PDT a_DT way_NN that_IN a_DT large_JJ part_NN of_IN it_PRP is_VBZ pre-computed_JJ ._.
The_DT resulting_VBG method_NN is_VBZ highly_RB scalable_JJ ._.
We_PRP observe_VBP it_PRP to_TO obtain_VB high_JJ predictive_JJ accuracy_NN for_IN problems_NNS with_IN strong_JJ mutual_JJ dependencies_NNS between_IN attributes_NNS ._.
We_PRP report_VBP on_IN experiments_NNS with_IN two_CD simulated_JJ and_CC seven_CD real_JJ data_NNS sets_NNS ._.
tely_RB be_VB solved_VBN by_IN a_DT single_JJ regression_NN model_NN ._.
Decision_NN tree_NN algorithms_NNS provide_VBP the_DT basis_NN for_IN recursively_RB partitioning_VBG the_DT data_NNS ,_, and_CC fitting_JJ local_JJ models_NNS in_IN the_DT leaves_NNS ._.
The_DT CART_NN decision_NN tree_NN algorithm_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: selects_VBZ the_DT split_NN variable_JJ and_CC split_JJ value_NN that_WDT minimizes_VBZ the_DT weighted_JJ sum_NN of_IN the_DT variances_NNS of_IN the_DT target_NN values_NNS in_IN the_DT two_CD subsets_NNS ._.
Identically_RB ,_, the_DT CART_NN algorithm_NN finds_VBZ a_DT split_NN that_WDT minimizes_VBZ the_DT R_NN
trees_NNS by_IN generating_VBG models_NNS in_IN the_DT leaf_NN nodes_NNS of_IN the_DT tree_NN rather_RB than_IN to_TO simply_RB predict_VB the_DT mean_NN ._.
M5_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, HTL_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, SECRET_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, incremental_JJ learning_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, SUPPORT_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, GUIDE_NNP -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, PHDRT_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- and_CC SMOTI_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP some_DT of_IN the_DT model_NN tree_NN algorithms_NNS that_WDT have_VBP been_VBN proposed_VBN ._.
We_PRP refer_VBP to_TO the_DT term_NN ``_`` model_NN tree_NN ''_'' as_IN the_DT more_RBR general_JJ form_NN of_IN Linear_JJ Regression_NN Trees_NNP ,_, where_WRB the_DT models_NNS at_IN each_DT leaf_NN node_NN of_IN the_DT partiti_NN
to_TO be_VB created_VBN ._.
Therefore_RB it_PRP is_VBZ imperative_JJ that_IN a_DT significant_JJ portion_NN of_IN the_DT calculations_NNS is_VBZ carried_VBN out_RP with_IN a_DT more_RBR efficient_JJ algorithm_NN ._.
There_EX are_VBP quicker_JJR alternatives_NNS to_TO Singular_JJ Value_NN Decomposition_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: such_JJ as_IN G.E._NNP -LRB-_-LRB- Gaussian_JJ Elimination_NN -RRB-_-RRB- or_CC Cholesky_NNP Decomposition_NNP if_IN the_DT matrix_NN R_NN is_VBZ Singular_JJ Positive_JJ Definite_NN ._.
We_PRP use_VBP G.E._NNP with_IN full_JJ pivoting_NN ._.
One_CD problem_NN that_WDT needs_VBZ to_TO be_VB addressed_VBN when_WRB using_VBG G.E._NNP is_VBZ
e_LS this_DT approach_NN scalable_JJ ,_, only_RB a_DT constant_JJ number_NN of_IN candidate_NN splits_VBZ is_VBZ considered_VBN for_IN each_DT attribute_NN ._.
Potts_NNP and_CC Sammut_NNP also_RB present_VBP an_DT online_JJ version_NN of_IN their_PRP$ algorithm_NN ._.
Similar_JJ to_TO this_DT paper_NN ,_, Torgo_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: addresses_VBZ the_DT issue_NN of_IN efficiently_RB evaluating_VBG the_DT candidate_NN splits_VBZ of_IN a_DT linear_JJ regression_NN tree_NN ._.
However_RB ,_, the_DT paper_NN briefly_NN describes_VBZ the_DT problem_NN setting_NN and_CC formulation_NN and_CC does_VBZ not_RB provide_VB implement_VB
ditional_JJ approaches_NNS will_MD fail_VB to_TO efficiently_RB fit_VB the_DT underlying_VBG patterns_NNS ._.
X_NN 1_CD is_VBZ a_DT binary_JJ attribute_NN -LRB-_-LRB- 50_CD %_NN of_IN values_NNS are_VBP 0_CD ,_, 50_CD %_NN of_IN values_NNS are_VBP 1_CD -RRB-_-RRB- ,_, and_CC X_NN 2_CD is_VBZ a_DT uniformly_RB distributed_VBN continuous_JJ attribute_NN =_JJ -_: =[_NN -1,1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
There_EX are_VBP 500_CD observations_NNS in_IN the_DT training_NN set_NN and_CC 500_CD observations_NNS in_IN the_DT validation_NN set_NN ._.
The_DT equation_NN for_IN the_DT underlying_JJ model_NN is_VBZ 3x2_NN ,_, x2_NN ,_, x_NN 0_CD 1_CD y_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- x_NN 1_CD 1_CD To_TO make_VB the_DT simulated_JJ data_NNS
split_NN and_CC the_DT models_NNS in_IN the_DT leaf_NN nodes_NNS to_TO minimize_VB the_DT global_JJ RSS_NN ._.
However_RB ,_, this_DT algorithm_NN is_VBZ often_RB cited_VBN as_IN being_VBG intractable_JJ and_CC non-scalable_JJ because_IN of_IN the_DT huge_JJ complexity_NN of_IN this_DT optimization_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =-[_CD 5_CD -RRB-_-RRB- ._.
In_IN Karalic_NNP 's_POS paper_NN ,_, the_DT largest_JJS sample_NN size_NN used_VBN for_IN experimentation_NN is_VBZ 300_CD examples_NNS ;_: this_DT provides_VBZ little_JJ evidence_NN to_TO support_VB the_DT algorithm_NN 's_POS scalability_NN ._.
Most_JJS papers_NNS cite_VBP RETIS_NN as_IN a_DT non-practic_JJ
methods_NNS to_TO improve_VB upon_IN the_DT accuracy_NN of_IN regression_NN trees_NNS by_IN generating_VBG models_NNS in_IN the_DT leaf_NN nodes_NNS of_IN the_DT tree_NN rather_RB than_IN to_TO simply_RB predict_VB the_DT mean_NN ._.
M5_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, HTL_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, SECRET_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, incremental_JJ learning_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, SUPPORT_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, GUIDE_NNP -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, PHDRT_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- and_CC SMOTI_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- are_VBP some_DT of_IN the_DT model_NN tree_NN algorithms_NNS that_WDT have_VBP been_VBN proposed_VBN ._.
We_PRP refer_VBP to_TO the_DT term_NN ``_`` model_NN tree_NN ''_'' as_IN the_DT more_RBR general_JJ form_NN of_IN Linear_JJ Regression_NN Trees_NNP ,_,
ple_NN authors_NNS have_VBP proposed_VBN methods_NNS to_TO improve_VB upon_IN the_DT accuracy_NN of_IN regression_NN trees_NNS by_IN generating_VBG models_NNS in_IN the_DT leaf_NN nodes_NNS of_IN the_DT tree_NN rather_RB than_IN to_TO simply_RB predict_VB the_DT mean_NN ._.
M5_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, HTL_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, SECRET_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, incremental_JJ learning_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, SUPPORT_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, GUIDE_NNP -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, PHDRT_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- and_CC SMOTI_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- are_VBP some_DT of_IN the_DT model_NN tree_NN algorithms_NNS that_WDT have_VBP been_VBN proposed_VBN ._.
We_PRP refer_VBP to_TO the_DT term_NN ``_`` model_NN tree_NN ''_'' as_IN the_DT more_RBR general_JJ form_NN o_NN
ser_NN to_TO a_DT globally_RB optimal_JJ partition_NN than_IN the_DT basic_JJ CART_NN optimization_NN criterion_NN ._.
The_DT principle_NN of_IN fitting_JJ models_NNS to_TO the_DT branches_NNS of_IN candidate_NN splits_VBZ is_VBZ not_RB new_JJ ._.
For_IN more_JJR than_IN a_DT decade_NN ,_, Karalic_NNP 's_POS RETIS_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: model_NN tree_NN algorithm_NN optimizes_VBZ the_DT overall_JJ RSS_NN ,_, simultaneously_RB optimizing_VBG the_DT split_NN and_CC the_DT models_NNS in_IN the_DT leaf_NN nodes_NNS to_TO minimize_VB the_DT global_JJ RSS_NN ._.
However_RB ,_, this_DT algorithm_NN is_VBZ often_RB cited_VBN as_IN being_VBG intrac_JJ
of_IN regression_NN trees_NNS by_IN generating_VBG models_NNS in_IN the_DT leaf_NN nodes_NNS of_IN the_DT tree_NN rather_RB than_IN to_TO simply_RB predict_VB the_DT mean_NN ._.
M5_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, HTL_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, SECRET_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, incremental_JJ learning_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, SUPPORT_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, GUIDE_NNP -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, PHDRT_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: and_CC SMOTI_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- are_VBP some_DT of_IN the_DT model_NN tree_NN algorithms_NNS that_WDT have_VBP been_VBN proposed_VBN ._.
We_PRP refer_VBP to_TO the_DT term_NN ``_`` model_NN tree_NN ''_'' as_IN the_DT more_RBR general_JJ form_NN of_IN Linear_JJ Regression_NN Trees_NNP ,_, where_WRB the_DT models_NNS at_IN each_DT leaf_NN node_NN
he_PRP accuracy_NN of_IN regression_NN trees_NNS by_IN generating_VBG models_NNS in_IN the_DT leaf_NN nodes_NNS of_IN the_DT tree_NN rather_RB than_IN to_TO simply_RB predict_VB the_DT mean_NN ._.
M5_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, HTL_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, SECRET_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, incremental_JJ learning_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, SUPPORT_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, GUIDE_NNP =_SYM -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: ,_, PHDRT_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- and_CC SMOTI_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- are_VBP some_DT of_IN the_DT model_NN tree_NN algorithms_NNS that_WDT have_VBP been_VBN proposed_VBN ._.
We_PRP refer_VBP to_TO the_DT term_NN ``_`` model_NN tree_NN ''_'' as_IN the_DT more_RBR general_JJ form_NN of_IN Linear_JJ Regression_NN Trees_NNP ,_, where_WRB the_DT models_NNS at_IN each_DT
model_NN ._.
Multiple_JJ authors_NNS have_VBP proposed_VBN methods_NNS to_TO improve_VB upon_IN the_DT accuracy_NN of_IN regression_NN trees_NNS by_IN generating_VBG models_NNS in_IN the_DT leaf_NN nodes_NNS of_IN the_DT tree_NN rather_RB than_IN to_TO simply_RB predict_VB the_DT mean_NN ._.
M5_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, HTL_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, SECRET_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, incremental_JJ learning_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, SUPPORT_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, GUIDE_NNP -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, PHDRT_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- and_CC SMOTI_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- are_VBP some_DT of_IN the_DT model_NN tree_NN algorithms_NNS that_WDT have_VBP been_VBN proposed_VBN ._.
We_PRP refer_VBP to_TO the_DT term_NN ``_`` model_NN tree_NN ''_'' as_IN the_DT more_JJR ge_NN
sparouhov@medai.com_NNP Tobias_NNP Scheffer_NNP Max_NNP Planck_NNP Institute_NNP for_IN Computer_NNP Science_NNP ,_, Saarbruecken_NNP ,_, Germany_NNP scheffer@mpi-inf.mpg.de_NN -LRB-_-LRB- the_DT mean_NN -RRB-_-RRB- for_IN each_DT of_IN the_DT subsets_NNS ._.
In_IN contrast_NN to_TO Linear_JJ Regression_NN Trees_NNP =_SYM -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT CART_NN regression_NN tree_NN algorithm_NN uses_VBZ the_DT mean_NN for_IN the_DT target_NN variable_NN as_IN the_DT prediction_NN for_IN each_DT of_IN the_DT subsets_NNS ._.
Therefore_RB ,_, the_DT CART_NN splitting_NN criterion_NN is_VBZ appropriate_JJ for_IN a_DT piecewise_JJ constant_JJ mo_NN
