Combinational_JJ collaborative_NN filtering_VBG for_IN personalized_JJ community_NN recommendation_NN
Rapid_JJ growth_NN in_IN the_DT amount_NN of_IN data_NNS available_JJ on_IN social_JJ networking_NN sites_NNS has_VBZ made_VBN information_NN retrieval_NN increasingly_RB challenging_JJ for_IN users_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT collaborative_JJ filtering_VBG method_NN ,_, Combinational_JJ Collaborative_JJ Filtering_NN -LRB-_-LRB- CCF_NN -RRB-_-RRB- ,_, to_TO perform_VB personalized_JJ community_NN recommendations_NNS by_IN considering_VBG multiple_JJ types_NNS of_IN co-occurrences_NNS in_IN social_JJ data_NNS at_IN the_DT same_JJ time_NN ._.
This_DT filtering_VBG method_NN fuses_VBZ semantic_JJ and_CC user_NN information_NN ,_, then_RB applies_VBZ a_DT hybrid_NN training_NN strategy_NN that_WDT combines_VBZ Gibbs_NNP sampling_NN and_CC Expectation-Maximization_NN algorithm_NN ._.
To_TO handle_VB the_DT large-scale_JJ dataset_NN ,_, parallel_JJ computing_NN is_VBZ used_VBN to_TO speed_VB up_RP the_DT model_NN training_NN ._.
Through_IN an_DT empirical_JJ study_NN on_IN the_DT Orkut_NNP dataset_NN ,_, we_PRP show_VBP CCF_NNP to_TO be_VB both_CC effective_JJ and_CC scalable_JJ ._.
sources_NNS to_TO alleviate_VB the_DT information_NN sparsity_NN problem_NN of_IN a_DT single_JJ source_NN ._.
Several_JJ other_JJ algorithms_NNS have_VBP been_VBN proposed_VBN to_TO model_NN publication_NN and_CC email_NN data_NNS 1_CD ._.
For_IN instance_NN ,_, the_DT author-topic_JJ -LRB-_-LRB- AT_NNP -RRB-_-RRB- model_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: employs_VBZ two_CD factors_NNS in_IN characterizing_VBG a_DT document_NN :_: the_DT document_NN 's_POS authors_NNS and_CC topics_NNS ._.
Modeling_VBG both_DT factors_NNS as_IN variables_NNS within_IN a_DT Bayesian_JJ network_NN allows_VBZ the_DT AT_NNP model_NN to_TO group_VB the_DT words_NNS used_VBN in_IN a_DT docu_NN
itive_JJ to_TO initialization_NN ._.
A_DT better_JJR initialization_NN tends_VBZ to_TO allow_VB EM_NNP to_TO find_VB a_DT ``_`` better_JJ ''_'' optimum_NN ._.
Second_NNP ,_, Gibbs_NNP sampling_NN is_VBZ too_RB slow_JJ to_TO be_VB effective_JJ for_IN large-scale_JJ datasets_NNS in_IN high-dimensional_JJ problems_NNS =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
A_DT hybrid_NN method_NN can_MD enjoy_VB the_DT advantages_NNS of_IN Gibbs_NNP and_CC EM_NNP ._.
Gibbs_NNP sampling_NN Gibbs_NNP sampling_NN is_VBZ a_DT simple_JJ and_CC widely_RB applicable_JJ Markov_NNP chain_NN Monte_NNP Carlo_NNP algorithm_NN ,_, which_WDT provides_VBZ a_DT simple_JJ method_NN for_IN obtai_NNS
descriptions_NNS ,_, respectively_RB ._.
2.2.1_CD Gibbs_NNP &_CC EM_NNP Hybrid_NNP Training_NNP Given_IN the_DT model_NN structure_NN ,_, the_DT next_JJ step_NN is_VBZ to_TO learn_VB model_NN parameters_NNS ._.
There_EX are_VBP some_DT standard_JJ learning_NN algorithms_NNS ,_, such_JJ as_IN Gibbs_NNP sampling_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Expectation-Maximization_NN -LRB-_-LRB- EM_NN -RRB-_-RRB- -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, and_CC Gradient_NN descent_NN ._.
For_IN CCF_NNP ,_, we_PRP propose_VBP a_DT hybrid_NN training_NN strategy_NN :_: We_PRP first_RB run_VBP Gibbs_NNP sampling_NN for_IN a_DT few_JJ iterations_NNS ,_, then_RB switch_NN to_TO EM_NNP ._.
The_DT model_NN trained_VBN by_IN Gib_NN
suitable_JJ for_IN parallelizing_VBG iterative_JJ algorithms_NNS than_IN MapReduce_NNP ._.
Since_IN standard_JJ MPI_NN implementations_NNS -LRB-_-LRB- MPICH2_NN -RRB-_-RRB- can_MD not_RB be_VB directly_RB ported_VBN to_TO our_PRP$ system_NN ,_, we_PRP implemented_VBD our_PRP$ own_JJ system_NN by_IN modifying_VBG MPICH2_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Parallel_JJ Gibbs_NNP sampling_NN We_PRP distribute_VBP the_DT computation_NN among_IN machines_NNS based_VBN on_IN community_NN IDs_NNS ._.
Thus_RB ,_, each_DT machine_NN i_FW only_JJ deals_NNS with_IN a_DT specified_VBN subset_NN of_IN communities_NNS ci_FW ,_, and_CC is_VBZ aware_JJ of_IN all_DT users_NNS u_VBP an_DT
C_NN ik_FW UZ_FW mk_FW ,_, C_NNP DZ_NNP n_NN −_NN C_NN ik_FW DZ_FW nk_FW -RRB-_-RRB- to_TO a_DT specified_VBN root_NN ,_, then_RB the_DT root_NN broadcasts_VBZ the_DT global_JJ difference_NN -LRB-_-LRB- sum_NN of_IN all_DT local_JJ differences_NNS -RRB-_-RRB- to_TO other_JJ machines_NNS to_TO update_VB global_JJ counts_NNS -LRB-_-LRB- C_NN UZ_NN mk_NN and_CC C_NN DZ_NN nk_NN -RRB-_-RRB- =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT is_VBZ a_DT MPI_NNP AllReduce_NNP operation_NN in_IN MPI_NNP ._.
We_PRP summarize_VBP the_DT process_NN in_IN Algorithm_NN 1_CD ._.
Algorithm_NN 1_CD :_: Parallel_JJ Gibbs_NNP Sampling_NNP of_IN CCF_NNP Input_NNP :_: N_NN ×_CD M_NN community-user_NN matrix_NN N_NN ×_NN V_NN community-description_NN matrix_NN ;_:
ibbs_NNS &_CC EM_FW Hybrid_FW Training_NN Given_IN the_DT model_NN structure_NN ,_, the_DT next_JJ step_NN is_VBZ to_TO learn_VB model_NN parameters_NNS ._.
There_EX are_VBP some_DT standard_JJ learning_NN algorithms_NNS ,_, such_JJ as_IN Gibbs_NNP sampling_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, Expectation-Maximization_NN -LRB-_-LRB- EM_NN -RRB-_-RRB- =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC Gradient_NN descent_NN ._.
For_IN CCF_NNP ,_, we_PRP propose_VBP a_DT hybrid_NN training_NN strategy_NN :_: We_PRP first_RB run_VBP Gibbs_NNP sampling_NN for_IN a_DT few_JJ iterations_NNS ,_, then_RB switch_NN to_TO EM_NNP ._.
The_DT model_NN trained_VBN by_IN Gibbs_NNP sampling_NN provides_VBZ the_DT initializa_NN
s_NN -LRB-_-LRB- PLSA_NN -RRB-_-RRB- -LRB-_-LRB- 7_CD -RRB-_-RRB- and_CC Latent_JJ DirichletAllocation_NN -LRB-_-LRB- LDA_NN -RRB-_-RRB- -LRB-_-LRB- 3_LS -RRB-_-RRB- model_NN document-word_NN co-occurrence_NN ,_, which_WDT is_VBZ similar_JJ to_TO the_DT bags_NN of_IN words_NNS community_NN view_NN ._.
Probabilistic_JJ Hypertext_NN Induced_VBN Topic_JJ Selection_NN -LRB-_-LRB- PHITS_NN -RRB-_-RRB- =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT variant_NN of_IN PLSA_NN ,_, models_NNS document-citation_NN co-occurrence_NN ,_, which_WDT is_VBZ similar_JJ to_TO the_DT bags_NN of_IN users_NNS community_NN view_NN ._.
However_RB ,_, a_DT system_NN that_WDT considers_VBZ just_RB bags_NN of_IN users_NNS can_MD not_RB take_VB advantage_NN of_IN content_NN
AT_NNP and_CC CLS_NNP ._.
The_DT entropies_NNS H_NN -LRB-_-LRB- CAT_NN -RRB-_-RRB- and_CC H_NN -LRB-_-LRB- CLS_NN -RRB-_-RRB- are_VBP used_VBN for_IN normalizing_VBG the_DT mutual_JJ information_NN to_TO be_VB in_IN the_DT range_NN -LRB-_-LRB- 0_CD ,_, 1_CD -RRB-_-RRB- ._.
In_IN practice_NN ,_, we_PRP made_VBD use_NN of_IN the_DT following_VBG formulation_NN to_TO estimate_VB the_DT NMI_NNP score_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: :_: PK_NN s_NN =_JJ 1_CD NMI_NN =_JJ q_FW `_`` P_NN s_NN PK_NN t_NN =_JJ 1_CD ns_NNS ,_, t_NN log_NN ns_NNS log_VBP ns_IN n_NN ´_CD `_`` P_NN t_NN ``_`` n_NN ·_FW ns_FW ,_, t_NN ns_FW ·_FW nt_NN ''_'' nt_NN log_NN nt_NN n_NN ´_NN ,_, -LRB-_-LRB- 17_CD -RRB-_-RRB- where_WRB n_NN is_VBZ the_DT number_NN of_IN communities_NNS ,_, ns_NNS and_CC nt_NNS denote_VBP the_DT numbers_NNS of_IN community_NN in_IN category_NN s_NN and_CC cluster_NN
ed_IN Work_NNP Several_JJ algorithms_NNS have_VBP been_VBN proposed_VBN to_TO deal_VB with_IN either_CC bags_NN of_IN words_NNS or_CC bags_NNS of_IN users_NNS ._.
Specifically_RB ,_, Probabilistic_JJ Latent_JJ Semantic_JJ Analysis_NN -LRB-_-LRB- PLSA_NN -RRB-_-RRB- -LRB-_-LRB- 7_CD -RRB-_-RRB- and_CC Latent_JJ DirichletAllocation_NN -LRB-_-LRB- LDA_NN -RRB-_-RRB- =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: model_JJ document-word_JJ co-occurrence_NN ,_, which_WDT is_VBZ similar_JJ to_TO the_DT bags_NN of_IN words_NNS community_NN view_NN ._.
Probabilistic_JJ Hypertext_NN Induced_VBN Topic_JJ Selection_NN -LRB-_-LRB- PHITS_NN -RRB-_-RRB- -LRB-_-LRB- 4_CD -RRB-_-RRB- ,_, a_DT variant_NN of_IN PLSA_NN ,_, models_NNS document-citation_NN co-oc_NN
s_NN effectiveness_NN and_CC scalability_NN ._.
1.1_CD Related_NNP Work_NNP Several_JJ algorithms_NNS have_VBP been_VBN proposed_VBN to_TO deal_VB with_IN either_CC bags_NN of_IN words_NNS or_CC bags_NNS of_IN users_NNS ._.
Specifically_RB ,_, Probabilistic_JJ Latent_JJ Semantic_JJ Analysis_NN -LRB-_-LRB- PLSA_NN -RRB-_-RRB- =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Latent_JJ DirichletAllocation_NN -LRB-_-LRB- LDA_NN -RRB-_-RRB- -LRB-_-LRB- 3_LS -RRB-_-RRB- model_NN document-word_NN co-occurrence_NN ,_, which_WDT is_VBZ similar_JJ to_TO the_DT bags_NN of_IN words_NNS community_NN view_NN ._.
Probabilistic_JJ Hypertext_NN Induced_VBN Topic_JJ Selection_NN -LRB-_-LRB- PHITS_NN -RRB-_-RRB- -LRB-_-LRB- 4_CD -RRB-_-RRB- ,_, a_DT varia_NN
sian_JJ network_NN allows_VBZ the_DT AT_NNP model_NN to_TO group_VB the_DT words_NNS used_VBN in_IN a_DT document_NN corpus_NN into_IN semantic_JJ topics_NNS ,_, and_CC to_TO determine_VB an_DT author_NN 's_POS topic_NN associations_NNS ._.
For_IN emails_NNS ,_, the_DT author-recipient-topic_JJ -LRB-_-LRB- ART_NN -RRB-_-RRB- model_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: considers_VBZ email_NN recipient_JJ as_IN an_DT additional_JJ factor_NN ._.
This_DT model_NN can_MD discover_VB relevant_JJ topics_NNS from_IN the_DT sender-recipient_JJ structure_NN in_IN emails_NNS ,_, and_CC enjoys_VBZ an_DT improved_VBN ability_NN to_TO measure_VB role-similarity_NN bet_NN
he_PRP number_NN of_IN EM_NN iterations_NNS -LRB-_-LRB- ranging_VBG from_IN 100_CD to_TO 500_CD -RRB-_-RRB- ._.
The_DT reported_VBN results_NNS are_VBP the_DT average_JJ performance_NN over_IN all_DT runs_NNS ._.
2_CD All_DT user_NN data_NNS were_VBD anonymized_VBN ,_, and_CC user_NN privacy_NN is_VBZ safeguarded_VBN ,_, as_IN performed_VBN in_IN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -.0.4_CD 0.35_CD 0.3_CD CCF_NN precision_NN C_NN −_NN U_NN precision_NN CCF_NN recall_NN C_NN −_NN U_NN recall_NN Percentage_NN 0.25_CD 0.2_CD 0.15_CD 0.1_CD 0.05_CD 0_CD 0_CD 50 100 150 200_CD Length_NN of_IN the_DT recommendation_NN list_NN Figure_NNP 5_CD :_: The_DT precision_NN and_CC recall_NN as_IN functions_NNS
