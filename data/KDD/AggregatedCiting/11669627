The_DT offset_VBN tree_NN for_IN learning_VBG with_IN partial_JJ labels_NNS
We_PRP present_VBP an_DT algorithm_NN ,_, called_VBD the_DT Offset_NNP Tree_NNP ,_, for_IN learning_VBG to_TO make_VB decisions_NNS in_IN situations_NNS where_WRB the_DT payoff_NN of_IN only_RB one_CD choice_NN is_VBZ observed_VBN ,_, rather_RB than_IN all_DT choices_NNS ._.
The_DT algorithm_NN reduces_VBZ this_DT setting_NN to_TO binary_JJ classification_NN ,_, allowing_VBG one_CD to_TO reuse_VB any_DT existing_VBG ,_, fully_RB supervised_VBN binary_JJ classification_NN algorithm_NN in_IN this_DT partial_JJ information_NN setting_NN ._.
We_PRP show_VBP that_IN the_DT Offset_NNP Tree_NNP is_VBZ an_DT optimal_JJ reduction_NN to_TO binary_JJ classification_NN ._.
In_IN particular_JJ ,_, it_PRP has_VBZ regret_NN at_IN most_JJS -LRB-_-LRB- k-1_NN -RRB-_-RRB- times_NNS the_DT regret_NN of_IN the_DT binary_JJ classifier_NN it_PRP uses_VBZ -LRB-_-LRB- where_WRB k_NN is_VBZ the_DT number_NN of_IN choices_NNS -RRB-_-RRB- ,_, and_CC no_DT reduction_NN to_TO binary_JJ classification_NN can_MD do_VB better_RBR ._.
This_DT reduction_NN is_VBZ also_RB computationally_RB optimal_JJ ,_, both_DT at_IN training_NN and_CC test_NN time_NN ,_, requiring_VBG just_RB O_NN -LRB-_-LRB- log2_NN k_NN -RRB-_-RRB- work_NN to_TO train_VB on_IN an_DT example_NN or_CC make_VB a_DT prediction_NN ._.
Experiments_NNS with_IN the_DT Offset_NNP Tree_NNP show_VBP that_IN it_PRP generally_RB performs_VBZ better_JJR than_IN several_JJ alternative_JJ approaches_NNS ._.
t_NN reduction_NN ._.
An_DT empirical_JJ comparison_NN of_IN the_DT three_CD approaches_NNS is_VBZ presented_VBN in_IN Section_NNP 5_CD ._.
Prior_JJ work_NN The_DT problem_NN considered_VBN here_RB is_VBZ a_DT noninteractive_JJ version_NN of_IN the_DT bandits_NNS problem_NN with_IN side_JJ information_NN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT has_VBZ been_VBN analyzed_VBN under_IN various_JJ additional_JJ assumptions_NNS about_IN how_WRB the_DT world_NN behaves_VBZ -LRB-_-LRB- 10_CD ,_, 13_CD ,_, 15_CD ,_, 17_CD -RRB-_-RRB- ,_, including_VBG payoffs_NNS as_IN a_DT linear_JJ function_NN of_IN the_DT side_NN information_NN -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- ._.
The_DT Exp4_NN algorithm_NN -LRB-_-LRB-
ntroduction_NN This_DT paper_NN is_VBZ about_IN a_DT new_JJ learning_NN setting_NN ,_, which_WDT can_MD be_VB thought_VBN of_IN as_IN an_DT offline_JJ variant_NN of_IN the_DT contextual_JJ k-armed_JJ bandit_NN problem_NN ._.
•_VB The_DT k-armed_JJ bandit_NN problem_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- has_VBZ been_VBN well_RB studied_VBN =_JJ -_: =[_NN 2_CD ,_, 3_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT basic_JJ setting_NN ,_, a_DT learner_NN repeatedly_RB chooses_VBZ one_CD of_IN k_NN actions_NNS ,_, and_CC then_RB learns_VBZ the_DT value_NN of_IN that_DT action_NN -LRB-_-LRB- but_CC not_RB the_DT value_NN of_IN other_JJ actions_NNS -RRB-_-RRB- ._.
The_DT goal_NN is_VBZ to_TO compete_VB with_IN the_DT best_JJS constant_JJ acti_NNS
he_PRP number_NN of_IN policies_NNS we_PRP want_VBP to_TO compete_VB with_IN is_VBZ large_JJ ,_, and_CC it_PRP relies_VBZ on_IN careful_JJ control_NN of_IN the_DT action_NN choosing_NN distribution_NN ._.
Sample_NN complexity_NN results_NNS for_IN policy_NN evaluation_NN in_IN reinforcement_NN learning_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: and_CC contextual_JJ bandits_NNS -LRB-_-LRB- 12_CD -RRB-_-RRB- show_VBP that_IN an_DT Empirical_JJ Risk_NN Minimization_NN type_NN algorithms_NNS can_MD succeed_VB in_IN this_DT setting_NN ._.
Compared_VBN to_TO these_DT results_NNS ,_, -LRB-_-LRB- 1_LS -RRB-_-RRB- we_PRP show_VBP that_IN it_PRP is_VBZ possible_JJ to_TO efficiently_RB reuse_VB exist_VBP
ork_VB The_DT problem_NN considered_VBN here_RB is_VBZ a_DT noninteractive_JJ version_NN of_IN the_DT bandits_NNS problem_NN with_IN side_NN information_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, which_WDT has_VBZ been_VBN analyzed_VBN under_IN various_JJ additional_JJ assumptions_NNS about_IN how_WRB the_DT world_NN behaves_VBZ =_JJ -_: =[_NN 10_CD ,_, 13_CD ,_, 15_CD ,_, 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, including_VBG payoffs_NNS as_IN a_DT linear_JJ function_NN of_IN the_DT side_NN information_NN -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- ._.
The_DT Exp4_NN algorithm_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- has_VBZ a_DT nice_JJ assumption-free_JJ analysis_NN ._.
However_RB ,_, it_PRP is_VBZ intractable_JJ when_WRB the_DT number_NN of_IN policies_NNS we_PRP want_VBP to_TO c_NN
ork_VB The_DT problem_NN considered_VBN here_RB is_VBZ a_DT noninteractive_JJ version_NN of_IN the_DT bandits_NNS problem_NN with_IN side_NN information_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, which_WDT has_VBZ been_VBN analyzed_VBN under_IN various_JJ additional_JJ assumptions_NNS about_IN how_WRB the_DT world_NN behaves_VBZ =_JJ -_: =[_NN 10_CD ,_, 13_CD ,_, 15_CD ,_, 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, including_VBG payoffs_NNS as_IN a_DT linear_JJ function_NN of_IN the_DT side_NN information_NN -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- ._.
The_DT Exp4_NN algorithm_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- has_VBZ a_DT nice_JJ assumption-free_JJ analysis_NN ._.
However_RB ,_, it_PRP is_VBZ intractable_JJ when_WRB the_DT number_NN of_IN policies_NNS we_PRP want_VBP to_TO c_NN
istributions_NNS ._.
Here_RB is_VBZ a_DT formal_JJ description_NN of_IN data_NNS generation_NN :_: 1_CD ._.
Some_DT unknown_JJ distribution_NN D_NN -LRB-_-LRB- x_NN ,_, r_NN -RRB-_-RRB- generates_VBZ a_DT feature_NN vector_NN x_NN and_CC a_DT vector_NN r_NN =_JJ -LRB-_-LRB- r1_NN ,_, r2_NN ,_, ..._: ,_, rk_NN -RRB-_-RRB- ,_, where_WRB for_IN i_FW ∈_FW -LCB-_-LRB- 1_CD ,_, ..._: ,_, k_NN -RCB-_-RRB- ,_, ri_FW ∈_FW =_SYM -_: =[_NN 0_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ the_DT reward_NN of_IN the_DT i-th_JJ action_NN ._.
2_CD ._.
The_DT offline_JJ policy_NN chooses_VBZ an_DT action_NN a_DT uniformly_RB at_IN random_JJ ._.
3_LS ._.
The_DT reward_NN ra_NN is_VBZ revealed_VBN ._.
The_DT goal_NN is_VBZ to_TO learn_VB a_DT policy_NN π_NN -LRB-_-LRB- x_NN -RRB-_-RRB- for_IN choosing_VBG action_NN a_DT given_VBN x_NN ,_, with_IN t_NN
ork_VB The_DT problem_NN considered_VBN here_RB is_VBZ a_DT noninteractive_JJ version_NN of_IN the_DT bandits_NNS problem_NN with_IN side_NN information_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, which_WDT has_VBZ been_VBN analyzed_VBN under_IN various_JJ additional_JJ assumptions_NNS about_IN how_WRB the_DT world_NN behaves_VBZ =_JJ -_: =[_NN 10_CD ,_, 13_CD ,_, 15_CD ,_, 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, including_VBG payoffs_NNS as_IN a_DT linear_JJ function_NN of_IN the_DT side_NN information_NN -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- ._.
The_DT Exp4_NN algorithm_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- has_VBZ a_DT nice_JJ assumption-free_JJ analysis_NN ._.
However_RB ,_, it_PRP is_VBZ intractable_JJ when_WRB the_DT number_NN of_IN policies_NNS we_PRP want_VBP to_TO c_NN
on_IN the_DT original_JJ partial_JJ label_NN problem_NN is_VBZ bounded_VBN by_IN k_NN times_NNS the_DT importance-weighted_JJ multiclass_JJ regret_NN -LRB-_-LRB- see_VB the_DT appendix_NN -RRB-_-RRB- ._.
When_WRB reduced_VBN to_TO binary_JJ classification_NN using_VBG the_DT standard_JJ all-pairs_JJ reduction_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP get_VBP a_DT bound_VBN of_IN k_NN -LRB-_-LRB- k_NN −_NN 1_CD -RRB-_-RRB- times_NNS the_DT binary_JJ regret_NN ._.
Other_JJ multiclass_NNS to_TO binary_JJ reductions_NNS can_MD improve_VB the_DT dependence_NN on_IN k_NN ,_, but_CC they_PRP all_DT yield_VBP worse_JJR results_NNS than_IN our_PRP$ approach_NN ._.
Our_PRP$ contribution_NN We_PRP prese_VBP
r_NN training_NN and_CC 1\/3_CD for_IN testing_NN ._.
Figure_NN 1_CD shows_VBZ the_DT error_NN rates_NNS -LRB-_-LRB- in_IN %_NN -RRB-_-RRB- of_IN Offset-Tree_NNP plotted_VBD against_IN the_DT error_NN rates_NNS of_IN Regression_NN -LRB-_-LRB- left_NN -RRB-_-RRB- and_CC Importance-Weighting_NN -LRB-_-LRB- right_NN -RRB-_-RRB- ._.
Decision_NN trees_NNS -LRB-_-LRB- J48_NN in_IN Weka_NN =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =--RRB-_NN were_VBD used_VBN as_IN a_DT base_NN binary_JJ learning_NN algorithm_NN for_IN both_DT Offset-Tree_NNP and_CC Importance-Weighting_NNP ._.
For_IN the_DT regression_NN approach_NN ,_, we_PRP learned_VBD a_DT separate_JJ regressor_NN for_IN each_DT of_IN the_DT k_NN choices_NNS ._.
-LRB-_-LRB- A_DT single_JJ regress_NN
alternative_JJ approaches_NNS ._.
1_CD Introduction_NN This_DT paper_NN is_VBZ about_IN a_DT new_JJ learning_NN setting_NN ,_, which_WDT can_MD be_VB thought_VBN of_IN as_IN an_DT offline_JJ variant_NN of_IN the_DT contextual_JJ k-armed_JJ bandit_NN problem_NN ._.
•_VB The_DT k-armed_JJ bandit_NN problem_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: has_VBZ been_VBN well_RB studied_VBN -LRB-_-LRB- 2_CD ,_, 3_CD ,_, 6_CD -RRB-_-RRB- ._.
In_IN the_DT basic_JJ setting_NN ,_, a_DT learner_NN repeatedly_RB chooses_VBZ one_CD of_IN k_NN actions_NNS ,_, and_CC then_RB learns_VBZ the_DT value_NN of_IN that_DT action_NN -LRB-_-LRB- but_CC not_RB the_DT value_NN of_IN other_JJ actions_NNS -RRB-_-RRB- ._.
The_DT goal_NN is_VBZ to_TO com_NN
n_NN step_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- of_IN Algorithm_NN 1_CD ,_, where_WRB importance_NN weighted_VBD binary_JJ examples_NNS are_VBP formed_VBN ._.
The_DT offset_VBN of_IN 1\/2_CD changes_NNS the_DT range_NN of_IN importances_NNS ,_, effectively_RB derandomizing_VBG the_DT induced_VBN problem_NN ._.
A_DT folk_NN theorem_NN -LRB-_-LRB- see_VB =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =--RRB-_NN says_VBZ that_IN for_IN any_DT importance-weighted_JJ distribution_NN P_NN ,_, there_EX exists_VBZ a_DT constant_JJ w_NN =_JJ E_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, w_NN -RRB-_-RRB- ∼_NN P_NN -LRB-_-LRB- w_NN -RRB-_-RRB- such_JJ that_IN for_IN any_DT predictor_NN h_NN :_: X_NN →_NN Y_NN ,_, E_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, w_NN -RRB-_-RRB- ∼_CD P_NN ′_NN -LRB-_-LRB- 1_CD -LRB-_-LRB- h_NN -LRB-_-LRB- x_NN -RRB-_-RRB- ̸_NN =_JJ y_NN -RRB-_-RRB- -RRB-_-RRB- =_JJ 1_CD w_NN E_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, w_NN -RRB-_-RRB- ∼_NN P_NN -LRB-_-LRB- w_NN ·_NN 1_CD -LRB-_-LRB- h_NN -LRB-_-LRB- x_NN -RRB-_-RRB- ̸_NN =_JJ
,_, described_VBN in_IN the_DT introduction_NN ._. -RRB-_-RRB-
Further_JJ details_NNS about_IN both_DT methods_NNS and_CC their_PRP$ analysis_NN are_VBP provided_VBN in_IN the_DT appendix_NN ._.
We_PRP performed_VBD the_DT comparison_NN on_IN a_DT number_NN of_IN publicly_RB available_JJ multiclass_JJ datasets_NNS =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN all_DT datasets_NNS ,_, we_PRP report_VBP the_DT average_JJ result_NN over_IN 10_CD random_JJ splits_VBZ -LRB-_-LRB- fixed_VBN for_IN all_DT methods_NNS -RRB-_-RRB- ,_, with_IN 2\/3_CD of_IN the_DT dataset_NN used_VBN for_IN training_NN and_CC 1\/3_CD for_IN testing_NN ._.
Figure_NN 1_CD shows_VBZ the_DT error_NN rates_NNS -LRB-_-LRB- in_IN %_NN -RRB-_-RRB- of_IN
ntroduction_NN This_DT paper_NN is_VBZ about_IN a_DT new_JJ learning_NN setting_NN ,_, which_WDT can_MD be_VB thought_VBN of_IN as_IN an_DT offline_JJ variant_NN of_IN the_DT contextual_JJ k-armed_JJ bandit_NN problem_NN ._.
•_VB The_DT k-armed_JJ bandit_NN problem_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- has_VBZ been_VBN well_RB studied_VBN =_JJ -_: =[_NN 2_CD ,_, 3_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT basic_JJ setting_NN ,_, a_DT learner_NN repeatedly_RB chooses_VBZ one_CD of_IN k_NN actions_NNS ,_, and_CC then_RB learns_VBZ the_DT value_NN of_IN that_DT action_NN -LRB-_-LRB- but_CC not_RB the_DT value_NN of_IN other_JJ actions_NNS -RRB-_-RRB- ._.
The_DT goal_NN is_VBZ to_TO compete_VB with_IN the_DT best_JJS constant_JJ acti_NNS
ork_VB The_DT problem_NN considered_VBN here_RB is_VBZ a_DT noninteractive_JJ version_NN of_IN the_DT bandits_NNS problem_NN with_IN side_NN information_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, which_WDT has_VBZ been_VBN analyzed_VBN under_IN various_JJ additional_JJ assumptions_NNS about_IN how_WRB the_DT world_NN behaves_VBZ =_JJ -_: =[_NN 10_CD ,_, 13_CD ,_, 15_CD ,_, 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, including_VBG payoffs_NNS as_IN a_DT linear_JJ function_NN of_IN the_DT side_NN information_NN -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- ._.
The_DT Exp4_NN algorithm_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- has_VBZ a_DT nice_JJ assumption-free_JJ analysis_NN ._.
However_RB ,_, it_PRP is_VBZ intractable_JJ when_WRB the_DT number_NN of_IN policies_NNS we_PRP want_VBP to_TO c_NN
k_NN actions_NNS ,_, and_CC then_RB learns_VBZ the_DT value_NN of_IN that_DT action_NN -LRB-_-LRB- but_CC not_RB the_DT value_NN of_IN other_JJ actions_NNS -RRB-_-RRB- ._.
The_DT goal_NN is_VBZ to_TO compete_VB with_IN the_DT best_JJS constant_JJ action_NN over_IN many_JJ rounds_NNS ._.
The_DT contextual_JJ k-armed_JJ bandit_NN problem_NN =_JJ -_: =[_NN 3_CD ,_, 12_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ a_DT variant_NN where_WRB the_DT algorithm_NN receives_VBZ a_DT feature_NN vector_NN x_NN before_IN choosing_VBG an_DT action_NN ,_, and_CC the_DT goal_NN is_VBZ to_TO compete_VB with_IN the_DT best_JJS policy_NN π_NN :_: x_NN →_CD -LCB-_-LRB- 1_CD ,_, ..._: ,_, k_NN -RCB-_-RRB- in_IN some_DT set_NN of_IN policies_NNS ._.
The_DT key_JJ difference_NN
need_VB to_TO modify_VB them_PRP ,_, and_CC -LRB-_-LRB- 2_LS -RRB-_-RRB- we_PRP show_VBP how_WRB to_TO best_RB do_VB this_DT ._.
Transformations_NNS from_IN partial_JJ label_NN problems_NNS to_TO fully_RB supervised_VBN problems_NNS can_MD be_VB thought_VBN of_IN as_IN methods_NNS for_IN dealing_VBG with_IN sample_NN selection_NN bias_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT are_VBP heavily_RB studied_VBN in_IN Economics_NNP and_CC Statistics_NNP -LRB-_-LRB- leading_VBG to_TO a_DT Nobel_NNP prize_NN in_IN Economics_NNP in_IN 2000_CD -RRB-_-RRB- ._.
2_CD The_DT Binary_NNP Case_NNP An_NNP importance-weighted_JJ k-class_NN classification_NN problem_NN is_VBZ defined_VBN by_IN a_DT distrib_NN
