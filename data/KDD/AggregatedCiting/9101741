Learning_NNP classifiers_NNS from_IN only_RB positive_JJ and_CC unlabeled_JJ data_NNS
The_DT input_NN to_TO an_DT algorithm_NN that_WDT learns_VBZ a_DT binary_JJ classifier_NN normally_RB consists_VBZ of_IN two_CD sets_NNS of_IN examples_NNS ,_, where_WRB one_CD set_NN consists_VBZ of_IN positive_JJ examples_NNS of_IN the_DT concept_NN to_TO be_VB learned_VBN ,_, and_CC the_DT other_JJ set_NN consists_VBZ of_IN negative_JJ examples_NNS ._.
However_RB ,_, it_PRP is_VBZ often_RB the_DT case_NN that_IN the_DT available_JJ training_NN data_NNS are_VBP an_DT incomplete_JJ set_NN of_IN positive_JJ examples_NNS ,_, and_CC a_DT set_NN of_IN unlabeled_JJ examples_NNS ,_, some_DT of_IN which_WDT are_VBP positive_JJ and_CC some_DT of_IN which_WDT are_VBP negative_JJ ._.
The_DT problem_NN solved_VBD in_IN this_DT paper_NN is_VBZ how_WRB to_TO learn_VB a_DT standard_JJ binary_JJ classifier_NN given_VBN a_DT nontraditional_JJ training_NN set_NN of_IN this_DT nature_NN ._.
Under_IN the_DT assumption_NN that_IN the_DT labeled_JJ examples_NNS are_VBP selected_VBN randomly_RB from_IN the_DT positive_JJ examples_NNS ,_, we_PRP show_VBP that_IN a_DT classifier_NN trained_VBN on_IN positive_JJ and_CC unlabeled_JJ examples_NNS predicts_VBZ probabilities_NNS that_WDT differ_VBP by_IN only_RB a_DT constant_JJ factor_NN from_IN the_DT true_JJ conditional_JJ probabilities_NNS of_IN being_VBG positive_JJ ._.
We_PRP show_VBP how_WRB to_TO use_VB this_DT result_NN in_IN two_CD different_JJ ways_NNS to_TO learn_VB a_DT classifier_NN from_IN a_DT nontraditional_JJ training_NN set_NN ._.
We_PRP then_RB apply_VBP these_DT two_CD new_JJ methods_NNS to_TO solve_VB a_DT real-world_JJ problem_NN :_: identifying_VBG protein_NN records_NNS that_WDT should_MD be_VB included_VBN in_IN an_DT incomplete_JJ specialized_JJ molecular_JJ biology_NN database_NN ._.
Our_PRP$ experiments_NNS in_IN this_DT domain_NN show_VBP that_IN models_NNS trained_VBN using_VBG the_DT new_JJ methods_NNS perform_VBP better_RBR than_IN the_DT current_JJ state-of-the-art_JJ biased_VBN SVM_NN method_NN for_IN learning_VBG from_IN positive_JJ and_CC unlabeled_JJ examples_NNS ._.
ber_NN of_IN true_JJ negatives_NNS ._.
Finally_RB ,_, precision_NN is_VBZ defined_VBN as_IN p_NN =_JJ a_DT \/_: -LRB-_-LRB- a_DT +_CC c_NN -RRB-_-RRB- and_CC recall_NN as_IN r_NN =_JJ a_DT \/_: -LRB-_-LRB- a_DT +_CC b_NN -RRB-_-RRB- ._.
The_DT basic_JJ learning_NN algorithm_NN for_IN each_DT method_NN is_VBZ an_DT SVM_NN with_IN a_DT linear_JJ kernel_NN as_IN implemented_VBN in_IN libSVM_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN approach_NN -LRB-_-LRB- ii_LS -RRB-_-RRB- we_PRP use_VBP Platt_NNP scaling_VBG to_TO get_VB probability_NN estimates_NNS which_WDT are_VBP then_RB adjusted_VBN using_VBG Lemma_NNP 1_CD ._.
For_IN approach_NN -LRB-_-LRB- iii_LS -RRB-_-RRB- we_PRP run_VBP libSVM_NN twice_RB ._.
The_DT first_JJ run_NN uses_VBZ Platt_NNP scaling_VBG to_TO get_VB probability_NN
0.9465_CD 0.9279_CD 0.9895_CD 621_CD and_CC then_RB -LRB-_-LRB- ii_LS -RRB-_-RRB- to_TO apply_VB a_DT standard_JJ learning_NN method_NN to_TO these_DT examples_NNS and_CC the_DT positive_JJ examples_NNS ;_: steps_NNS -LRB-_-LRB- i_LS -RRB-_-RRB- and_CC -LRB-_-LRB- ii_LS -RRB-_-RRB- may_MD be_VB iterated_VBN ._.
Papers_NNP using_VBG this_DT general_JJ approach_NN include_VBP =_JJ -_: =[_NN 24_CD ,_, 20_CD ,_, 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC the_DT idea_NN has_VBZ been_VBN rediscovered_VBN independently_RB a_DT few_JJ times_NNS ,_, most_RBS recently_RB in_IN -LRB-_-LRB- 22_CD ,_, Section_NNP 2.4_CD -RRB-_-RRB- ._.
The_DT approach_NN is_VBZ sometimes_RB extended_VBN to_TO identify_VB also_RB additional_JJ positive_JJ examples_NNS in_IN the_DT unlabeled_JJ se_FW
st_IN algorithms_NNS based_VBN on_IN this_DT assumption_NN need_VBP p_NN -LRB-_-LRB- y_NN =_JJ 1_CD -RRB-_-RRB- to_TO be_VB an_DT additional_JJ input_NN piece_NN of_IN information_NN ;_: a_DT recent_JJ paper_NN emphasizes_VBZ the_DT importance_NN of_IN p_NN -LRB-_-LRB- y_NN =_JJ 1_CD -RRB-_-RRB- for_IN learning_VBG from_IN positive_JJ and_CC unlabeled_JJ data_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN Section_NN 3_CD above_RB ,_, we_PRP show_VBP how_WRB to_TO estimate_VB p_NN -LRB-_-LRB- y_NN =_JJ 1_CD -RRB-_-RRB- empirically_RB ._.
The_DT most_RBS similar_JJ previous_JJ work_NN to_TO ours_PRP is_VBZ -LRB-_-LRB- 26_CD -RRB-_-RRB- ._.
Their_PRP$ zi_NN 219_CD approach_NN also_RB makes_VBZ the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN and_CC
e_LS result_NN has_VBZ not_RB been_VBN published_VBN before_RB ,_, and_CC it_PRP is_VBZ not_RB obvious_JJ ._.
The_DT reason_NN perhaps_RB that_IN the_DT result_NN is_VBZ novel_JJ is_VBZ that_IN although_IN the_DT learning_NN scenario_NN has_VBZ been_VBN discussed_VBN in_IN many_JJ previous_JJ papers_NNS ,_, including_VBG =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 11_CD ,_, 26_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC these_DT papers_NNS do_VBP make_VB the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN either_CC explicitly_RB or_CC implicitly_RB ,_, the_DT scenario_NN has_VBZ not_RB previously_RB been_VBN formalized_VBN using_VBG a_DT random_JJ variable_JJ s_NN to_TO represent_VB the_DT fa_NN
hat_NN contains_VBZ most_JJS of_IN the_DT available_JJ positive_JJ examples_NNS ._.
Unfortunately_RB ,_, the_DT outcome_NN of_IN these_DT methods_NNS is_VBZ sensitive_JJ to_TO the_DT values_NNS chosen_VBN for_IN tuning_NN parameters_NNS ,_, and_CC no_DT good_JJ way_NN is_VBZ known_VBN to_TO set_VB these_DT values_NNS =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Moreover_RB ,_, the_DT biased_VBN SVM_NN method_NN has_VBZ been_VBN reported_VBN to_TO do_VB better_RBR experimentally_RB -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
7_CD ._.
CONCLUSIONS_NNS The_DT central_JJ contribution_NN of_IN this_DT paper_NN is_VBZ Lemma_NNP 1_CD ,_, which_WDT shows_VBZ that_IN if_IN positive_JJ training_NN examples_NNS a_DT
oach_NN is_VBZ to_TO assign_VB weights_NNS somehow_RB to_TO the_DT unlabeled_JJ examples_NNS ,_, and_CC then_RB to_TO train_VB a_DT classifier_NN with_IN the_DT unlabeled_JJ examples_NNS interpreted_VBN as_IN weighted_JJ negative_JJ examples_NNS ._.
This_DT approach_NN is_VBZ used_VBN for_IN example_NN by_IN =_JJ -_: =[_NN 8_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT first_JJ approach_NN can_MD be_VB viewed_VBN as_IN a_DT special_JJ case_NN of_IN the_DT second_JJ approach_NN ,_, where_WRB each_DT weight_NN is_VBZ either_CC 0_CD or_CC 1_CD ._.
The_DT second_JJ approach_NN is_VBZ similar_JJ to_TO the_DT method_NN we_PRP suggest_VBP in_IN Section_NN 3_CD above_IN ,_, with_IN three_CD
e_LS result_NN has_VBZ not_RB been_VBN published_VBN before_RB ,_, and_CC it_PRP is_VBZ not_RB obvious_JJ ._.
The_DT reason_NN perhaps_RB that_IN the_DT result_NN is_VBZ novel_JJ is_VBZ that_IN although_IN the_DT learning_NN scenario_NN has_VBZ been_VBN discussed_VBN in_IN many_JJ previous_JJ papers_NNS ,_, including_VBG =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 11_CD ,_, 26_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC these_DT papers_NNS do_VBP make_VB the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN either_CC explicitly_RB or_CC implicitly_RB ,_, the_DT scenario_NN has_VBZ not_RB previously_RB been_VBN formalized_VBN using_VBG a_DT random_JJ variable_JJ s_NN to_TO represent_VB the_DT fa_NN
type_NN ._.
The_DT first_JJ approach_NN is_VBZ to_TO do_VB probability_NN density_NN estimation_NN ,_, but_CC this_DT is_VBZ well-known_JJ to_TO be_VB a_DT very_RB difficult_JJ task_NN for_IN high-dimensional_JJ data_NNS ._.
The_DT second_JJ approach_NN is_VBZ to_TO use_VB a_DT so-called_JJ one-class_JJ SVM_NN =_JJ -_: =[_NN 16_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT aim_NN of_IN these_DT methods_NNS is_VBZ to_TO model_VB a_DT region_NN that_WDT contains_VBZ most_JJS of_IN the_DT available_JJ positive_JJ examples_NNS ._.
Unfortunately_RB ,_, the_DT outcome_NN of_IN these_DT methods_NNS is_VBZ sensitive_JJ to_TO the_DT values_NNS chosen_VBN for_IN tuning_NN paramete_NN
sitive_JJ example_NN is_VBZ labeled_VBN ._.
This_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN is_VBZ analogous_JJ to_TO the_DT ``_`` missing_VBG completely_RB at_IN random_JJ ''_'' assumption_NN that_WDT is_VBZ often_RB made_VBN when_WRB learning_VBG from_IN data_NNS with_IN missing_VBG values_NNS =_JJ -_: =[_NN 10_CD ,_, 17_CD ,_, 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Another_DT way_NN of_IN stating_VBG the_DT assumption_NN is_VBZ that_IN s_NN and_CC x_NN are_VBP conditionally_RB independent_JJ given_VBN y._NN 214_CD So_RB ,_, a_DT training_NN set_NN is_VBZ a_DT random_JJ sample_NN from_IN a_DT distribution_NN p_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, s_NNS -RRB-_-RRB- that_WDT satisfies_VBZ Equations_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- and_CC
ilability_NN of_IN explicit_JJ negative_JJ examples_NNS ._.
However_RB ,_, in_IN many_JJ real-world_JJ domains_NNS ,_, the_DT concept_NN of_IN a_DT negative_JJ example_NN is_VBZ not_RB natural_JJ ._.
For_IN example_NN ,_, over_IN 1000_CD specialized_JJ databases_NNS exist_VBP in_IN molecular_JJ biology_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Each_DT of_IN these_DT defines_VBZ a_DT set_NN of_IN positive_JJ examples_NNS ,_, namely_RB the_DT set_NN of_IN genes_NNS or_CC proteins_NNS included_VBN in_IN the_DT database_NN ._.
In_IN each_DT case_NN ,_, it_PRP would_MD be_VB useful_JJ to_TO learn_VB a_DT classifier_NN that_WDT can_MD recognize_VB additional_JJ g_NN
sitive_JJ example_NN is_VBZ labeled_VBN ._.
This_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN is_VBZ analogous_JJ to_TO the_DT ``_`` missing_VBG completely_RB at_IN random_JJ ''_'' assumption_NN that_WDT is_VBZ often_RB made_VBN when_WRB learning_VBG from_IN data_NNS with_IN missing_VBG values_NNS =_JJ -_: =[_NN 10_CD ,_, 17_CD ,_, 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Another_DT way_NN of_IN stating_VBG the_DT assumption_NN is_VBZ that_IN s_NN and_CC x_NN are_VBP conditionally_RB independent_JJ given_VBN y._NN 214_CD So_RB ,_, a_DT training_NN set_NN is_VBZ a_DT random_JJ sample_NN from_IN a_DT distribution_NN p_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, s_NNS -RRB-_-RRB- that_WDT satisfies_VBZ Equations_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- and_CC
e_LS result_NN has_VBZ not_RB been_VBN published_VBN before_RB ,_, and_CC it_PRP is_VBZ not_RB obvious_JJ ._.
The_DT reason_NN perhaps_RB that_IN the_DT result_NN is_VBZ novel_JJ is_VBZ that_IN although_IN the_DT learning_NN scenario_NN has_VBZ been_VBN discussed_VBN in_IN many_JJ previous_JJ papers_NNS ,_, including_VBG =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 11_CD ,_, 26_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC these_DT papers_NNS do_VBP make_VB the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN either_CC explicitly_RB or_CC implicitly_RB ,_, the_DT scenario_NN has_VBZ not_RB previously_RB been_VBN formalized_VBN using_VBG a_DT random_JJ variable_JJ s_NN to_TO represent_VB the_DT fa_NN
0.9465_CD 0.9279_CD 0.9895_CD 621_CD and_CC then_RB -LRB-_-LRB- ii_LS -RRB-_-RRB- to_TO apply_VB a_DT standard_JJ learning_NN method_NN to_TO these_DT examples_NNS and_CC the_DT positive_JJ examples_NNS ;_: steps_NNS -LRB-_-LRB- i_LS -RRB-_-RRB- and_CC -LRB-_-LRB- ii_LS -RRB-_-RRB- may_MD be_VB iterated_VBN ._.
Papers_NNP using_VBG this_DT general_JJ approach_NN include_VBP =_JJ -_: =[_NN 24_CD ,_, 20_CD ,_, 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC the_DT idea_NN has_VBZ been_VBN rediscovered_VBN independently_RB a_DT few_JJ times_NNS ,_, most_RBS recently_RB in_IN -LRB-_-LRB- 22_CD ,_, Section_NNP 2.4_CD -RRB-_-RRB- ._.
The_DT approach_NN is_VBZ sometimes_RB extended_VBN to_TO identify_VB also_RB additional_JJ positive_JJ examples_NNS in_IN the_DT unlabeled_JJ se_FW
o_NN not_RB ._.
Fortunately_RB ,_, the_DT outputs_NNS of_IN these_DT other_JJ methods_NNS can_MD typically_RB be_VB postprocessed_VBN into_IN calibrated_VBN probabilities_NNS ._.
The_DT two_CD most_RBS common_JJ postprocessing_VBG methods_NNS for_IN calibration_NN are_VBP isotonic_JJ regression_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC fitting_JJ a_DT one-dimensional_JJ logistic_JJ regression_NN function_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
We_PRP apply_VBP the_DT latter_JJ 216_CD method_NN ,_, which_WDT is_VBZ often_RB called_VBN Platt_NNP scaling_NN ,_, to_TO SVM_NNP classifiers_NNS in_IN Section_NNP 5_CD below_IN ._.
4_LS ._.
AN_DT ILLUSTRATION_NN To_TO ill_RB
ot_IN involved_VBN in_IN this_DT process_NN ,_, the_DT only_JJ answer_NN is_VBZ ``_`` all_DT other_JJ proteins_NNS ._. ''_''
To_TO make_VB this_DT answer_NN operational_JJ ,_, we_PRP could_MD take_VB all_DT proteins_NNS mentioned_VBN in_IN a_DT comprehensive_JJ unspecialized_JJ database_NN such_JJ as_IN SwissProt_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
But_CC these_DT proteins_NNS are_VBP unlabeled_JJ examples_NNS ,_, not_RB negative_JJ examples_NNS ,_, because_IN some_DT of_IN them_PRP are_VBP proteins_NNS that_WDT should_MD be_VB in_IN TCDB_NNP ._.
Our_PRP$ goal_NN is_VBZ precisely_RB to_TO discover_VB these_DT proteins_NNS ._.
This_DT paper_NN is_VBZ organized_VBN
type_NN ._.
The_DT first_JJ approach_NN is_VBZ to_TO do_VB probability_NN density_NN estimation_NN ,_, but_CC this_DT is_VBZ well-known_JJ to_TO be_VB a_DT very_RB difficult_JJ task_NN for_IN high-dimensional_JJ data_NNS ._.
The_DT second_JJ approach_NN is_VBZ to_TO use_VB a_DT so-called_JJ one-class_JJ SVM_NN =_JJ -_: =[_NN 16_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT aim_NN of_IN these_DT methods_NNS is_VBZ to_TO model_VB a_DT region_NN that_WDT contains_VBZ most_JJS of_IN the_DT available_JJ positive_JJ examples_NNS ._.
Unfortunately_RB ,_, the_DT outcome_NN of_IN these_DT methods_NNS is_VBZ sensitive_JJ to_TO the_DT values_NNS chosen_VBN for_IN tuning_NN paramete_NN
lly_RB be_VB postprocessed_VBN into_IN calibrated_VBN probabilities_NNS ._.
The_DT two_CD most_RBS common_JJ postprocessing_VBG methods_NNS for_IN calibration_NN are_VBP isotonic_JJ regression_NN -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, and_CC fitting_JJ a_DT one-dimensional_JJ logistic_JJ regression_NN function_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP apply_VBP the_DT latter_JJ 216_CD method_NN ,_, which_WDT is_VBZ often_RB called_VBN Platt_NNP scaling_NN ,_, to_TO SVM_NNP classifiers_NNS in_IN Section_NNP 5_CD below_IN ._.
4_LS ._.
AN_DT ILLUSTRATION_NNP To_TO illustrate_VB the_DT method_NN proposed_VBN in_IN Section_NN 2_CD above_RB ,_, we_PRP generate_VBP 500_CD p_NN
ain_VB any_DT explicit_JJ set_NN of_IN examples_NNS that_WDT should_MD not_RB be_VB included_VBN ,_, and_CC it_PRP is_VBZ unnatural_JJ to_TO ask_VB a_DT human_JJ expert_NN to_TO identify_VB such_PDT a_DT set_NN ._.
Consider_VB the_DT database_NN that_IN we_PRP are_VBP associated_VBN with_IN ,_, which_WDT is_VBZ called_VBN TCDB_NN =_SYM -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT database_NN contains_VBZ information_NN about_IN over_IN 4000_CD proteins_NNS that_WDT are_VBP involved_VBN in_IN signaling_NN across_IN cellular_JJ membranes_NNS ._.
If_IN we_PRP ask_VBP a_DT biologist_NN for_IN examples_NNS of_IN proteins_NNS that_WDT are_VBP not_RB involved_VBN in_IN this_DT pro_NN
0.9465_CD 0.9279_CD 0.9895_CD 621_CD and_CC then_RB -LRB-_-LRB- ii_LS -RRB-_-RRB- to_TO apply_VB a_DT standard_JJ learning_NN method_NN to_TO these_DT examples_NNS and_CC the_DT positive_JJ examples_NNS ;_: steps_NNS -LRB-_-LRB- i_LS -RRB-_-RRB- and_CC -LRB-_-LRB- ii_LS -RRB-_-RRB- may_MD be_VB iterated_VBN ._.
Papers_NNP using_VBG this_DT general_JJ approach_NN include_VBP =_JJ -_: =[_NN 24_CD ,_, 20_CD ,_, 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC the_DT idea_NN has_VBZ been_VBN rediscovered_VBN independently_RB a_DT few_JJ times_NNS ,_, most_RBS recently_RB in_IN -LRB-_-LRB- 22_CD ,_, Section_NNP 2.4_CD -RRB-_-RRB- ._.
The_DT approach_NN is_VBZ sometimes_RB extended_VBN to_TO identify_VB also_RB additional_JJ positive_JJ examples_NNS in_IN the_DT unlabeled_JJ se_FW
e_LS result_NN has_VBZ not_RB been_VBN published_VBN before_RB ,_, and_CC it_PRP is_VBZ not_RB obvious_JJ ._.
The_DT reason_NN perhaps_RB that_IN the_DT result_NN is_VBZ novel_JJ is_VBZ that_IN although_IN the_DT learning_NN scenario_NN has_VBZ been_VBN discussed_VBN in_IN many_JJ previous_JJ papers_NNS ,_, including_VBG =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 11_CD ,_, 26_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC these_DT papers_NNS do_VBP make_VB the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN either_CC explicitly_RB or_CC implicitly_RB ,_, the_DT scenario_NN has_VBZ not_RB previously_RB been_VBN formalized_VBN using_VBG a_DT random_JJ variable_JJ s_NN to_TO represent_VB the_DT fa_NN
because_IN in_IN previous_JJ work_NN we_PRP did_VBD in_IN fact_NN manually_RB identify_VBP the_DT subset_NN of_IN actual_JJ positive_JJ examples_NNS inside_IN U_NN ;_: call_VB this_DT subset_NN Q._NNP The_NNP procedure_NN used_VBN to_TO identify_VB Q_NNP ,_, which_WDT has_VBZ 348_CD members_NNS ,_, is_VBZ explained_VBN in_IN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Let_VB N_NN =_JJ U_NNP \_NNP Q_NNP so_IN the_DT cardinality_NN of_IN N_NN is_VBZ 4558_CD ._.
The_DT three_CD sets_NNS of_IN records_NNS N_NN ,_, P_NN ,_, and_CC Q_NNP are_VBP available_JJ at_IN www.cs.ucsd.edu\/users\/elkan\/posonly_NN ._.
The_DT P_NN and_CC U_NN datasets_NNS were_VBD obtained_VBN separately_RB ,_, and_CC U_NN is_VBZ a_DT sa_NN
sitive_JJ example_NN is_VBZ labeled_VBN ._.
This_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN is_VBZ analogous_JJ to_TO the_DT ``_`` missing_VBG completely_RB at_IN random_JJ ''_'' assumption_NN that_WDT is_VBZ often_RB made_VBN when_WRB learning_VBG from_IN data_NNS with_IN missing_VBG values_NNS =_JJ -_: =[_NN 10_CD ,_, 17_CD ,_, 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Another_DT way_NN of_IN stating_VBG the_DT assumption_NN is_VBZ that_IN s_NN and_CC x_NN are_VBP conditionally_RB independent_JJ given_VBN y._NN 214_CD So_RB ,_, a_DT training_NN set_NN is_VBZ a_DT random_JJ sample_NN from_IN a_DT distribution_NN p_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, s_NNS -RRB-_-RRB- that_WDT satisfies_VBZ Equations_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- and_CC
e_LS result_NN has_VBZ not_RB been_VBN published_VBN before_RB ,_, and_CC it_PRP is_VBZ not_RB obvious_JJ ._.
The_DT reason_NN perhaps_RB that_IN the_DT result_NN is_VBZ novel_JJ is_VBZ that_IN although_IN the_DT learning_NN scenario_NN has_VBZ been_VBN discussed_VBN in_IN many_JJ previous_JJ papers_NNS ,_, including_VBG =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 11_CD ,_, 26_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC these_DT papers_NNS do_VBP make_VB the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN either_CC explicitly_RB or_CC implicitly_RB ,_, the_DT scenario_NN has_VBZ not_RB previously_RB been_VBN formalized_VBN using_VBG a_DT random_JJ variable_JJ s_NN to_TO represent_VB the_DT fa_NN
n_NN words_NNS ,_, the_DT probability_NN that_IN an_DT example_NN x_NN appears_VBZ in_IN the_DT labeled_JJ set_NN is_VBZ zero_CD if_IN y_NN =_JJ 0_CD ._.
There_EX is_VBZ a_DT subtle_JJ but_CC important_JJ difference_NN between_IN the_DT scenario_NN considered_VBN here_RB ,_, and_CC the_DT scenario_NN considered_VBN in_IN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT scenario_NN here_RB is_VBZ that_IN the_DT training_NN data_NNS are_VBP drawn_VBN randomly_RB from_IN p_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, s_NNS -RRB-_-RRB- ,_, but_CC for_IN each_DT tuple_FW 〈_FW x_NN ,_, y_NN ,_, s_NN 〉_NN that_WDT is_VBZ drawn_VBN ,_, only_RB 〈_CD x_NN ,_, s_NN 〉_NN is_VBZ recorded_VBN ._.
The_DT scenario_NN of_IN -LRB-_-LRB- 21_CD -RRB-_-RRB- is_VBZ that_IN two_CD training_NN sets_NNS ar_IN
