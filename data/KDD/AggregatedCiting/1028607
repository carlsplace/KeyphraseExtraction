Towards_IN scalable_JJ support_NN vector_NN machines_NNS using_VBG squashing_VBG
wer_NN memory_NN cost_NN and_CC yields_NNS performance_NN close_RB to_TO that_DT of_IN SMO_NN on_IN the_DT full_JJ data_NNS ._.
In_IN this_DT paper_NN we_PRP suggest_VBP another_DT method_NN for_IN scaling_VBG SVMs_NNS up_RB based_VBN on_IN squashing_NN ._.
DuMouchel_NNP et_NNP ._.
al._FW -LRB-_-LRB- 5_CD -RRB-_-RRB- and_CC Madigan_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: recently_RB introduced_VBN squashing_NN as_IN a_DT technique_NN that_WDT allows_VBZ one_CD to_TO scale_VB a_DT dataset_NN down_RP while_IN preserving_VBG its_PRP$ statistical_JJ properties_NNS ._.
In_IN particular_JJ ,_, likelihood-based_JJ squashing_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- assumes_VBZ a_DT particular_JJ st_NN
ational_JJ cost_NN is_VBZ to_TO use_VB approximation_NN methods_NNS ._.
The_DT simplest_JJS method_NN is_VBZ sampling_NN from_IN the_DT original_JJ dataset_NN and_CC using_VBG the_DT sample_NN to_TO train_VB the_DT SVM_NNP ._.
An_DT extension_NN of_IN this_DT idea_NN was_VBD explored_VBN by_IN Pavlov_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: who_WP reported_VBD results_NNS on_IN the_DT application_NN of_IN boosting_VBG to_TO training_NN SVMs_NNS ._.
Boost-SMO_JJ algorithm_NN trains_VBZ a_DT sequence_NN of_IN SVM_NN classifiers_NNS on_IN samples_NNS of_IN data_NNS so_IN that_IN each_DT subsequent_JJ classifier_NN concentrates_VBZ mostl_NN
Ms_NN ,_, has_VBZ substantially_RB lower_JJR memory_NN cost_NN and_CC yields_NNS performance_NN close_RB to_TO that_DT of_IN SMO_NN on_IN the_DT full_JJ data_NNS ._.
In_IN this_DT paper_NN we_PRP suggest_VBP another_DT method_NN for_IN scaling_VBG SVMs_NNS up_RB based_VBN on_IN squashing_NN ._.
DuMouchel_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Madigan_NNP et_NNP ._.
al._FW -LRB-_-LRB- 7_CD -RRB-_-RRB- recently_RB introduced_VBD squashing_VBG as_IN a_DT technique_NN that_WDT allows_VBZ one_CD to_TO scale_VB a_DT dataset_NN down_RP while_IN preserving_VBG its_PRP$ statistical_JJ properties_NNS ._.
In_IN particular_JJ ,_, likelihood-based_JJ squashing_NN -LRB-_-LRB- 7_CD -RRB-_-RRB-
lved_VBN in_IN solving_VBG quadratic_JJ programming_NN problem_NN arising_VBG in_IN their_PRP$ training_NN ._.
Various_JJ training_NN algorithms_NNS have_VBP been_VBN proposed_VBN to_TO speed_VB up_RP the_DT training_NN ,_, including_VBG chunking_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, Osuna_NNP 's_POS decomposition_NN method_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC Sequential_JJ Minimal_JJ Optimization_NN -LRB-_-LRB- SMO_NN -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
Although_IN these_DT algorithms_NNS accelerate_VBP the_DT training_NN ,_, they_PRP do_VBP not_RB scale_VB well_RB with_IN the_DT size_NN of_IN the_DT training_NN data_NNS ._.
Another_DT approach_NN to_TO reducing_VBG the_DT computa_NN
ing_NN Methodologies_NNS -RRB-_-RRB- :_: Artificial_NNP Intelligence_NNP --_: Learning_NNP General_NNP Terms_NNS support_VBP vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- ,_, scalability_NN ,_, squashing_NN ,_, boosting_VBG 1_CD ._.
INTRODUCTION_NN Following_VBG the_DT pioneering_JJ work_NN of_IN Vladimir_NNP Vapnik_NNP =_SYM -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, support_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- are_VBP steadily_RB gaining_VBG popularity_NN in_IN the_DT machine_NN learning_NN community_NN ._.
SVMs_NNS have_VBP been_VBN proven_VBN to_TO exhibit_VB several_JJ attractive_JJ theoretical_JJ properties_NNS including_VBG maximum_NN margin_NN
f_LS boosting_VBG to_TO training_NN SVMs_NNS ._.
Boost-SMO_JJ algorithm_NN trains_VBZ a_DT sequence_NN of_IN SVM_NN classifiers_NNS on_IN samples_NNS of_IN data_NNS so_IN that_IN each_DT subsequent_JJ classifier_NN concentrates_VBZ mostly_RB on_IN the_DT errors_NNS made_VBN by_IN the_DT previous_JJ ones_NNS =_JJ -_: =[_NN 12_CD ,_, 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
While_IN this_DT method_NN is_VBZ not_RB optimal_JJ in_IN general_JJ ,_, it_PRP allows_VBZ for_IN fast_JJ training_NN of_IN SVMs_NNS ,_, has_VBZ substantially_RB lower_JJR memory_NN cost_NN and_CC yields_NNS performance_NN close_RB to_TO that_DT of_IN SMO_NN on_IN the_DT full_JJ data_NNS ._.
In_IN this_DT paper_NN we_PRP s_VBZ
l_NN and_CC tries_VBZ to_TO preserve_VB the_DT behavior_NN of_IN the_DT likelihood_NN function_NN of_IN the_DT original_JJ data_NNS -LRB-_-LRB- referred_VBN to_TO as_IN the_DT ``_`` mother-data_NN ''_'' -RRB-_-RRB- in_IN the_DT neighborhood_NN of_IN the_DT maximum_NN likelihood_NN solution_NN ._.
A_DT recent_JJ pape_NN =_JJ -_: =_JJ r_NN by_IN Owen_NNP -LRB-_-LRB- Owe99_NN -RRB-_-RRB- -_: =_SYM -_: analyzes_VBZ why_WRB and_CC when_WRB squashing_VBG may_MD reduce_VB the_DT complexity_NN of_IN learning_VBG parameters_NNS of_IN the_DT model_NN and_CC concludes_VBZ that_IN a_DT method_NN that_WDT uses_VBZ global_JJ -LRB-_-LRB- as_IN opposed_VBN to_TO local_JJ -RRB-_-RRB- features_NNS of_IN the_DT data_NNS will_MD be_VB likely_JJ to_TO
number_NN and_CC location_NN of_IN the_DT points_NNS in_IN the_DT parameter_NN space_NN to_TO evaluate_VB the_DT likelihoods_NNS at_IN ,_, and_CC the_DT number_NN of_IN squashed_JJ data_NNS points_NNS to_TO ensure_VB a_DT sufficiently_RB good_JJ approximation_NN ._.
Various_JJ factorial_JJ designs_NNS =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: in_IN the_DT parameter_NN space_NN are_VBP suggested_VBN in_IN -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
While_IN this_DT method_NN is_VBZ universal_JJ ,_, as_IN we_PRP noted_VBD above_IN ,_, for_IN SVMs_NNS we_PRP can_MD sample_NN the_DT values_NNS of_IN w_NN from_IN the_DT prior_JJ distribution_NN in_IN equation_NN 4_CD ._.
The_DT choice_NN of_IN the_DT int_NN
on_IN between_IN the_DT classes_NNS is_VBZ equivalent_JJ to_TO the_DT minimization_NN of_IN kwk_NN 2_CD L_NN 2_CD with_IN the_DT additional_JJ constraints_NNS y_FW i_FW -LRB-_-LRB- !_.
w_NN ;_: x_NN i_LS ?_.
+_CC b_LS -RRB-_-RRB- s1_NN to_TO ensure_VB that_IN all_PDT the_DT patterns_NNS in_IN the_DT training_NN set_NN are_VBP classified_VBN correctly_RB =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, in_IN most_JJS cases_NNS perfect_JJ separation_NN is_VBZ impossible_JJ and_CC we_PRP need_VBP to_TO trade_VB errors_NNS on_IN the_DT individual_JJ training_NN patterns_NNS for_IN the_DT maximum_NN margin_NN ._.
The_DT optimization_NN problem_NN becomes_VBZ minw_NN ;_: bE_NN =_JJ minw_NN ;_: bf_NN 1_CD 2_CD
referred_VBN to_TO as_IN the_DT ``_`` mother-data_NN ''_'' -RRB-_-RRB- in_IN the_DT neighborhood_NN of_IN the_DT maximum_NN likelihood_NN solution_NN ._.
To_TO apply_VB likelihood-based_JJ squashing_NN to_TO SVMs_NNS it_PRP is_VBZ necessary_JJ to_TO have_VB a_DT probabilistic_JJ interpretation_NN -LRB-_-LRB- =_JJ -_: =_JJ see_VB ,_, e.g._FW ,_, -LRB-_-LRB- 6_CD ,_, 16_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
We_PRP use_VBP an_DT interpretation_NN of_IN the_DT SVM_NN training_NN procedure_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- as_IN a_DT problem_NN of_IN finding_VBG maximum_JJ aposteriori_NN values_NNS for_IN the_DT parameters_NNS of_IN the_DT SVM_NN ._.
We_PRP show_VBP that_IN the_DT probabilistic_JJ interpretation_NN of_IN SVM_NN tr_NN
rising_VBG in_IN their_PRP$ training_NN ._.
Various_JJ training_NN algorithms_NNS have_VBP been_VBN proposed_VBN to_TO speed_VB up_RP the_DT training_NN ,_, including_VBG chunking_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, Osuna_NNP 's_POS decomposition_NN method_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, and_CC Sequential_JJ Minimal_JJ Optimization_NN -LRB-_-LRB- SMO_NN -RRB-_-RRB- =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Although_IN these_DT algorithms_NNS accelerate_VBP the_DT training_NN ,_, they_PRP do_VBP not_RB scale_VB well_RB with_IN the_DT size_NN of_IN the_DT training_NN data_NNS ._.
Another_DT approach_NN to_TO reducing_VBG the_DT computational_JJ cost_NN is_VBZ to_TO use_VB approximation_NN methods_NNS ._.
The_DT
ffered_VBN by_IN squashSMO_NN and_CC boost-SMO_NN might_MD be_VB even_RB more_RBR significant_JJ for_IN non-linear_JJ SVMs_NNS that_WDT take_VBP much_RB longer_JJR to_TO train_VB ._.
Several_JJ authors_NNS have_VBP studied_VBN the_DT question_NN of_IN feature_NN selection_NN for_IN SVMs_NNS -LRB-_-LRB- see_VB ,_, e.g._FW =_JJ -_: =_JJ -LRB-_-LRB- BO98_NN -RRB-_-RRB- -_: =--RRB-_NN ._.
The_DT main_JJ idea_NN is_VBZ to_TO interpret_VB the_DT norm_NN of_IN w_NN as_IN a_DT norm_NN in_IN L_NN 1_CD space_NN and_CC reduce_VB the_DT training_NN task_NN to_TO linear_JJ programming_NN optimization_NN problem_NN ._.
We_PRP think_VBP that_IN squashing_NN might_MD offer_VB a_DT principled_JJ way_NN to_TO d_NN
data_NNS from_IN squashing_VBG ._.
4_CD Experiments_NNS We_PRP have_VBP run_VBN experiments_NNS on_IN four_CD datasets_NNS ,_, one_CD of_IN which_WDT is_VBZ synthetic_JJ and_CC the_DT rest_NN are_VBP publicly_RB available_JJ at_IN either_DT UCI_NNP machine_NN learning_NN -LRB-_-LRB- BM98_NN -RRB-_-RRB- or_CC UCI_NN KDD_NN repository_NN =_JJ -_: =_JJ -LRB-_-LRB- Bay99_NN -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP 6_CD evaluated_VBD the_DT performance_NN of_IN the_DT full-SMO_NN -LRB-_-LRB- SMO_NN on_IN the_DT full_JJ training_NN data_NNS -RRB-_-RRB- ,_, srs-SMO_NN -LRB-_-LRB- SMO_NN on_IN a_DT simple_JJ random_JJ sample_NN -RRB-_-RRB- ,_, squash-SMO_NN -LRB-_-LRB- SMO_NN on_IN the_DT squashed_JJ data_NNS -RRB-_-RRB- and_CC boost-SMO_NN -LRB-_-LRB- SMO_NN on_IN the_DT boosted_VBN sampl_NN
ained_VBN quadratic_JJ programming_NN problem_NN ._.
The_DT standard_JJ SMO_NN algorithm_NN solves_VBZ the_DT dual_JJ of_IN problem_NN 2_CD and_CC 3_CD by_IN decomposing_VBG it_PRP into_IN the_DT smaller_JJR problems_NNS that_WDT can_MD be_VB solved_VBN analytically_RB ._.
SMO_NN is_VBZ provably_RB optimal_JJ =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Several_JJ authors_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 16_CD ,_, 14_CD -RRB-_-RRB- -RRB-_-RRB- have_VBP shown_VBN that_IN the_DT right_JJ side_NN of_IN Equation_NN 2_CD can_MD be_VB treated_VBN as_IN a_DT log-posterior_NN on_IN the_DT parameters_NNS w_NN and_CC b_NN ,_, with_IN the_DT first_JJ term_NN corresponding_VBG to_TO a_DT prior_JJ on_IN w_NN and_CC the_DT s_NN
directly_RB on_IN the_DT weighed_VBN data_NNS from_IN squashing_VBG ._.
4_LS ._.
EXPERIMENTS_NNS We_PRP have_VBP run_VBN experiments_NNS on_IN four_CD datasets_NNS ,_, one_CD of_IN which_WDT is_VBZ synthetic_JJ and_CC the_DT rest_NN are_VBP publicly_RB available_JJ at_IN either_CC the_DT UCI_NNP machine_NN learning_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: or_CC UCI_NNP KDD_NNP repositories_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- ._.
We_PRP evaluated_VBD the_DT performance_NN of_IN the_DT full-SMO_NN -LRB-_-LRB- SMO_NN on_IN the_DT full_JJ training_NN data_NNS -RRB-_-RRB- ,_, srs-SMO_NN -LRB-_-LRB- SMO_NN on_IN a_DT simple_JJ random_JJ sample_NN -RRB-_-RRB- ,_, squash-SMO_NN -LRB-_-LRB- SMO_NN on_IN the_DT squashed_JJ data_NNS -RRB-_-RRB- and_CC boost-SMO_NN
referred_VBN to_TO as_IN the_DT ``_`` mother-data_NN ''_'' -RRB-_-RRB- in_IN the_DT neighborhood_NN of_IN the_DT maximum_NN likelihood_NN solution_NN ._.
To_TO apply_VB likelihood-based_JJ squashing_NN to_TO SVMs_NNS it_PRP is_VBZ necessary_JJ to_TO have_VB a_DT probabilistic_JJ interpretation_NN -LRB-_-LRB- =_JJ -_: =_JJ see_VB ,_, e.g._FW ,_, -LRB-_-LRB- 6_CD ,_, 16_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
We_PRP use_VBP an_DT interpretation_NN of_IN the_DT SVM_NN training_NN procedure_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- as_IN a_DT problem_NN of_IN finding_VBG maximum_JJ aposteriori_NN values_NNS for_IN the_DT parameters_NNS of_IN the_DT SVM_NN ._.
We_PRP show_VBP that_IN the_DT probabilistic_JJ interpretation_NN of_IN SVM_NN tr_NN
mum_JJ likelihood_NN solution_NN ._.
To_TO apply_VB likelihood-based_JJ squashing_NN to_TO SVMs_NNS it_PRP is_VBZ necessary_JJ to_TO have_VB a_DT probabilistic_JJ interpretation_NN -LRB-_-LRB- see_VB ,_, e.g._FW ,_, -LRB-_-LRB- 6_CD ,_, 16_CD -RRB-_-RRB- -RRB-_-RRB- ._.
We_PRP use_VBP an_DT interpretation_NN of_IN the_DT SVM_NN training_NN procedure_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: as_IN a_DT problem_NN of_IN finding_VBG maximum_JJ aposteriori_NN values_NNS for_IN the_DT parameters_NNS of_IN the_DT SVM_NN ._.
We_PRP show_VBP that_IN the_DT probabilistic_JJ interpretation_NN of_IN SVM_NN training_NN in_IN conjunction_NN with_IN likelihood-based_JJ squashing_NN allows_VBZ o_NN
