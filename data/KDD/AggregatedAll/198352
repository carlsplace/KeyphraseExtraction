Efficient_JJ progressive_JJ sampling_NN
the_DT model_NN 's_POS performance_NN at_IN each_DT epoch_NN ._.
To_TO accomplish_VB this_DT ,_, we_PRP compute_VBP x-validated_JJ accuracy_NN of_IN the_DT current_JJ model_NN on_IN the_DT available_JJ pool_NN of_IN instances_NNS ._.
Progress_NNP of_IN learning_VBG curve_NN is_VBZ estimated_VBN empirically_RB =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Here_RB we_PRP use_VBP LOESS_JJ regression_NN in_IN order_NN to_TO smooth_VB the_DT variances_NNS in_IN estimated_VBN learning_NN rates_NNS at_IN each_DT epoch_NN ._.
More_RBR succinctly_RB ,_, the_DT learning_NN rate_NN at_IN any_DT point_NN is_VBZ estimated_VBN by_IN computing_VBG the_DT slope_NN of_IN a_DT leas_NN
statistics_NNS under_IN the_DT name_NN of_IN sequential_JJ sampling_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- and_CC more_RBR recently_RB in_IN the_DT database_NN community_NN -LRB-_-LRB- 14_CD ,_, 13_CD -RRB-_-RRB- ._.
In_IN the_DT KDD_NNP literature_NN ,_, related_JJ methods_NNS are_VBP described_VBN under_IN the_DT name_NN of_IN progressive_JJ sampling_NN =_JJ -_: =[_NN 11_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Details_NNS on_IN how_WRB to_TO use_VB sampling_NN for_IN selecting_VBG the_DT stump_NN at_IN each_DT boosting_VBG iteration_NN are_VBP provided_VBN in_IN Section_NN 3_CD ._.
In_IN this_DT paper_NN we_PRP provide_VBP a_DT preliminary_JJ experimental_JJ evaluation_NN of_IN this_DT learning_NN system_NN ._.
F_NN
ing_RB ,_, but_CC it_PRP does_VBZ not_RB venture_VB intosearching_VBG among_IN all_DT possible_JJ combinations_NNS of_IN parameter_NN settings_NNS exhaustively_RB on_IN all_DT training_NN data_NNS available_JJ ._.
Rather_RB ,_, it_PRP borrows_VBZ a_DT heuristic_NN from_IN progressive_JJ sampling_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT goal_NN of_IN progressive_JJ sampling_NN is_VBZ to_TO iteratively_RB seek_VB a_DT data_NN set_NN size_NN at_IN which_WDT generalization_NN performance_NN on_IN test_NN material_NN converges_VBZ ._.
The_DT method_NN is_VBZ to_TO start_VB at_IN a_DT small_JJ sample_NN size_NN training_NN set_NN ,_, an_DT
rmance_NN at_IN a_DT reasonable_JJ time_NN ._.
Approaches_NNS to_TO tackle_VB this_DT problem_NN range_NN from_IN statistical_JJ estmiates_NNS for_IN appropriate_JJ subsample_JJ sizes_NNS -LRB-_-LRB- 15_CD ,_, 29_CD ,_, 24_CD ,_, 9_CD -RRB-_-RRB- ,_, over_IN attempts_NNS to_TO model_VB the_DT shape_NN of_IN the_DT learning_NN curve_NN =_JJ -_: =[_NN 13_CD ,_, 22_CD -RRB-_-RRB- -_: =_JJ -_: ,_, to_TO active_JJ learning_NN and_CC windowing_NN techniques_NNS that_WDT give_VBP the_DT learning_NN algorithm_NN itself_PRP control_VBP over_IN the_DT subsampling_JJ process_NN -LRB-_-LRB- 8_CD ,_, 10_CD -RRB-_-RRB- ._.
However_RB ,_, the_DT main_JJ focus_NN of_IN previous_JJ works_NNS on_IN this_DT topic_NN has_VBZ been_VBN on_IN
then_RB applying_VBG voting_NN to_TO classify_VB new_JJ examples_NNS will_MD allow_VB an_DT accurate_JJ model_NN to_TO be_VB built_VBN on_IN a_DT single_JJ computer_NN ._.
Others_NNS have_VBP suggested_VBN that_IN in_IN very_RB large_JJ data_NNS sets_VBZ many_JJ of_IN the_DT examples_NNS will_MD be_VB redundant_JJ =_JJ -_: =[_NN 7_CD ,_, 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT context_NN of_IN the_DT above_JJ work_NN ,_, the_DT use_NN of_IN subsets_NNS or_CC perhaps_RB a_DT single_JJ well-chosen_JJ subset_NN for_IN training_NN may_MD be_VB the_DT right_JJ approach_NN ._.
In_IN this_DT paper_NN ,_, we_PRP consider_VBP the_DT case_NN where_WRB either_CC all_PDT the_DT data_NNS -LRB-_-LRB- of_IN
and_CC feature_NN selection_NN ,_, that_WDT combines_VBZ classifier_NN wrapping_NN -LRB-_-LRB- using_VBG the_DT training_NN material_NN internally_RB to_TO test_VB experimental_JJ variants_NNS -RRB-_-RRB- -LRB-_-LRB- Kohavi_NNP and_CC John_NNP ,_, 1997_CD -RRB-_-RRB- with_IN progressive_JJ sampling_NN of_IN training_NN material_NN -LRB-_-LRB- =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN ._.
We_PRP start_VBP with_IN a_DT large_JJ pool_NN of_IN experiments_NNS ,_, each_DT with_IN a_DT unique_JJ combination_NN of_IN input_NN features_NNS and_CC algorithmic_JJ parameter_NN settings_NNS ._.
In_IN the_DT first_JJ step_NN ,_, each_DT attempted_VBN setting_NN is_VBZ applied_VBN to_TO a_DT small_JJ amoun_NN
sub-sampling_NN and_CC compression_NN for_IN reducing_VBG the_DT volume_NN of_IN data_NNS these_DT algorithms_NNS must_MD examine_VB ._.
While_IN serial_JJ techniques_NNS for_IN sub-sampling_NN and_CC compression_NN have_VBP been_VBN developed_VBN and_CC applied_VBN with_IN some_DT success_NN =_JJ -_: =[_NN 15_CD ,_, 24_CD ,_, 32_CD ,_, 36_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT variety_NN of_IN application_NN characteristics_NNS necessitate_VBP the_DT development_NN of_IN corresponding_VBG distributed_VBN formulations_NNS ._.
These_DT include_VBP :_: â€¢_NNP Data_NNP volume_NN :_: Datasets_NNS often_RB reside_VBP at_IN geographically_RB distributed_VBN loca_NN
and_CC feature_NN selection_NN ,_, that_WDT combines_VBZ classifier_NN wrapping_NN -LRB-_-LRB- using_VBG the_DT training_NN material_NN internally_RB to_TO test_VB experimental_JJ variants_NNS -RRB-_-RRB- -LRB-_-LRB- Kohavi_NNP and_CC John_NNP ,_, 1997_CD -RRB-_-RRB- with_IN progressive_JJ sampling_NN of_IN training_NN material_NN -LRB-_-LRB- =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN ._.
We_PRP start_VBP with_IN a_DT large_JJ pool_NN of_IN experiments_NNS ,_, each_DT with_IN a_DT unique_JJ combination_NN of_IN input_NN features_NNS and_CC algorithmic_JJ parameter_NN settings_NNS ._.
In_IN the_DT first_JJ step_NN ,_, each_DT attempted_VBN setting_NN is_VBZ applied_VBN to_TO a_DT small_JJ amoun_NN
geometric_JJ sampling_NN scheme_NN is_VBZ motivated_VBN by_IN previous_JJ work_NN on_IN progressive_JJ sampling_NN ,_, which_WDT shows_VBZ that_IN given_VBN certain_JJ assumptions_NNS -LRB-_-LRB- which_WDT do_VBP not_RB hold_VB in_IN this_DT paper_NN -RRB-_-RRB- ,_, this_DT schedule_NN is_VBZ asymptotically_RB optimal_JJ =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT previous_JJ work_NN on_IN progressive_JJ sampling_NN is_VBZ described_VBN in_IN Section_NN 6_CD ._.
For_IN sampling_NN schedules_NNS S1_NN and_CC S2_NN ,_, in_IN addition_NN to_TO evaluating_VBG the_DT training_NN set_NN sizes_NNS just_RB described_VBN ,_, the_DT largest_JJS possible_JJ training_NN
ny_IN unknown_JJ properties_NNS of_IN an_DT instance_NN ._.
Other_JJ results_NNS seeking_VBG to_TO reduce_VB the_DT sample_NN complexity_NN for_IN learning_NN include_VBP decision_NN theoretic_JJ subsampling_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, on-line_JJ stopping_VBG rules_NNS -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, progressive_JJ sampling_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC active_JJ feature_NN value_NN acquisition_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
We_PRP note_VBP that_IN these_DT techniques_NNS differ_VBP from_IN our_PRP$ approach_NN because_IN we_PRP place_VBP a_DT firm_JJ prior_JJ budget_NN on_IN the_DT learner_NN 's_POS ability_NN to_TO acquire_VB information_NN ,_, while_IN these_DT ap_NN
for_IN model-based_JJ clustering_NN methods_NNS are_VBP usually_RB smooth_JJ ._.
In_IN situations_NNS where_WRB the_DT learning_NN curves_NNS are_VBP less_RBR smooth_JJ ,_, techniques_NNS that_WDT use_VBP additional_JJ samples_NNS to_TO assess_VB the_DT shape_NN of_IN the_DT learning_NN curve_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =_JJ Provost_NNP ,_, et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN may_MD be_VB useful_JJ and_CC the_DT use_NN of_IN an_DT abbreviated_JJ training_NN method_NN can_MD reduce_VB the_DT cost_NN of_IN doing_VBG so_RB ._.
6_CD ._.
Empirical_JJ Study_NN In_IN this_DT section_NN ,_, we_PRP describe_VBP an_DT evaluation_NN of_IN our_PRP$ learning-curve_JJ sampling_NN method_NN and_CC e_SYM
designing_VBG data_NNS mining_NN systems_NNS for_IN very_RB large_JJ data_NNS sets_NNS ._.
The_DT machine_NN learning_NN community_NN has_VBZ essentially_RB focused_VBN on_IN two_CD directions_NNS to_TO deal_VB with_IN massive_JJ data_NNS sets_NNS :_: data_NNS subsampling_VBG -LRB-_-LRB- Musick_NNP et_FW al._FW ,_, 1993_CD ;_: =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN ,_, and_CC the_DT design_NN of_IN parallel_NN or_CC distributed_VBN approaches_NNS capable_JJ of_IN handling_VBG all_PDT the_DT data_NNS -LRB-_-LRB- Chan_NNP and_CC Stolfo_NNP ,_, 1993_CD ;_: Provost_NNP and_CC Hennessy_NNP ,_, 1996_CD ;_: Hall_NNP et_FW al._FW ,_, 1999_CD ;_: Chawla_NNP et_FW al._FW ,_, 2000_CD -RRB-_-RRB- ._.
The_DT subsampling_JJ ap_NN
l_NN efficiency_NN may_MD not_RB equate_VB to_TO an_DT improvement_NN in_IN prediction_NN performance_NN ._.
A_DT critical_JJ assumption_NN underlies_VBZ many_JJ current_JJ attempts_NNS to_TO tackle_VB large_JJ data_NNS sets_NNS by_IN creating_VBG algorithms_NNS that_WDT are_VBP more_RBR efficient_JJ =_JJ -_: =[_NN 1_CD ,_, 2_CD ,_, 3_CD ,_, 4_CD ,_, 5_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: :_: that_IN the_DT learning_VBG biases_NNS of_IN existing_VBG algorithms_NNS are_VBP suitable_JJ for_IN use_NN with_IN large_JJ data_NNS sets_NNS ._.
Increasing_VBG the_DT efficiency_NN of_IN an_DT algorithm_NN assumes_VBZ that_IN the_DT existing_VBG algorithm_NN only_RB requires_VBZ more_JJR time_NN ,_, rath_NN
ny_IN unknown_JJ properties_NNS of_IN an_DT instance_NN ._.
Other_JJ results_NNS seeking_VBG to_TO reduce_VB the_DT sample_NN complexity_NN for_IN learning_NN include_VBP decision_NN theoretic_JJ subsampling_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, on-line_JJ stopping_VBG rules_NNS -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, progressive_JJ sampling_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC active_JJ feature_NN value_NN acquisition_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
We_PRP note_VBP that_IN these_DT techniques_NNS differ_VBP from_IN our_PRP$ approach_NN because_IN we_PRP place_VBP a_DT firm_JJ budget_NN on_IN the_DT learner_NN 's_POS ability_NN to_TO acquire_VB information_NN ,_, while_IN these_DT approach_NN
arning_VBG algorithm_NN on_IN the_DT basis_NN of_IN a_DT sequence_NN of_IN samples_NNS ._.
Some_DT authors_NNS have_VBP used_VBN samples_NNS that_WDT increase_VBP by_IN a_DT fixed_JJ amount_NN -LRB-_-LRB- John_NNP &_CC Langley_NNP ,_, 1996_CD -RRB-_-RRB- ,_, while_IN others_NNS have_VBP used_VBN progressively_RB increasing_VBG samples_NNS -LRB-_-LRB- =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =_JJ -_: ;_: Leite_NNP &_CC Brazdil_NNP ,_, 2004_CD -RRB-_-RRB- ._.
Usually_RB the_DT aim_NN is_VBZ to_TO determine_VB the_DT sample_NN size_NN in_IN which_WDT the_DT accuracy_NN does_VBZ not_RB increase_VB any_DT more_JJR ,_, called_VBD a_DT optimal_JJ sample_NN size_NN ._.
Fig._NN 1_CD shows_VBZ a_DT typical_JJ learning_NN curve_NN and_CC the_DT
in_IN no_DT significant_JJ increase_NN in_IN model_NN accuracy_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
Another_DT recent_JJ work_NN on_IN improving_VBG the_DT efficiency_NN of_IN tree_NN building_NN for_IN large_JJ data_NNS sets_NNS is_VBZ Progressive_NNP Sampling_NNP -LRB-_-LRB- PS_NNP for_IN short_JJ -RRB-_-RRB- proposed_VBN by_IN Provost_NNP et_FW al_FW =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
By_IN means_NNS of_IN a_DT learning_NN curve_NN -LRB-_-LRB- see_VB an_DT example_NN learning_NN curve_NN in_IN Figure_NNP 1_LS -RRB-_-RRB- which_WDT depicts_VBZ the_DT relationship_NN between_IN sample_NN size_NN and_CC \*_NN Email_NN :_: gubh@comp.nus.edu.sg;_NNP School_NNP of_IN Computing_NNP ,_, National_NNP University_NNP
in_IN no_DT significant_JJ increase_NN in_IN model_NN accuracy_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
Another_DT recent_JJ work_NN on_IN improving_VBG the_DT efficiency_NN of_IN tree_NN building_NN for_IN large_JJ data_NNS sets_NNS is_VBZ Progressive_NNP Sampling_NNP -LRB-_-LRB- PS_NNP for_IN short_JJ -RRB-_-RRB- proposed_VBN by_IN Provost_NNP et_FW al_FW =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
By_IN means_NNS of_IN a_DT learning_NN curve_NN -LRB-_-LRB- see_VB an_DT example_NN learning_NN curve_NN in_IN Figure_NNP 1_LS -RRB-_-RRB- which_WDT depicts_VBZ the_DT relationship_NN between_IN sample_NN size_NN and_CC âˆ—_NNP Email_NNP :_: gubh@comp.nus.edu.sg;_NNP School_NNP of_IN Computing_NNP ,_, National_NNP University_NNP
nerating_VBG distinct_JJ samples_NNS at_IN each_DT site_NN ._.
We_PRP then_RB merge_VBP these_DT samples_NNS to_TO finally_RB train_VB a_DT classifier_NN on_IN the_DT resulting_VBG sample_NN ._.
The_DT literature_NN in_IN data_NN mining_NN is_VBZ filled_VBN with_IN various_JJ sampling_NN algorithms_NNS -LRB-_-LRB- 3_LS -RRB-_-RRB- =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 4_LS -RRB-_-RRB- ,_, which_WDT can_MD be_VB grouped_VBN ,_, with_IN respect_NN to_TO how_WRB the_DT samples_NNS are_VBP formed_VBN ,_, to_TO form_VB three_CD distinct_JJ families_NNS :_: static_JJ ,_, dynamic_JJ and_CC active_JJ sampling_NN ._.
Static_JJ Sampling_NN refers_VBZ to_TO a_DT sampling_NN that_WDT is_VBZ performed_VBN wit_NN
a_DT heuristic_NN wrapped-based_JJ method_NN to_TO set_VB them_PRP automatically_RB for_IN all_DT experiments_NNS ._.
Wrapped_VBN progressive_JJ sampling_NN -LRB-_-LRB- wps_NNS -RRB-_-RRB- -LRB-_-LRB- 22_CD -RRB-_-RRB- combines_VBZ classifier_NN wrapping_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- with_IN progressive_JJ sampling_NN of_IN training_NN material_NN =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
wps_NN starts_VBZ with_IN a_DT large_JJ pool_NN of_IN experiments_NNS ,_, each_DT with_IN one_CD systematically_RB generated_VBN recombination_NN of_IN tested_VBN algorithmic_JJ parameter_NN settings_NNS ._.
In_IN the_DT first_JJ step_NN of_IN wps_NNS ,_, each_DT attempted_VBN setting_NN is_VBZ applie_NN
and_CC feature_NN selection_NN ,_, that_WDT combines_VBZ classifier_NN wrapping_NN -LRB-_-LRB- using_VBG the_DT training_NN material_NN internally_RB to_TO test_VB experimental_JJ variants_NNS -RRB-_-RRB- -LRB-_-LRB- Kohavi_NNP and_CC John_NNP ,_, 1997_CD -RRB-_-RRB- with_IN progressive_JJ sampling_NN of_IN training_NN material_NN -LRB-_-LRB- =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN ._.
We_PRP start_VBP with_IN a_DT large_JJ pool_NN of_IN experiments_NNS ,_, each_DT with_IN a_DT unique_JJ combination_NN of_IN input_NN features_NNS and_CC algorithmic_JJ parameter_NN settings_NNS ._.
In_IN the_DT first_JJ step_NN ,_, each_DT attempted_VBN setting_NN is_VBZ applied_VBN to_TO a_DT small_JJ amoun_NN
the_DT first_JJ place_NN ._.
Geometric_JJ sampling_NN was_VBD proposed_VBN as_IN an_DT improved_JJ sampling_NN mechanism_NN ._.
It_PRP uses_VBZ the_DT following_JJ schedule_NN :_: Sgeom_NN =_JJ a_FW i_FW Â·_NN n0_NN =_JJ -LCB-_-LRB- n0_NN ,_, a_DT Â·_FW n0_FW ,_, a_DT 2_CD Â·_FW n0_FW ,_, a_DT 3_CD Â·_NN n0_NN ,_, ..._: -RCB-_-RRB- where_WRB a_DT and_CC n0_NN are_VBP constants_NNS =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN instance_NN :_: -LCB-_-LRB- 100_CD ,_, 200_CD ,_, 400_CD ,_, 800_CD ,_, ..._: -RCB-_-RRB- ._.
Although_IN a_DT ,_, which_WDT we_PRP will_MD call_VB it_PRP the_DT geometric_JJ factor_NN ,_, can_MD be_VB any_DT constant_JJ ,_, by_IN default_NN it_PRP has_VBZ been_VBN taken_VBN to_TO be_VB equal_JJ to_TO 2_CD ._.
Its_PRP$ main_JJ drawback_NN is_VBZ that_IN it_PRP is_VBZ
finite_JJ one_NN -RRB-_-RRB- as_IN the_DT reference_NN for_IN loss_NN ,_, and_CC their_PRP$ approach_NN is_VBZ based_VBN on_IN fitting_JJ a_DT learning_NN curve_NN ,_, offering_VBG no_DT guarantees_NNS that_IN the_DT criterion_NN will_MD be_VB satisfied_VBN ._.
Learning_NNP curve_NN methods_NNS -LRB-_-LRB- Meek_NNP et_FW al._FW ,_, 2002_CD ,_, =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_CD produce_NN models_NNS on_IN successively_RB larger_JJR sub-samples_NNS of_IN the_DT training_NN set_VBN until_IN model_NN quality_NN appears_VBZ to_TO asymptote_VB ._.
Progressive_JJ sampling_NN approaches_NNS attempt_VBP to_TO iteratively_RB determine_VB the_DT best_JJS number_NN of_IN ex_FW
ped_VBN by_IN detecting_VBG diminishing_VBG improvement_NN in_IN the_DT quality_NN of_IN the_DT models_NNS being_VBG built_VBN ._.
Convergence_NN detection_NN has_VBZ been_VBN studied_VBN for_IN the_DT case_NN of_IN random_JJ sampling_NN by_IN estimating_VBG the_DT slope_NN of_IN the_DT learning_NN curve_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT learning_VBG curve_NN maynotbewell_NN behaved_VBD in_IN the_DT active_JJ learning_NN case_NN making_VBG this_DT task_NN more_RBR complicated_JJ ._.
This_DT also_RB makes_VBZ the_DT more_RBR general_JJ problem_NN of_IN determining_VBG a_DT good_JJ schedule_NN for_IN adding_VBG labeled_JJ poin_NN
e_LS with_IN the_DT power_NN law_NN ._.
One_CD limitation_NN is_VBZ that_IN the_DT dynamic_JJ sampling_NN requires_VBZ the_DT learning_VBG algorithm_NN to_TO be_VB incremental_JJ ._.
Progressive_JJ Sampling_NN :_: The_DT progressive_JJ sampling_NN was_VBD first_RB proposed_VBN by_IN Provost_NNP et_FW al_FW =_JJ -_: =[_NN 45_CD -RRB-_-RRB- -_: =_SYM -_: for_IN scaling_VBG up_RP inductive_JJ algorithms_NNS ._.
Basically_RB it_PRP uses_VBZ the_DT idea_NN similar_JJ to_TO the_DT dynamic_JJ sampling_NN ,_, but_CC it_PRP does_VBZ not_RB require_VB the_DT algorithm_NN to_TO be_VB incremental_JJ ._.
It_PRP starts_VBZ learning_VBG with_IN a_DT small_JJ sample_NN ,_, and_CC p_NN
ther_JJR enlargements_NNS of_IN the_DT window_NN ._.
In_IN terms_NNS of_IN run-time_NN ,_, the_DT authors_NNS note_VBP that_IN this_DT technique_NN can_MD in_IN general_JJ only_JJ gain_NN efficiency_NN for_IN incremental_JJ algorithm_NN ._.
In_IN the_DT most_RBS recent_JJ work_NN of_IN Provost_NNP and_CC et_FW al_FW =_JJ -_: =[_NN 36_CD -RRB-_-RRB- -_: =_JJ -_: ,_, they_PRP prompted_VBD a_DT new_JJ procedure_NN named_VBN ``_`` progressive_JJ sampling_NN ''_'' ._.
The_DT basic_JJ idea_NN is_VBZ when_WRB sample_NN size_NN increases_VBZ the_DT learning_NN accuracy_NN will_MD follow_VB the_DT famous_JJ ``_`` learning_NN curve_NN ''_'' so_IN that_IN there_EX should_MD be_VB a_DT conve_NN
The_DT experimental_JJ result_NN on_IN census_NN :_: the_DT error_NN rate_NN -LRB-_-LRB- %_NN -RRB-_-RRB- ._.
Archive_NNP -LRB-_-LRB- 3_CD -RRB-_-RRB- ._.
Led_NNP -LRB-_-LRB- 10_CD %_NN -RRB-_-RRB- and_CC waveform_NN are_VBP selected_VBN from_IN UCI_NNP Machine_NNP Learning_NNP Repository_NNP -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
These_DT data_NNS sets_NNS are_VBP used_VBN in_IN the_DT paper_NN of_IN Provost_NNP et_FW al._FW =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
``_`` Led_NNP -LRB-_-LRB- 10_CD %_NN -RRB-_-RRB- ''_'' means_VBZ that_IN 10_CD %_NN of_IN instances_NNS are_VBP includes_VBZ noise_NN in_IN one_CD of_IN the_DT attribute_NN values_NNS ._.
The_DT specifications_NNS of_IN the_DT data_NNS sets_NNS are_VBP summarized_VBN in_IN Table_NNP 2_CD ._.
The_DT experimental_JJ results_NNS on_IN prediction_NN accura_NN
ly_RB ,_, adhoc_JJ sampling_NN queries_NNS that_WDT project_VBP the_DT data_NNS set_VBN along_IN space_NN and_CC time_NN have_VBP been_VBN used_VBN to_TO effectively_RB summarize_VB data_NNS for_IN tasks_NNS such_JJ as_IN anomaly_NN detection_NN and_CC network_NN monitoring_NN ._.
Progressive_JJ sampling_NN =_JJ -_: =[_NN 27_CD -RRB-_-RRB- -_: =_JJ -_: and_CC similar_JJ techniques_NNS can_MD mitigate_VB the_DT issues_NNS related_VBN to_TO the_DT quality_NN of_IN the_DT sample_NN ,_, but_CC they_PRP make_VBP it_PRP imperative_JJ that_IN sample_NN generation_NN be_VB inexpensive_JJ ._.
Despite_IN the_DT recognized_VBN importance_NN of_IN sampling_NN i_FW
he_PRP combined_VBD cost_NN of_IN training_NN -LRB-_-LRB- building_VBG the_DT model_NN -RRB-_-RRB- and_CC operating_NN -LRB-_-LRB- using_VBG the_DT model_NN -RRB-_-RRB- as_IN a_DT function_NN of_IN training_NN set_NN size_NN ._.
We_PRP can_MD then_RB optimize_VB the_DT size_NN of_IN the_DT training_NN set_VBN to_TO minimize_VB this_DT combined_JJ cost_NN -LRB-_-LRB- =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN ._.
Alternatively_RB ,_, an_DT adaptive_JJ learning_NN system_NN ,_, given_VBN -LRB-_-LRB- 1_LS -RRB-_-RRB- the_DT expected_VBN number_NN of_IN classifications_NNS that_IN the_DT learned_VBN model_NN will_MD make_VB when_WRB embedded_VBN in_IN the_DT operational_JJ system_NN ,_, -LRB-_-LRB- 2_LS -RRB-_-RRB- the_DT cost_NN of_IN misclassificat_NN
geometric_JJ sampling_NN scheme_NN is_VBZ motivated_VBN by_IN previous_JJ work_NN on_IN progressive_JJ sampling_NN ,_, which_WDT shows_VBZ that_IN given_VBN certain_JJ assumptions_NNS -LRB-_-LRB- which_WDT do_VBP not_RB hold_VB in_IN this_DT paper_NN -RRB-_-RRB- ,_, this_DT schedule_NN is_VBZ asymptotically_RB optimal_JJ =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT previous_JJ work_NN on_IN progressive_JJ sampling_NN is_VBZ described_VBN in_IN Section_NN 6_CD ._.
For_IN sampling_NN schedules_NNS S1_NN and_CC S2_NN ,_, in_IN addition_NN to_TO evaluating_VBG the_DT training_NN set_NN sizes_NNS just_RB described_VBN ,_, the_DT largest_JJS possible_JJ training_NN
3_CD and_CC 4_CD ,_, the_DT batch_NN size_NN -LRB-_-LRB- i.e._FW the_DT number_NN of_IN examples_NNS labeled_VBN per_IN iteration_NN -RRB-_-RRB- was_VBD fixed_VBN for_IN each_DT dataset_NN ._.
This_DT is_VBZ typical_JJ of_IN many_JJ active_JJ learning_NN studies_NNS -LRB-_-LRB- e.g._FW -LRB-_-LRB- 1_CD ,_, 25_CD ,_, 30_CD ,_, 47_CD ,_, 49_CD ,_, 56_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Provost_NNP et_FW al._FW =_SYM -_: =[_NN 41_CD -RRB-_-RRB- -_: =_SYM -_: examine_VB methods_NNS for_IN progressive_JJ sampling_NN ,_, in_IN which_WDT successively_RB larger_JJR samples_NNS are_VBP drawn_VBN ._.
This_DT research_NN does_VBZ not_RB involve_VB active_JJ learning_NN ,_, but_CC is_VBZ instead_RB concerned_VBN with_IN learning_VBG from_IN a_DT very_RB large_JJ -LRB-_-LRB- lab_NN
ressive_JJ sampling_NN ,_, henceforth_NN referred_VBD to_TO as_IN wps_NNS -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
We_PRP briefly_RB describe_VBP wps_NNS here_RB ._.
Wps_NNP is_VBZ a_DT heuristic_NN ,_, non-exhaustive_JJ variant_NN of_IN exhaustive_JJ wrapping_NN ._.
It_PRP borrows_VBZ a_DT heuristic_NN from_IN progressive_JJ sampling_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT goal_NN of_IN progressive_JJ sampling_NN is_VBZ to_TO iteratively_RB seek_VB a_DT data_NN set_NN size_NN at_IN which_WDT generalization_NN performance_NN on_IN test_NN material_NN converges_VBZ ._.
More_RBR generally_RB ,_, wps_NN borrows_VBZ from_IN the_DT idea_NN that_IN aspects_NNS of_IN lear_NN
the_DT hope_NN of_IN capturing_VBG the_DT point_NN where_WRB most_JJS of_IN the_DT concept_NN is_VBZ learned_VBN ._.
Usually_RB ,_, the_DT rate_NN of_IN of_IN improvement_NN at_IN the_DT final_JJ stages_NNS of_IN learning_NN ,_, before_IN the_DT concept_NN is_VBZ fully_RB learned_VBN ,_, is_VBZ very_RB slow_JJ -LRB-_-LRB- see_VB eg._FW ,_, =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =--RRB-_NN with_IN several_JJ thousands_NNS of_IN instances_NNS contributing_VBG to_TO a_DT tiny_JJ improvement_NN in_IN performance_NN ,_, unnecessarily_RB inflating_VBG the_DT complexity_NN score_NN -LRB-_-LRB- See_NNP Figure_NNP 2_CD -LRB-_-LRB- a_DT -RRB-_-RRB- -RRB-_-RRB- ._.
Using_VBG a_DT log_NN scale_NN for_IN ni_NNS makes_VBZ the_DT scale_NN like_IN t_NN
ed_VBN sampling_NN or_CC cluster_NN sampling_NN -LRB-_-LRB- see_VB -LRB-_-LRB- 3_CD -RRB-_-RRB- for_IN details_NNS of_IN these_DT sampling_NN techniques_NNS -RRB-_-RRB- ._.
There_EX have_VBP also_RB been_VBN some_DT data_NNS reduction_NN methods_NNS that_WDT incorporate_VBP random_JJ sampling_NN with_IN adaptive_JJ procedures_NNS -LRB-_-LRB- 8_CD -RRB-_-RRB- -LRB-_-LRB- 10_CD -RRB-_-RRB- =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 18_CD -RRB-_-RRB- ;_: but_CC these_DT methods_NNS were_VBD intended_VBN for_IN use_NN in_IN conjunction_NN with_IN a_DT specific_JJ data_NN mining_NN technique_NN such_JJ as_IN decision_NN trees_NNS or_CC association_NN rule_NN algorithms_NNS ._.
There_EX is_VBZ a_DT weakness_NN in_IN almost_RB all_DT of_IN the_DT exis_NN
presents_VBZ an_DT improvement_NN in_IN relation_NN to_TO previous_JJ research_NN ._.
For_IN example_NN ,_, the_DT C4_NN :5_CD algorithm_NN applied_VBN by_IN Soinov_NNP et_FW al._FW -LRB-_-LRB- 10_CD -RRB-_-RRB- has_VBZ a_DT complexity_NN of_IN OÃ°n2_NN logÃ°nÃžÃž_NN for_IN problems_NNS with_IN continuous-valued_JJ attributes_NNS =_JJ -_: =[_NN 32_CD -RRB-_-RRB- -_: =_SYM -_: ._.
4_CD RESULTS_NNS AND_CC DISCUSSION_NN The_DT predictive_JJ performance_NN of_IN GRNCOP_NN was_VBD tested_VBN using_VBG the_DT microarray_NN data_NNS in_IN -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, which_WDT also_RB includes_VBZ data_NNS from_IN S._FW cerevisiae_FW cell_NN cultures_NNS -LRB-_-LRB- 27_CD -RRB-_-RRB- ._.
These_DT data_NNS were_VBD synchroniz_NN
._.
Locality_NN is_VBZ defined_VBN within_IN a_DT particular_JJ progressive_JJ sampling_NN procedure_NN ._.
Not_RB all_DT learning_VBG curves_NNS are_VBP well_RB behaved_VBN ._.
For_IN example_NN ,_, theoretical_JJ analyses_NNS of_IN learning_VBG curves_NNS based_VBN on_IN statistical_JJ mechanics_NNS =_JJ -_: =[_NN 7_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP shown_VBN that_IN sudden_JJ increases_NNS in_IN accuracy_NN are_VBP possible_JJ ,_, particularly_RB on_IN small_JJ samples_NNS ._.
However_RB ,_, empirical_JJ studies_NNS of_IN the_DT application_NN of_IN standard_JJ induction_NN algorithms_NNS to_TO large_JJ data_NNS sets_NNS --_: those_DT of_IN
t._FW In_FW their_PRP$ paper_NN on_IN arithmetic_NN sampling_NN ,_, John_NNP and_CC Langley_NNP -LRB-_-LRB- 8_CD -RRB-_-RRB- model_NN the_DT learning_VBG curve_NN as_IN sampling_NN progresses_VBZ ._.
They_PRP determine_VBP convergence_NN using_VBG a_DT stopping_VBG criterion_NN modeled_VBD after_IN the_DT work_NN of_IN Valiant_NNP =_SYM -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Specifically_RB ,_, convergence_NN is_VBZ reached_VBN when_WRB P_NN r_NN -LRB-_-LRB- -LRB-_-LRB- acc_NN -LRB-_-LRB- N_NN -RRB-_-RRB- \_FW Gamma_FW acc_NN -LRB-_-LRB- n_NN i_LS -RRB-_-RRB- -RRB-_-RRB- ?_.
ffl_NN -RRB-_-RRB- sffi_NN ,_, where_WRB acc_NN -LRB-_-LRB- x_NN -RRB-_-RRB- is_VBZ the_DT accuracy_NN of_IN the_DT model_NN that_IN an_DT algorithm_NN produces_VBZ after_IN seeing_VBG x_NN instances_NNS ,_, ffl_NN refers_VBZ to_TO the_DT m_NN
m_NN on_IN subsets_NNS of_IN size_NN 2_CD We_PRP follow_VBP the_DT reasoning_NN of_IN Korf_NNP ,_, who_WP shows_VBZ that_IN progressive_JJ deepening_VBG is_VBZ an_DT optimal_JJ schedule_NN for_IN conducting_VBG depth-first_JJ search_NN when_WRB the_DT smallest_JJS sufficient_JJ depth_NN is_VBZ unknown_JJ -LRB-_-LRB- 9_CD -RRB-_-RRB- =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
a_DT i_FW \_FW Delta_NN n_NN 0_CD for_IN i_LS =_JJ 0_CD ;_: 1_CD ;_: :_: :_: :_: ;_: b_NN ,_, where_WRB b_NN +_CC 1_CD is_VBZ the_DT number_NN of_IN samples_NNS processed_VBN before_IN detecting_VBG convergence_NN ._.
Now_RB ,_, we_PRP assume_VBP convergence_NN is_VBZ well_RB detected_VBN ,_, so_IN a_DT b_NN \_NNP Gamma1_NNP \_NNP Delta_NNP n_NN 0_CD !_.
nminsa_NN b_NN
un-time_JJ complexity_NN -LRB-_-LRB- in_IN n_NN -RRB-_-RRB- of_IN the_DT underlying_VBG induction_NN algorithm_NN ._.
Run-time_JJ complexity_NN models_NNS are_VBP not_RB always_RB easy_JJ to_TO obtain_VB ._.
For_IN example_NN ,_, our_PRP$ empirical_JJ results_NNS below_RB use_VBP the_DT decisiontree_JJ algorithm_NN C4_NN .5_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, for_IN which_WDT reported_VBD time_NN complexity_NN varies_VBZ widely_RB ._.
Moreover_RB ,_, DP_NN sampling_NN requires_VBZ the_DT actual_JJ runtime_NN complexity_NN for_IN the_DT problem_NN in_IN question_NN ,_, rather_RB than_IN a_DT worst-case_JJ complexity_NN ._.
We_PRP obtained_VBD empirical_JJ
died_VBD the_DT general_JJ case_NN ._.
Methods_NNS for_IN active_JJ sampling_NN ,_, choosing_VBG subsequent_JJ samples_NNS based_VBN upon_IN the_DT models_NNS learned_VBD previously_RB ,_, are_VBP of_IN particular_JJ interest_NN ._.
A_DT classic_JJ example_NN of_IN active_JJ sampling_NN is_VBZ windowing_VBG =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, wherein_WRB subsequent_JJ sampling_NN chooses_VBZ instances_NNS for_IN which_WDT the_DT current_JJ model_NN makes_VBZ errors_NNS ._.
Active_JJ sampling_NN changes_VBZ the_DT learning_NN curve_NN ._.
For_IN example_NN ,_, on_IN noisy_JJ data_NNS ,_, windowing_VBG learning_NN curves_NNS are_VBP notoriou_JJ
on_IN small_JJ samples_NNS ._.
However_RB ,_, empirical_JJ studies_NNS of_IN the_DT application_NN of_IN standard_JJ induction_NN algorithms_NNS to_TO large_JJ data_NNS sets_NNS --_: those_DT of_IN relevance_NN to_TO this_DT paper_NN --_: have_VBP shown_VBN learning_VBG curves_NNS to_TO be_VB well_RB behaved_VBD =_JJ -_: =[_NN 3_CD ,_, 4_CD ,_, 6_CD ,_, 12_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN ,_, practical_JJ progressive_JJ sampling_NN demands_NNS only_RB that_IN learning_VBG curves_NNS are_VBP well_RB behaved_VBN at_IN the_DT level_NN Compute_VB schedule_NN S_NN =_JJ fn_NN 0_CD ;_: n_NN 1_CD ;_: n_NN 2_CD ;_: :_: :_: :_: ;_: n_NN k_NN g_NN of_IN sample_NN sizes_NNS n_NN \/_: n_NN 0_CD M_NN \/_: model_NN in_IN
on_IN small_JJ samples_NNS ._.
However_RB ,_, empirical_JJ studies_NNS of_IN the_DT application_NN of_IN standard_JJ induction_NN algorithms_NNS to_TO large_JJ data_NNS sets_NNS --_: those_DT of_IN relevance_NN to_TO this_DT paper_NN --_: have_VBP shown_VBN learning_VBG curves_NNS to_TO be_VB well_RB behaved_VBD =_JJ -_: =[_NN 3_CD ,_, 4_CD ,_, 6_CD ,_, 12_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN ,_, practical_JJ progressive_JJ sampling_NN demands_NNS only_RB that_IN learning_VBG curves_NNS are_VBP well_RB behaved_VBN at_IN the_DT level_NN Compute_VB schedule_NN S_NN =_JJ fn_NN 0_CD ;_: n_NN 1_CD ;_: n_NN 2_CD ;_: :_: :_: :_: ;_: n_NN k_NN g_NN of_IN sample_NN sizes_NNS n_NN \/_: n_NN 0_CD M_NN \/_: model_NN in_IN
can_MD be_VB done_VBN on_IN convergence_NN detection_NN ._.
With_IN very_RB slow_JJ sampling_NN ,_, the_DT efficiency_NN of_IN progressive_JJ sampling_NN will_MD be_VB the_DT same_JJ as_IN if_IN nmin_NN were_VBD known_VBN a_DT priori_FW ._.
7_CD Other_JJ related_JJ work_NN The_DT method_NN of_IN Musick_NNP et_FW al._FW =_SYM -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: for_IN determining_VBG the_DT best_JJS attribute_NN at_IN each_DT decision-tree_JJ node_NN can_MD be_VB seen_VBN as_IN an_DT instance_NN of_IN the_DT generic_JJ progressive_JJ sampling_NN algorithm_NN shown_VBN in_IN figure_NN 2_CD ,_, if_IN we_PRP regard_VBP each_DT node_NN of_IN the_DT decision_NN tree_NN a_DT
5_CD run_NN time_NN is_VBZ often_RB claimed_VBN to_TO be_VB linear_JJ in_IN the_DT number_NN of_IN instances_NNS ,_, for_IN non-numeric_JJ data_NNS sets_NNS ._.
This_DT claim_NN is_VBZ based_VBN on_IN an_DT analysis_NN by_IN Utgoff_NNP ,_, where_WRB he_PRP shows_VBZ the_DT asymptotic_JJ time_NN complexity_NN to_TO be_VB O_NN -LRB-_-LRB- n_NN -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =_JJ Utgoff_NNP ,_, 1989_CD -_: =--RRB-_NN ._.
With_IN numeric_JJ data_NNS ,_, sorting_NN adds_VBZ a_DT log_NN n_NN term_NN at_IN each_DT node_NN ._.
However_RB ,_, C4_NN .5_NN has_VBZ been_VBN observed_VBN to_TO be_VB worse_JJR than_IN O_NN -LRB-_-LRB- n_NN 2_CD -RRB-_-RRB- -LRB-_-LRB- Catlett_NNP ,_, 1991a_CD -RRB-_-RRB- ._.
One_CD explanation_NN for_IN the_DT discrepancy_NN is_VBZ that_IN Utgoff_NNP actually_RB sho_VBP
rithm_NN on_IN subsets_NNS of_IN size_NN 2_CD We_PRP follow_VBP the_DT reasoning_NN of_IN Korf_NNP ,_, who_WP shows_VBZ that_IN progressive_JJ deepening_VBG is_VBZ an_DT optimal_JJ schedule_NN for_IN conducting_VBG depth-first_JJ search_NN when_WRB the_DT smallest_JJS sufficient_JJ depth_NN is_VBZ unknown_JJ =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 15_CD -RRB-_-RRB- ._.
a_DT i_FW \_FW Delta_NN n_NN 0_CD for_IN i_LS =_JJ 0_CD ;_: 1_CD ;_: :_: :_: :_: ;_: b_NN ,_, where_WRB b_NN +_CC 1_CD is_VBZ the_DT number_NN of_IN samples_NNS processed_VBN before_IN detecting_VBG convergence_NN ._.
Now_RB ,_, we_PRP assume_VBP convergence_NN is_VBZ well_RB detected_VBN ,_, so_IN a_DT b_NN \_NNP Gamma1_NNP \_NNP Delta_NNP n_NN 0_CD !_.
nmin_NN
urve_NN ._.
For_IN example_NN ,_, on_IN noisy_JJ data_NNS ,_, windowing_VBG learning_NN curves_NNS are_VBP notoriously_RB ill_RB behaved_VBN :_: subsequent_JJ samples_NNS contain_VBP increasing_VBG amounts_NNS of_IN noise_NN ,_, and_CC performance_NN often_RB decreases_VBZ as_IN sampling_NN progresses_VBZ =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
It_PRP would_MD be_VB interesting_JJ to_TO examine_VB more_RBR closely_RB the_DT use_NN of_IN the_DT techniques_NNS outlined_VBN above_RB in_IN the_DT context_NN of_IN active_JJ sampling_NN ,_, and_CC the_DT potential_JJ synergies_NNS ._.
8_CD Conclusion_NN With_IN this_DT work_NN we_PRP have_VBP made_VBN subs_NNS
ves_NNS typically_RB have_VBP a_DT steeply_RB sloping_VBG portion_NN early_RB in_IN the_DT curve_NN ,_, a_DT more_RBR gently_RB sloping_VBG middle_JJ portion_NN ,_, and_CC a_DT plateau_NN late_RB in_IN the_DT curve_NN ._.
The_DT middle_JJ portion_NN can_MD be_VB extremely_RB large_JJ in_IN some_DT curves_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 2_CD ,_, 3_CD ,_, 6_CD -RRB-_-RRB- -_: =--RRB-_NN and_CC almost_RB entirely_RB missing_VBG in_IN others_NNS ._.
The_DT plateau_NN occurs_VBZ when_WRB adding_VBG additional_JJ data_NNS instances_NNS does_VBZ not_RB improve_VB accuracy_NN ._.
The_DT plateau_NN ,_, and_CC even_RB the_DT entire_JJ middle_JJ portion_NN ,_, can_MD be_VB missing_VBG from_IN curves_NNS
._.
For_IN each_DT value_NN of_IN n_NN ,_, from_IN 1_CD to_TO N_NN ,_, a_DT model_NN can_MD either_RB be_VB built_VBN or_CC not_RB ,_, leading_VBG to_TO 2_CD N_NN possible_JJ schedules_NNS ._.
However_RB ,_, identification_NN of_IN the_DT optimal_JJ schedule_NN can_MD be_VB cast_VBN in_IN terms_NNS of_IN dynamic_JJ programming_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, yielding_VBG an_DT algorithm_NN that_WDT requires_VBZ O_NN -LRB-_-LRB- N_NN 2_CD -RRB-_-RRB- space_NN and_CC O_NN -LRB-_-LRB- N_NN 3_CD -RRB-_-RRB- time_NN ._.
Let_VB f_LS -LRB-_-LRB- n_NN -RRB-_-RRB- be_VB the_DT cost_NN of_IN building_VBG a_DT model_NN with_IN n_NN instances_NNS and_CC determining_VBG whether_IN accuracy_NN has_VBZ converged_VBN ._.
As_IN described_VBN above_IN ,_, let_VB
use_NN for_IN comparison_NN the_DT schedule_NN composed_VBN of_IN a_DT single_JJ data_NN set_VBN with_IN all_DT instances_NNS ,_, SN_NN =_JJ fNg_NN ._.
We_PRP also_RB will_MD consider_VB the_DT simple_JJ schedule_NN generated_VBN by_IN an_DT omniscient_JJ oracle_NN ,_, SO_NN =_JJ fnmin_FW g._FW John_NNP and_CC Langley_NNP =_SYM -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: define_VB a_DT progressive_JJ sampling_NN approach_NN we_PRP call_VBP arithmetic_NN sampling_NN using_VBG the_DT schedule_NN S_NN a_DT =_JJ n_NN 0_CD +_CC -LRB-_-LRB- i_FW \_FW Delta_NN n_NN ffi_NN -RRB-_-RRB- =_JJ fn_NN 0_CD ;_: n_NN 0_CD +_CC n_NN ffi_NN ;_: n_NN 0_CD +_CC 2n_FW ffi_FW ;_: :_: :_: :_: ;_: n_NN 0_CD +_CC k_NN \_CD Delta_NNP n_NN ffi_NN g._NN 1_CD An_DT example_NN
on_IN small_JJ samples_NNS ._.
However_RB ,_, empirical_JJ studies_NNS of_IN the_DT application_NN of_IN standard_JJ induction_NN algorithms_NNS to_TO large_JJ data_NNS sets_NNS --_: those_DT of_IN relevance_NN to_TO this_DT paper_NN --_: have_VBP shown_VBN learning_VBG curves_NNS to_TO be_VB well_RB behaved_VBD =_JJ -_: =[_NN 3_CD ,_, 4_CD ,_, 6_CD ,_, 12_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN ,_, practical_JJ progressive_JJ sampling_NN demands_NNS only_RB that_IN learning_VBG curves_NNS are_VBP well_RB behaved_VBN at_IN the_DT level_NN Compute_VB schedule_NN S_NN =_JJ fn_NN 0_CD ;_: n_NN 1_CD ;_: n_NN 2_CD ;_: :_: :_: :_: ;_: n_NN k_NN g_NN of_IN sample_NN sizes_NNS n_NN \/_: n_NN 0_CD M_NN \/_: model_NN in_IN
ves_NNS typically_RB have_VBP a_DT steeply_RB sloping_VBG portion_NN early_RB in_IN the_DT curve_NN ,_, a_DT more_RBR gently_RB sloping_VBG middle_JJ portion_NN ,_, and_CC a_DT plateau_NN late_RB in_IN the_DT curve_NN ._.
The_DT middle_JJ portion_NN can_MD be_VB extremely_RB large_JJ in_IN some_DT curves_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 2_CD ,_, 3_CD ,_, 6_CD -RRB-_-RRB- -_: =--RRB-_NN and_CC almost_RB entirely_RB missing_VBG in_IN others_NNS ._.
The_DT plateau_NN occurs_VBZ when_WRB adding_VBG additional_JJ data_NNS instances_NNS does_VBZ not_RB improve_VB accuracy_NN ._.
The_DT plateau_NN ,_, and_CC even_RB the_DT entire_JJ middle_JJ portion_NN ,_, can_MD be_VB missing_VBG from_IN curves_NNS
ves_NNS typically_RB have_VBP a_DT steeply_RB sloping_VBG portion_NN early_RB in_IN the_DT curve_NN ,_, a_DT more_RBR gently_RB sloping_VBG middle_JJ portion_NN ,_, and_CC a_DT plateau_NN late_RB in_IN the_DT curve_NN ._.
The_DT middle_JJ portion_NN can_MD be_VB extremely_RB large_JJ in_IN some_DT curves_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 2_CD ,_, 3_CD ,_, 6_CD -RRB-_-RRB- -_: =--RRB-_NN and_CC almost_RB entirely_RB missing_VBG in_IN others_NNS ._.
The_DT plateau_NN occurs_VBZ when_WRB adding_VBG additional_JJ data_NNS instances_NNS does_VBZ not_RB improve_VB accuracy_NN ._.
The_DT plateau_NN ,_, and_CC even_RB the_DT entire_JJ middle_JJ portion_NN ,_, can_MD be_VB missing_VBG from_IN curves_NNS
n_NN ,_, and_CC low_JJ values_NNS for_IN very_RB large_JJ n._NN For_IN example_NN ,_, data_NNS from_IN Oates_NNP and_CC Jensen_NNP -LRB-_-LRB- 12_CD ,_, 13_CD -RRB-_-RRB- show_VBP that_IN the_DT distribution_NN of_IN the_DT number_NN of_IN instances_NNS needed_VBN for_IN convergence_NN over_IN a_DT large_JJ set_NN of_IN the_DT UCI_NNP databases_NNS =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ roughly_RB log-normal_JJ ._.
Given_VBN such_JJ expectations_NNS about_IN nmin_NN ,_, is_VBZ it_PRP possible_JJ to_TO construct_VB the_DT schedule_NN with_IN the_DT minimum_NN expected_VBD cost_NN of_IN convergence_NN ?_.
This_DT seems_VBZ a_DT daunting_JJ task_NN ._.
For_IN each_DT value_NN of_IN n_NN ,_, fr_NN
._.
Locality_NN is_VBZ defined_VBN within_IN a_DT particular_JJ progressive_JJ sampling_NN procedure_NN ._.
Not_RB all_DT learning_VBG curves_NNS are_VBP well_RB behaved_VBN ._.
For_IN example_NN ,_, theoretical_JJ analyses_NNS of_IN learning_VBG curves_NNS based_VBN on_IN statistical_JJ mechanics_NNS =_JJ -_: =[_NN 7_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP shown_VBN that_IN sudden_JJ increases_NNS in_IN accuracy_NN are_VBP possible_JJ ,_, particularly_RB on_IN small_JJ samples_NNS ._.
However_RB ,_, empirical_JJ studies_NNS of_IN the_DT application_NN of_IN standard_JJ induction_NN algorithms_NNS to_TO large_JJ data_NNS sets_NNS --_: those_DT of_IN
can_MD overshoot_VB nmin_NN greatly_RB ,_, they_PRP then_RB calculate_VBP n_NN ffi_NN for_IN an_DT efficient_JJ arithmetic_NN schedule_NN ,_, and_CC revise_VB the_DT estimate_NN after_IN executing_VBG each_DT schedule_NN point_NN ._.
Other_JJ sequential_JJ multi-sample_JJ learning_NN methods_NNS =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP degenerate_JJ instances_NNS of_IN progressive_JJ sampling_NN ,_, typically_RB using_VBG fixed_JJ arithmetic_NN schedules_NNS and_CC treating_VBG convergence_NN detection_NN simplistically_RB ,_, if_IN at_IN all_DT ._.
For_IN this_DT paper_NN ,_, we_PRP have_VBP considered_VBN only_JJ draw_NN
