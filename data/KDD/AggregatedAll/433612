Time_NN and_CC sample_NN efficient_JJ discovery_NN of_IN Markov_NNP blankets_NNS and_CC direct_JJ causal_JJ relations_NNS
Data_NNP Mining_NNP with_IN Bayesian_NNP Network_NNP learning_NN has_VBZ two_CD important_JJ characteristics_NNS :_: under_IN conditions_NNS learned_VBD edges_NNS between_IN variables_NNS correspond_VBP to_TO casual_JJ influences_NNS ,_, and_CC second_JJ ,_, for_IN every_DT variable_JJ T_NN in_IN the_DT network_NN a_DT special_JJ subset_NN -LRB-_-LRB- Markov_NNP Blanket_NNP -RRB-_-RRB- identifiable_JJ by_IN the_DT network_NN is_VBZ the_DT minimal_JJ variable_JJ set_NN required_VBN to_TO predict_VB T._NNP However_RB ,_, all_DT known_JJ algorithms_NNS learning_VBG a_DT complete_JJ BN_NN do_VBP not_RB scale_VB up_RP beyond_IN a_DT few_JJ hundred_CD variables_NNS ._.
On_IN the_DT other_JJ hand_NN ,_, all_DT known_JJ sound_JJ algorithms_NNS learning_VBG a_DT local_JJ region_NN of_IN the_DT network_NN require_VBP an_DT exponential_JJ number_NN of_IN training_NN instances_NNS to_TO the_DT size_NN of_IN the_DT learned_VBN region_NN ._.
The_DT contribution_NN of_IN this_DT paper_NN is_VBZ two-fold_JJ ._.
We_PRP introduce_VBP a_DT novel_JJ local_JJ algorithm_NN that_WDT returns_VBZ all_DT variables_NNS with_IN direct_JJ edges_NNS to_TO and_CC from_IN a_DT target_NN variable_JJ T_NN as_RB well_RB as_IN a_DT local_JJ algorithm_NN that_WDT returns_VBZ the_DT Markov_NNP Blanket_NNP of_IN T._NNP Both_DT algorithms_NNS -LRB-_-LRB- i_LS -RRB-_-RRB- are_VBP sound_JJ ,_, -LRB-_-LRB- ii_LS -RRB-_-RRB- can_MD be_VB run_VBN efficiently_RB in_IN datasets_NNS with_IN thousands_NNS of_IN variables_NNS ,_, and_CC -LRB-_-LRB- iii_LS -RRB-_-RRB- significantly_RB outperform_JJ in_IN terms_NNS of_IN approximating_VBG the_DT true_JJ neighborhood_NN previous_JJ state-of-the-art_JJ algorithms_NNS using_VBG only_RB a_DT fraction_NN of_IN the_DT training_NN size_NN required_VBN by_IN the_DT existing_VBG methods_NNS ._.
A_DT fundamental_JJ difference_NN between_IN our_PRP$ approach_NN and_CC existing_VBG ones_NNS is_VBZ that_IN the_DT required_JJ sample_NN depends_VBZ on_IN the_DT generating_VBG graph_NN connectivity_NN and_CC not_RB the_DT size_NN of_IN the_DT local_JJ region_NN ;_: this_DT yields_VBZ up_RP to_TO exponential_JJ savings_NNS in_IN sample_NN relative_JJ to_TO previously_RB known_VBN algorithms_NNS ._.
The_DT results_NNS presented_VBN here_RB are_VBP promising_VBG not_RB only_RB for_IN discovery_NN of_IN local_JJ causal_JJ structure_NN ,_, and_CC variable_JJ selection_NN for_IN classification_NN ,_, but_CC also_RB for_IN the_DT induction_NN of_IN complete_JJ BNs_NNS ._.
on_IN ._.
Other_JJ algorithms_NNS include_VBP the_DT GS_NN algorithm_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, originally_RB developed_VBN for_IN learning_NN of_IN the_DT structure_NN of_IN a_DT Bayesian_JJ network_NN of_IN a_DT domain_NN ,_, and_CC extensions_NNS to_TO it_PRP -LRB-_-LRB- 11_CD -RRB-_-RRB- including_VBG the_DT recent_JJ MMMB_NN algorithm_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Meinshausen_NNP and_CC BÃ¼hlmann_NNP -LRB-_-LRB- 13_CD -RRB-_-RRB- recently_RB proposed_VBD an_DT optimal_JJ theoretical_JJ solution_NN to_TO the_DT problem_NN of_IN learning_VBG the_DT neighborhood_NN of_IN a_DT Markov_NNP network_NN when_WRB the_DT distribution_NN of_IN the_DT domain_NN can_MD be_VB assumed_VBN to_TO
ents_NNS for_IN thousands_NNS of_IN genes_NNS -LRB-_-LRB- Hughes_NNP et_FW al._FW ,_, 2000_CD ;_: Spellman_NNP et_FW al._FW ,_, 1998_CD -RRB-_-RRB- ,_, but_CC the_DT existing_VBG algorithms_NNS for_IN learning_VBG BNs_NNS from_IN data_NNS do_VBP not_RB scale_VB to_TO such_JJ highdimensional_JJ databases_NNS -LRB-_-LRB- Friedman_NNP et_FW al._FW ,_, 1999_CD ;_: =_JJ -_: =_JJ Tsamardinos_NNPS et_FW al._FW ,_, 2003_CD -_: =--RRB-_NN ._.
This_DT implies_VBZ that_IN in_IN the_DT references_NNS cited_VBD above_IN ,_, for_IN instance_NN ,_, the_DT authors_NNS have_VBP to_TO decide_VB in_IN advance_NN which_WDT genes_NNS are_VBP included_VBN in_IN the_DT learning_NN process_NN -LRB-_-LRB- in_IN all_PDT the_DT cases_NNS -LRB-_-LRB- 1000_CD -RRB-_-RRB- and_CC which_WDT genes_NNS are_VBP ex_FW
he_PRP pseudocode_VBD for_IN the_DT CD-B_NN algorithm_NN is_VBZ provided_VBN in_IN Appendix_NNP A._NNP 1_CD ._.
2.2_CD CD-H_NN algorithm_NN The_DT CD-H_NN algorithm_NN replaces_VBZ the_DT initial_JJ steps_NNS of_IN the_DT CD-B_NN algorithm_NN for_IN finding_VBG the_DT PC_NN -LRB-_-LRB- X_NN -RRB-_-RRB- with_IN the_DT MMPC_NN algorithm_NN -LRB-_-LRB- =_JJ -_: =_JJ Tsamardinos_NNPS et_FW al._FW ,_, 2003_CD -_: =_JJ -_: ,_, 2006_CD -RRB-_-RRB- ._.
The_DT MMPC_NNP uses_VBZ a_DT two-phase_JJ search_NN procedure_NN based_VBN on_IN tests_NNS of_IN independence\/dependence_NN ._.
In_IN the_DT first_JJ phase_NN of_IN search_NN a_DT candidate_NN set_NN of_IN parents_NNS and_CC children_NNS called_VBN CPC_NNP is_VBZ estimated_VBN which_WDT is_VBZ a_DT s_NN
l_NN generalization_NN ._.
A_DT principle_JJ solution_NN to_TO the_DT feature_NN reduction_NN problem_NN is_VBZ to_TO determine_VB a_DT subset_NN of_IN features_NNS that_WDT can_MD render_VB of_IN the_DT rest_NN of_IN whole_JJ features_NNS independent_JJ of_IN the_DT variable_NN of_IN interest_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- ._.
From_IN a_DT theoretical_JJ perspective_NN ,_, it_PRP is_VBZ known_VBN that_IN optimal_JJ feature_NN reduction_NN for_IN supervised_JJ learning_NN problems_NNS requires_VBZ an_DT exhaustive_JJ search_NN of_IN all_DT possible_JJ subsets_NNS of_IN features_NNS of_IN the_DT chosen_JJ cardi_NN
ed_VBN in_IN 2003_CD ,_, and_CC the_DT most_JJS difference_NN from_IN GROUP_NNP I_PRP is_VBZ that_IN topology_NN information_NN is_VBZ considered_VBN by_IN algorithms_NNS of_IN GROUP_NNP II_NNP ._.
A._NN MMPC\/MB_NN and_CC HITON-PC\/MB_NN MMPC\/MB_NN ,_, in_IN fact_NN ,_, are_VBP two_CD algorithms_NNS proposed_VBN in_IN 2003_CD =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, with_IN MMPC_NN -LRB-_-LRB- Max-Min_NNP Parents_NNP and_CC Children_NNP -RRB-_-RRB- for_IN the_DT induction_NN of_IN parents_NNS and_CC children_NNS of_IN ,_, and_CC MMMB_NNP -LRB-_-LRB- Max-Min_NNP Markov_NNP Blanket_NNP -RRB-_-RRB- for_IN the_DT induction_NN of_IN Markov_NNP blanket_NN of_IN ._.
For_IN the_DT first_JJ time_NN ,_, given_VBN faithfulne_NN
et_FW al._FW ,_, 2000_CD -RRB-_-RRB- ,_, and_CC algorithms_NNS that_WDT learn_VBP the_DT Markov_NNP blanket_NN as_IN a_DT step_NN in_IN learning_VBG the_DT Bayesian_JJ network_NN structure_NN such_JJ as_IN Grow-Shrink_NN -LRB-_-LRB- GS_NN -RRB-_-RRB- algorithm_NN -LRB-_-LRB- Margaritis_NNP &_CC Thrun_NNP ,_, 2000_CD -RRB-_-RRB- ,_, IAMB_NN and_CC its_PRP$ variants_NNS -LRB-_-LRB- =_JJ -_: =_JJ Tsamardinos_NNPS ,_, Aliferis_NNP ,_, &_CC Statnikov_NNP ,_, 2003_CD -_: =_SYM -_: a_LS -RRB-_-RRB- ,_, HITON-PC_NN and_CC HITON-MB_NN -LRB-_-LRB- Aliferis_NNP ,_, Tsamardinos_NNP ,_, &_CC Statnikov_NNP ,_, 2003_CD -RRB-_-RRB- ,_, MMPC_NNP and_CC MMMB_NNP -LRB-_-LRB- Tsamardinos_NNP ,_, Aliferis_NNP ,_, &_CC Statnikov_NNP ,_, 2003b_CD -RRB-_-RRB- ,_, and_CC max-min_JJ hill_NN climbing_NN -LRB-_-LRB- MMHC_NN -RRB-_-RRB- -LRB-_-LRB- Tsamardinos_NNP ,_, Brown_NNP ,_, &_CC Aliferis_NNP ,_, 2006_CD -RRB-_-RRB-
Tsamardinos_NNP et_FW al._FW discuss_VBP the_DT possibility_NN of_IN interpreting_VBG the_DT MB_NN as_IN direct_JJ causal_JJ relationships_NNS but_CC the_DT algorithm_NN in_IN that_DT paper_NN does_VBZ not_RB specifically_RB distinguish_VB between_IN causes_NNS and_CC effects_NNS of_IN a_DT node_NN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
To_TO the_DT best_JJS of_IN our_PRP$ knowledge_NN there_EX is_VBZ no_DT Bayesian_JJ local_JJ causal_JJ discovery_NN algorithm_NN described_VBN in_IN the_DT literature_NN ._.
Experimental_JJ Methods_NNS In_IN this_DT study_NN we_PRP apply_VBP BLCD_NN to_TO the_DT Infant_NNP Birth_NNP and_CC Death_NNP dataset_NN
is_VBZ to_TO be_VB learned_VBN from_IN data_NNS ,_, then_RB knowing_VBG the_DT relevant_JJ features_NNS reduces_VBZ the_DT dimension_NN of_IN the_DT search_NN space_NN ._.
In_IN this_DT paper_NN ,_, we_PRP are_VBP interested_JJ in_IN solving_VBG the_DT FSS_NN problem_NN following_VBG the_DT approach_NN proposed_VBN in_IN =_JJ -_: =[_NN 9_CD ,_, 10_CD ,_, 11_CD -RRB-_-RRB- -_: =_JJ -_: :_: Since_IN the_DT Markov_NNP boundary_NN of_IN C_NN ,_, MB_NN -LRB-_-LRB- C_NN -RRB-_-RRB- ,_, is_VBZ defined_VBN as_IN any_DT minimal_JJ subset_NN of_IN F_NN such_JJ that_IN C_NN is_VBZ conditionally_RB independent_JJ of_IN the_DT rest_NN of_IN F_NN given_VBN MB_NN -LRB-_-LRB- C_NN -RRB-_-RRB- ,_, then_RB MB_NN -LRB-_-LRB- C_NN -RRB-_-RRB- is_VBZ a_DT solution_NN to_TO the_DT FSS_NN problem_NN ._.
Under_IN
tem_RB ,_, namely_RB ,_, several_JJ simple_JJ gene_NN expression_NN normalization_NN methods_NNS ,_, area_NN under_IN ROC_NN curve_NN performance_NN metric_NN -LRB-_-LRB- for_IN binary_JJ diagnostic_JJ problems_NNS -RRB-_-RRB- ,_, and_CC two_CD state_NN of_IN the_DT art_NN local_JJ causal_JJ discovery_NN algorithms_NNS =_JJ -_: =[_NN 17,18_CD -RRB-_-RRB- -_: =_SYM -_: shown_VBN with_IN boldface_NN in_IN Table_NNP 4_CD ._.
To_TO guide_VB the_DT user_NN 's_POS choices_NNS according_VBG to_TO the_DT available_JJ computational_JJ power_NN and_CC time_NN ,_, the_DT system_NN outputs_VBZ the_DT number_NN of_IN models_NNS to_TO be_VB generated_VBN while_IN the_DT user_NN is_VBZ selectin_NN
y_NN is_VBZ a_DT Markov_NNP blanket_NN ._.
The_DT trivial_JJ Markov_NNP blanket_NN is_VBZ of_IN course_NN X_NN itself_PRP ._.
Unfortunately_RB ,_, this_DT terminology_NN is_VBZ not_RB standardized_JJ :_: some_DT authors_NNS instead_RB refer_VBP to_TO the_DT Markov_NNP boundary_NN as_IN ''_'' the_DT Markov_NNP blanket_NN ''_'' =_SYM -_: =[_NN 175_CD -RRB-_-RRB- -_: =_JJ -_: ,_, so_IN care_NN must_MD be_VB taken_VBN when_WRB consulting_VBG the_DT literature_NN ._.
Guyon_NNP et_FW al._FW -LRB-_-LRB- 69_CD -RRB-_-RRB- also_RB refer_VBP to_TO the_DT Markov_NNP boundary_NN as_IN a_DT ''_'' surely_RB sufficient_JJ feature_NN subset_NN ''_'' ._.
An_DT important_JJ fact_NN for_IN devising_VBG feature_NN selection_NN al_FW
samples_NNS as_IN negative_JJ ._.
Thus_RB ,_, a_DT combined_JJ measure_NN is_VBZ required_VBN ._.
The_DT measure_NN we_PRP used_VBD is_VBZ the_DT proximity_NN of_IN the_DT sensitivity_NN and_CC specificity_NN of_IN the_DT algorithm_NN to_TO perfect_JJ sensitivity_NN and_CC specificity_NN expressed_VBN as_IN =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_JJ -_: :_: Note_VB ,_, that_IN we_PRP can_MD not_RB use_VB AUCs_NNS or_CC fix_VB the_DT measures_NNS ._.
First_RB ,_, AUC_NN 's_POS can_MD not_RB be_VB generated_VBN for_IN the_DT CQF_NNP filters_NNS because_IN the_DT documents_NNS are_VBP not_RB ranked_VBN ._.
Either_CC the_DT query_NN is_VBZ satisfied_JJ or_CC not_RB ._.
Second_RB ,_, fixing_VBG sen_NN
above_IN property_NN ,_, the_DT Markov_NNP Blanket_NNP is_VBZ inextricably_RB connected_VBN to_TO the_DT variable_JJ selection_NN problem_NN ,_, i.e._FW ,_, the_DT problem_NN of_IN choosing_VBG a_DT minimum_JJ set_NN of_IN predictors_NNS that_WDT optimally_RB classify_VBP T_NN ._.
In_IN particular_JJ ,_, in_IN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: we_PRP prove_VBP that_IN under_IN certain_JJ broad_JJ conditions_NNS the_DT Markov_NNP Blanket_NNP is_VBZ the_DT solution_NN to_TO the_DT variable_JJ selection_NN problem_NN ._.
Several_JJ algorithms_NNS are_VBP currently_RB available_JJ that_WDT can_MD induce_VB the_DT BN_NN that_WDT captures_VBZ the_DT
ests_NNS and_CC considering_VBG the_DT d-separation_NN relations_NNS they_PRP entail_VBP ,_, one_PRP can_MD reconstruct_VB the_DT BN_NN that_WDT captures_VBZ the_DT data_NNS generating_VBG process_NN ._.
This_DT is_VBZ the_DT main_JJ idea_NN behind_IN constraint-based_JJ BN_NN learning_NN approaches_VBZ =_JJ -_: =[_NN 8_CD ,_, 3_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT following_VBG theorem_NN in_IN -LRB-_-LRB- 8_CD -RRB-_-RRB- is_VBZ foundational_JJ for_IN the_DT both_CC the_DT PC_NN and_CC the_DT MMPC_NNP algorithms_NNS of_IN Section_NN 3_CD :_: Theorem_NN 2_CD ._.
If_IN a_DT BN_NN N_NN is_VBZ faithful_JJ to_TO a_DT joint_JJ probability_NN distribution_NN J_NN then_RB :_: 1_CD ._.
There_EX is_VBZ an_DT edg_NN
iously_RB known_VBN algorithms_NNS for_IN inducing_VBG Markov_NNP Blankets_NNPS ,_, namely_RB the_DT Incremental_JJ Association_NN Markov_NN Blanket_NN -LRB-_-LRB- IAMB_NN -RRB-_-RRB- algorithm_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, the_DT Grow-Shrink_NN -LRB-_-LRB- GS_NN -RRB-_-RRB- algorithm_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, and_CC the_DT Koller-Sahami_NNP algorithm_NN -LRB-_-LRB- KS_NN -RRB-_-RRB- =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
MMMB_NN trades-off_NN computation_NN time_NN for_IN required_VBN training_NN sample_NN size_NN ,_, while_IN still_RB being_VBG efficient_JJ enough_RB to_TO scale_VB up_RP to_TO thousands_NNS of_IN variables_NNS ._.
MMMB_NN yields_NNS up_IN to_TO exponential_JJ savings_NNS in_IN sample_NN relative_NN
smaller_JJR real_JJ BNs_NNS in_IN a_DT way_NN that_WDT retains_VBZ their_PRP$ structural_JJ and_CC probabilistic_JJ properties_NNS ,_, hoping_VBG that_IN the_DT simulated_JJ network_NN will_MD exhibit_VB the_DT same_JJ characteristics_NNS as_IN the_DT real_JJ BN_NN tiles_NNS -LRB-_-LRB- the_DT details_NNS are_VBP in_IN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
More_RBR specifically_RB ,_, we_PRP randomly_RB generated_VBD ALARM-5K_NN by_IN tiling_VBG 135_CD copies_NNS of_IN ALARM_NN ._.
Similarly_RB ,_, for_IN Pigs-5K_NN -LRB-_-LRB- 11_CD copies_NNS of_IN original_JJ Pigs_NNS -RRB-_-RRB- and_CC Hailfinder5K_NN -LRB-_-LRB- 89_CD copies_NNS of_IN original_JJ Hailfinder_NN -RRB-_-RRB- ._.
We_PRP did_VBD not_RB
ales_NNS up_RB to_TO datasets_NNS with_IN thousands_NNS of_IN variables_NNS ._.
MMMB_NN is_VBZ compared_VBN with_IN all_DT previously_RB known_VBN algorithms_NNS for_IN inducing_VBG Markov_NNP Blankets_NNPS ,_, namely_RB the_DT Incremental_JJ Association_NN Markov_NN Blanket_NN -LRB-_-LRB- IAMB_NN -RRB-_-RRB- algorithm_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT Grow-Shrink_NN -LRB-_-LRB- GS_NN -RRB-_-RRB- algorithm_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, and_CC the_DT Koller-Sahami_NNP algorithm_NN -LRB-_-LRB- KS_NN -RRB-_-RRB- -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
MMMB_NN trades-off_NN computation_NN time_NN for_IN required_VBN training_NN sample_NN size_NN ,_, while_IN still_RB being_VBG efficient_JJ enough_RB to_TO scale_VB up_RP to_TO th_DT
