Proximal_JJ support_NN vector_NN machine_NN classifiers_NNS
Instead_RB of_IN a_DT standard_JJ support_NN vector_NN machine_NN -LRB-_-LRB- SVM_NN -RRB-_-RRB- that_WDT classifies_VBZ points_NNS by_IN assigning_VBG them_PRP to_TO one_CD of_IN two_CD disjoint_NN half-spaces_NNS ,_, points_NNS are_VBP classified_VBN by_IN assigning_VBG them_PRP to_TO the_DT closest_JJS of_IN two_CD parallel_JJ planes_NNS -LRB-_-LRB- in_IN input_NN or_CC feature_NN space_NN -RRB-_-RRB- that_WDT are_VBP pushed_VBN apart_RB as_RB far_RB as_IN possible_JJ ._.
This_DT formulation_NN ,_, which_WDT can_MD also_RB be_VB interpreted_VBN as_IN regularized_VBN least_JJS squares_NNS and_CC considered_VBN in_IN the_DT much_RB more_RBR general_JJ context_NN of_IN regularized_VBN networks_NNS -LRB-_-LRB- 8_CD ,_, 9_CD -RRB-_-RRB- ,_, leads_VBZ to_TO an_DT extremely_RB fast_JJ and_CC simple_JJ algorithm_NN for_IN generating_VBG a_DT linear_JJ or_CC nonlinear_JJ classifier_NN that_WDT merely_RB requires_VBZ the_DT solution_NN of_IN a_DT single_JJ system_NN of_IN linear_JJ equations_NNS ._.
In_IN contrast_NN ,_, standard_JJ SVMs_NNS solve_VBP a_DT quadratic_JJ or_CC a_DT linear_JJ program_NN that_WDT require_VBP considerably_RB longer_JJR computational_JJ time_NN ._.
Computational_JJ results_NNS on_IN publicly_RB available_JJ datasets_NNS indicate_VBP that_IN the_DT proposed_JJ proximal_JJ SVM_NN classifier_NN has_VBZ comparable_JJ test_NN set_VBD correctness_NN to_TO that_DT of_IN standard_JJ SVM_NN classifiers_NNS ,_, but_CC with_IN considerably_RB faster_JJR computational_JJ time_NN that_WDT can_MD be_VB an_DT order_NN of_IN magnitude_NN faster_RBR ._.
The_DT linear_JJ proximal_JJ SVM_NN can_MD easily_RB handle_VB large_JJ datasets_NNS as_IN indicated_VBN by_IN the_DT classification_NN of_IN a_DT 2_CD million_CD point_NN 10-attribute_JJ set_NN in_IN 20.8_CD seconds_NNS ._.
All_DT computational_JJ results_NNS are_VBP based_VBN on_IN 6_CD lines_NNS of_IN MATLAB_NNP code_NN ._.
olleagues_NNS proposed_VBD several_JJ variations_NNS of_IN standard_JJ support_NN vector_NN machines_NNS by_IN modifying_VBG the_DT objective_JJ function_NN ,_, together_RB with_IN several_JJ very_RB efficient_JJ training_NN algorithms_NNS -LRB-_-LRB- -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 20_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 14_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 12_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Since_IN the_DT optimization_NN problems_NNS given_VBN by_IN these_DT modifications_NNS are_VBP different_JJ from_IN that_DT of_IN standard_JJ support_NN vector_NN machines_NNS ,_, we_PRP will_MD not_RB address_VB them_PRP in_IN this_DT paper_NN ._.
In_IN this_DT paper_NN ,_, we_PRP pr_VBP
last_RB two_CD test_NN problems_NNS are_VBP described_VBN in_IN -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
We_PRP first_JJ focus_NN on_IN the_DT test_NN problems_NNS 1_CD --_: 7_CD ,_, for_IN which_WDT we_PRP have_VBP performed_VBN a_DT ten_CD --_: fold_JJ cross_NN --_: validation_NN ._.
In_IN table_NN 2_CD we_PRP compare_VBP our_PRP$ results_NNS with_IN those_DT reported_VBN in_IN =_JJ -_: =[_NN 4_CD ,_, 5_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN particular_JJ ,_, the_DT classification_NN error_NN is_VBZ averaged_VBN over_IN the_DT results_NNS of_IN the_DT ten_CD --_: fold_JJ cross_NN --_: validation_NN ._.
The_DT reported_VBN results_NNS of_IN our_PRP$ algorithm_NN are_VBP the_DT best_JJS ones_NNS for_IN the_DT different_JJ tested_VBN values_NNS of_IN the_DT
olleagues_NNS proposed_VBD several_JJ variations_NNS of_IN standard_JJ support_NN vector_NN machines_NNS by_IN modifying_VBG the_DT objective_JJ function_NN ,_, together_RB with_IN several_JJ very_RB efficient_JJ training_NN algorithms_NNS -LRB-_-LRB- -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 20_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 14_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 12_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Since_IN the_DT optimization_NN problems_NNS given_VBN by_IN these_DT modifications_NNS are_VBP different_JJ from_IN that_DT of_IN standard_JJ support_NN vector_NN machines_NNS ,_, we_PRP will_MD not_RB address_VB them_PRP in_IN this_DT paper_NN ._.
In_IN this_DT paper_NN ,_, we_PRP pr_VBP
omposition_NN methods_NNS to_TO the_DT extreme_NN :_: the_DT working_VBG set_NN is_VBZ only_RB composed_VBN of_IN two_CD observations_NNS for_IN which_WDT the_DT optimization_NN process_NN is_VBZ performed_VBN analytically_RB at_IN every_DT iteration_NN ._.
Mangasarian_NN and_CC his_PRP$ colleagues_NNS =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 9_CD -RRB-_-RRB- proposed_VBD several_JJ variations_NNS of_IN standard_JJ support_NN vector_NN machines_NNS by_IN modifying_VBG the_DT objective_JJ function_NN ,_, together_RB with_IN several_JJ very_RB efficient_JJ training_NN algorithms_NNS ._.
For_IN more_RBR detailed_JJ survey_NN ,_, see_VB -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
A_DT
sing_VB results_NNS ._.
However_RB ,_, although_IN the_DT SVM_NNP often_RB performs_VBZ superbly_RB ,_, it_PRP is_VBZ not_RB always_RB perfect_JJ ,_, especially_RB when_WRB a_DT single_JJ hyperplane_NN does_VBZ not_RB fit_VB the_DT data_NNS well_RB ._.
To_TO overcome_VB this_DT problem_NN ,_, Mangasarian_JJ et_FW al._FW =_SYM -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT multisurface_JJ version_NN that_WDT uses_VBZ multiple_JJ hyperplanes_NNS to_TO fit_VB the_DT data_NNS with_IN large_JJ margin_NN and_CC small_JJ variance_NN ._.
In_IN addition_NN ,_, ensemble_NN methods_NNS -LRB-_-LRB- such_JJ as_IN bagging_VBG and_CC boosting_VBG -RRB-_-RRB- that_WDT combine_VBP multiple_JJ
RSVM_NN was_VBD introduced_VBN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, which_WDT randomly_RB selected_VBD a_DT subset_NN from_IN the_DT training_NN set_VBN and_CC trained_VBN on_IN this_DT reduced_VBN subset_NN ._.
The_DT proximal_JJ SVM_NN -LRB-_-LRB- PSVM_NN -RRB-_-RRB- was_VBD introduced_VBN as_IN an_DT alternative_NN to_TO quadratic_JJ programming_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, as_IN it_PRP only_RB depends_VBZ on_IN linear_JJ algebraic_JJ operations_NNS ._.
Incremental_JJ training_NN methods_NNS were_VBD proposed_VBN for_IN PSVM_NNP to_TO save_VB space_NN and_CC to_TO unlearn_VB old_JJ data_NNS -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, where_WRB only_RB linear_JJ classifiers_NNS without_IN kernels_NNS were_VBD c_NN
classification_NN via_IN the_DT loss_NN function_NN V_NN -LRB-_-LRB- f_FW -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- -RRB-_-RRB- =_JJ -LRB-_-LRB- f_LS -LRB-_-LRB- x_NN -RRB-_-RRB- −_FW y_FW -RRB-_-RRB- 2_CD ._.
This_DT classification_NN scheme_NN was_VBD used_VBN at_IN least_JJS as_RB early_RB as_IN 1989_CD -LRB-_-LRB- for_IN reviews_NNS see_VBP -LRB-_-LRB- 7_CD ,_, 40_CD -RRB-_-RRB- and_CC then_RB rediscovered_VBD again_RB by_IN many_JJ others_NNS -LRB-_-LRB- se_FW =_JJ -_: =_JJ e_LS -LRB-_-LRB- 21_CD ,_, 49_CD -RRB-_-RRB- -RRB-_-RRB- ,_, inclu_NN -_: =_JJ -_: ding_NN Mangasarian_NN -LRB-_-LRB- who_WP refers_VBZ to_TO square_VB loss_NN regularization_NN as_IN ``_`` proximal_JJ vector_NN machines_NNS ''_'' -RRB-_-RRB- and_CC Suykens_NNP -LRB-_-LRB- who_WP uses_VBZ the_DT name_NN ``_`` least_JJS square_JJ SVMs_NNS ''_'' -RRB-_-RRB- ._.
Rifkin_NNP -LRB-_-LRB- -LRB-_-LRB- 47_CD -RRB-_-RRB- -RRB-_-RRB- has_VBZ confirmed_VBN the_DT interesting_JJ empi_NNS
ining_NN ._.
5.3_CD A_DT fast_JJ unconstrained_JJ quadratic_JJ formulation_NN for_IN semisupervised_JJ Classification_NN :_: PLIAM_NN We_PRP can_MD speed_VB up_RP the_DT 2-norm_JJ formulation_NN as_IN follows_VBZ :_: Setting_VBG p_NN =_JJ 2_CD and_CC following_VBG the_DT same_JJ idea_NN proposed_VBN in_IN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP can_MD slightly_RB modify_VB the_DT inequalities_NNS in_IN formulation_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- and_CC substitute_VB them_PRP by_IN equalities_NNS to_TO obtain_VB :_: min_NN -LRB-_-LRB- w_NN ,_, γ_NN ,_, y_NN ,_, z_SYM -RRB-_-RRB- ‖_FW w_FW ‖_FW 2_CD 2_CD +_CC νy_NN +_CC µz_NN +_CC α_FW ‖_FW ˜_FW LCw_FW ‖_FW 2_CD 2_CD +_CC γ2_FW s.t._FW ∓_NN -LRB-_-LRB- A_NN ∓_FW w_FW −_FW eγ_FW -RRB-_-RRB- +_CC y_NN ∓_NN =_JJ e_SYM ∓_CD R_NN ∓_NN -LRB-_-LRB- Uw_NN −_NN eγ_NN -RRB-_-RRB- +_CC z_SYM ∓_NN
the_DT training_NN set_VBN by_IN cross-validation_NN works_VBZ well_RB ._.
Again_RB we_PRP have_VBP almost_RB the_DT same_JJ results_NNS with_IN linear_NN and_CC d-linear_NN 14sSparse_NN Grid_NNP Combination_NNP Technique_NNP Results_NNS with_IN d-linear_NN linear_NN lin_NN ._.
dim_NN 3_CD refined_JJ SVM_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: Level_NN 10-fold_JJ time_NN -LRB-_-LRB- sec_NN -RRB-_-RRB- time_NN -LRB-_-LRB- sec_NN -RRB-_-RRB- time_NN -LRB-_-LRB- sec_NN -RRB-_-RRB- linear_JJ non-linear_JJ 1_CD train_NN 77.7_CD %_NN 2.2_CD 82.3_CD %_NN 0.1_CD 78.7_CD %_NN 0.2_CD 70.2_CD %_NN 75.8_CD %_NN test_NN 71.8_CD %_NN 72.4_CD %_NN 73.9_CD %_NN 70.0_CD %_NN 73.7_CD %_NN 2_CD train_NN 84.3_CD %_NN 27.0_CD 80.0_CD %_NN 1.0_CD 84.1_CD %_NN 1_CD
ing_NN c_NN -LRB-_-LRB- f_FW ,_, D_NN -RRB-_-RRB- =_JJ ∑_CD m_NN i_LS =_JJ 1_CD -LRB-_-LRB- yi_FW −_FW f_FW -LRB-_-LRB- xi_NN -RRB-_-RRB- -RRB-_-RRB- 2_CD -RRB-_-RRB- ._.
The_DT RLS_NN algorithm_NN with_IN slight_JJ modifications_NNS -LRB-_-LRB- e.g._FW including_VBG the_DT bias_NN term_NN -RRB-_-RRB- is_VBZ also_RB known_VBN as_IN least-squares_JJ support_NN vector_NN machines_NNS -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, proximal_JJ vector_NN machines_NNS =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: ,_, kernel_NN ridge_NN regression_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, and_CC is_VBZ closely_RB related_JJ to_TO many_JJ other_JJ methods_NNS ._.
It_PRP has_VBZ been_VBN shown_VBN that_IN the_DT RLS_NN algorithm_NN have_VBP a_DT classification_NN performance_NN similar_JJ to_TO the_DT regular_JJ SVMs_NNS -LRB-_-LRB- see_VB e.g._FW -LRB-_-LRB- 11_CD ,_, 28_CD -RRB-_-RRB- -RRB-_-RRB-
matrix_NN for_IN p_obs_NNS ,_, at_IN t_NN =_JJ 0_CD p_obs0_NN =_JJ cbind_NN -LRB-_-LRB- as_IN ._.
vector_NN -LRB-_-LRB- matrix_NN -LRB-_-LRB- rep_NN -LRB-_-LRB- s1_NN ,_, length_NN -LRB-_-LRB- s2_NN -RRB-_-RRB- -RRB-_-RRB- ,_, byrow_NN =_JJ TRUE_JJ ,_, nrow_NN =_JJ length_NN -LRB-_-LRB- s2_NN -RRB-_-RRB- -RRB-_-RRB- -RRB-_-RRB- ,_, rep_NN -LRB-_-LRB- s2_NN ,_, length_NN -LRB-_-LRB- s1_NN -RRB-_-RRB- -RRB-_-RRB- -RRB-_-RRB- p_obs_NN =_JJ cbind_NN -LRB-_-LRB- p_obs0_NN ,0_CD -RRB-_-RRB- #observation_NN O_fun_NN =_JJ function_NN -LRB-_-LRB- vec_NN -RRB-_-RRB- -LCB-_-LRB- return_NN -LRB-_-LRB- obs_NN -LRB-_-LRB- vec_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ,_, vec_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =-]_CD -RRB-_-RRB- -RCB-_-RRB- F_fun_NN =_JJ function_NN -LRB-_-LRB- vec_NN -RRB-_-RRB- -LCB-_-LRB- return_NN -LRB-_-LRB- forc_NN -LRB-_-LRB- vec_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ,_, vec_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ,_, vec_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- +2_CD -RRB-_-RRB- -RRB-_-RRB- -RCB-_-RRB- U_fun_NN =_JJ function_NN -LRB-_-LRB- pi_vec_NN ,_, s_vec_NN -RRB-_-RRB- -LCB-_-LRB- r_NN =_JJ sqrt_NN -LRB-_-LRB- sum_NN -LRB-_-LRB- -LRB-_-LRB- -LRB-_-LRB- pi_vec-s_vec_NN -RRB-_-RRB- \*_NN c_NN -LRB-_-LRB- 1\/grdinter_NN ,1_NN \/_: grdinter_NN ,1_CD -RRB-_-RRB- -RRB-_-RRB- ^_NN 2_CD -RRB-_-RRB- -RRB-_-RRB- #r_NN =_JJ |_FW pi-s_FW |_FW return_NN -LRB-_-LRB- r_NN ^_NN 3_CD -RRB-_-RRB- -RCB-_-RRB- K_NN =_JJ matrix_NN -LRB-_-LRB- 0_CD ,_, nrow_NN =_JJ nrow_NN -LRB-_-LRB- p_NN
ds_JJ knowledge_NN sets_NNS ,_, support_NN vector_NN machine_NN classifier_NN ,_, knowledge-based_JJ classifier_NN ,_, linear_JJ programming_NN 1_CD ._.
INTRODUCTION_NN Support_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- have_VBP played_VBN a_DT major_JJ role_NN in_IN classification_NN problems_NNS =_JJ -_: =[_NN 24_CD ,_, 3_CD ,_, 12_CD ,_, 1_CD ,_, 13_CD ,_, 5_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB unlike_IN other_JJ classification_NN tools_NNS such_JJ as_IN knowledge-based_JJ neural_JJ networks_NNS -LRB-_-LRB- 21_CD ,_, 22_CD ,_, 17_CD ,_, 7_CD -RRB-_-RRB- ,_, little_JJ work_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- has_VBZ gone_VBN into_IN incorporating_VBG prior_JJ knowledge_NN into_IN support_NN vector_NN machines_NNS ._.
In_IN this_DT
er_IN generalization_NN ability_NN than_IN a_DT conventional_JJ SVM_NN ._.
This_DT reduced_VBN kernel_NN technique_NN has_VBZ been_VBN successfully_RB applied_VBN to_TO other_JJ kernel-based_JJ learning_NN algorithm_NN ,_, such_JJ as_IN proximal_JJ support_NN vector_NN machine_NN -LRB-_-LRB- PSVM_NN -RRB-_-RRB- =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: and_CC ɛ-smooth_NN support_NN vector_NN regression_NN -LRB-_-LRB- ɛ-SSVR_NN -RRB-_-RRB- -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
In_IN -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, the_DT reduced_VBN set_NN is_VBZ selected_VBN randomly_RB from_IN the_DT entire_JJ dataset_NN with_IN a_DT user_NN pre-specified_JJ reduced_VBN set_NN size_NN ¯_FW m._FW It_PRP is_VBZ typically_RB much_RB smaller_JJR t_NN
al_FW input_NN space_NN into_IN two_CD classes_NNS in_IN less_JJR than_IN 2.5_CD hours_NNS on_IN a_DT 400_CD MHz_NN Pentium_NN II_CD processor_NN ._.
Keywords_NNS incremental_JJ classifier_NN ,_, massive_JJ data_NNS classification_NN ,_, support_NN vector_NN machines_NNS 1_CD Introduction_NN Recently_RB =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: a_DT fast_RB simple_JJ classification_NN algorithm_NN was_VBD proposed_VBN that_WDT classifies_VBZ points_NNS by_IN assigning_VBG them_PRP to_TO the_DT closest_JJS of_IN two_CD parallel_JJ planes_NNS -LRB-_-LRB- in_IN input_NN or_CC feature_NN space_NN -RRB-_-RRB- that_WDT are_VBP pushed_VBN apart_RB as_RB far_RB as_IN possible_JJ
-LRB-_-LRB- 1_LS -RRB-_-RRB- leads_VBZ to_TO the_DT so-called_JJ Regularized_NNP Least-Squares_NNP Classification_NN which_WDT requires_VBZ only_RB the_DT solution_NN of_IN a_DT linear_JJ system_NN ._.
The_DT algorithm_NN has_VBZ been_VBN rediscovered_VBN several_JJ times_NNS and_CC has_VBZ many_JJ different_JJ names_NNS =_JJ -_: =[_NN 11_CD ,_, 10_CD ,_, 4_CD ,_, 15_CD -RRB-_-RRB- ._.
In_IN -_: =_JJ -_: this_DT paper_NN ,_, we_PRP stick_VBP to_TO the_DT term_NN ``_`` RLSC_NNP ''_'' for_IN consistency_NN ._.
It_PRP has_VBZ been_VBN shown_VBN in_IN -LRB-_-LRB- 11_CD ,_, 4_CD -RRB-_-RRB- that_IN RLSC_NN achieves_VBZ comparable_JJ accuracy_NN in_IN binary_JJ classification_NN problems_NNS to_TO the_DT popular_JJ SVMs_NNS ._.
If_IN we_PRP substitut_VBP
omparison_NN of_IN error_NN rates_NNS for_IN entities_NNS not_RB sharing_VBG and_CC sharing_VBG their_PRP$ datasets_NNS using_VBG a_DT 1-norm_JJ nonlinear_JJ Gaussian_JJ SVM_NN ..._: ..._: ..._: ..._: ..._: ._.
79_CD 7.1_CD Linear_JJ kernel_NN GEPSVM_NN ,_, PSVM_NN =_JJ -_: =[_NN 27_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC SVM-Light_NN -LRB-_-LRB- 49_CD -RRB-_-RRB- ten-fold_JJ testing_NN correctness_NN and_CC p-values_NN ._.
The_DT p-values_NNS are_VBP from_IN a_DT t-test_NN comparing_VBG each_DT algorithm_NN to_TO GEPSVM_NNP ._.
Best_NN correctness_NN results_NNS are_VBP in_IN bold_JJ ._.
An_DT asterisk_NN -LRB-_-LRB- \*_NN -RRB-_-RRB- denotes_VBZ signifi_NNS
classification_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- and_CC soft-decay_JJ decremental_JJ learning_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
Proximal_JJ SVMs_NNS has_VBZ also_RB been_VBN shown_VBN to_TO perform_VB at_IN a_DT similar_JJ level_NN of_IN accuracy_NN as_IN regular_JJ SVMs_NNS and_CC at_IN the_DT same_JJ time_NN being_VBG significantly_RB faster_RBR =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
sIn_NN this_DT paper_NN we_PRP propose_VBP and_CC compare_VBP two_CD parallelization_JJ approaches_NNS of_IN the_DT Incremental_JJ SVM_NN classifier_NN ._.
The_DT algorithms_NNS presented_VBN are_VBP based_VBN on_IN heaps_NNS of_IN CPUs_NNS represented_VBN as_IN tree_NN topologies_NNS ._.
2_CD Backgroun_NN
upport_NN of_IN incremental_JJ multicategorical_JJ classification_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
Proximal_JJ SVMs_NNS has_VBZ also_RB been_VBN shown_VBN to_TO perform_VB at_IN a_DT similar_JJ level_NN of_IN accuracy_NN as_IN regular_JJ SVMs_NNS and_CC at_IN the_DT same_JJ time_NN being_VBG significantly_RB faster_RBR =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT paper_NN we_PRP propose_VBP a_DT computationally_RB efficient_JJ algorithm_NN that_WDT enables_VBZ decremental_JJ support_NN for_IN Incremental_JJ PSVMs_NNS using_VBG a_DT weight_NN decay_NN coefficient_NN ._.
The_DT suggested_JJ approach_NN is_VBZ compared_VBN the_DT current_JJ
em_NN to_TO one_CD of_IN two_CD disjoint_JJ halfspaces_NNS in_IN either_CC the_DT original_JJ input_NN space_NN of_IN the_DT problem_NN for_IN linear_JJ classifiers_NNS ,_, or_CC in_IN a_DT higher_JJR dimensional_JJ feature_NN space_NN for_IN nonlinear_JJ classifiers_NNS -LRB-_-LRB- 26_CD ,_, 8_CD ,_, 17_CD -RRB-_-RRB- ._.
Recently_RB =_JJ -_: =[_NN 22_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: much_RB simpler_JJR classifiers_NNS ,_, the_DT least_JJS squares_NNS and_CC the_DT proximal_JJ support_NN vector_NN machine_NN -LRB-_-LRB- PSVM_NN -RRB-_-RRB- ,_, were_VBD implemented_VBN wherein_WRB each_DT class_NN of_IN points_NNS is_VBZ assigned_VBN to_TO the_DT closest_JJS of_IN two_CD parallel_JJ planes_NNS -LRB-_-LRB- in_IN input_NN o_NN
g_NN the_DT SVs_NNS with_IN the_DT corresponding_JJ weights_NNS to_TO describe_VB the_DT class_NN boundary_NN ._.
There_EX have_VBP been_VBN many_JJ attempts_NNS to_TO revise_VB the_DT original_JJ QP_NN formulation_NN such_JJ that_IN it_PRP can_MD be_VB solved_VBN by_IN a_DT QP_NN solver_VBZ more_RBR efficiently_RB =_JJ -_: =[_NN 8_CD ,_, 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
-LRB-_-LRB- See_NNP Section_NNP 6_CD for_IN more_JJR details_NNS ._. -RRB-_-RRB-
We_PRP do_VBP not_RB revise_VB the_DT original_JJ QP_NN formulation_NN of_IN SVMs_NNS ._.
Instead_RB ,_, we_PRP try_VBP to_TO provide_VB a_DT smallerbut_JJ high_JJ quality_NN data_NNS set_NN that_WDT is_VBZ beneficial_JJ to_TO computing_VBG the_DT SVM_NN boundar_NN
onvex_JJ ,_, piecewise_JJ quadratic_JJ function_NN on_IN the_DT n-dimensional_JJ real_JJ space_NN R_NNP n._NNP Such_PDT a_DT problem_NN is_VBZ a_DT fundamental_JJ one_CD in_IN generating_VBG a_DT linear_JJ or_CC nonlinear_JJ kernel_NN classifier_NN for_IN data_NNS mining_NN and_CC machine_NN learning_NN =_JJ -_: =[_NN 23_CD ,_, 2_CD ,_, 16_CD ,_, 17_CD ,_, 18_CD ,_, 10_CD ,_, 11_CD ,_, 19_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT work_NN is_VBZ motivated_VBN by_IN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, where_WRB a_DT smoothed_VBN version_NN of_IN the_DT present_JJ algorithm_NN was_VBD shown_VBN to_TO converge_VB globally_RB and_CC quadratically_RB but_CC was_VBD observed_VBN to_TO terminate_VB in_IN a_DT few_JJ steps_NNS even_RB when_WRB the_DT smoothin_NN
el_NN matrix_NN which_WDT is_VBZ used_VBN in_IN the_DT nonlinear_JJ SVM_NN formulation_NN to_TO avoid_VB the_DT computational_JJ difficulties_NNS ._.
This_DT reducedskernel_NN technique_NN has_VBZ been_VBN successfully_RB applied_VBN to_TO other_JJ kernel-based_JJ learning_NN algorithms_NNS =_JJ -_: =[_NN 3_CD ,_, 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT paper_NN ,_, we_PRP use_VBP a_DT systematic_JJ sampling_NN mechanism_NN to_TO select_VB a_DT reduced_VBN set_NN which_WDT is_VBZ the_DT most_RBS important_JJ ingredient_NN of_IN RSVM_NN and_CC name_VB it_PRP as_IN Systematic_JJ Sampling_NN RSVM_NN -LRB-_-LRB- SSRSVM_NN -RRB-_-RRB- ._.
This_DT algorithm_NN is_VBZ inspi_NN
attractive_JJ because_IN they_PRP assure_VBP a_DT continuous_JJ decrease_NN in_IN the_DT primal_JJ objective_NN function_NN ._.
Recently_RB some_DT promising_JJ primal_JJ algorithms_NNS have_VBP been_VBN given_VBN for_IN training_NN linear_NN classifiers_NNS ._.
Fung_NNP and_CC Mangasarian_NNP -LRB-_-LRB- =_JJ -_: =_JJ Fung_NNP and_CC Mangasarian_NNP ,_, 2001_CD -_: =--RRB-_NN have_VBP given_VBN a_DT primal_JJ version_NN of_IN the_DT least_JJS squares_NNS formulation_NN of_IN SVMs_NNS given_VBN by_IN Suykens_NNP and_CC Vandewalle_NNP -LRB-_-LRB- 1999_CD -RRB-_-RRB- ._.
Komarek_NN -LRB-_-LRB- 2004_CD -RRB-_-RRB- has_VBZ effectively_RB applied_VBN conjugate_NN gradient_NN schemes_NNS to_TO logistic_JJ regression_NN ._.
l._NN A_NN PP-SVM_NN solution_NN for_IN linear_JJ kernels_NNS has_VBZ been_VBN proposed_VBN in_IN -LRB-_-LRB- 36_CD -RRB-_-RRB- ._.
However_RB ,_, it_PRP is_VBZ not_RB extendible_JJ to_TO nonlinear_JJ kernels_NNS because_IN its_PRP$ solution_NN is_VBZ based_VBN on_IN the_DT optimization_NN formulation_NN of_IN the_DT proximal_JJ SVM_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Our_PRP$ approach_NN is_VBZ new_JJ and_CC specifically_RB designed_VBN for_IN SVM_NN nonlinear_JJ kernels_NNS ._.
Our_PRP$ method_NN is_VBZ currently_RB limited_VBN to_TO the_DT nonlinear_JJ kernels_NNS whose_WP$ kernel_NN matrix_NN can_MD be_VB constructed_VBN from_IN the_DT gram_NN matrix_NN ._.
However_RB
linear_JJ classifier_NN regardless_RB of_IN the_DT algorithm_NN or_CC criteria_NNS used_VBN to_TO construct_VB the_DT classifier_NN ,_, including_VBG Linear_NNP Fisher_NNP Discriminant_NNP -LRB-_-LRB- LFD_NNP -RRB-_-RRB- -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, Least_NNP squares_NNS SVM_NNP 's_POS -LRB-_-LRB- LS-SVMs_NN -RRB-_-RRB- -LRB-_-LRB- 21_CD -RRB-_-RRB- or_CC Proximal_JJ SVMs_NNS -LRB-_-LRB- PSVM_NN -RRB-_-RRB- =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Denote_VB by_IN P_NN −_NN -LRB-_-LRB- w_NN ,_, γ_NN ,_, I_PRP -RRB-_-RRB- the_DT problem_NN of_IN constructing_VBG rules_NNS for_IN the_DT classifier_NN for_IN the_DT region_NN :_: I_NN =_JJ -LCB-_-LRB- x_NN s.t._NN w_NN ′_CD x_NN -LRB-_-LRB- γ_NN ,_, li_FW ≤_FW xi_FW ≤_FW ui_FW ,_, 1_CD ≤_CD i_FW ≤_FW n_NN -RCB-_-RRB- based_VBN on_IN the_DT classification_NN hyperplane_NN w_FW ′_FW x_NN =_JJ γ_NN obtained_VBD b_NN
e_LS can_MD be_VB regarded_VBN as_IN a_DT special_JJ type_NN of_IN regularization_NN term_NN ._.
Based_VBN on_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- ,_, we_PRP can_MD know_VB that_DT many_JJ other_JJ popular_JJ kernel_NN machines_NNS later_RB invented_VBD such_JJ as_IN KRR_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, LS-SVM_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, KFD_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, RLSC_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- and_CC PSVM_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP equivalent_JJ to_TO GPR_NN model_NN in_IN nature_NN ._.
It_PRP is_VBZ easy_JJ to_TO see_VB that_IN the_DT loss_NN function_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- arrives_VBZ at_IN its_PRP$ optimal_JJ value_NN at_IN -LRB-_-LRB- 2_LS -RRB-_-RRB- α_NN =_JJ -LRB-_-LRB- K_NN ⊤_NN K_NN +_CC σ_NN 2_CD K_NN -RRB-_-RRB- −_NN 1_CD K_NN ⊤_CD t_NN =_JJ -LRB-_-LRB- K_NN +_CC σ_NN 2_CD Idn_NN -RRB-_-RRB- −_NN 1_CD t._NN -LRB-_-LRB- 3_CD -RRB-_-RRB- 160_CD After_IN obtaining_VBG the_DT
ssion_NN -LRB-_-LRB- KRR_NN -RRB-_-RRB- -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, Least_NNP Squares_NNPS Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- LS-SVM_NN -RRB-_-RRB- -LRB-_-LRB- 31_CD -RRB-_-RRB- ,_, Kernel_NNP Fisher_NNP Discriminant_NNP -LRB-_-LRB- 18_CD -RRB-_-RRB- ,_, Regularised_VBN Least_FW Squares_FW Classification_NN -LRB-_-LRB- RLSC_NN -RRB-_-RRB- -LRB-_-LRB- 23_CD -RRB-_-RRB- and_CC Proximal_JJ Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- PSVM_NNP -RRB-_-RRB- =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, are_VBP equivalent_JJ to_TO the_DT GPR_NN model_NN in_IN essence_NN ._.
Since_IN the_DT matrix_NN -LRB-_-LRB- σ_NN 2_CD K_NN +_CC K_NN ⊤_NN K_NN -RRB-_-RRB- in_IN -LRB-_-LRB- 1.11_CD -RRB-_-RRB- is_VBZ symmetric_JJ and_CC the_DT objective_NN is_VBZ a_DT quadratic_JJ function_NN ,_, it_PRP is_VBZ straightforward_JJ to_TO exploit_VB the_DT well-known_JJ Conjuga_NN
imbalance_NN in_IN the_DT number_NN of_IN representatives_NNS in_IN each_DT class_NN ,_, we_PRP modify_VBP the_DT diagonal_JJ elements_NNS of_IN the_DT kernel_NN matrices_NNS as_IN suggested_VBN by_IN Veropoulos_NNP ,_, Campbell_NNP and_CC Cristianini_NNP -LRB-_-LRB- 1999_CD -RRB-_-RRB- ._.
We_PRP used_VBD the_DT Proximal_JJ SVM_NN -LRB-_-LRB- =_JJ -_: =_JJ Fung_NNP and_CC Mangasarian_NNP 2001_CD -_: =--RRB-_NN to_TO perform_VB the_DT actual_JJ SVM_NN classification_NN ._.
Gaussian_NNP Naïve_NNP Bayesian_NNP Network_NNP Analysis_NNP Two_CD separate_JJ GNBNs_NNS were_VBD constructed_VBN ,_, GNBNh_NN and_CC GNBNa_NN ._.
GNBNh_NN was_VBD trained_VBN on_IN Dh_FW ∪_FW Dd_NN and_CC was_VBD used_VBN to_TO classify_VB whether_IN
functions_NNS lead_VBP to_TO different_JJ learning_NN algorithms_NNS ._.
For_IN example_NN ,_, when_WRB used_VBN for_IN classification_NN ,_, a_DT squared-loss_JJ -LRB-_-LRB- y_FW −_FW f_FW -LRB-_-LRB- x_NN -RRB-_-RRB- -RRB-_-RRB- 2_CD brings_VBZ about_IN the_DT regularized_VBN least-squares_NNS classification_NN -LRB-_-LRB- RLSC_NN -RRB-_-RRB- algorithm_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =-[_NN 15_CD -RRB-_-RRB- ;_: while_IN a_DT hinge_NN loss_NN -LRB-_-LRB- 1_CD −_NN yf_NN -LRB-_-LRB- x_NN -RRB-_-RRB- -RRB-_-RRB- +_CC ≡_FW max_FW -LRB-_-LRB- 1_CD −_NN yf_NN -LRB-_-LRB- x_NN -RRB-_-RRB- ,_, 0_CD -RRB-_-RRB- corresponds_VBZ to_TO the_DT classical_JJ support_NN vector_NN machines_NNS -LRB-_-LRB- SVM_NN -RRB-_-RRB- ._.
Using_VBG this_DT model_NN ,_, data_NNS are_VBP implicitly_RB projected_VBN onto_IN the_DT hypothesis_NN space_NN HK_NN v_LS
and_CC has_VBZ been_VBN used_VBN for_IN text_NN classification_NN :_: see_VB ,_, e.g._FW ,_, Zhang_NNP and_CC Peng_NNP -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, Poggio_NNP and_CC Smale_NNP -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, Rifkin_NNP ,_, et_NNP ._.
al._FW -LRB-_-LRB- 27_CD -RRB-_-RRB- ,_, Fung_NNP and_CC Mangasarian_NNP -LRB-_-LRB- who_WP call_VBP the_DT procedure_NN a_DT Proximal_JJ Support_NN Vector_NNP Machine_NNP -RRB-_-RRB- =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Agarwal_NNP -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Zhang_NNP and_CC Oles_NNP -LRB-_-LRB- 42_CD -RRB-_-RRB- ,_, and_CC Suykens_NNP and_CC Vandewalle_NNP -LRB-_-LRB- who_WP call_VBP the_DT procedure_NN a_DT Least_FW Squares_FW Support_NN Vector_NNP Machine_NNP -RRB-_-RRB- -LRB-_-LRB- 33_CD -RRB-_-RRB- ._.
In_IN particular_JJ ,_, RLSC_NN performs_VBZ comparable_JJ to_TO the_DT popular_JJ Support_NN Vect_NN
and_CC has_VBZ been_VBN used_VBN for_IN text_NN classification_NN :_: see_VB ,_, e.g._FW ,_, Zhang_NNP and_CC Peng_NNP -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, Poggio_NNP and_CC Smale_NNP -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, Rifkin_NNP ,_, et_NNP ._.
al._FW -LRB-_-LRB- 27_CD -RRB-_-RRB- ,_, Fung_NNP and_CC Mangasarian_NNP -LRB-_-LRB- who_WP call_VBP the_DT procedure_NN a_DT Proximal_JJ Support_NN Vector_NNP Machine_NNP -RRB-_-RRB- =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Agarwal_NNP -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Zhang_NNP and_CC Oles_NNP -LRB-_-LRB- 42_CD -RRB-_-RRB- ,_, and_CC Suykens_NNP and_CC Vandewalle_NNP -LRB-_-LRB- who_WP call_VBP the_DT procedure_NN a_DT Least_FW Squares_FW Support_NN Vector_NNP Machine_NNP -RRB-_-RRB- -LRB-_-LRB- 33_CD -RRB-_-RRB- ._.
In_IN particular_JJ ,_, RLSC_NN performs_VBZ comparable_JJ to_TO the_DT popular_JJ Support_NN Vect_NN
this_DT paper_NN ,_, we_PRP present_VBP an_DT algorithm_NN that_WDT can_MD classify_VB large-scale_JJ text_NN data_NNS with_IN high_JJ classification_NN quality_NN and_CC fast_JJ training_NN speed_NN ._.
Our_PRP$ method_NN is_VBZ based_VBN on_IN a_DT novel_JJ extension_NN of_IN the_DT proximal_JJ SVM_NN mode_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Previous_JJ studies_NNS on_IN proximal_JJ SVM_NN have_VBP focused_VBN on_IN classification_NN for_IN low_JJ dimensional_JJ data_NNS and_CC did_VBD not_RB consider_VB the_DT unbalanced_JJ data_NN cases_NNS ._.
Such_JJ methods_NNS will_MD meet_VB difficulties_NNS when_WRB classifying_VBG unbalanc_NN
nce_VB Foundation_NNP of_IN China_NNP under_IN the_DT grants_NNS NSFC_NN 60375022_CD and_CC NSFC_NNP 60473040_CD ._.
sTable_JJ 1_CD ._.
The_DT contingency_NN table_NN label_NN y_NN =_JJ 0_CD label_NN y_NN =_JJ 1_CD prediction_NN h_NN -LRB-_-LRB- x_NN -RRB-_-RRB- =_JJ 0_CD T_NN p_NN F_NN p_NN prediction_NN h_NN -LRB-_-LRB- x_NN -RRB-_-RRB- =_JJ 1_CD F_NN n_NN T_NN n_NN gramme_NN problem_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 7_CD -RRB-_-RRB- -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, and_CC using_VBG geometric_JJ algorithms_NNS -LRB-_-LRB- 9_CD -RRB-_-RRB- -LRB-_-LRB- 10_CD -RRB-_-RRB- ._.
Divide-andconquer_JJ principle_NN is_VBZ applied_VBN to_TO scale_VB SVMs_NNS too_RB ._.
Divide-and-conquer_JJ principle_NN implemented_VBN in_IN serial_NN include_VBP the_DT standard_JJ SVMs_NNS training_NN metho_NN
structure_NN presented_VBN here_RB is_VBZ ,_, in_IN spirit_NN ,_, similar_JJ to_TO the_DT work_NN presented_VBN by_IN Viola_NNP and_CC Jones_NNP using_VBG AdaBoost_NNP based_VBN classifiers_NNS -LRB-_-LRB- 57_CD -RRB-_-RRB- ._.
To_TO implement_VB the_DT low_JJ false_JJ negative_JJ classifiers_NNS a_DT modified_VBN proximal_JJ SVM_NN =_JJ -_: =[_NN 69_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ used_VBN to_TO remove_VB the_DT ``_`` peripheral_JJ ''_'' examples_NNS ,_, and_CC a_DT standard_JJ SVM_NN with_IN RBF_NN kernel_NN is_VBZ used_VBN to_TO perform_VB the_DT final_JJ classification_NN ._.
As_IN seen_VBN from_IN Fig._NNP 29_CD ,_, the_DT data_NNS points_NNS that_WDT do_VBP not_RB lie_VB between_IN the_DT two_CD prox_NN
ares_NNS problem_NN ,_, also_RB known_VBN as_IN ridge_NN regression_NN in_IN statistics_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ._.
Regularized_VBN least_JJS squares_NNS classification_NN -LRB-_-LRB- 26_CD -RRB-_-RRB- is_VBZ closely_RB related_JJ to_TO Regularized_NNP Network_NNP -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, Least_JJ squares_NNS SVM_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, and_CC proximal_JJ SVMs_NNS =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP have_VBP conducted_VBN extensive_JJ experiments_NNS using_VBG five_CD lowdimensional_JJ datasets_NNS and_CC nine_CD high-dimensional_JJ datasets_NNS from_IN various_JJ data_NNS sources_NNS ,_, including_VBG text_NN documents_NNS ,_, face_NN images_NNS ,_, and_CC microarray_NN gene_NN ex_FW
ING_NN AND_CC KNOWLEDGE_NN DISCOVERY_NN :_: AN_DT INTERNATIONAL_NNP JOURNAL_NNP ,_, MAY_NNP ._.
2005_CD 104_CD There_EX have_VBP been_VBN many_JJ attempts_NNS to_TO revise_VB the_DT original_JJ QP_NN formulation_NN so_IN that_IN it_PRP can_MD be_VB solved_VBN by_IN a_DT QP_NN solver_VBZ more_RBR efficiently_RB -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
-LRB-_-LRB- See_NNP Section_NNP VII_NNP for_IN more_JJR details_NNS ._. -RRB-_-RRB-
In_IN contrast_NN to_TO those_DT works_NNS ,_, we_PRP do_VBP not_RB revise_VB the_DT original_JJ QP_NN formulation_NN of_IN SVM_NNP ._.
Instead_RB ,_, we_PRP try_VBP to_TO provide_VB a_DT smaller_JJR but_CC high_JJ quality_NN data_NNS set_NN that_WDT is_VBZ bene_NN
hyper_JJ plane_NN and_CC closest_JJS data_NNS points_NNS -LRB-_-LRB- support_NN vectors_NNS -RRB-_-RRB- ._.
To_TO maximize_VB the_DT margin_NN denoted_VBN by_IN 1_CD |_CD |_CD w_NN |_NNP |_NNP while_IN minimizing_VBG the_DT error_NN ,_, the_DT standard_JJ SVM_NN solution_NN is_VBZ formulated_VBN into_IN the_DT following_JJ primal_JJ program_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: :_: min_NN w_NN ,_, y_NN Fig._NNP 3_CD ._.
15_CD measurements_NNS 1_CD 2_CD w_NN ′_CD w_NN +_CC νe_FW ′_FW y_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- s.t._NN D_NN -LRB-_-LRB- Aw_UH −_FW eγ_FW -RRB-_-RRB- +_CC y_FW ≥_FW e_LS and_CC y_FW ≥_FW 0_CD -LRB-_-LRB- 3_CD -RRB-_-RRB- which_WDT minimizes_VBZ the_DT reciprocal_JJ of_IN the_DT margin_NN -LRB-_-LRB- w_NN ′_CD w_NN -RRB-_-RRB- andthe_NN error_NN -LRB-_-LRB- e_SYM ′_FW y_FW -RRB-_-RRB- ._.
An_DT m_NN ×_CD n_NN matrix_NN A_NN represents_VBZ m_NN dat_NN
l._NN A_NN PP-SVM_NN solution_NN for_IN linear_JJ kernels_NNS has_VBZ been_VBN proposed_VBN in_IN -LRB-_-LRB- 36_CD -RRB-_-RRB- ._.
However_RB ,_, it_PRP is_VBZ not_RB extendible_JJ to_TO nonlinear_JJ kernels_NNS because_IN its_PRP$ solution_NN is_VBZ based_VBN on_IN the_DT optimization_NN formulation_NN of_IN the_DT proximalsSVM_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Our_PRP$ approach_NN is_VBZ new_JJ and_CC specifically_RB designed_VBN for_IN SVM_NN nonlinear_JJ kernels_NNS ._.
Our_PRP$ method_NN is_VBZ currently_RB limited_VBN to_TO the_DT nonlinear_JJ kernels_NNS whose_WP$ kernel_NN matrix_NN can_MD be_VB constructed_VBN from_IN the_DT gram_NN matrix_NN ._.
However_RB
nding_NN ''_'' that_WDT underlies_VBZ our_PRP$ approach_NN ._.
Such_JJ bounding_VBG applies_VBZ to_TO any_DT kernel_NN machine_NN of_IN the_DT form_NN f_FW -LRB-_-LRB- x_NN -RRB-_-RRB- =_JJ P_NN isi_NN K_NN -LRB-_-LRB- X_NN i_LS ;_: x_LS -RRB-_-RRB- b_NN ,_, including_VBG not_RB only_JJ SVMs_NNS ,_, but_CC also_RB recent_JJ ones_NNS such_JJ as_IN KFD_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, MPM_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- and_CC =_JJ -_: =_JJ PSVM_NN -LRB-_-LRB- 8_CD -_: =-]_CD ._.
Furthermore_RB ,_, our_PRP$ anytime_RB bounding_VBG is_VBZ applicable_JJ regardless_RB of_IN the_DT task_NN -LRB-_-LRB- e.g._FW classication_NN ,_, regression_NN ,_, etc._NN -RRB-_-RRB- ,_, since_IN this_DT same_JJ kernel_NN machine_NN form_NN is_VBZ common_JJ to_TO all_DT such_JJ uses_NNS ._.
For_IN 7_CD We_PRP also_RB observe_VBP t_NN
rding_VBG this_DT fuzziness_NN can_MD improve_VB its_PRP$ performance_NN and_CC lessen_VB the_DT eFect_NN of_IN outliers_NNS ._.
Traditional_JJ SVM-based_JJ classi1ers_NNS lack_VBP such_JJ a_DT mechanism_NN ._.
Fung_NNP and_CC Mangasarian_NNP recently_RB proposed_VBD proximal_JJ SVMs_NNS -LRB-_-LRB- PSVMs_NNS -RRB-_-RRB- =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT may_MD be_VB thought_VBN of_IN as_IN a_DT kind_NN of_IN regularized_VBN least_JJS squares_NNS SVM_NNP ._.
PSVMs_NNS require_VBP the_DT solution_NN of_IN a_DT single_JJ set_NN of_IN linear_JJ equations_NNS ,_, and_CC can_MD thus_RB be_VB considerably_RB faster_RBR than_IN traditional_JJ SVMs_NNS ._.
The_DT sol_NN
2007_CD ._.
Copyright_NN 2007_CD by_IN the_DT author_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- \/_: owner_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
Fung_NNP &_CC Mangasarian_NNP ,_, 2002_CD ;_: Laskov_NNP et_FW al._FW ,_, 2006_CD -RRB-_-RRB- ,_, parallel_JJ techniques_NNS -LRB-_-LRB- Collobert_NNP et_FW al._FW ,_, 2001_CD ;_: Graf_NNP et_FW al._FW ,_, 2004_CD -RRB-_-RRB- ,_, and_CC employing_VBG an_DT approximate_JJ formula_NN -LRB-_-LRB- =_JJ -_: =_JJ Fung_NNP &_CC Mangasarian_NNP ,_, 2001_CD -_: =_JJ -_: ;_: Lee_NNP &_CC Mangasarian_NNP ,_, 2001_CD -RRB-_-RRB- ._.
Another_DT approach_NN ,_, in_IN which_WDT only_RB the_DT representatives_NNS are_VBP used_VBN for_IN training_NN ,_, also_RB aims_VBZ at_IN large-scale_JJ classification_NN problems_NNS ._.
Active_JJ learning_NN -LRB-_-LRB- Schohn_NNP &_CC Cohn_NNP ,_, 2000_CD -RRB-_-RRB- chooses_VBZ t_NN
RSVM_NN was_VBD introduced_VBN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, which_WDT randomly_RB selected_VBD a_DT subset_NN from_IN the_DT training_NN set_VBN and_CC trained_VBN on_IN this_DT reduced_VBN subset_NN ._.
The_DT proximal_JJ SVM_NN -LRB-_-LRB- PSVM_NN -RRB-_-RRB- was_VBD introduced_VBN as_IN an_DT alternative_NN to_TO quadratic_JJ programming_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, as_IN it_PRP only_RB depends_VBZ on_IN linear_JJ algebraic_JJ operations_NNS ._.
Incremental_JJ training_NN methods_NNS were_VBD proposed_VBN for_IN PSVM_NNP to_TO save_VB space_NN and_CC to_TO unlearn_VB old_JJ data_NNS -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, where_WRB only_RB linear_JJ classifiers_NNS without_IN kernels_NNS were_VBD c_NN
-LRB-_-LRB- 31_CD -RRB-_-RRB- as_RB well_RB as_IN the_DT Occam_NNP 's_POS razor_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ._.
The_DT technique_NN of_IN using_VBG a_DT reduced_VBN kernel_NN matrix_NN has_VBZ been_VBN successfully_RB applied_VBN to_TO other_JJ kernel-based_JJ learning_NN algorithms_NNS ,_, such_JJ as_IN proximal_JJ support_NN vector_NN machine_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, ɛ-smooth_NN support_NN vector_NN regression_NN -LRB-_-LRB- ɛ-SSVR_NN -RRB-_-RRB- -LRB-_-LRB- 18_CD -RRB-_-RRB- ,_, Lagrangian_JJ support_NN vector_NN machine_NN -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, least-square_JJ support_NN vector_NN machine_NN -LRB-_-LRB- 38_CD ,_, 39_CD ,_, 23_CD -RRB-_-RRB- ._.
Also_RB ,_, there_EX were_VBD experimental_JJ studies_NNS on_IN RSVM_NN -LRB-_-LRB- 23_CD ,_, 18_CD -RRB-_-RRB- tha_NN
olleagues_NNS proposed_VBD several_JJ variations_NNS of_IN standard_JJ support_NN vector_NN machines_NNS by_IN modifying_VBG the_DT objective_JJ function_NN ,_, together_RB with_IN several_JJ very_RB efficient_JJ training_NN algorithms_NNS -LRB-_-LRB- -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 20_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 14_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 12_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Since_IN the_DT optimization_NN problems_NNS given_VBN by_IN these_DT modifications_NNS are_VBP different_JJ from_IN that_DT of_IN standard_JJ support_NN vector_NN machines_NNS ,_, we_PRP will_MD not_RB address_VB them_PRP in_IN this_DT paper_NN ._.
In_IN this_DT paper_NN ,_, we_PRP pr_VBP
factors_NNS 6_CD 4_CD New_NNP Interpretation_NNP to_TO the_DT standard_JJ SVM_NN in_IN vector_NN space_NN 8s1_NN Introduction_NN 1.1_CD related_JJ work_NN review_NN 1.1.1_CD SVM_NN Note_NN :_: SVM_NN has_VBZ high_JJ generalization_NN ability_NN 1.1.2_CD Proximal_JJ SVM_NN -LRB-_-LRB- PSVM_NN -RRB-_-RRB- Proximal_JJ SVM_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: do_VB an_DT extremely_RB fast_JJ SVM_NNP construction_NN ._.
And_CC its_PRP$ support_NN vectors_NNS can_MD be_VB set_VBN to_TO the_DT whole_JJ training_NN data-set_NN ._.
1.1.3_CD Semi-supervised_JJ -LRB-_-LRB- Transductive_JJ -RRB-_-RRB- SVM_NN TSVM_NN exhaustively_RB tests_VBZ the_DT label_NN assignment_NN of_IN any_DT t_NN
criteria_NNS with_IN the_DT partial_JJ fuzzy_JJ scores_NNS represented_VBN by_IN TFNs_NNS ,_, shown_VBN in_IN Table_NNP 5_CD ._.
Table_NNP 5_CD ._.
Two_CD Projects_NNP Evaluated_NNP in_IN Two_CD Fuzzy_NNP Criteria_NNP Projects_NNPS PIN_NN -LRB-_-LRB- %_NN -RRB-_-RRB- EIN_NN -LRB-_-LRB- 0_CD --_: 10_CD -RRB-_-RRB- Proj1_NN -LRB-_-LRB- 40_CD ,_, 50_CD ,_, 60_CD -RRB-_-RRB- -LRB-_-LRB- 3.8_CD ,_, 4_CD ,_, 4.3_CD -RRB-_-RRB- Proj2_NN =_JJ -_: =[_NN 25_CD ,_, 28_CD ,_, 32_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 7_CD ,_, 8_CD ,_, 8.8_CD -RRB-_-RRB- A_DT first_JJ calculation_NN could_MD be_VB made_VBN using_VBG the_DT central_JJ α_NN =_JJ 1_CD scores_NNS ,_, i.e._FW ,_, Proj1_NN -LRB-_-LRB- 50_CD ;_: 4_CD -RRB-_-RRB- and_CC Proj2_NN -LRB-_-LRB- 28_CD ;_: 8_CD -RRB-_-RRB- ._.
If_IN it_PRP is_VBZ decided_VBN to_TO consider_VB only_JJ data_NNS for_IN which_WDT MG_NN ≥_NN 0.5_CD ,_, the_DT α-cut_NN with_IN α_NN =_JJ 0.5_CD is_VBZ
el_IN matrix_NN ._.
DK-PLS_NN explicitly_RB produces_VBZ a_DT low_JJ rank_NN approximation_NN of_IN the_DT kernel_NN matrix_NN ._.
Thus_RB it_PRP is_VBZ more_RBR closely_RB related_JJ to_TO other_JJ kernel_NN matrix_NN approximation_NN approaches_NNS based_VBN on_IN sampling_NN or_CC factorization_NN =_JJ -_: =[_NN 16_CD ,_, 10_CD ,_, 17_CD ,_, 9_CD ,_, 27_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
DK-PLS_NN has_VBZ the_DT advantage_NN that_IN the_DT kernel_NN does_VBZ not_RB need_VB to_TO be_VB square_JJ ._.
When_WRB combined_VBN with_IN sampling_NN of_IN the_DT columns_NNS of_IN the_DT kernel_NN matrix_NN such_JJ as_IN in_IN -LRB-_-LRB- 17_CD ,_, 16_CD ,_, 10_CD -RRB-_-RRB- ,_, it_PRP is_VBZ more_RBR scalable_JJ than_IN the_DT original_JJ KP_NN
exercise_NN of_IN deriving_VBG the_DT dual_JJ may_MD seem_VB somewhat_RB pointless_JJ ,_, its_PRP$ value_NN will_MD become_VB clear_JJ in_IN later_JJ sections_NNS ,_, when_WRB we_PRP use_VBP this_DT dual_JJ approach_NN to_TO make_VB connections_NNS to_TO other_JJ ideas_NNS ._.
Additionally_RB ,_, some_DT authors_NNS =_JJ -_: =[_NN 39_CD ,_, 38_CD -RRB-_-RRB- -_: =_JJ -_: who_WP have_VBP published_VBN on_IN this_DT topic_NN derive_VBP the_DT dual_JJ in_IN order_NN to_TO find_VB the_DT solution_NN ,_, possibly_RB in_IN order_NN to_TO make_VB their_PRP$ least-squares_NNS formulation_NN look_VBP as_RB much_JJ like_IN 76sthe_JJ standard_JJ SVM_NN formulation_NN as_IN possible_JJ
the_DT matrix_NN are_VBP zero_CD ._.
This_DT is_VBZ not_RB always_RB true_JJ and_CC its_PRP$ violation_NN might_MD incur_VB limited_JJ reduction_NN in_IN computational_JJ time_NN and_CC memory_NN ._.
To_TO provide_VB a_DT deterministic_JJ method_NN to_TO speed_VB up_RP kernel_NN machines_NNS learning_VBG ,_, -LRB-_-LRB- =_JJ -_: =_JJ Fung_NNP &_CC Mangasarian_NNP ,_, 2001_CD -_: =--RRB-_NN used_VBD Sherman-Morrison-Woodbury_JJ formula_NN to_TO calculate_VB the_DT inverse_NN of_IN -LRB-_-LRB- K_NN +_CC γmI_NN -RRB-_-RRB- ._.
This_DT approach_NN works_VBZ well_RB on_IN a_DT linear_JJ kernel_NN with_IN a_DT small_JJ d_NN -LRB-_-LRB- recall_NN d_NN is_VBZ the_DT number_NN of_IN dimensions_NNS -RRB-_-RRB- ._.
For_IN other_JJ problems_NNS ,_, p_NN
that_IN use_NN chunking_NN and_CC decomposition_NN methods_NNS efficiently_RB ._.
There_EX are_VBP also_RB efficient_JJ algorithms_NNS that_WDT exploit_VBP the_DT special_JJ structure_NN of_IN the_DT optimization_NN problem_NN such_JJ as_IN Generalized_NNP Proximal_JJ SVMs_NNS -LRB-_-LRB- GEPSVM_NN -RRB-_-RRB- =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT binary_JJ classification_NN problem_NN can_MD be_VB formulated_VBN as_IN a_DT generalized_JJ eigenvalue_NN problem_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- ._.
This_DT formulation_NN differs_VBZ from_IN SVMs_NNS since_IN ,_, instead_RB of_IN finding_VBG one_CD hyperplane_NN that_WDT separates_VBZ the_DT two_CD class_NN
nd_IN the_DT loss_NN function_NN is_VBZ V_NN -LRB-_-LRB- f_FW -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- -RRB-_-RRB- =_JJ -LRB-_-LRB- f_LS -LRB-_-LRB- x_NN -RRB-_-RRB- −_FW y_FW -RRB-_-RRB- 2_CD ,_, we_PRP obtain_VBP the_DT proximal_JJ SVM_NN ,_, and_CC with_IN V_NN -LRB-_-LRB- f_FW -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- -RRB-_-RRB- =_JJ -LRB-_-LRB- 1_CD −_NN yf_NN -LRB-_-LRB- x_NN -RRB-_-RRB- -RRB-_-RRB- +_CC SVM_NN classification_NN ._.
On_IN the_DT other_JJ side_NN ,_, in_IN a_DT recent_JJ technical_JJ report_NN by_IN Mangasarian_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, it_PRP is_VBZ showed_VBN how_WRB to_TO treat_VB a_DT binary_JJ classification_NN problem_NN :_: Aw_UH −_FW γ_FW min_NN ,_, -LRB-_-LRB- 6_CD -RRB-_-RRB- w_NN ,_, γ_NN =_JJ 0_CD Bw_NN −_FW γ_FW in_IN which_WDT the_DT solution_NN is_VBZ a_DT pair_NN of_IN parallel_JJ hyperplanes_NNS separating_VBG A_NN and_CC B_NN sets_NNS ,_, as_IN a_DT generalized_JJ eigenv_NN
lows_NNS us_PRP to_TO solve_VB problems_NNS with_IN millions_NNS of_IN points_NNS by_IN merely_RB inverting_VBG much_RB smaller_JJR matrices_NNS of_IN the_DT order_NN of_IN n._NNP This_NNP approach_NN has_VBZ been_VBN used_VBN by_IN various_JJ algorithms_NNS such_JJ as_IN ASVM_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, LSVM_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, and_CC PSVM_NN =_JJ -_: =[_NN 11_CD -_: =-]_CD ,_, as_RB well_RB as_IN in_IN an_DT interior_JJ point_NN approach_NN by_IN Ferris_NNP and_CC Munson_NNP -LRB-_-LRB- 10_CD -RRB-_-RRB- ._.
Burges_NNP also_RB used_VBD an_DT active_JJ set_NN method_NN for_IN support_NN vector_NN machines_NNS for_IN classication_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ._.
We_PRP note_VBP that_IN an_DT active_JJ set_NN computational_JJ
,_, various_JJ studies_NNS applied_VBD the_DT kernel_NN trick_NN to_TO the_DT LS_NN loss_NN to_TO obtain_VB mathematically_RB equivalent_JJ kernel_NN classifiers_NNS under_IN different_JJ names_NNS ,_, including_VBG the_DT least_JJS square_JJ SVM_NN -LRB-_-LRB- LSSVM_NN -LRB-_-LRB- 29_CD -RRB-_-RRB- -RRB-_-RRB- ,_, proximal_JJ SVM_NN -LRB-_-LRB- PSVM_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, Kernel_NNP Ridge_NNP Regression_NN -LRB-_-LRB- KRR_NN ,_, -LRB-_-LRB- 1_LS -RRB-_-RRB- -RRB-_-RRB- and_CC Kernel_NNP Fisher_NNP Discriminant_NNP etc._NN ._.
Though_IN very_RB promising_JJ results_NNS were_VBD reported_VBN ,_, a_DT crucial_JJ drawback_NN is_VBZ that_IN the_DT resulted_VBN kernel_NN machines_NNS lose_VBP the_DT important_JJ sparse_NN
nt_NN past_NN ,_, much_JJ research_NN has_VBZ been_VBN devoted_VBN to_TO incorporating_VBG the_DT concept_NN of_IN kernels_NNS and_CC new_JJ forms_NNS of_IN regularization_NN in_IN these_DT methods_NNS -LRB-_-LRB- Saunders_NNP et_FW al._FW ,_, 1998_CD ;_: Mika_NNP et_FW al._FW ,_, 1999_CD ;_: Suykens_NNP &_CC Vandewalle_NNP ,_, 1999_CD ;_: =_JJ -_: =_JJ Fung_NNP &_CC Mangasarian_NNP ,_, 2001_CD -_: =--RRB-_NN ._.
Let_VB A_DT ∈_NN ℜ_NN n_NN ×_FW d_FW be_VB the_DT data_NNS matrix_NN representing_VBG the_DT training_NN samples_NNS ,_, S._FW Each_DT row_NN represents_VBZ a_DT sample_NN and_CC each_DT column_NN represents_VBZ a_DT feature_NN ._.
Notice_NNP that_IN if_IN we_PRP use_VBP the_DT kernel_NN extension_NN of_IN least_JJS squares_NNS
s_NN but_CC is_VBZ computationally_RB more_RBR attractive_JJ since_IN it_PRP solves_VBZ a_DT linear_JJ optimization_NN problem_NN instead_RB of_IN a_DT quadratic_JJ problem_NN ._.
We_PRP briefly_RB discuss_VBP the_DT MPSVM_NNP formulation_NN in_IN this_DT section_NN ._.
Our_PRP$ discussion_NN follows_VBZ =_JJ -_: =[_NN 5,6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Let_VB ,_, X_NN =_JJ -LCB-_-LRB- x1_NN ,_, x2_NN ,_, ..._: xm_IN -RCB-_-RRB- be_VB a_DT set_NN of_IN m_NN points_NNS in_IN n-dimensional_JJ real_JJ space_NN R_NN n_NN represented_VBN by_IN a_DT m_NN ×_CD n_NN matrix_NN A._NN We_PRP consider_VBP the_DT problem_NN of_IN classifying_VBG these_DT points_NNS according_VBG to_TO the_DT membership_NN of_IN e_SYM
and_CC has_VBZ been_VBN used_VBN for_IN text_NN classification_NN :_: see_VB ,_, e.g._FW ,_, Zhang_NNP and_CC Peng_NNP -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, Poggio_NNP and_CC Smale_NNP -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, Rifkin_NNP ,_, et_NNP ._.
al._FW -LRB-_-LRB- 27_CD -RRB-_-RRB- ,_, Fung_NNP and_CC Mangasarian_NNP -LRB-_-LRB- who_WP call_VBP the_DT procedure_NN a_DT Proximal_JJ Support_NN Vector_NNP Machine_NNP -RRB-_-RRB- =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Agarwal_NNP -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Zhang_NNP and_CC Oles_NNP -LRB-_-LRB- 42_CD -RRB-_-RRB- ,_, and_CC Suykens_NNP and_CC Vandewalle_NNP -LRB-_-LRB- who_WP call_VBP the_DT procedure_NN a_DT Least_FW Squares_FW Support_NN Vector_NNP Machine_NNP -RRB-_-RRB- -LRB-_-LRB- 33_CD -RRB-_-RRB- ._.
In_IN particular_JJ ,_, RLSC_NN performs_VBZ comparable_JJ to_TO the_DT popular_JJ Support_NN Vect_NN
nt_NN past_NN ,_, much_JJ research_NN has_VBZ been_VBN devoted_VBN to_TO incorporating_VBG the_DT concept_NN of_IN kernels_NNS and_CC new_JJ forms_NNS of_IN regularization_NN in_IN these_DT methods_NNS -LRB-_-LRB- Saunders_NNP et_FW al._FW ,_, 1998_CD ;_: Mika_NNP et_FW al._FW ,_, 1999_CD ;_: Suykens_NNP &_CC Vandewalle_NNP ,_, 1999_CD ;_: =_JJ -_: =_JJ Fung_NNP &_CC Mangasarian_NNP ,_, 2001_CD -_: =--RRB-_NN ._.
Let_VB A_DT ∈_NN ℜ_NN n_NN ×_FW d_FW be_VB the_DT data_NNS matrix_NN representing_VBG the_DT training_NN samples_NNS ,_, S._FW Each_DT row_NN represents_VBZ a_DT sample_NN and_CC each_DT column_NN represents_VBZ a_DT feature_NN ._.
Notice_NNP that_IN if_IN we_PRP use_VBP the_DT kernel_NN extension_NN of_IN least_JJS squares_NNS
oints_NNS intertwined_VBN in_IN the_DT shape_NN of_IN a_DT spiral_NN is_VBZ a_DT synthetic_JJ dataset_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ._.
However_RB ,_, it_PRP apparently_RB is_VBZ a_DT difficult_JJ test_NN case_NN for_IN data_NN mining_NN algorithms_NNS and_CC is_VBZ known_VBN to_TO give_VB neural_JJ networks_NNS severe_JJ problems_NNS =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN contrast_NN ,_, a_DT sharp_JJ separation_NN was_VBD obtained_VBN using_VBG PSVM_NN as_IN can_MD be_VB seen_VBN in_IN Figure_NNP 3_CD ._.
5_CD ._.
Table_NNP 2_CD :_: Nonlinear_JJ Classifier_NN Comparison_NN using_VBG ISVM_NN ,_, SSVM_NN and_CC LSVM_NN For_IN this_DT experiment_NN we_PRP chose_VBD four_CD datasets_NNS f_SYM
sification_NN ,_, generalized_JJ eigenvalues_NNS ._.
1_CD INTRODUCTION_NN SUPPORT_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 4_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 27_CD -RRB-_-RRB- constitute_VBP the_DT method_NN of_IN choice_NN for_IN classification_NN problems_NNS while_IN the_DT generalized_JJ eigenvalue_NN problem_NN =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- is_VBZ a_DT simple_JJ problem_NN of_IN classical_JJ linear_JJ algebra_NN solvable_JJ by_IN a_DT single_JJ command_NN of_IN MATLAB_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- or_CC Scilab_NNP -LRB-_-LRB- 24_CD -RRB-_-RRB- or_CC by_IN using_VBG standard_JJ linear_JJ algebra_NN software_NN such_JJ LAPACK_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
In_IN proximal_JJ support_NN vecto_NN
is_VBZ a_DT version_NN of_IN the_DT US_NNP Census_NNP Bureau_NNP ``_`` Adult_JJ ''_'' dataset_NN ,_, which_WDT is_VBZ publicly_RB available_JJ from_IN the_DT Silicon_NNP Graphics_NNP website_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
sThe_NNP Galaxy_NNP Dim_NNP dataset_NN used_VBN in_IN galaxy_NN discrimination_NN with_IN neural_JJ netw_NN =_JJ -_: =_JJ orks_NNS from_IN -LRB-_-LRB- 30_CD -RRB-_-RRB- -_: =_JJ -_: sTwo_NN large_JJ datasets_NNS -LRB-_-LRB- 2_CD million_CD points_NNS and_CC 10_CD attributes_NNS -RRB-_-RRB- created_VBN using_VBG David_NNP Musicant_NNP 's_POS NDC_NNP Data_NNP Generator_NNP -LRB-_-LRB- 29_CD -RRB-_-RRB- ._.
sThe_FW Spiral_FW dataset_NN proposed_VBN by_IN Alexis_NNP Wieland_NNP of_IN the_DT MITRE_NNP Corporation_NNP and_CC available_JJ fr_NN
of_IN the_DT number_NN of_IN data_NNS points_NNS needs_VBZ to_TO be_VB solved_VBN ._.
This_DT allows_VBZ us_PRP to_TO easily_RB classify_VB datasets_NNS with_IN as_RB many_JJ as_IN a_DT few_JJ thousand_CD of_IN points_NNS ._.
For_IN larger_JJR datasets_NNS ,_, data_NNS selection_NN and_CC reduction_NN methods_NNS such_JJ as_IN =_JJ -_: =[_NN 11_CD ,_, 17_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: can_MD be_VB utilized_VBN as_IN indicated_VBN by_IN some_DT of_IN our_PRP$ numerical_JJ results_NNS and_CC will_MD be_VB the_DT subject_NN of_IN future_JJ work_NN ._.
Our_PRP$ computational_JJ results_NNS demonstrate_VBP that_IN PSVM_NN classifiers_NNS obtain_VBP test_NN set_NN correctness_NN statistic_NN
sets_NNS are_VBP f0_JJ 0Š_NN ;_: 1_CD 1Šg_NN and_CC f1_NN 0Š_NN ;_: 0_CD 1Šg_NN leads_VBZ to_TO an_DT exact_JJ classification_NN by_IN two_CD nonparallel_JJ proximal_JJ lines_NNS each_DT going_VBG through_IN the_DT two_CD points_NNS of_IN each_DT set_NN ._.
Related_JJ work_NN is_VBZ the_DT k-plane_NN clustering_NN of_IN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, where_WRB clusters_NNS are_VBP determined_VBN by_IN proximity_NN to_TO various_JJ nonparallel_JJ planes_NNS based_VBN on_IN the_DT smallest_JJS eigenvector_NN of_IN a_DT matrix_NN generated_VBN by_IN given_VBN data_NNS points_NNS ._.
We_PRP also_RB note_VBP that_IN ,_, in_IN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, the_DT generalized_JJ eig_NN
zed_VBN instead_RB of_IN the_DT 1-norm_NN ,_, and_CC the_DT margin_NN between_IN the_DT bounding_VBG planes_NNS is_VBZ maximized_VBN with_IN respect_NN to_TO both_CC orientation_NN w_NN and_CC relative_JJ location_NN to_TO the_DT origin_NN if_IN ._.
Extensive_JJ computational_JJ experience_NN ,_, as_IN in_IN =_JJ -_: =[_NN 22_CD ,_, 23_CD ,_, 24_CD ,_, 18_CD ,_, 17_CD -RRB-_-RRB- -_: =_SYM -_: indicates_VBZ that_IN this_DT formulation_NN is_VBZ just_RB as_RB good_JJ as_IN the_DT classical_JJ formulation_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- with_IN some_DT added_VBN advantages_NNS such_JJ as_IN strong_JJ convexity_NN of_IN the_DT objective_JJ function_NN ._.
Our_PRP$ key_JJ idea_NN in_IN this_DT present_JJ paper_NN is_VBZ t_NN
site_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
sThe_NNP Galaxy_NNP Dim_NNP dataset_NN used_VBN in_IN galaxy_NN discrimination_NN with_IN neural_JJ networks_NNS from_IN -LRB-_-LRB- 30_CD -RRB-_-RRB- sTwo_NN large_JJ datasets_NNS -LRB-_-LRB- 2_CD million_CD points_NNS and_CC 10_CD attributes_NNS -RRB-_-RRB- created_VBN using_VBG David_NNP Musicant_NNP 's_POS NDC_NNP Data_NNP Generator_NN =_JJ -_: =[_NN 29_CD -RRB-_-RRB- -_: =_SYM -_: ._.
sThe_FW Spiral_FW dataset_NN proposed_VBN by_IN Alexis_NNP Wieland_NNP of_IN the_DT MITRE_NNP Corporation_NNP and_CC available_JJ from_IN the_DT CMU_NNP Artificial_NNP Intelligence_NNP Repository_NNP -LRB-_-LRB- 37_CD -RRB-_-RRB- ._.
We_PRP outline_VBP our_PRP$ computational_JJ results_NNS now_RB in_IN five_CD groups_NNS as_IN
leveland_VBN Heart_NNP ,_, Pima_NNP Indians_NNPS ,_, BUPA_NN Liver_NN ,_, Mushroom_NNP ,_, Tic-Tac-Toe_NNP ._.
sThe_NNP Census_NNP dataset_NN is_VBZ a_DT version_NN of_IN the_DT US_NNP Census_NNP Bureau_NNP ``_`` Adult_JJ ''_'' dataset_NN ,_, which_WDT is_VBZ publicly_RB available_JJ from_IN the_DT Silicon_NNP Graphic_NNP =_SYM -_: =_JJ s_NN website_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
sThe_NNP Galaxy_NNP Dim_NNP dataset_NN used_VBN in_IN galaxy_NN discrimination_NN with_IN neural_JJ networks_NNS from_IN -LRB-_-LRB- 30_CD -RRB-_-RRB- sTwo_NN large_JJ datasets_NNS -LRB-_-LRB- 2_CD million_CD points_NNS and_CC 10_CD attributes_NNS -RRB-_-RRB- created_VBN using_VBG David_NNP Musicant_NNP 's_POS NDC_NNP Data_NNP Generator_NNP -LRB-_-LRB- 29_CD -RRB-_-RRB- ._.
sTh_NN
ttributes_NNS -RRB-_-RRB- created_VBN using_VBG David_NNP Musicant_NNP 's_POS NDC_NNP Data_NNP Generator_NNP -LRB-_-LRB- 29_CD -RRB-_-RRB- ._.
sThe_FW Spiral_FW dataset_NN proposed_VBN by_IN Alexis_NNP Wieland_NNP of_IN the_DT MITRE_NNP Corporation_NNP and_CC available_JJ from_IN the_DT CMU_NNP Artificial_NNP Intelligence_NNP Repository_NNP =_SYM -_: =[_NN 37_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP outline_VBP our_PRP$ computational_JJ results_NNS now_RB in_IN five_CD groups_NNS as_IN follows_VBZ ._.
1_CD ._.
Table_NNP 1_CD :_: Comparison_NN of_IN seven_CD different_JJ methods_NNS on_IN the_DT Adult_JJ dataset_NN In_IN this_DT experiment_NN we_PRP compared_VBD the_DT performance_NN of_IN seven_CD dif_NN
cause_VB the_DT solution_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- for_IN u_NN entails_VBZ the_DT inversion_NN of_IN a_DT possibly_RB massive_JJ m_NN x_NN m_NN matrix_NN ,_, we_PRP make_VBP immediate_JJ use_NN of_IN the_DT Sherman-Morrison-Woodbury_JJ formula_NN -LRB-_-LRB- 14_CD ,_, p._NN 51_CD -RRB-_-RRB- for_IN matrix_NN inversion_NN ,_, as_IN was_VBD done_VBN in_IN =_JJ -_: =[_NN 23_CD ,_, 10_CD ,_, 24_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT results_VBZ in_IN :_: u_NN ,_, -LRB-_-LRB- I_PRP -_: H_NN -LRB-_-LRB- +_CC H_NN `_`` H_NN -RRB-_-RRB- H_NN '_'' -RRB-_-RRB- e._NN -LRB-_-LRB- 15_CD -RRB-_-RRB- This_DT expression_NN ,_, as_RB well_RB as_IN another_DT simple_JJ expression_NN -LRB-_-LRB- 29_CD -RRB-_-RRB- for_IN -LRB-_-LRB- -RRB-_-RRB- below_IN ,_, involve_VBP the_DT inversion_NN of_IN a_DT much_RB smaller_JJR dimensional_JJ matrLx_NN of_IN order_NN -LRB-_-LRB- n_NN +_CC 1_LS -RRB-_-RRB- x_NN
ult_FW dataset_FW In_IN this_DT experiment_NN we_PRP compared_VBD the_DT performance_NN of_IN seven_CD different_JJ methods_NNS for_IN linear_JJ classification_NN on_IN different_JJ sized_VBN versions_NNS of_IN the_DT Adult_NN dataset_NN ._.
Reported_VBN results_NNS on_IN the_DT SOR_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, SMO_NN =_JJ -_: =[_NN 31_CD -RRB-_-RRB- and_CC SVM_NN a_DT -_: =_JJ -_: gent_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- are_VBP from_IN -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
F_NN -LRB-_-LRB- esults_NNS for_IN LSVM_NN -LRB-_-LRB- 24_CD -RRB-_-RRB- resuks_NNS were_VBD computed_VBN here_RB using_VBG ``_`` 1ocopl_NN ''_'' ,_, whereas_IN SSVM_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- and_CC F_NN -LRB-_-LRB- LP_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- are_VBP from_IN -LRB-_-LRB- 18_CD -RRB-_-RRB- ._.
The_DT SMO_NN experiments_NNS were_VBD run_VBN on_IN a_DT 266_CD MHz_NN Pentium_NN II_CD p_NN
nt_NN Ai_NN in_IN the_DT class_NN A_NN +_CC or_CC A_DT -_: as_IN specified_VBN by_IN a_DT given_VBN m_NN x_NN m_NN diagonal_JJ matrLx_NN D_NN with_IN plus_CC ones_NNS or_CC minus_NN ones_NNS along_IN its_PRP$ diagonal_NN ._.
For_IN this_DT problem_NN ,_, the_DT standard_JJ support_NN vector_NN machine_NN with_IN a_DT linear_JJ kernel_NN =_JJ -_: =[_NN 35_CD ,_, 6_CD -RRB-_-RRB- is_VBZ g_NN -_: =_JJ -_: iven_VBN by_IN the_DT following_JJ quadratic_JJ program_NN with_IN parameters0_NN :_: i_LS \/_: min_NN e\/y_NN +_CC w_NN w_NN -LRB-_-LRB- w_NN ,_, ''_'' \/_: ,_, y_NN -RRB-_-RRB- GRn_NN +_CC l_NN +_CC m_NN s.t._NN D_NN -LRB-_-LRB- Aw_UH -_: ef_NN -RRB-_-RRB- +_CC y_FW __FW e_LS -LRB-_-LRB- 2_CD -RRB-_-RRB- 71_CD __NN -RRB-_-RRB- 0_CD ._.
As_IN depicted_VBN in_IN Figure_NNP 1_CD ,_, w_NN is_VBZ the_DT normal_JJ to_TO the_DT bounding_VBG planes_NNS :_: x_NN '_''
seconds_NNS ._.
All_DT computational_JJ results_NNS are_VBP based_VBN on_IN 6_CD lines_NNS of_IN MATLAB_NNP code_NN ._.
Keywords_NNS data_NNS classification_NN ,_, tions_NNS support_VBP vector_NN machines_NNS ,_, linear_JJ equa1_NN ._.
INTRODUCTION_NNP Standard_NNP support_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- =_JJ -_: =[_NN 36_CD ,_, 6_CD ,_, 3_CD ,_, 5_CD ,_, 20_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT are_VBP powerful_JJ tools_NNS for_IN data_NNS classification_NN ,_, classify_VB points_NNS by_IN assigning_VBG them_PRP to_TO one_CD of_IN two_CD disjoint_NN halfspaces_NNS ._.
These_DT halfspaces_NNS are_VBP either_CC in_IN the_DT original_JJ input_NN space_NN of_IN the_DT problem_NN for_IN line_NN
-RRB-_-RRB- constitute_VBP the_DT method_NN of_IN choice_NN for_IN classification_NN problems_NNS while_IN the_DT generalized_JJ eigenvalue_NN problem_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- is_VBZ a_DT simple_JJ problem_NN of_IN classical_JJ linear_JJ algebra_NN solvable_JJ by_IN a_DT single_JJ command_NN of_IN MATLAB_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: or_CC Scilab_NNP -LRB-_-LRB- 24_CD -RRB-_-RRB- or_CC by_IN using_VBG standard_JJ linear_JJ algebra_NN software_NN such_JJ LAPACK_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
In_IN proximal_JJ support_NN vector_NN classification_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, two_CD parallel_JJ planes_NNS are_VBP generated_VBN such_JJ that_IN each_DT plane_NN is_VBZ closes_NNS
Simplifying_VBG -LRB-_-LRB- 3_CD -RRB-_-RRB- gives_VBZ :_: kAw_NN min_NN ðw_NN ;_: Þ60_NN e_LS k_NN 2_CD kBw_NN :_: 2_CD e_LS k_NN ð4Þ_NN We_PRP now_RB introduce_VBP a_DT Tikhonov_NNP regularization_NN term_NN -LRB-_-LRB- 26_CD -RRB-_-RRB- that_WDT is_VBZ often_RB used_VBN to_TO regularize_VB least_JJS squares_NNS and_CC mathematical_JJ programming_NN problems_NNS =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 25_CD -RRB-_-RRB- that_WDT reduces_VBZ the_DT norm_NN of_IN the_DT problem_NN variables_NNS ðw_NN ;_: Þ_NN that_WDT determine_VBD the_DT proximal_JJ planes_NNS -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
Thus_RB ,_, for_IN a_DT nonnegative_JJ parameter_NN we_PRP regularize_VBP our_PRP$ problem_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- as_IN follows_VBZ :_: ,_, kAw_NN e_SYM k_NN min_NN
parating_VBG plane_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- as_IN depicted_VBN in_IN Figure_NNP 1_CD ._.
This_DT plane_NN acts_VBZ as_IN a_DT linear_JJ classifier_NN as_IN follows_VBZ :_: 0_CD ,_, thenxEA_NN +_CC ,_, x_NN '_'' w_SYM -_: 7_CD 0_CD ,_, then_RB x_NN E_NN A_NN -_: ,_, -LRB-_-LRB- 7_CD -RRB-_-RRB- 0_CD ,_, thenxEA_NN +_CC orxEA_NN -_: ._.
Our_PRP$ point_NN of_IN departure_NN is_VBZ similar_JJ to_TO that_DT of_IN =_JJ -_: =[_NN 23_CD ,_, 24_CD -RRB-_-RRB- ,_, whe_NN -_: =_SYM -_: re_IN the_DT optimization_NN problem_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- is_VBZ replaced_VBN by_IN the_DT following_JJ problem_NN :_: -LCB-_-LRB- -LRB-_-LRB- $_$ ,_, $_$ +_CC min_NN 117111_CD +_CC -LRB-_-LRB- ,_, ,_, y_NN -RRB-_-RRB- +_CC +_CC ''_'' -LRB-_-LRB- 8_CD -RRB-_-RRB- s.t._NN D_NN -LRB-_-LRB- Aw_UH -_: e7_NN -RRB-_-RRB- +_CC y_FW __FW -RRB-_-RRB- e_SYM Note_VB that_IN no_DT explicit_JJ nonnegativky_JJ constraint_NN is_VBZ needed_VBN on_IN y_NN ,_, becaus_NN
approach_NN in_IN both_CC computation_NN time_NN and_CC test_NN set_NN correctness_NN ._.
Index_NN Terms_NNS --_: Support_NN vector_NN machines_NNS ,_, proximal_JJ classification_NN ,_, generalized_JJ eigenvalues_NNS ._.
1_CD INTRODUCTION_NN SUPPORT_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 27_CD -RRB-_-RRB- constitute_VBP the_DT method_NN of_IN choice_NN for_IN classification_NN problems_NNS while_IN the_DT generalized_JJ eigenvalue_NN problem_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- is_VBZ a_DT simple_JJ problem_NN of_IN classical_JJ linear_JJ algebra_NN solvable_JJ by_IN a_DT single_JJ command_NN of_IN MA_NN
ances_NNS in_IN the_DT ðw_NN ;_: Þ-space_NN of_IN points_NNS in_IN class_NN 1_CD to_TO the_DT plane_NN x_NN 0_CD w_NN 0_CD ,_, while_IN the_DT denominator_NN of_IN -LRB-_-LRB- 3_CD -RRB-_-RRB- is_VBZ the_DT sum_NN of_IN squares_NNS of_IN twonorm_NN distances_NNS in_IN the_DT ðw_NN ;_: Þ-space_NN of_IN points_NNS in_IN class_NN 2_CD to_TO the_DT same_JJ plane_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Simplifying_VBG -LRB-_-LRB- 3_CD -RRB-_-RRB- gives_VBZ :_: kAw_NN min_NN ðw_NN ;_: Þ60_NN e_LS k_NN 2_CD kBw_NN :_: 2_CD e_LS k_NN ð4Þ_NN We_PRP now_RB introduce_VBP a_DT Tikhonov_NNP regularization_NN term_NN -LRB-_-LRB- 26_CD -RRB-_-RRB- that_WDT is_VBZ often_RB used_VBN to_TO regularize_VB least_JJS squares_NNS and_CC mathematical_JJ programming_NN problem_NN
oposed_VBN approach_NN in_IN both_CC computation_NN time_NN and_CC test_NN set_NN correctness_NN ._.
Index_NN Terms_NNS --_: Support_NN vector_NN machines_NNS ,_, proximal_JJ classification_NN ,_, generalized_JJ eigenvalues_NNS ._.
1_CD INTRODUCTION_NN SUPPORT_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 4_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 27_CD -RRB-_-RRB- constitute_VBP the_DT method_NN of_IN choice_NN for_IN classification_NN problems_NNS while_IN the_DT generalized_JJ eigenvalue_NN problem_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- is_VBZ a_DT simple_JJ problem_NN of_IN classical_JJ linear_JJ algebra_NN solvable_JJ by_IN a_DT single_JJ command_NN
versions_NNS of_IN the_DT Adult_NN dataset_NN ._.
Reported_VBN results_NNS on_IN the_DT SOR_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, SMO_NN -LRB-_-LRB- 31_CD -RRB-_-RRB- and_CC SVM_NN agent_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- are_VBP from_IN -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
F_NN -LRB-_-LRB- esults_NNS for_IN LSVM_NN -LRB-_-LRB- 24_CD -RRB-_-RRB- resuks_NNS were_VBD computed_VBN here_RB using_VBG ``_`` 1ocopl_NN ''_'' ,_, whereas_IN SSVM_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- =_JJ -_: =_JJ and_CC F_NN -LRB-_-LRB- LP_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP from_IN -LRB-_-LRB- 18_CD -RRB-_-RRB- ._.
The_DT SMO_NN experiments_NNS were_VBD run_VBN on_IN a_DT 266_CD MHz_NN Pentium_NN II_CD processor_NN under_IN Windows_NNP NT_NNP 4_CD using_VBG Microsoft_NNP 's_POS Visual_JJ C_NN +_CC +_CC 5.0_CD compiler_NN ._.
The_DT SOl_FW :[_FW experiments_NNS were_VBD run_VBN on_IN a_DT 200_CD MHz_NN Pentium_FW Pro_FW wit_NN
tion_NN ,_, generalized_JJ eigenvalues_NNS ._.
1_CD INTRODUCTION_NN SUPPORT_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 4_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 27_CD -RRB-_-RRB- constitute_VBP the_DT method_NN of_IN choice_NN for_IN classification_NN problems_NNS while_IN the_DT generalized_JJ eigenvalue_NN problem_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ a_DT simple_JJ problem_NN of_IN classical_JJ linear_JJ algebra_NN solvable_JJ by_IN a_DT single_JJ command_NN of_IN MATLAB_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- or_CC Scilab_NNP -LRB-_-LRB- 24_CD -RRB-_-RRB- or_CC by_IN using_VBG standard_JJ linear_JJ algebra_NN software_NN such_JJ LAPACK_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
In_IN proximal_JJ support_NN vector_NN cla_NN
ts_NNS with_IN respect_NN to_TO iv_NN and_CC 7_CD ._.
We_PRP turn_VBP now_RB to_TO our_PRP$ computations_NNS ._.
The_DT datasets_NNS used_VBN for_IN our_PRP$ numerical_JJ tests_NNS were_VBD the_DT following_NN :_: sSeven_NN publicly_RB available_JJ datasets_NNS from_IN the_DT UCI_NNP Machine_NNP Learning_NNP F_NN -LRB-_-LRB- epository_NN =_JJ -_: =[_NN 28_CD -RRB-_-RRB- :_: WPBC_NN ,_, Io_NNP -_: =_JJ -_: nosphere_NN ,_, Cleveland_NNP Heart_NNP ,_, Pima_NNP Indians_NNPS ,_, BUPA_NN Liver_NN ,_, Mushroom_NNP ,_, Tic-Tac-Toe_NNP ._.
sThe_NNP Census_NNP dataset_NN is_VBZ a_DT version_NN of_IN the_DT US_NNP Census_NNP Bureau_NNP ``_`` Adult_JJ ''_'' dataset_NN ,_, which_WDT is_VBZ publicly_RB available_JJ from_IN the_DT Sili_NN
larger_JJR ._.
For_IN an_DT interior_JJ point_NN method_NN used_VBN for_IN solving_VBG a_DT two-norm_JJ SVM_NN quadratic_JJ program_NN ,_, the_DT complexity_NN is_VBZ of_IN order_NN n_NN 3:5_CD based_VBN on_IN a_DT linear_JJ complementarity_NN problem_NN formulation_NN of_IN the_DT quadratic_JJ program_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
These_DT facts_NNS help_VBP explain_VB the_DT computation_NN times_NNS of_IN Table_NNP 3_CD ,_, where_WRB PSVM_NN is_VBZ over_IN one_CD order_NN of_IN magnitude_NN faster_JJR TABLE_NN 3_CD Average_NNP Time_NNP to_TO Learn_VB One_CD Linear_NNP Kernel_NNP GEPSVM_NNP ,_, PSVM_NNP -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, and_CC SVM-Light_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- on_IN the_DT
k-plane_NN clustering_NN of_IN -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, where_WRB clusters_NNS are_VBP determined_VBN by_IN proximity_NN to_TO various_JJ nonparallel_JJ planes_NNS based_VBN on_IN the_DT smallest_JJS eigenvector_NN of_IN a_DT matrix_NN generated_VBN by_IN given_VBN data_NNS points_NNS ._.
We_PRP also_RB note_VBP that_IN ,_, in_IN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT generalized_JJ eigenvalue_JJ formulation_NN was_VBD used_VBN for_IN protein_NN fold_NN recognition_NN to_TO determine_VB an_DT optimal_JJ transformation_NN of_IN a_DT permutation_NN matrix_NN based_VBN on_IN simultaneously_RB minimizing_VBG within-class_NN ._.
O.L._NNP Man_NNP
fying_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- gives_VBZ :_: kAw_NN min_NN ðw_NN ;_: Þ60_NN e_LS k_NN 2_CD kBw_NN :_: 2_CD e_LS k_NN ð4Þ_NN We_PRP now_RB introduce_VBP a_DT Tikhonov_NNP regularization_NN term_NN -LRB-_-LRB- 26_CD -RRB-_-RRB- that_WDT is_VBZ often_RB used_VBN to_TO regularize_VB least_JJS squares_NNS and_CC mathematical_JJ programming_NN problems_NNS -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 25_CD -RRB-_-RRB- that_WDT reduces_VBZ the_DT norm_NN of_IN the_DT problem_NN variables_NNS ðw_NN ;_: Þ_NN that_WDT determine_VBD the_DT proximal_JJ planes_NNS -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
Thus_RB ,_, for_IN a_DT nonnegative_JJ parameter_NN we_PRP regularize_VBP our_PRP$ problem_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- as_IN follows_VBZ :_: ,_, kAw_NN e_SYM k_NN min_NN ðw_NN ;_: Þ_NN
e_LS method_NN of_IN choice_NN for_IN classification_NN problems_NNS while_IN the_DT generalized_JJ eigenvalue_NN problem_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- is_VBZ a_DT simple_JJ problem_NN of_IN classical_JJ linear_JJ algebra_NN solvable_JJ by_IN a_DT single_JJ command_NN of_IN MATLAB_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- or_CC Scilab_NN =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_JJ -_: or_CC by_IN using_VBG standard_JJ linear_JJ algebra_NN software_NN such_JJ LAPACK_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
In_IN proximal_JJ support_NN vector_NN classification_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, two_CD parallel_JJ planes_NNS are_VBP generated_VBN such_JJ that_IN each_DT plane_NN is_VBZ closest_JJS to_TO one_CD of_IN two_CD
3632_CD ,_, by_IN Air_NNP Force_NNP Office_NNP of_IN Scientific_NNP Research_NNP Grant_NNP F49620-00-1-0085_NN and_CC by_IN the_DT Microsof_NNP Corporation_NNP ._.
We_PRP are_VBP grateful_JJ to_TO Professor_NNP C.-J_NNP ._.
Lin_NNP of_IN National_NNP Taiwan_NNP University_NNP who_WP pointed_VBD out_RP reference_NN =_JJ -_: =[_NN 33_CD -RRB-_-RRB- -_: =_JJ -_: ,_, upon_IN reading_VBG the_DT original_JJ version_NN of_IN this_DT paper_NN ._.
Least_NNP squares_NNS are_VBP also_RB used_VBN in_IN -LRB-_-LRB- 33_CD -RRB-_-RRB- to_TO construct_VB an_DT SVM_NN ,_, but_CC with_IN the_DT explicit_JJ requirement_NN of_IN Mercer_NNP 's_POS positive_JJ definiteness_NN condition_NN -LRB-_-LRB- 35_CD -RRB-_-RRB- ,_, which_WDT is_VBZ
e_LS space_NN -RRB-_-RRB- that_WDT are_VBP pushed_VBN apart_RB as_RB far_RB as_IN possible_JJ ._.
This_DT formulation_NN ,_, which_WDT can_MD also_RB be_VB interpreted_VBN as_IN regularized_VBN least_JJS squares_NNS and_CC considered_VBN in_IN the_DT much_RB more_RBR general_JJ context_NN of_IN regularized_VBN networks_NNS =_JJ -_: =[_NN 8_CD ,_, 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, leads_VBZ to_TO an_DT extremely_RB fast_JJ and_CC simple_JJ algorithm_NN for_IN generating_VBG a_DT linear_JJ or_CC nonlinear_JJ classifier_NN that_WDT merely_RB requires_VBZ the_DT solution_NN of_IN a_DT single_JJ system_NN of_IN linear_JJ equations_NNS ._.
In_IN contrast_NN ,_, standard_JJ SVMs_NNS s_NN
seconds_NNS ._.
All_DT computational_JJ results_NNS are_VBP based_VBN on_IN 6_CD lines_NNS of_IN MATLAB_NNP code_NN ._.
Keywords_NNS data_NNS classification_NN ,_, tions_NNS support_VBP vector_NN machines_NNS ,_, linear_JJ equa1_NN ._.
INTRODUCTION_NNP Standard_NNP support_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- =_JJ -_: =[_NN 36_CD ,_, 6_CD ,_, 3_CD ,_, 5_CD ,_, 20_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT are_VBP powerful_JJ tools_NNS for_IN data_NNS classification_NN ,_, classify_VB points_NNS by_IN assigning_VBG them_PRP to_TO one_CD of_IN two_CD disjoint_NN halfspaces_NNS ._.
These_DT halfspaces_NNS are_VBP either_CC in_IN the_DT original_JJ input_NN space_NN of_IN the_DT problem_NN for_IN line_NN
seconds_NNS ._.
All_DT computational_JJ results_NNS are_VBP based_VBN on_IN 6_CD lines_NNS of_IN MATLAB_NNP code_NN ._.
Keywords_NNS data_NNS classification_NN ,_, tions_NNS support_VBP vector_NN machines_NNS ,_, linear_JJ equa1_NN ._.
INTRODUCTION_NNP Standard_NNP support_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- =_JJ -_: =[_NN 36_CD ,_, 6_CD ,_, 3_CD ,_, 5_CD ,_, 20_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT are_VBP powerful_JJ tools_NNS for_IN data_NNS classification_NN ,_, classify_VB points_NNS by_IN assigning_VBG them_PRP to_TO one_CD of_IN two_CD disjoint_NN halfspaces_NNS ._.
These_DT halfspaces_NNS are_VBP either_CC in_IN the_DT original_JJ input_NN space_NN of_IN the_DT problem_NN for_IN line_NN
of_IN the_DT number_NN of_IN data_NNS points_NNS needs_VBZ to_TO be_VB solved_VBN ._.
This_DT allows_VBZ us_PRP to_TO easily_RB classify_VB datasets_NNS with_IN as_RB many_JJ as_IN a_DT few_JJ thousand_CD of_IN points_NNS ._.
For_IN larger_JJR datasets_NNS ,_, data_NNS selection_NN and_CC reduction_NN methods_NNS such_JJ as_IN =_JJ -_: =[_NN 11_CD ,_, 17_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: can_MD be_VB utilized_VBN as_IN indicated_VBN by_IN some_DT of_IN our_PRP$ numerical_JJ results_NNS and_CC will_MD be_VB the_DT subject_NN of_IN future_JJ work_NN ._.
Our_PRP$ computational_JJ results_NNS demonstrate_VBP that_IN PSVM_NN classifiers_NNS obtain_VBP test_NN set_NN correctness_NN statistic_NN
ares_NNS of_IN twonorm_NN distances_NNS in_IN the_DT ðw_NN ;_: Þ-space_NN of_IN points_NNS in_IN class_NN 2_CD to_TO the_DT same_JJ plane_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
Simplifying_VBG -LRB-_-LRB- 3_CD -RRB-_-RRB- gives_VBZ :_: kAw_NN min_NN ðw_NN ;_: Þ60_NN e_LS k_NN 2_CD kBw_NN :_: 2_CD e_LS k_NN ð4Þ_NN We_PRP now_RB introduce_VBP a_DT Tikhonov_NNP regularization_NN term_NN =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_SYM -_: that_WDT is_VBZ often_RB used_VBN to_TO regularize_VB least_JJS squares_NNS and_CC mathematical_JJ programming_NN problems_NNS -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 25_CD -RRB-_-RRB- that_WDT reduces_VBZ the_DT norm_NN of_IN the_DT problem_NN variables_NNS ðw_NN ;_: Þ_NN that_WDT determine_VBD the_DT proximal_JJ planes_NNS -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
seconds_NNS ._.
All_DT computational_JJ results_NNS are_VBP based_VBN on_IN 6_CD lines_NNS of_IN MATLAB_NNP code_NN ._.
Keywords_NNS data_NNS classification_NN ,_, tions_NNS support_VBP vector_NN machines_NNS ,_, linear_JJ equa1_NN ._.
INTRODUCTION_NNP Standard_NNP support_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- =_JJ -_: =[_NN 36_CD ,_, 6_CD ,_, 3_CD ,_, 5_CD ,_, 20_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT are_VBP powerful_JJ tools_NNS for_IN data_NNS classification_NN ,_, classify_VB points_NNS by_IN assigning_VBG them_PRP to_TO one_CD of_IN two_CD disjoint_NN halfspaces_NNS ._.
These_DT halfspaces_NNS are_VBP either_CC in_IN the_DT original_JJ input_NN space_NN of_IN the_DT problem_NN for_IN line_NN
which_WDT points_NNS of_IN the_DT sets_NNS A_NN +_CC and_CC A_NN -_: cluster_NN and_CC which_WDT are_VBP pushed_VBN apart_RB by_IN the_DT optimization_NN problem_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
We_PRP note_VBP that_IN our_PRP$ formulation_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- can_MD be_VB also_RB interpreted_VBN as_IN a_DT regularized_VBN least_JJS squares_NNS solution_NN =_JJ -_: =[_NN 34_CD -RRB-_-RRB- -_: =_SYM -_: of_IN the_DT system_NN of_IN linear_JJ equations_NNS D_NN -LRB-_-LRB- Aw_UH -_: el_NN -RRB-_-RRB- e_LS ,_, that_WDT is_VBZ finding_VBG an_DT approximate_JJ solution_NN -LRB-_-LRB- w_NN ,_, 7_CD -RRB-_-RRB- with_IN least_JJS 2-norm_JJ ._.
Similarly_RB the_DT standard_JJ SVM_NN formulation_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- can_MD be_VB interpreted_VBN ,_, by_IN using_VBG linear_JJ progra_NN
experiment_NN we_PRP compared_VBD the_DT performance_NN of_IN seven_CD different_JJ methods_NNS for_IN linear_JJ classification_NN on_IN different_JJ sized_VBN versions_NNS of_IN the_DT Adult_NN dataset_NN ._.
Reported_VBN results_NNS on_IN the_DT SOR_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, SMO_NN -LRB-_-LRB- 31_CD -RRB-_-RRB- and_CC SVM_NN agent_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- are_VBP from_IN -_: =-[_CD 22_CD -RRB-_-RRB- ._.
F_NN -LRB-_-LRB- esults_NNS for_IN LSVM_NN -LRB-_-LRB- 24_CD -RRB-_-RRB- resuks_NNS were_VBD computed_VBN here_RB using_VBG ``_`` 1ocopl_NN ''_'' ,_, whereas_IN SSVM_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- and_CC F_NN -LRB-_-LRB- LP_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- are_VBP from_IN -LRB-_-LRB- 18_CD -RRB-_-RRB- ._.
The_DT SMO_NN experiments_NNS were_VBD run_VBN on_IN a_DT 266_CD MHz_NN Pentium_NN II_CD processor_NN under_IN Wind_NN
zed_VBN instead_RB of_IN the_DT 1-norm_NN ,_, and_CC the_DT margin_NN between_IN the_DT bounding_VBG planes_NNS is_VBZ maximized_VBN with_IN respect_NN to_TO both_CC orientation_NN w_NN and_CC relative_JJ location_NN to_TO the_DT origin_NN if_IN ._.
Extensive_JJ computational_JJ experience_NN ,_, as_IN in_IN =_JJ -_: =[_NN 22_CD ,_, 23_CD ,_, 24_CD ,_, 18_CD ,_, 17_CD -RRB-_-RRB- -_: =_SYM -_: indicates_VBZ that_IN this_DT formulation_NN is_VBZ just_RB as_RB good_JJ as_IN the_DT classical_JJ formulation_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- with_IN some_DT added_VBN advantages_NNS such_JJ as_IN strong_JJ convexity_NN of_IN the_DT objective_JJ function_NN ._.
Our_PRP$ key_JJ idea_NN in_IN this_DT present_JJ paper_NN is_VBZ t_NN
generalized_VBN eigenvalue_JJ problem_NN -LRB-_-LRB- 28_CD -RRB-_-RRB- corresponding_VBG to_TO a_DT smallest_JJS eigenvalue_NN ._.
We_PRP note_VBP immediately_RB that_IN ,_, if_IN either_CC m1_NN or_CC m2_NN are_VBP large_JJ ,_, the_DT techniques_NNS of_IN the_DT reduced_VBN support_NN vector_NN machine_NN classification_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: can_MD be_VB easily_RB applied_VBN to_TO reduce_VB the_DT dimensionality_NN m_NN þ_NN 1_CD m1_NN þ_NN m2_NN þ_NN 1_CD of_IN the_DT generalized_JJ eigenvalue_NN problem_NN -LRB-_-LRB- 27_CD -RRB-_-RRB- to_TO m_NN þ_NN 1_CD by_IN replacing_VBG the_DT kernels_NNS KðA_NN ;_: C_NN 0_CD Þ_NN ,_, TABLE_NN 1_CD Linear_NNP Kernel_NNP GEPSVM_NNP ,_, PSVM_NNP -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_,
parating_VBG plane_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- as_IN depicted_VBN in_IN Figure_NNP 1_CD ._.
This_DT plane_NN acts_VBZ as_IN a_DT linear_JJ classifier_NN as_IN follows_VBZ :_: 0_CD ,_, thenxEA_NN +_CC ,_, x_NN '_'' w_SYM -_: 7_CD 0_CD ,_, then_RB x_NN E_NN A_NN -_: ,_, -LRB-_-LRB- 7_CD -RRB-_-RRB- 0_CD ,_, thenxEA_NN +_CC orxEA_NN -_: ._.
Our_PRP$ point_NN of_IN departure_NN is_VBZ similar_JJ to_TO that_DT of_IN =_JJ -_: =[_NN 23_CD ,_, 24_CD -RRB-_-RRB- ,_, whe_NN -_: =_SYM -_: re_IN the_DT optimization_NN problem_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- is_VBZ replaced_VBN by_IN the_DT following_JJ problem_NN :_: -LCB-_-LRB- -LRB-_-LRB- $_$ ,_, $_$ +_CC min_NN 117111_CD +_CC -LRB-_-LRB- ,_, ,_, y_NN -RRB-_-RRB- +_CC +_CC ''_'' -LRB-_-LRB- 8_CD -RRB-_-RRB- s.t._NN D_NN -LRB-_-LRB- Aw_UH -_: e7_NN -RRB-_-RRB- +_CC y_FW __FW -RRB-_-RRB- e_SYM Note_VB that_IN no_DT explicit_JJ nonnegativky_JJ constraint_NN is_VBZ needed_VBN on_IN y_NN ,_, becaus_NN
