Learning_NNP classifiers_NNS from_IN only_RB positive_JJ and_CC unlabeled_JJ data_NNS
The_DT input_NN to_TO an_DT algorithm_NN that_WDT learns_VBZ a_DT binary_JJ classifier_NN normally_RB consists_VBZ of_IN two_CD sets_NNS of_IN examples_NNS ,_, where_WRB one_CD set_NN consists_VBZ of_IN positive_JJ examples_NNS of_IN the_DT concept_NN to_TO be_VB learned_VBN ,_, and_CC the_DT other_JJ set_NN consists_VBZ of_IN negative_JJ examples_NNS ._.
However_RB ,_, it_PRP is_VBZ often_RB the_DT case_NN that_IN the_DT available_JJ training_NN data_NNS are_VBP an_DT incomplete_JJ set_NN of_IN positive_JJ examples_NNS ,_, and_CC a_DT set_NN of_IN unlabeled_JJ examples_NNS ,_, some_DT of_IN which_WDT are_VBP positive_JJ and_CC some_DT of_IN which_WDT are_VBP negative_JJ ._.
The_DT problem_NN solved_VBD in_IN this_DT paper_NN is_VBZ how_WRB to_TO learn_VB a_DT standard_JJ binary_JJ classifier_NN given_VBN a_DT nontraditional_JJ training_NN set_NN of_IN this_DT nature_NN ._.
Under_IN the_DT assumption_NN that_IN the_DT labeled_JJ examples_NNS are_VBP selected_VBN randomly_RB from_IN the_DT positive_JJ examples_NNS ,_, we_PRP show_VBP that_IN a_DT classifier_NN trained_VBN on_IN positive_JJ and_CC unlabeled_JJ examples_NNS predicts_VBZ probabilities_NNS that_WDT differ_VBP by_IN only_RB a_DT constant_JJ factor_NN from_IN the_DT true_JJ conditional_JJ probabilities_NNS of_IN being_VBG positive_JJ ._.
We_PRP show_VBP how_WRB to_TO use_VB this_DT result_NN in_IN two_CD different_JJ ways_NNS to_TO learn_VB a_DT classifier_NN from_IN a_DT nontraditional_JJ training_NN set_NN ._.
We_PRP then_RB apply_VBP these_DT two_CD new_JJ methods_NNS to_TO solve_VB a_DT real-world_JJ problem_NN :_: identifying_VBG protein_NN records_NNS that_WDT should_MD be_VB included_VBN in_IN an_DT incomplete_JJ specialized_JJ molecular_JJ biology_NN database_NN ._.
Our_PRP$ experiments_NNS in_IN this_DT domain_NN show_VBP that_IN models_NNS trained_VBN using_VBG the_DT new_JJ methods_NNS perform_VBP better_RBR than_IN the_DT current_JJ state-of-the-art_JJ biased_VBN SVM_NN method_NN for_IN learning_VBG from_IN positive_JJ and_CC unlabeled_JJ examples_NNS ._.
sparser_JJR query_NN vectors_NNS ;_: both_DT and_CC work_NN much_RB better_JJR than_IN the_DT Rocchio_NNP algorithm_NN ,_, with_IN being_VBG much_RB faster_RBR than_IN ._.
4_LS ._.
Related_NNP Work_NNP PU_NN Learning_NNP has_VBZ been_VBN studied_VBN recently_RB in_IN the_DT context_NN of_IN text_NN classification_NN =_JJ -_: =[_NN 3_CD ,_, 2_CD ,_, 5_CD ,_, 12_CD ,_, 18_CD ,_, 17_CD ,_, 15_CD ,_, 7_CD ,_, 35_CD ,_, 34_CD ,_, 37_CD ,_, 38_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Actually_RB •_FW •_FW •_FW it_PRP should_MD be_VB more_RBR effective_JJ because_IN it_PRP directly_RB optimises_VBZ meaningful_JJ multivariate_JJ performance_NN measures_NNS ;_: it_PRP is_VBZ hundreds_NNS of_IN times_NNS more_RBR efficient_JJ because_IN it_PRP only_RB needs_VBZ to_TO train_VB one_CD SVM_NN c_NN
tive_JJ training_NN data_NNS ,_, we_PRP use_VBP PU_NN learning_VBG to_TO solve_VB the_DT problem_NN -LRB-_-LRB- Liu_NNP et_FW al._FW 2002_CD ;_: Yu_NNP et_FW al._FW 2002_CD ;_: Denis_NNP et_FW al._FW 2002_CD ;_: Li_NNP et_FW al._FW 2003_CD ;_: Lee_NNP and_CC Liu_NNP ,_, 2003_CD ;_: Liu_NNP et_FW al._FW 2003_CD ;_: Denis_NNP et_FW al._FW 2003_CD ;_: Li_NNP et_FW al._FW 2007_CD ;_: =_JJ -_: =_JJ Elkan_NNP and_CC Noto_NNP ,_, 2008_CD -_: =_JJ -_: ;_: Li_NNP et_FW al._FW 2009_CD ;_: Li_NNP et_FW al._FW 2010_CD -RRB-_-RRB- ._.
We_PRP will_MD discuss_VB this_DT learning_NN model_NN further_RBR in_IN Section_NNP 3_CD ._.
Another_DT related_JJ work_NN to_TO ours_PRP is_VBZ transfer_NN learning_NN or_CC domain_NN adaptation_NN ._.
Unlike_IN our_PRP$ problem_NN setting_NN ,_, transfe_NN
articles_NNS ._.
The_DT system_NN then_RB adjusts_VBZ the_DT trained_JJ model_NN mathematically_RB to_TO account_VB for_IN the_DT fact_NN that_IN a_DT small_JJ fraction_NN of_IN unlabeled_JJ articles_NNS are_VBP actually_RB positive_JJ ;_: for_IN details_NNS see_VBP Elkan_NNP and_CC Noto_NNP ,_, KDD_NNP 2008_CD =_SYM -_: =[_NN 68_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT features_NNS that_IN PMAC_NN extracts_NNS from_IN PubMed_NNP are_VBP •_JJ Words_NNS in_IN an_DT article_NN 's_POS abstract_JJ ,_, •_JJ Words_NNS in_IN its_PRP$ title_NN ,_, •_CD Author_NN names_NNS and_CC affiliations_NNS ,_, •_NNP Journal_NNP name_NN and_CC publication_NN type_NN ,_, •_NNP Chemical_NNP substances_NNS me_PRP
ied_VBD unbalanced_JJ weights_NNS to_TO positive_JJ and_CC unlabeled_JJ observations_NNS under_IN a_DT maximum_JJ margin_NN framework_NN ._.
This_DT approach_NN doubly_RB penalized_VBD a_DT SVM_NN and_CC demonstrated_VBD good_JJ empirical_JJ performance_NN ._.
A_DT more_RBR recently_RB paper_NN =_JJ -_: =[_NN 61_CD -RRB-_-RRB- -_: =_SYM -_: discussed_VBD an_DT alternative_JJ Bayesian_JJ approach_NN to_TO learning_VBG a_DT classifier_NN from_IN only_RB positive_JJ and_CC unlabeled_JJ observations_NNS ._.
Unfortunately_RB ,_, all_PDT these_DT methods_NNS concentrated_VBD on_IN the_DT very_RB limited_JJ positive_JJ labels_NNS an_DT
ust_NN preferences_NNS of_IN individual_JJ researchers_NNS ._.
Previous_JJ work_NN has_VBZ also_RB considered_VBN the_DT more_RBR general_JJ ,_, yet_RB related_JJ ,_, problem_NN of_IN taking_VBG positive_JJ examples_NNS of_IN membership_NN in_IN a_DT set_NN and_CC using_VBG them_PRP to_TO expand_VB the_DT set_NN =_JJ -_: =[_NN 17_CD ,_, 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
While_IN such_JJ approacheshave_NN been_VBN applied_VBN to_TO the_DT domain_NN of_IN research_NN literature_NN ,_, they_PRP do_VBP not_RB explicitly_RB model_VB the_DT particular_JJ characteristics_NNS of_IN our_PRP$ problem_NN ,_, e.g._FW ,_, the_DT effect_NN of_IN citations_NNS ,_, publication_NN ve_NN
ignificance_JJ standard_JJ supervised_JJ classification_NN methods_NNS assuming_VBG that_IN all_DT unlabeled_JJ examples_NNS are_VBP negative_JJ and_CC using_VBG cross-validation_NN to_TO obtain_VB non-trivial_JJ predictions_NNS for_IN the_DT unlabeled_JJ examples_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 15_CD ,_, 9_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
It_PRP is_VBZ shown_VBN in_IN -LRB-_-LRB- 9_CD -RRB-_-RRB- that_IN ,_, under_IN the_DT assumption_NN that_IN the_DT labeled_JJ examples_NNS are_VBP selected_VBN randomly_RB from_IN the_DT positive_JJ examples_NNS ,_, this_DT approach_NN predicts_VBZ class_NN conditional_JJ probabilities_NNS that_WDT differ_VBP by_IN only_RB
ever_RB ,_, the_DT issue_NN has_VBZ not_RB yet_RB been_VBN investigated_VBN in_IN the_DT context_NN of_IN the_DT task_NN of_IN identifying_VBG articles_NNS that_WDT are_VBP relevant_JJ to_TO a_DT biomedical_JJ database_NN ._.
We_PRP provide_VBP a_DT comparison_NN with_IN the_DT aforementioned_JJ methods_NNS in_IN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC we_PRP believe_VBP that_IN our_PRP$ recent_JJ formalization_NN -LRB-_-LRB- explained_VBN in_IN detail_NN Section_NN 3.2_CD -RRB-_-RRB- demonstrates_VBZ that_IN our_PRP$ approaches_NNS are_VBP well-suited_JJ to_TO this_DT task_NN ._.
s6_NNP Noto_NNP et_FW al._FW 3.1_CD Iterative_JJ relabeling_VBG Our_PRP$ first_JJ approac_NN
eled_VBN negative_JJ set_NN which_WDT is_VBZ typically_RB also_RB provided_VBN as_IN part_NN of_IN the_DT input_NN to_TO a_DT learning_NN algorithm_NN ._.
However_RB ,_, we_PRP recently_RB showed_VBD why_WRB we_PRP were_VBD able_JJ to_TO do_VB almost_RB as_RB well_RB by_IN using_VBG unlabeled_JJ Medline_NNP documents_NNS =_JJ -_: =-LRB-_NN 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT features_NNS that_IN we_PRP use_VBP are_VBP words_NNS that_WDT are_VBP associated_VBN with_IN each_DT document_NN ,_, either_CC by_IN appearing_VBG in_IN the_DT document_NN itself_PRP ,_, or_CC by_IN being_VBG part_NN of_IN a_DT set_NN of_IN keywords_NNS associated_VBN with_IN the_DT document_NN ._.
That_DT is_VBZ ,_, ea_FW
mpletion_NN ''_'' -RRB-_-RRB- ,_, our_PRP$ task_NN more_RBR closely_RB resembles_VBZ traditional_JJ predictive_JJ modeling_NN of_IN a_DT specific_JJ target_NN variable_NN ,_, but_CC with_IN a_DT massive_JJ number_NN of_IN variables_NNS ,_, and_CC technically_RB only_RB positive_JJ and_CC unlabeled_JJ examples_NNS =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Nonetheless_RB ,_, it_PRP may_MD be_VB that_IN CF-style_JJ dimensionality_NN reduction_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- can_MD further_RB improve_VB audience_NN selection_NN ._.
Although_IN we_PRP have_VBP tried_VBN to_TO design_VB the_DT experiments_NNS carefully_RB ,_, there_EX may_MD be_VB some_DT residual_JJ bias_NN
ber_NN of_IN true_JJ negatives_NNS ._.
Finally_RB ,_, precision_NN is_VBZ defined_VBN as_IN p_NN =_JJ a_DT \/_: -LRB-_-LRB- a_DT +_CC c_NN -RRB-_-RRB- and_CC recall_NN as_IN r_NN =_JJ a_DT \/_: -LRB-_-LRB- a_DT +_CC b_NN -RRB-_-RRB- ._.
The_DT basic_JJ learning_NN algorithm_NN for_IN each_DT method_NN is_VBZ an_DT SVM_NN with_IN a_DT linear_JJ kernel_NN as_IN implemented_VBN in_IN libSVM_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN approach_NN -LRB-_-LRB- ii_LS -RRB-_-RRB- we_PRP use_VBP Platt_NNP scaling_VBG to_TO get_VB probability_NN estimates_NNS which_WDT are_VBP then_RB adjusted_VBN using_VBG Lemma_NNP 1_CD ._.
For_IN approach_NN -LRB-_-LRB- iii_LS -RRB-_-RRB- we_PRP run_VBP libSVM_NN twice_RB ._.
The_DT first_JJ run_NN uses_VBZ Platt_NNP scaling_VBG to_TO get_VB probability_NN
0.9465_CD 0.9279_CD 0.9895_CD 621_CD and_CC then_RB -LRB-_-LRB- ii_LS -RRB-_-RRB- to_TO apply_VB a_DT standard_JJ learning_NN method_NN to_TO these_DT examples_NNS and_CC the_DT positive_JJ examples_NNS ;_: steps_NNS -LRB-_-LRB- i_LS -RRB-_-RRB- and_CC -LRB-_-LRB- ii_LS -RRB-_-RRB- may_MD be_VB iterated_VBN ._.
Papers_NNP using_VBG this_DT general_JJ approach_NN include_VBP =_JJ -_: =[_NN 24_CD ,_, 20_CD ,_, 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC the_DT idea_NN has_VBZ been_VBN rediscovered_VBN independently_RB a_DT few_JJ times_NNS ,_, most_RBS recently_RB in_IN -LRB-_-LRB- 22_CD ,_, Section_NNP 2.4_CD -RRB-_-RRB- ._.
The_DT approach_NN is_VBZ sometimes_RB extended_VBN to_TO identify_VB also_RB additional_JJ positive_JJ examples_NNS in_IN the_DT unlabeled_JJ se_FW
st_IN algorithms_NNS based_VBN on_IN this_DT assumption_NN need_VBP p_NN -LRB-_-LRB- y_NN =_JJ 1_CD -RRB-_-RRB- to_TO be_VB an_DT additional_JJ input_NN piece_NN of_IN information_NN ;_: a_DT recent_JJ paper_NN emphasizes_VBZ the_DT importance_NN of_IN p_NN -LRB-_-LRB- y_NN =_JJ 1_CD -RRB-_-RRB- for_IN learning_VBG from_IN positive_JJ and_CC unlabeled_JJ data_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN Section_NN 3_CD above_RB ,_, we_PRP show_VBP how_WRB to_TO estimate_VB p_NN -LRB-_-LRB- y_NN =_JJ 1_CD -RRB-_-RRB- empirically_RB ._.
The_DT most_RBS similar_JJ previous_JJ work_NN to_TO ours_PRP is_VBZ -LRB-_-LRB- 26_CD -RRB-_-RRB- ._.
Their_PRP$ zi_NN 219_CD approach_NN also_RB makes_VBZ the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN and_CC
e_LS result_NN has_VBZ not_RB been_VBN published_VBN before_RB ,_, and_CC it_PRP is_VBZ not_RB obvious_JJ ._.
The_DT reason_NN perhaps_RB that_IN the_DT result_NN is_VBZ novel_JJ is_VBZ that_IN although_IN the_DT learning_NN scenario_NN has_VBZ been_VBN discussed_VBN in_IN many_JJ previous_JJ papers_NNS ,_, including_VBG =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 11_CD ,_, 26_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC these_DT papers_NNS do_VBP make_VB the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN either_CC explicitly_RB or_CC implicitly_RB ,_, the_DT scenario_NN has_VBZ not_RB previously_RB been_VBN formalized_VBN using_VBG a_DT random_JJ variable_JJ s_NN to_TO represent_VB the_DT fa_NN
hat_NN contains_VBZ most_JJS of_IN the_DT available_JJ positive_JJ examples_NNS ._.
Unfortunately_RB ,_, the_DT outcome_NN of_IN these_DT methods_NNS is_VBZ sensitive_JJ to_TO the_DT values_NNS chosen_VBN for_IN tuning_NN parameters_NNS ,_, and_CC no_DT good_JJ way_NN is_VBZ known_VBN to_TO set_VB these_DT values_NNS =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Moreover_RB ,_, the_DT biased_VBN SVM_NN method_NN has_VBZ been_VBN reported_VBN to_TO do_VB better_RBR experimentally_RB -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
7_CD ._.
CONCLUSIONS_NNS The_DT central_JJ contribution_NN of_IN this_DT paper_NN is_VBZ Lemma_NNP 1_CD ,_, which_WDT shows_VBZ that_IN if_IN positive_JJ training_NN examples_NNS a_DT
oach_NN is_VBZ to_TO assign_VB weights_NNS somehow_RB to_TO the_DT unlabeled_JJ examples_NNS ,_, and_CC then_RB to_TO train_VB a_DT classifier_NN with_IN the_DT unlabeled_JJ examples_NNS interpreted_VBN as_IN weighted_JJ negative_JJ examples_NNS ._.
This_DT approach_NN is_VBZ used_VBN for_IN example_NN by_IN =_JJ -_: =[_NN 8_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT first_JJ approach_NN can_MD be_VB viewed_VBN as_IN a_DT special_JJ case_NN of_IN the_DT second_JJ approach_NN ,_, where_WRB each_DT weight_NN is_VBZ either_CC 0_CD or_CC 1_CD ._.
The_DT second_JJ approach_NN is_VBZ similar_JJ to_TO the_DT method_NN we_PRP suggest_VBP in_IN Section_NN 3_CD above_IN ,_, with_IN three_CD
e_LS result_NN has_VBZ not_RB been_VBN published_VBN before_RB ,_, and_CC it_PRP is_VBZ not_RB obvious_JJ ._.
The_DT reason_NN perhaps_RB that_IN the_DT result_NN is_VBZ novel_JJ is_VBZ that_IN although_IN the_DT learning_NN scenario_NN has_VBZ been_VBN discussed_VBN in_IN many_JJ previous_JJ papers_NNS ,_, including_VBG =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 11_CD ,_, 26_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC these_DT papers_NNS do_VBP make_VB the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN either_CC explicitly_RB or_CC implicitly_RB ,_, the_DT scenario_NN has_VBZ not_RB previously_RB been_VBN formalized_VBN using_VBG a_DT random_JJ variable_JJ s_NN to_TO represent_VB the_DT fa_NN
type_NN ._.
The_DT first_JJ approach_NN is_VBZ to_TO do_VB probability_NN density_NN estimation_NN ,_, but_CC this_DT is_VBZ well-known_JJ to_TO be_VB a_DT very_RB difficult_JJ task_NN for_IN high-dimensional_JJ data_NNS ._.
The_DT second_JJ approach_NN is_VBZ to_TO use_VB a_DT so-called_JJ one-class_JJ SVM_NN =_JJ -_: =[_NN 16_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT aim_NN of_IN these_DT methods_NNS is_VBZ to_TO model_VB a_DT region_NN that_WDT contains_VBZ most_JJS of_IN the_DT available_JJ positive_JJ examples_NNS ._.
Unfortunately_RB ,_, the_DT outcome_NN of_IN these_DT methods_NNS is_VBZ sensitive_JJ to_TO the_DT values_NNS chosen_VBN for_IN tuning_NN paramete_NN
sitive_JJ example_NN is_VBZ labeled_VBN ._.
This_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN is_VBZ analogous_JJ to_TO the_DT ``_`` missing_VBG completely_RB at_IN random_JJ ''_'' assumption_NN that_WDT is_VBZ often_RB made_VBN when_WRB learning_VBG from_IN data_NNS with_IN missing_VBG values_NNS =_JJ -_: =[_NN 10_CD ,_, 17_CD ,_, 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Another_DT way_NN of_IN stating_VBG the_DT assumption_NN is_VBZ that_IN s_NN and_CC x_NN are_VBP conditionally_RB independent_JJ given_VBN y._NN 214_CD So_RB ,_, a_DT training_NN set_NN is_VBZ a_DT random_JJ sample_NN from_IN a_DT distribution_NN p_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, s_NNS -RRB-_-RRB- that_WDT satisfies_VBZ Equations_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- and_CC
ilability_NN of_IN explicit_JJ negative_JJ examples_NNS ._.
However_RB ,_, in_IN many_JJ real-world_JJ domains_NNS ,_, the_DT concept_NN of_IN a_DT negative_JJ example_NN is_VBZ not_RB natural_JJ ._.
For_IN example_NN ,_, over_IN 1000_CD specialized_JJ databases_NNS exist_VBP in_IN molecular_JJ biology_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Each_DT of_IN these_DT defines_VBZ a_DT set_NN of_IN positive_JJ examples_NNS ,_, namely_RB the_DT set_NN of_IN genes_NNS or_CC proteins_NNS included_VBN in_IN the_DT database_NN ._.
In_IN each_DT case_NN ,_, it_PRP would_MD be_VB useful_JJ to_TO learn_VB a_DT classifier_NN that_WDT can_MD recognize_VB additional_JJ g_NN
sitive_JJ example_NN is_VBZ labeled_VBN ._.
This_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN is_VBZ analogous_JJ to_TO the_DT ``_`` missing_VBG completely_RB at_IN random_JJ ''_'' assumption_NN that_WDT is_VBZ often_RB made_VBN when_WRB learning_VBG from_IN data_NNS with_IN missing_VBG values_NNS =_JJ -_: =[_NN 10_CD ,_, 17_CD ,_, 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Another_DT way_NN of_IN stating_VBG the_DT assumption_NN is_VBZ that_IN s_NN and_CC x_NN are_VBP conditionally_RB independent_JJ given_VBN y._NN 214_CD So_RB ,_, a_DT training_NN set_NN is_VBZ a_DT random_JJ sample_NN from_IN a_DT distribution_NN p_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, s_NNS -RRB-_-RRB- that_WDT satisfies_VBZ Equations_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- and_CC
e_LS result_NN has_VBZ not_RB been_VBN published_VBN before_RB ,_, and_CC it_PRP is_VBZ not_RB obvious_JJ ._.
The_DT reason_NN perhaps_RB that_IN the_DT result_NN is_VBZ novel_JJ is_VBZ that_IN although_IN the_DT learning_NN scenario_NN has_VBZ been_VBN discussed_VBN in_IN many_JJ previous_JJ papers_NNS ,_, including_VBG =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 11_CD ,_, 26_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC these_DT papers_NNS do_VBP make_VB the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN either_CC explicitly_RB or_CC implicitly_RB ,_, the_DT scenario_NN has_VBZ not_RB previously_RB been_VBN formalized_VBN using_VBG a_DT random_JJ variable_JJ s_NN to_TO represent_VB the_DT fa_NN
0.9465_CD 0.9279_CD 0.9895_CD 621_CD and_CC then_RB -LRB-_-LRB- ii_LS -RRB-_-RRB- to_TO apply_VB a_DT standard_JJ learning_NN method_NN to_TO these_DT examples_NNS and_CC the_DT positive_JJ examples_NNS ;_: steps_NNS -LRB-_-LRB- i_LS -RRB-_-RRB- and_CC -LRB-_-LRB- ii_LS -RRB-_-RRB- may_MD be_VB iterated_VBN ._.
Papers_NNP using_VBG this_DT general_JJ approach_NN include_VBP =_JJ -_: =[_NN 24_CD ,_, 20_CD ,_, 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC the_DT idea_NN has_VBZ been_VBN rediscovered_VBN independently_RB a_DT few_JJ times_NNS ,_, most_RBS recently_RB in_IN -LRB-_-LRB- 22_CD ,_, Section_NNP 2.4_CD -RRB-_-RRB- ._.
The_DT approach_NN is_VBZ sometimes_RB extended_VBN to_TO identify_VB also_RB additional_JJ positive_JJ examples_NNS in_IN the_DT unlabeled_JJ se_FW
o_NN not_RB ._.
Fortunately_RB ,_, the_DT outputs_NNS of_IN these_DT other_JJ methods_NNS can_MD typically_RB be_VB postprocessed_VBN into_IN calibrated_VBN probabilities_NNS ._.
The_DT two_CD most_RBS common_JJ postprocessing_VBG methods_NNS for_IN calibration_NN are_VBP isotonic_JJ regression_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC fitting_JJ a_DT one-dimensional_JJ logistic_JJ regression_NN function_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
We_PRP apply_VBP the_DT latter_JJ 216_CD method_NN ,_, which_WDT is_VBZ often_RB called_VBN Platt_NNP scaling_NN ,_, to_TO SVM_NNP classifiers_NNS in_IN Section_NNP 5_CD below_IN ._.
4_LS ._.
AN_DT ILLUSTRATION_NN To_TO ill_RB
ot_IN involved_VBN in_IN this_DT process_NN ,_, the_DT only_JJ answer_NN is_VBZ ``_`` all_DT other_JJ proteins_NNS ._. ''_''
To_TO make_VB this_DT answer_NN operational_JJ ,_, we_PRP could_MD take_VB all_DT proteins_NNS mentioned_VBN in_IN a_DT comprehensive_JJ unspecialized_JJ database_NN such_JJ as_IN SwissProt_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
But_CC these_DT proteins_NNS are_VBP unlabeled_JJ examples_NNS ,_, not_RB negative_JJ examples_NNS ,_, because_IN some_DT of_IN them_PRP are_VBP proteins_NNS that_WDT should_MD be_VB in_IN TCDB_NNP ._.
Our_PRP$ goal_NN is_VBZ precisely_RB to_TO discover_VB these_DT proteins_NNS ._.
This_DT paper_NN is_VBZ organized_VBN
type_NN ._.
The_DT first_JJ approach_NN is_VBZ to_TO do_VB probability_NN density_NN estimation_NN ,_, but_CC this_DT is_VBZ well-known_JJ to_TO be_VB a_DT very_RB difficult_JJ task_NN for_IN high-dimensional_JJ data_NNS ._.
The_DT second_JJ approach_NN is_VBZ to_TO use_VB a_DT so-called_JJ one-class_JJ SVM_NN =_JJ -_: =[_NN 16_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT aim_NN of_IN these_DT methods_NNS is_VBZ to_TO model_VB a_DT region_NN that_WDT contains_VBZ most_JJS of_IN the_DT available_JJ positive_JJ examples_NNS ._.
Unfortunately_RB ,_, the_DT outcome_NN of_IN these_DT methods_NNS is_VBZ sensitive_JJ to_TO the_DT values_NNS chosen_VBN for_IN tuning_NN paramete_NN
lly_RB be_VB postprocessed_VBN into_IN calibrated_VBN probabilities_NNS ._.
The_DT two_CD most_RBS common_JJ postprocessing_VBG methods_NNS for_IN calibration_NN are_VBP isotonic_JJ regression_NN -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, and_CC fitting_JJ a_DT one-dimensional_JJ logistic_JJ regression_NN function_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP apply_VBP the_DT latter_JJ 216_CD method_NN ,_, which_WDT is_VBZ often_RB called_VBN Platt_NNP scaling_NN ,_, to_TO SVM_NNP classifiers_NNS in_IN Section_NNP 5_CD below_IN ._.
4_LS ._.
AN_DT ILLUSTRATION_NNP To_TO illustrate_VB the_DT method_NN proposed_VBN in_IN Section_NN 2_CD above_RB ,_, we_PRP generate_VBP 500_CD p_NN
ain_VB any_DT explicit_JJ set_NN of_IN examples_NNS that_WDT should_MD not_RB be_VB included_VBN ,_, and_CC it_PRP is_VBZ unnatural_JJ to_TO ask_VB a_DT human_JJ expert_NN to_TO identify_VB such_PDT a_DT set_NN ._.
Consider_VB the_DT database_NN that_IN we_PRP are_VBP associated_VBN with_IN ,_, which_WDT is_VBZ called_VBN TCDB_NN =_SYM -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT database_NN contains_VBZ information_NN about_IN over_IN 4000_CD proteins_NNS that_WDT are_VBP involved_VBN in_IN signaling_NN across_IN cellular_JJ membranes_NNS ._.
If_IN we_PRP ask_VBP a_DT biologist_NN for_IN examples_NNS of_IN proteins_NNS that_WDT are_VBP not_RB involved_VBN in_IN this_DT pro_NN
0.9465_CD 0.9279_CD 0.9895_CD 621_CD and_CC then_RB -LRB-_-LRB- ii_LS -RRB-_-RRB- to_TO apply_VB a_DT standard_JJ learning_NN method_NN to_TO these_DT examples_NNS and_CC the_DT positive_JJ examples_NNS ;_: steps_NNS -LRB-_-LRB- i_LS -RRB-_-RRB- and_CC -LRB-_-LRB- ii_LS -RRB-_-RRB- may_MD be_VB iterated_VBN ._.
Papers_NNP using_VBG this_DT general_JJ approach_NN include_VBP =_JJ -_: =[_NN 24_CD ,_, 20_CD ,_, 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC the_DT idea_NN has_VBZ been_VBN rediscovered_VBN independently_RB a_DT few_JJ times_NNS ,_, most_RBS recently_RB in_IN -LRB-_-LRB- 22_CD ,_, Section_NNP 2.4_CD -RRB-_-RRB- ._.
The_DT approach_NN is_VBZ sometimes_RB extended_VBN to_TO identify_VB also_RB additional_JJ positive_JJ examples_NNS in_IN the_DT unlabeled_JJ se_FW
e_LS result_NN has_VBZ not_RB been_VBN published_VBN before_RB ,_, and_CC it_PRP is_VBZ not_RB obvious_JJ ._.
The_DT reason_NN perhaps_RB that_IN the_DT result_NN is_VBZ novel_JJ is_VBZ that_IN although_IN the_DT learning_NN scenario_NN has_VBZ been_VBN discussed_VBN in_IN many_JJ previous_JJ papers_NNS ,_, including_VBG =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 11_CD ,_, 26_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC these_DT papers_NNS do_VBP make_VB the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN either_CC explicitly_RB or_CC implicitly_RB ,_, the_DT scenario_NN has_VBZ not_RB previously_RB been_VBN formalized_VBN using_VBG a_DT random_JJ variable_JJ s_NN to_TO represent_VB the_DT fa_NN
because_IN in_IN previous_JJ work_NN we_PRP did_VBD in_IN fact_NN manually_RB identify_VBP the_DT subset_NN of_IN actual_JJ positive_JJ examples_NNS inside_IN U_NN ;_: call_VB this_DT subset_NN Q._NNP The_NNP procedure_NN used_VBN to_TO identify_VB Q_NNP ,_, which_WDT has_VBZ 348_CD members_NNS ,_, is_VBZ explained_VBN in_IN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Let_VB N_NN =_JJ U_NNP \_NNP Q_NNP so_IN the_DT cardinality_NN of_IN N_NN is_VBZ 4558_CD ._.
The_DT three_CD sets_NNS of_IN records_NNS N_NN ,_, P_NN ,_, and_CC Q_NNP are_VBP available_JJ at_IN www.cs.ucsd.edu\/users\/elkan\/posonly_NN ._.
The_DT P_NN and_CC U_NN datasets_NNS were_VBD obtained_VBN separately_RB ,_, and_CC U_NN is_VBZ a_DT sa_NN
sitive_JJ example_NN is_VBZ labeled_VBN ._.
This_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN is_VBZ analogous_JJ to_TO the_DT ``_`` missing_VBG completely_RB at_IN random_JJ ''_'' assumption_NN that_WDT is_VBZ often_RB made_VBN when_WRB learning_VBG from_IN data_NNS with_IN missing_VBG values_NNS =_JJ -_: =[_NN 10_CD ,_, 17_CD ,_, 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Another_DT way_NN of_IN stating_VBG the_DT assumption_NN is_VBZ that_IN s_NN and_CC x_NN are_VBP conditionally_RB independent_JJ given_VBN y._NN 214_CD So_RB ,_, a_DT training_NN set_NN is_VBZ a_DT random_JJ sample_NN from_IN a_DT distribution_NN p_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, s_NNS -RRB-_-RRB- that_WDT satisfies_VBZ Equations_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- and_CC
e_LS result_NN has_VBZ not_RB been_VBN published_VBN before_RB ,_, and_CC it_PRP is_VBZ not_RB obvious_JJ ._.
The_DT reason_NN perhaps_RB that_IN the_DT result_NN is_VBZ novel_JJ is_VBZ that_IN although_IN the_DT learning_NN scenario_NN has_VBZ been_VBN discussed_VBN in_IN many_JJ previous_JJ papers_NNS ,_, including_VBG =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 11_CD ,_, 26_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC these_DT papers_NNS do_VBP make_VB the_DT ``_`` selected_VBN completely_RB at_IN random_JJ ''_'' assumption_NN either_CC explicitly_RB or_CC implicitly_RB ,_, the_DT scenario_NN has_VBZ not_RB previously_RB been_VBN formalized_VBN using_VBG a_DT random_JJ variable_JJ s_NN to_TO represent_VB the_DT fa_NN
n_NN words_NNS ,_, the_DT probability_NN that_IN an_DT example_NN x_NN appears_VBZ in_IN the_DT labeled_JJ set_NN is_VBZ zero_CD if_IN y_NN =_JJ 0_CD ._.
There_EX is_VBZ a_DT subtle_JJ but_CC important_JJ difference_NN between_IN the_DT scenario_NN considered_VBN here_RB ,_, and_CC the_DT scenario_NN considered_VBN in_IN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT scenario_NN here_RB is_VBZ that_IN the_DT training_NN data_NNS are_VBP drawn_VBN randomly_RB from_IN p_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, s_NNS -RRB-_-RRB- ,_, but_CC for_IN each_DT tuple_FW 〈_FW x_NN ,_, y_NN ,_, s_NN 〉_NN that_WDT is_VBZ drawn_VBN ,_, only_RB 〈_CD x_NN ,_, s_NN 〉_NN is_VBZ recorded_VBN ._.
The_DT scenario_NN of_IN -LRB-_-LRB- 21_CD -RRB-_-RRB- is_VBZ that_IN two_CD training_NN sets_NNS ar_IN
