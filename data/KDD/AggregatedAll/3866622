Supervised_VBN probabilistic_JJ principal_JJ component_NN analysis_NN
Principal_NN component_NN analysis_NN -LRB-_-LRB- PCA_NN -RRB-_-RRB- has_VBZ been_VBN extensively_RB applied_VBN in_IN data_NNS mining_NN ,_, pattern_NN recognition_NN and_CC information_NN retrieval_NN for_IN unsupervised_JJ dimensionality_NN reduction_NN ._.
When_WRB labels_NNS of_IN data_NNS are_VBP available_JJ ,_, e.g._FW ,_, in_IN a_DT classification_NN or_CC regression_NN task_NN ,_, PCA_NN is_VBZ however_RB not_RB able_JJ to_TO use_VB this_DT information_NN ._.
The_DT problem_NN is_VBZ more_RBR interesting_JJ if_IN only_JJ part_NN of_IN the_DT input_NN data_NNS are_VBP labeled_VBN ,_, i.e._FW ,_, in_IN a_DT semi-supervised_JJ setting_NN ._.
In_IN this_DT paper_NN we_PRP propose_VBP a_DT supervised_JJ PCA_NN model_NN called_VBN SPPCA_NNP and_CC a_DT semi-supervised_JJ PCA_NN model_NN called_VBN S2PPCA_NNP ,_, both_DT of_IN which_WDT are_VBP extensions_NNS of_IN a_DT probabilistic_JJ PCA_NN model_NN ._.
The_DT proposed_VBN models_NNS are_VBP able_JJ to_TO incorporate_VB the_DT label_NN information_NN into_IN the_DT projection_NN phase_NN ,_, and_CC can_MD naturally_RB handle_VB multiple_JJ outputs_NNS -LRB-_-LRB- i.e._FW ,_, in_IN multi-task_JJ learning_NN problems_NNS -RRB-_-RRB- ._.
We_PRP derive_VBP an_DT efficient_JJ EM_NN learning_NN algorithm_NN for_IN both_DT models_NNS ,_, and_CC also_RB provide_VBP theoretical_JJ justifications_NNS of_IN the_DT model_NN behaviors_NNS ._.
SPPCA_NN and_CC S2PPCA_NN are_VBP compared_VBN with_IN other_JJ supervised_JJ projection_NN methods_NNS on_IN various_JJ learning_VBG tasks_NNS ,_, and_CC show_VBP not_RB only_RB promising_JJ performance_NN but_CC also_RB good_JJ scalability_NN ._.
for_IN data_NNS mining_NN and_CC some_DT PCA-based_JJ clustering_NN methods_NNS have_VBP been_VBN developed_VBN in_IN the_DT past_NN -LRB-_-LRB- 13_CD ,_, 14_CD -RRB-_-RRB- ._.
PCA_NNP has_VBZ also_RB been_VBN extensively_RB applied_VBN extensively_RB in_IN the_DT field_NN of_IN face_NN recognition_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- ._.
The_DT authors_NNS in_IN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT supervised_JJ PCA_NN model_NN called_VBN SPPCA_NNP ,_, whereby_WRB they_PRP extended_VBD PCA_NNP to_TO incorporate_VB label\/class_NN information_NN into_IN the_DT projection_NN phase_NN ._.
Correlation_NN Clustering_NN :_: Correlation_NN clustering_NN aims_VBZ at_IN groupi_NNS
ed_VBN outside_JJ dimensionality_NN reduction_NN ,_, to_TO e.g._FW mixture_NN modeling_NN -LRB-_-LRB- 10_CD ,_, 11_CD -RRB-_-RRB- ._.
For_IN dimensionality_NN reduction_NN ,_, semi-supervised_JJ learning_NN based_VBN on_IN joint_JJ density_NN estimation_NN in_IN the_DT original_JJ space_NN has_VBZ been_VBN used_VBN in_IN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC based_VBN on_IN discriminative_JJ modeling_NN of_IN pairwise_JJ constraints_NNS but_CC no_DT labeled_JJ or_CC unlabeled_JJ samples_NNS in_IN -LRB-_-LRB- 3_CD -RRB-_-RRB- ._.
Metrics_NNS have_VBP been_VBN learned_VBN based_VBN on_IN pairwise_JJ constraints_NNS in_IN -LRB-_-LRB- 13_CD -RRB-_-RRB- ._.
In_IN this_DT paper_NN we_PRP introduce_VBP a_DT
bilistic_JJ framework_NN constrained_VBN on_IN the_DT vector_NN mean_NN and_CC the_DT covariance_NN matrix_NN of_IN the_DT latent_JJ space_NN ._.
More_RBR recently_RB ,_, Ioffe_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- has_VBZ proposed_VBN a_DT probabilistic_JJ approach_NN for_IN LDA_NNP and_CC in_IN the_DT same_JJ year_NN ,_, Yu_NNP et_FW al._FW =_SYM -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP adapted_VBN the_DT framework_NN of_IN probabilistic_JJ principal_JJ component_NN analysis_NN -LRB-_-LRB- PPCA_NN -RRB-_-RRB- developed_VBN by_IN Tipping_VBG et_FW al._FW -LRB-_-LRB- 7_CD -RRB-_-RRB- in_IN a_DT supervised_JJ context_NN and_CC have_VBP found_VBN that_IN the_DT maximum_NN likelihood_NN of_IN their_PRP$ approach_NN is_VBZ
ther_NN context_NN as_RB well_RB ._.
Very_RB similar_JJ problems_NNS arise_VBP in_IN fMRI_NN data_NNS ,_, for_IN instance_NN ,_, and_CC dPCA_NN could_MD provide_VB a_DT useful_JJ alternative_NN to_TO other_JJ dimensionality_NN reduction_NN methods_NNS such_JJ as_IN CCA_NNP ,_, PLS_NNP ,_, or_CC Supervised_JJ PCA_NN =_JJ -_: =[_NN 1_CD ,_, 12_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Furthermore_RB ,_, the_DT general_JJ aim_NN of_IN demixing_JJ dependencies_NNS could_MD likely_RB be_VB extended_VBN to_TO other_JJ methods_NNS -LRB-_-LRB- such_JJ as_IN ICA_NN -RRB-_-RRB- as_RB well_RB ._.
Ultimately_RB ,_, we_PRP see_VBP dPCA_NN as_IN a_DT particular_JJ data_NN visualization_NN technique_NN that_WDT will_MD
be_VB seen_VBN as_IN special_JJ cases_NNS of_IN the_DT proposed_VBN model_NN ._.
Replacing_VBG the_DT separate_JJ ARD_NNP terms_NNS of_IN the_DT two_CD data_NNS sources_NNS with_IN a_DT singleBayesian_JJ CCA_NNP via_IN Group_NNP Sparsity_NNP one_CD results_VBZ in_IN a_DT variant_NN of_IN the_DT supervised_JJ PCA_NN -LRB-_-LRB- =_JJ -_: =_JJ Yu_FW et_FW al._FW ,_, 2006_CD -_: =--RRB-_NN ;_: it_PRP assumes_VBZ all_DT components_NNS to_TO be_VB shared_VBN ,_, and_CC the_DT amount_NN of_IN supervision_NN can_MD be_VB controlled_VBN by_IN specifying_VBG different_JJ noise_NN variances_NNS for_IN the_DT two_CD sources_NNS ._.
Assuming_VBG additionally_RB equal_JJ variances_NNS the_DT model_NN
ed_VBN outside_JJ dimensionality_NN reduction_NN ,_, to_TO e.g._FW mixture_NN modeling_NN -LRB-_-LRB- 10_CD ,_, 11_CD -RRB-_-RRB- ._.
For_IN dimensionality_NN reduction_NN ,_, semi-supervised_JJ learning_NN based_VBN on_IN joint_JJ density_NN estimation_NN in_IN the_DT original_JJ space_NN has_VBZ been_VBN used_VBN in_IN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC based_VBN on_IN discriminative_JJ modeling_NN of_IN pairwise_JJ constraints_NNS but_CC no_DT labeled_JJ or_CC unlabeled_JJ samples_NNS in_IN -LRB-_-LRB- 3_CD -RRB-_-RRB- ._.
Metrics_NNS have_VBP been_VBN learned_VBN based_VBN on_IN pairwise_JJ constraints_NNS in_IN -LRB-_-LRB- 13_CD -RRB-_-RRB- ._.
In_IN this_DT paper_NN we_PRP introduce_VBP a_DT
ion_NN ._.
Existing_VBG supervised_JJ methods_NNS include_VBP linear_JJ regression_NN analysis_NN -LRB-_-LRB- LRA_NN -RRB-_-RRB- -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, linear_JJ discriminant_JJ analysis_NN -LRB-_-LRB- LDA_NN -RRB-_-RRB- -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, principal_JJ component_NN regression_NN -LRB-_-LRB- PCR_NN -RRB-_-RRB- -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, supervise_VBP probabilistic_JJ PCA_NN -LRB-_-LRB- SPPCA_NN -RRB-_-RRB- =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_JJ -_: and_CC many_JJ others_NNS -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
LRA_NN and_CC LDA_NN find_VBP the_DT linear_JJ combinations_NNS of_IN the_DT input_NN -LRB-_-LRB- predictor_NN -RRB-_-RRB- features_NNS which_WDT best_RB explain_VBP the_DT target_NN -LRB-_-LRB- dependent_JJ -RRB-_-RRB- feature_NN ._.
In_IN these_DT methods_NNS ,_, the_DT input_NN features_NNS are_VBP generally_RB a_DT
ss_RB ,_, with_IN an_DT orthogonality_NN constraint_NN on_IN the_DT shared_JJ factors_NNS ._.
Principal_NN components_NNS analysis_NN ,_, which_WDT factors_VBZ a_DT doubly_RB centered_JJ matrix_NN under_IN squared_VBN loss_NN ,_, has_VBZ also_RB been_VBN extended_VBN to_TO the_DT three-factor_JJ schema_NN =_JJ -_: =[_NN 38_CD -RRB-_-RRB- -_: =_SYM -_: ._.
An_DT extension_NN of_IN pLSI_NN to_TO two_CD related_JJ matrices_NNS ,_, pLSI-pHITS_NN ,_, consists_VBZ of_IN two_CD pLSI_NN models_NNS that_WDT share_VBP latent_JJ variables_NNS -LRB-_-LRB- 39_CD -RRB-_-RRB- ._.
While_IN our_PRP$ choice_NN of_IN a_DT bilinear_JJ form_NN UV_NN T_NN is_VBZ common_JJ ,_, it_PRP is_VBZ not_RB the_DT only_JJ option_NN ._.
ss_RB ,_, with_IN an_DT orthogonality_NN constraint_NN on_IN the_DT shared_JJ factors_NNS ._.
Principal_NN components_NNS analysis_NN ,_, which_WDT factors_VBZ a_DT doubly_RB centered_JJ matrix_NN under_IN squared_VBN loss_NN ,_, has_VBZ also_RB been_VBN extended_VBN to_TO the_DT three-factor_JJ schema_NN =_JJ -_: =[_NN 36_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Another_DT interesting_JJ type_NN of_IN schema_NN contains_VBZ multiple_JJ parallel_JJ relations_NNS between_IN two_CD entity_NN types_NNS ._.
An_DT example_NN of_IN this_DT sort_NN of_IN schema_NN is_VBZ max-margin_JJ matrix_NN factorization_NN -LRB-_-LRB- MMMF_NN -RRB-_-RRB- -LRB-_-LRB- 30_CD -RRB-_-RRB- ._.
In_IN MMMF_NNP ,_, the_DT goal_NN
m._NN A_NN few_JJ supervised_JJ dimensionality_NN reduction_NN methods_NNS based_VBN on_IN exponential_JJ family_NN models_NNS have_VBP been_VBN proposed_VBN in_IN the_DT literature_NN ._.
For_IN example_NN ,_, a_DT supervised_JJ probabilistic_JJ PCA_NN -LRB-_-LRB- SPPCA_NN -RRB-_-RRB- model_NN was_VBD proposed_VBN in_IN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
SPPCA_NN extends_VBZ probabilistic_JJ PCA_NN by_IN assuming_VBG that_IN both_DT features_NNS and_CC labels_NNS have_VBP Gaussian_JJ distributions_NNS and_CC are_VBP generated_VBN independently_RB from_IN the_DT latent_JJ low_JJ dimensional_JJ space_NN through_IN linear_JJ 1transform_NN
A_DT model_NN to_TO capture_VB the_DT label_NN correlations_NNS and_CC to_TO learn_VB better_JJR predictive_JJ features_NNS from_IN the_DT data_NNS by_IN projective_JJ it_PRP to_TO a_DT subspace_NN directed_VBN by_IN label_NN information_NN ._.
It_PRP has_VBZ been_VBN empirically_RB and_CC theoretically_RB =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_SYM -_: shown_VBN that_IN incorporating_VBG label_NN information_NN in_IN dimensionality_NN reduction_NN indeed_RB leads_VBZ to_TO better_JJR projections_NNS if_IN the_DT final_JJ goal_NN is_VBZ prediction_NN ._.
More_RBR concretely_RB ,_, let_VB X_NN =_JJ -LRB-_-LRB- x1_NN ,_, ..._: ,_, xN_NN -RRB-_-RRB- be_VB an_DT D_NN Ã—_NN N_NN matrix_NN of_IN p_NN
