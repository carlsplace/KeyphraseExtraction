Data_NN selection_NN for_IN support_NN vector_NN machine_NN classifiers_NNS
igh-dimensional_JJ and_CC large_JJ datasets_NNS ._.
An_DT algorithm_NN that_WDT selects_VBZ the_DT minimal_JJ number_NN of_IN samples_NNS -LRB-_-LRB- observations_NNS -RRB-_-RRB- from_IN a_DT large_JJ dataset_NN in_IN order_NN to_TO build_VB a_DT Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- Classifier_NNP is_VBZ given_VBN in_IN =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
On_IN the_DT other_JJ hand_NN ,_, some_DT classification_NN algorithms_NNS such_JJ as_IN Decision_NNP Trees_NNP perform_VB dimensionality_NN reduction_NN by_IN selecting_VBG the_DT set_NN of_IN features_NNS that_WDT increases_VBZ the_DT classifier_NN accuracy_NN ._.
3.4.3_FW Summary_FW It_PRP is_VBZ
Approaches_NNS combine_VBP learning_NN and_CC feature_NN selection_NN through_IN prior_JJ or_CC posterior_JJ regularization_NN ._.
-LRB-_-LRB- Tibshirani_NNP ,_, 1994_CD -RRB-_-RRB- pioneered_VBD the_DT use_NN of_IN L1_NN regularization_NN ,_, amenable_JJ to_TO Linear_JJ Programming-based_JJ learning_NN -LRB-_-LRB- =_JJ -_: =_JJ Fung_NNP &_CC Mangasarian_NNP ,_, 2000_CD -_: =--RRB-_NN and_CC paving_VBG the_DT way_NN for_IN the_DT so-called_JJ least_JJS absolute_JJ shrinkage_NN and_CC selection_NN operator_NN -LRB-_-LRB- Lasso_NN -RRB-_-RRB- approach_NN -LRB-_-LRB- Efron_NNP et_FW al._FW ,_, 2004_CD -RRB-_-RRB- ._.
Quite_RB a_DT few_JJ other_JJ regularization_NN approaches_NNS ,_, mostly_RB considering_VBG linear_JJ hyp_NN
ds_JJ knowledge_NN sets_NNS ,_, support_NN vector_NN machine_NN classifier_NN ,_, knowledge-based_JJ classifier_NN ,_, linear_JJ programming_NN 1_CD ._.
INTRODUCTION_NN Support_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- have_VBP played_VBN a_DT major_JJ role_NN in_IN classification_NN problems_NNS =_JJ -_: =[_NN 24_CD ,_, 3_CD ,_, 12_CD ,_, 1_CD ,_, 13_CD ,_, 5_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB unlike_IN other_JJ classification_NN tools_NNS such_JJ as_IN knowledge-based_JJ neural_JJ networks_NNS -LRB-_-LRB- 21_CD ,_, 22_CD ,_, 17_CD ,_, 7_CD -RRB-_-RRB- ,_, little_JJ work_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- has_VBZ gone_VBN into_IN incorporating_VBG prior_JJ knowledge_NN into_IN support_NN vector_NN machines_NNS ._.
In_IN this_DT
on_IN the_DT locations_NNS xi_RB include_VBP γ_NN -LRB-_-LRB- θ_NN -RRB-_-RRB- =_JJ 1_CD −_CD e_LS p_NN |_CD θ_NN |_NN with_IN p_NN -RRB-_-RRB- 0_CD -LRB-_-LRB- feature_NN selection_NN prior_JJ -RRB-_-RRB- ,_, -LRB-_-LRB- 98_CD -RRB-_-RRB- γ_NN -LRB-_-LRB- θ_NN -RRB-_-RRB- =_JJ θ_NN 2_CD -LRB-_-LRB- weight_NN decay_NN prior_JJ -RRB-_-RRB- ,_, -LRB-_-LRB- 99_CD -RRB-_-RRB- γ_NN -LRB-_-LRB- θ_NN -RRB-_-RRB- =|_FW θ_FW |_NN -LRB-_-LRB- Laplacian_NN prior_JJ -RRB-_-RRB- ._.
-LRB-_-LRB- 100_CD -RRB-_-RRB- The_DT prior_RB given_VBN by_IN -LRB-_-LRB- 98_CD -RRB-_-RRB- was_VBD int_NN =_JJ -_: =_JJ roduced_NN in_IN -LRB-_-LRB- 5_CD ,_, 14_CD -RRB-_-RRB- -_: =_JJ -_: and_CC is_VBZ log-concave_JJ ._.
While_IN the_DT latter_JJ characteristic_NN is_VBZ unfavorable_JJ in_IN general_JJ ,_, since_IN the_DT corresponding_JJ optimization_NN problem_NN exhibits_VBZ many_JJ local_JJ minima_NN ,_, the_DT negative_JJ log-posterior_NN becomes_VBZ strictly_RB con_VB
of_IN the_DT number_NN of_IN data_NNS points_NNS needs_VBZ to_TO be_VB solved_VBN ._.
This_DT allows_VBZ us_PRP to_TO easily_RB classify_VB datasets_NNS with_IN as_RB many_JJ as_IN a_DT few_JJ thousand_CD of_IN points_NNS ._.
For_IN larger_JJR datasets_NNS ,_, data_NNS selection_NN and_CC reduction_NN methods_NNS such_JJ as_IN =_JJ -_: =[_NN 11_CD ,_, 17_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: can_MD be_VB utilized_VBN as_IN indicated_VBN by_IN some_DT of_IN our_PRP$ numerical_JJ results_NNS and_CC will_MD be_VB the_DT subject_NN of_IN future_JJ work_NN ._.
Our_PRP$ computational_JJ results_NNS demonstrate_VBP that_IN PSVM_NN classifiers_NNS obtain_VBP test_NN set_NN correctness_NN statistic_NN
ellent_JJ book_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ._.
The_DT extensive_JJ connections_NNS between_IN mathematical_JJ programming_NN ,_, in_IN particular_JJ linear_JJ and_CC quadratic_JJ programming_NN ,_, and_CC classification_NN ,_, have_VBP been_VBN successfully_RB explored_VBN by_IN a_DT number_NN of_IN authors_NNS =_JJ -_: =[_NN 2_CD ,_, 5_CD ,_, 13_CD ,_, 10_CD ,_, 4_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Our_PRP$ work_NN is_VBZ in_IN this_DT line_NN ,_, but_CC with_IN an_DT emphasis_NN on_IN exploiting_VBG the_DT specific_JJ unknown-butbounded_JJ type_NN of_IN information_NN ,_, that_WDT describes_VBZ data_NNS within_IN this_DT interval_JJ matrix_NN model_NN ._.
As_IN a_DT result_NN ,_, we_PRP end_VBP up_RP using_VBG
paratively_RB evaluated_VBN on_IN a_DT number_NN of_IN artificial_JJ and_CC real_JJ world_NN two_CD class_NN problem_NN datasets_NNS ._.
The_DT GMEB_NN algorithm_NN was_VBD compared_VBN to_TO the_DT linear_JJ SVM_NN -LRB-_-LRB- standard_JJ SVM_NN with_IN linear_JJ kernel_NN -RRB-_-RRB- and_CC the_DT l1_NN SVM_NN classifier_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
4.1_CD Experimental_JJ Methodology_NN The_DT algorithms_NNS are_VBP compared_VBN by_IN two_CD criteria_NNS :_: the_DT number_NN of_IN selected_VBN features_NNS and_CC the_DT error_NN rates_NNS ._.
The_DT weight_NN assigned_VBN by_IN a_DT linear_JJ classifier_NN to_TO a_DT feature_NN j_NN ,_, determines_VBZ w_NN
eal-world_JJ problems_NNS ._.
This_DT issue_NN has_VBZ been_VBN addressed_VBN in_IN the_DT LP_NN and_CC SVM_NN literature_NN over_IN the_DT years_NNS ,_, providing_VBG a_DT wide_JJ array_NN of_IN speed-up_JJ approaches_NNS for_IN optimization_NN problems_NNS in_IN the_DT context_NN of_IN classification_NN =_JJ -_: =[_NN 2_CD ,_, 11_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, the_DT nature_NN of_IN the_DT ranking_JJ problem_NN introduces_VBZ a_DT different_JJ challenge_NN making_VBG traditional_JJ constraint_NN reduction_NN methods_NNS ,_, typically_RB used_VBN in_IN classification_NN problems_NNS ,_, inappropriate_JJ ._.
In_IN this_DT work_NN we_PRP
of_IN the_DT number_NN of_IN data_NNS points_NNS needs_VBZ to_TO be_VB solved_VBN ._.
This_DT allows_VBZ us_PRP to_TO easily_RB classify_VB datasets_NNS with_IN as_RB many_JJ as_IN a_DT few_JJ thousand_CD of_IN points_NNS ._.
For_IN larger_JJR datasets_NNS ,_, data_NNS selection_NN and_CC reduction_NN methods_NNS such_JJ as_IN =_JJ -_: =[_NN 11_CD ,_, 17_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: can_MD be_VB utilized_VBN as_IN indicated_VBN by_IN some_DT of_IN our_PRP$ numerical_JJ results_NNS and_CC will_MD be_VB the_DT subject_NN of_IN future_JJ work_NN ._.
Our_PRP$ computational_JJ results_NNS demonstrate_VBP that_IN PSVM_NN classifiers_NNS obtain_VBP test_NN set_NN correctness_NN statistic_NN
vely_RB large_JJ number_NN of_IN features_NNS and_CC their_PRP$ correlative_JJ nature_NN ,_, we_PRP initially_RB examined_VBD possibilities_NNS of_IN feature_NN selection_NN ._.
In_IN particular_JJ ,_, several_JJ state-of-the-art_JJ feature_NN selection_NN algorithms_NNS ,_, both_DT linear_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 14_CD -RRB-_-RRB- and_CC non-linear_JJ -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, were_VBD tested_VBN and_CC compared_VBN to_TO a_DT support_NN vector_NN machine_NN -LRB-_-LRB- SVM_NN -RRB-_-RRB- -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, applied_VBN on_IN all_PDT the_DT features_NNS ._.
These_DT initial_JJ experiments_NNS indicated_VBD that_IN feature_NN selection_NN only_RB decrease_VB
xample_RB ,_, although_IN oftentimes_RB speed-up_JJ heuristics_NNS and_CC approximations_NNS exist_VBP that_WDT alleviate_VBP this_DT problem_NN ._.
Examples_NNS of_IN typically_RB batch_NN algorithms_NNS with_IN online_JJ versions_NNS are_VBP decision_NN trees_NNS -LRB-_-LRB- 11_CD ,_, 20_CD ,_, 39_CD -RRB-_-RRB- ,_, SVMs_NNS =_SYM -_: =[_NN 10_CD ,_, 14_CD ,_, 30_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC bagging_VBG and_CC boosting_VBG -LRB-_-LRB- 31_CD ,_, 32_CD -RRB-_-RRB- ._.
An_DT alternative_JJ approach_NN is_VBZ what_WP we_PRP refer_VBP to_TO as_IN the_DT wrapper_NN approach_NN ._.
In_IN the_DT wrapper_NN approach_NN ,_, the_DT batch_NN machine_NN learning_NN algorithms_NNS are_VBP not_RB modified_VBN but_CC are_VBP used_VBN w_NN
ers_NNPS ,_, such_JJ as_IN multi-layer_JJ perceptron_NN ,_, nearest_JJS neighbor_NN ,_, C4_NN .5_NN ,_, and_CC SVMs_NNS ._.
However_RB ,_, random_JJ selection_NN method_NN can_MD sometimes_RB cause_VB a_DT catastrophic_JJ decrease_NN in_IN accuracy_NN ._.
Minimal_JJ support_NN vector_NN machines_NNS -LRB-_-LRB- MSVM_NN -RRB-_-RRB- =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: remove_VB redundant_JJ data_NNS and_CC finds_VBZ a_DT minimal_JJ set_NN of_IN support_NN vectors_NNS ,_, which_WDT is_VBZ based_VBN on_IN linear_JJ programming_NN instead_RB of_IN the_DT standard_JJ quadratic_JJ programming_NN formulation_NN by_IN representing_VBG the_DT margin_NN in_IN terms_NNS of_IN
s_NN that_IN the_DT L1-norm_NN of_IN the_DT weight_NN vector_NN is_VBZ minimized_VBN instead_RB of_IN the_DT squared_VBN L2-norm_NN ._.
This_DT modification_NN is_VBZ motivated_VBN similarly_RB as_IN for_IN the_DT LASSO_NNP ._.
The_DT Liknon_NNP classifier_NN is_VBZ therefore_RB a_DT linear_JJ L1-SVM_NN ,_, see_VBP =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 28_CD -RRB-_-RRB- for_IN more_JJR on_IN L1-SVM_NN 's_POS ._.
We_PRP now_RB elaborate_VBP on_IN the_DT Liknon_NNP classifier_NN ,_, since_IN it_PRP resembles_VBZ LESS_NNP to_TO a_DT certain_JJ extent_NN ._.
Formally_RB ,_, Liknon_NNP is_VBZ defined_VBN as_IN follows_VBZ :_: subject_JJ to_TO :_: min_NN |_CD w_NN |_NN +_CC C_NN n_NN i_FW =_JJ 1_CD ξi_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- ∀_NN
d_NN instead_RB of_IN thesVeenman_FW et_FW ._.
al_FW :_: LESS_NN :_: a_DT Model-Based_NNP Classifier_NNP for_IN Sparse_NNP Subspaces_NNP 8_CD squared_VBD L2-norm_NN ._.
As_IN such_JJ Liknon_NNP is_VBZ considered_VBN an_DT L1-SVM_NN ._.
For_IN more_JJR on_IN L1-SVM_NN classifiers_NNS see_VBP for_IN instance_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- and_CC =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT motivation_NN for_IN applying_VBG the_DT L1-norm_NN is_VBZ the_DT same_JJ as_IN for_IN the_DT LASSO_NNP -LRB-_-LRB- see_VB previous_JJ Section_NN -RRB-_-RRB- ._.
That_DT is_VBZ ,_, the_DT L1-norm_NN forces_VBZ the_DT weight_NN entries_NNS wj_VBP to_TO 0_CD so_IN the_DT classifier_NN explicitly_RB selects_VBZ a_DT feature_NN sub_NN
ne_NN that_WDT would_MD be_VB built_VBN from_IN all_PDT the_DT data_NNS ._.
Other_JJ approaches_NNS to_TO scaling_VBG include_VBP those_DT developed_VBN for_IN support_NN vector_NN machines_NNS by_IN Mangasarian_NN and_CC colleagues_NNS ,_, including_VBG chunking_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- and_CC instance_NN selection_NN =_JJ -_: =[_NN 11_CD -_: =-]_CD ._.
Finally_RB ,_, the_DT many_JJ formulations_NNS for_IN feature_NN selection_NN in_IN machine_NN learning_NN could_MD be_VB considered_VBN scaling_VBG mechanisms_NNS ,_, since_IN they_PRP result_VBP in_IN dimensionality_NN reduction_NN ,_, although_IN most_JJS were_VBD motivated_VBN by_IN overt_JJ
was_VBD also_RB utilized_VBN for_IN feature_NN selection_NN ._.
A_DT similar_JJ idea_NN was_VBD used_VBN in_IN SVMRecursive_FW Feature_FW Elimination_NN -LRB-_-LRB- SVM-RFE_NN -RRB-_-RRB- -LRB-_-LRB- Guyon_NNP et_FW al._FW ,_, 2002_CD -RRB-_-RRB- where_WRB features_NNS with_IN smallest_JJS weights_NNS were_VBD removed_VBN iteratively_RB ._.
In_IN -LRB-_-LRB- =_JJ -_: =_JJ Fung_NNP &_CC Mangasarian_NNP ,_, 2000_CD -_: =_JJ -_: ;_: Ng_NN ,_, 2004_CD -RRB-_-RRB- ,_, L1norm_NN of_IN weights_NNS was_VBD suggested_VBN to_TO replace_VB L2-norm_NN for_IN feature_NN selection_NN when_WRB learning_VBG an_DT SVM_NN model_NN ._.
Another_DT feature_NN selection_NN model_NN related_JJ to_TO L1-norm_NN is_VBZ lasso_NN -LRB-_-LRB- Tibshirani_NNP ,_, 1996_CD -RRB-_-RRB- ,_, which_WDT
on_IN seven_CD datasets_NNS ,_, the_DT first_JJ five_CD of_IN which_WDT ,_, WPBC_NNP ,_, Ionosphere_NNP ,_, Cleveland_NNP Heart_NNP ,_, Pima_NNP Indians_NNPS ,_, and_CC BUPA_NN Liver_NN are_VBP from_IN the_DT Irvine_NNP Machine_NNP Learning_NNP Repository_NNP -LRB-_-LRB- 18_CD -RRB-_-RRB- ,_, while_IN the_DT Galaxy_NNP Dim_NNP dataset_NN is_VBZ from_IN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- ,_, and_CC the_DT -_: =_JJ -_: Census_NNP dataset_NN is_VBZ a_DT version_NN of_IN the_DT US_NNP Census_NNP Bureau_NNP ``_`` Adult_JJ ''_'' dataset_NN ,_, which_WDT is_VBZ publicly_RB available_JJ from_IN Silicon_NNP Graphics_NNP '_POS website_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
For_IN WPBC_NN -LRB-_-LRB- 60_CD -RRB-_-RRB- ,_, 110_CD breast_NN cancer_NN patients_NNS were_VBD classified_VBN
ing_NN Repository_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- ,_, while_IN the_DT Galaxy_NNP Dim_NNP dataset_NN is_VBZ from_IN -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, and_CC the_DT Census_NNP dataset_NN is_VBZ a_DT version_NN of_IN the_DT US_NNP Census_NNP Bureau_NNP ``_`` Adult_JJ ''_'' dataset_NN ,_, which_WDT is_VBZ publicly_RB available_JJ from_IN Silicon_NNP Graphics_NNP =_SYM -_: =_JJ '_'' website_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN WPBC_NN -LRB-_-LRB- 60_CD -RRB-_-RRB- ,_, 110_CD breast_NN cancer_NN patients_NNS were_VBD classified_VBN into_IN those_DT who_WP had_VBD a_DT recurrence_NN of_IN the_DT disease_NN within_IN 60_CD months_NNS and_CC those_DT who_WP had_VBD not_RB ._.
For_IN the_DT Census_NNP dataset_NN ,_, ten_CD features_NNS were_VBD used_VBN to_TO pred_VB
rging_VBG and_CC processing_VBG it_PRP with_IN new_JJ incoming_JJ data_NNS ._.
Keywords_NNS support_VBP vector_NN machines_NNS ,_, data_NNS classification_NN ,_, data_NNS selection_NN ,_, concave_NN minimization_NN ,_, linear_JJ programming_NN 1_CD ._.
INTRODUCTION_NN Support_NN vector_NN machines_NNS =_JJ -_: =[_NN 20_CD ,_, 8_CD ,_, 3_CD ,_, 7_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP powerful_JJ tools_NNS for_IN data_NNS classification_NN ._.
Classification_NN is_VBZ achieved_VBN by_IN a_DT linear_JJ or_CC nonlinear_JJ separating_VBG surface_NN in_IN the_DT input_NN space_NN of_IN the_DT dataset_NN ._.
The_DT separating_VBG surface_NN depends_VBZ only_RB on_IN a_DT subset_NN o_NN
rging_VBG and_CC processing_VBG it_PRP with_IN new_JJ incoming_JJ data_NNS ._.
Keywords_NNS support_VBP vector_NN machines_NNS ,_, data_NNS classification_NN ,_, data_NNS selection_NN ,_, concave_NN minimization_NN ,_, linear_JJ programming_NN 1_CD ._.
INTRODUCTION_NN Support_NN vector_NN machines_NNS =_JJ -_: =[_NN 20_CD ,_, 8_CD ,_, 3_CD ,_, 7_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP powerful_JJ tools_NNS for_IN data_NNS classification_NN ._.
Classification_NN is_VBZ achieved_VBN by_IN a_DT linear_JJ or_CC nonlinear_JJ separating_VBG surface_NN in_IN the_DT input_NN space_NN of_IN the_DT dataset_NN ._.
The_DT separating_VBG surface_NN depends_VBZ only_RB on_IN a_DT subset_NN o_NN
nd_NN allows_VBZ the_DT removal_NN of_IN redundant_JJ data_NNS ._.
This_DT dependence_NN on_IN a_DT small_JJ subset_NN of_IN a_DT given_VBN dataset_NN ,_, which_WDT often_RB leads_VBZ to_TO an_DT improved_JJ classifier_NN ,_, can_MD be_VB utilized_VBN in_IN an_DT incremental_JJ approach_NN such_JJ as_IN chunking_NN =_JJ -_: =[_NN 3_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: where_WRB a_DT small_JJ fraction_NN of_IN the_DT data_NN is_VBZ maintained_VBN before_IN merging_VBG and_CC processing_VBG it_PRP with_IN new_JJ incoming_JJ data_NNS ._.
For_IN the_DT sake_NN of_IN simplicity_NN and_CC getting_VBG basic_JJ ideas_NNS across_IN we_PRP shall_MD confine_VB ourselves_PRP here_RB to_TO
-LRB-_-LRB- 7_CD -RRB-_-RRB- This_NNP S_NNP VMII_NNP ._.
I_NN reformulation_NN in_IN effect_NN maximizes_VBZ the_DT margin_NN ,_, the_DT distance_NN between_IN the_DT two_CD bounding_VBG planes_NNS of_IN Figures_NNS 1_CD and_CC 2_CD ,_, using_VBG a_DT different_JJ norm_NN ,_, the_DT oo-norm_NN ,_, and_CC results_VBZ with_IN 2_CD instead_RB of_IN 2_CD =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
a_DT margin_NN in_IN terms_NNS of_IN the_DT 1-norm_NN ,_, ,_, ,_, ,_, The_DT mathematical_JJ program_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- is_VBZ easily_RB converted_VBN to_TO a_DT linear_JJ program_NN as_IN follows_VBZ :_: min_NN yy_NN +_CC v_LS +_CC i_FW y._FW -LRB-_-LRB- w_NN ,_, \/_: ,_, y_NN ,_, v_LS -RRB-_-RRB- GRn_NN +_CC l_NN +_CC m_NN +_CC n_NN s.t._NN D_NN -LRB-_-LRB- Aw_UH -_: 7_CD -RRB-_-RRB- +_CC Y_NN -_: s_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- v_LS -RRB-_-RRB- w_NN -RRB-_-RRB- --_: v_LS -RRB-_-RRB- 0_CD ._.
Y_NN
I._NN I1_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- and_CC FSV_NN -LRB-_-LRB- 2_CD ,_, Eqn_NN ._.
-LRB-_-LRB- 8_CD -RRB-_-RRB- -RRB-_-RRB- were_VBD tested_VBN on_IN seven_CD datasets_NNS ,_, the_DT first_JJ five_CD of_IN which_WDT ,_, WPBC_NNP ,_, Ionosphere_NNP ,_, Cleveland_NNP Heart_NNP ,_, Pima_NNP Indians_NNPS ,_, and_CC BUPA_NN Liver_NN are_VBP from_IN the_DT Irvine_NNP Machine_NNP Learning_NNP Repository_NNP =_SYM -_: =[_NN 18_CD -RRB-_-RRB- ,_, while_IN th_DT -_: =_SYM -_: e_LS Galaxy_NNP Dim_NNP dataset_NN is_VBZ from_IN -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, and_CC the_DT Census_NNP dataset_NN is_VBZ a_DT version_NN of_IN the_DT US_NNP Census_NNP Bureau_NNP ``_`` Adult_JJ ''_'' dataset_NN ,_, which_WDT is_VBZ publicly_RB available_JJ from_IN Silicon_NNP Graphics_NNP '_POS website_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
For_IN WPBC_NN -LRB-_-LRB- 60_CD -RRB-_-RRB- ,_,
p_NN function_NN term_NN ey_NN ._.
in_IN -LRB-_-LRB- 9_CD -RRB-_-RRB- can_MD be_VB handled_VBN directly_RB by_IN an_DT algorithm_NN such_JJ as_IN that_DT of_IN -LRB-_-LRB- 12_CD ,_, Algorithm_NN 1_CD SLA_NN -RRB-_-RRB- ,_, we_PRP prefer_VBP to_TO approximate_JJ it_PRP here_RB by_IN a_DT smooth_JJ concave_NN exponential_NN on_IN the_DT nonnegative_JJ real_JJ line_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: as_IN was_VBD done_VBN in_IN the_DT feature_NN selection_NN approach_NN of_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
For_IN y_FW __FW -RRB-_-RRB- 0_CD ,_, the_DT approximation_NN of_IN the_DT step_NN vector_NN y._NN of_IN -LRB-_-LRB- 9_CD -RRB-_-RRB- by_IN the_DT concave_NN exponential_NN ,_, yi_FW ._.
s1_NN -_: sv_NN ,_, i_FW 1_CD ,_, ..._: ,_, m_NN ,_, that_WDT is_VBZ :_: y._NN e_SYM -_: v_LS ,_, a_DT -RRB-_-RRB- 0_CD ,_, -LRB-_-LRB- 14_CD -RRB-_-RRB- wheresi_NNS
rging_VBG and_CC processing_VBG it_PRP with_IN new_JJ incoming_JJ data_NNS ._.
Keywords_NNS support_VBP vector_NN machines_NNS ,_, data_NNS classification_NN ,_, data_NNS selection_NN ,_, concave_NN minimization_NN ,_, linear_JJ programming_NN 1_CD ._.
INTRODUCTION_NN Support_NN vector_NN machines_NNS =_JJ -_: =[_NN 20_CD ,_, 8_CD ,_, 3_CD ,_, 7_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP powerful_JJ tools_NNS for_IN data_NNS classification_NN ._.
Classification_NN is_VBZ achieved_VBN by_IN a_DT linear_JJ or_CC nonlinear_JJ separating_VBG surface_NN in_IN the_DT input_NN space_NN of_IN the_DT dataset_NN ._.
The_DT separating_VBG surface_NN depends_VBZ only_RB on_IN a_DT subset_NN o_NN
rging_VBG and_CC processing_VBG it_PRP with_IN new_JJ incoming_JJ data_NNS ._.
Keywords_NNS support_VBP vector_NN machines_NNS ,_, data_NNS classification_NN ,_, data_NNS selection_NN ,_, concave_NN minimization_NN ,_, linear_JJ programming_NN 1_CD ._.
INTRODUCTION_NN Support_NN vector_NN machines_NNS =_JJ -_: =[_NN 20_CD ,_, 8_CD ,_, 3_CD ,_, 7_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP powerful_JJ tools_NNS for_IN data_NNS classification_NN ._.
Classification_NN is_VBZ achieved_VBN by_IN a_DT linear_JJ or_CC nonlinear_JJ separating_VBG surface_NN in_IN the_DT input_NN space_NN of_IN the_DT dataset_NN ._.
The_DT separating_VBG surface_NN depends_VBZ only_RB on_IN a_DT subset_NN o_NN
NE_NN In_IN order_NN to_TO make_VB use_NN of_IN a_DT faster_JJR linear_JJ programming_NN based_VBN approach_NN ,_, instead_RB of_IN the_DT standard_JJ quadratic_JJ programming_NN formulation_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ,_, we_PRP reformulate_VBP -LRB-_-LRB- 1_LS -RRB-_-RRB- by_IN replacing_VBG the_DT 2-norm_NN by_IN a_DT 1-norm_NN as_IN follows_VBZ =_JJ -_: =[_NN 14_CD ,_, 2_CD -RRB-_-RRB- -_: =_JJ -_: :_: min_NN -LRB-_-LRB- w_NN ,_, \/_: ,_, y_NN -RRB-_-RRB- CRn_NN +_CC l_NN +_CC m_NN ._.
c_NN '_POS y_NN +_CC Ilmll_NN ._.
Y_NN -RRB-_-RRB- y_NN +_CC Y_NN -RRB-_-RRB- lml_FW i_FW =_JJ i_FW j_FW =_JJ i_FW D_NN -LRB-_-LRB- Aw_UH -_: e7_NN -RRB-_-RRB- +_CC y_FW __FW -RRB-_-RRB- e_SYM y_FW __FW -RRB-_-RRB- 0_CD ._.
-LRB-_-LRB- 7_CD -RRB-_-RRB- This_NNP S_NNP VMII_NNP ._.
I_NN reformulation_NN in_IN effect_NN maximizes_VBZ the_DT margin_NN ,_, the_DT distance_NN between_IN the_DT two_CD bounding_VBG planes_NNS of_IN Figure_NNP
rging_VBG and_CC processing_VBG it_PRP with_IN new_JJ incoming_JJ data_NNS ._.
Keywords_NNS support_VBP vector_NN machines_NNS ,_, data_NNS classification_NN ,_, data_NNS selection_NN ,_, concave_NN minimization_NN ,_, linear_JJ programming_NN 1_CD ._.
INTRODUCTION_NN Support_NN vector_NN machines_NNS =_JJ -_: =[_NN 20_CD ,_, 8_CD ,_, 3_CD ,_, 7_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP powerful_JJ tools_NNS for_IN data_NNS classification_NN ._.
Classification_NN is_VBZ achieved_VBN by_IN a_DT linear_JJ or_CC nonlinear_JJ separating_VBG surface_NN in_IN the_DT input_NN space_NN of_IN the_DT dataset_NN ._.
The_DT separating_VBG surface_NN depends_VBZ only_RB on_IN a_DT subset_NN o_NN
tive_JJ multipliers_NNS u_NN C_NN R_NN TM_NN associated_VBN with_IN the_DT first_JJ set_NN of_IN constraints_NNS of_IN the_DT linear_JJ program_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, and_CC multipliers_NNS -LRB-_-LRB- r_NN ,_, s_NN -RRB-_-RRB- C_NN R_NN '_'' +_CC '_'' for_IN the_DT second_JJ set_NN of_IN constraints_NNS of_IN -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, then_RB the_DT dual_JJ linear_JJ program_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: associated_VBN with_IN the_DT linear_JJ SVM_NN formulation_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- is_VBZ the_DT following_NN :_: max_NN -LRB-_-LRB- u_NN ,_, r_NN ,_, s_NNS -RRB-_-RRB- ERm_NNS +_CC n_NN +_CC n_NN s.t._FW A'Du_FW -_: r_NN +_CC s_NN 0_CD -_: Du_NNP 0_CD usT_NN '_'' 4-8sO_NN ._.
-LRB-_-LRB- 11_CD -RRB-_-RRB- Equality_NN of_IN the_DT primal_JJ objective_JJ function_NN of_IN -LRB-_-LRB- 8_CD -RRB-_-RRB- and_CC the_DT dual_JJ objective_NN
