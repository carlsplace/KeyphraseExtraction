The_DT distributed_VBN boosting_VBG algorithm_NN
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT general_JJ framework_NN for_IN distributed_VBN boosting_VBG intended_JJ for_IN efficient_JJ integrating_VBG specialized_JJ classifiers_NNS learned_VBD over_IN very_RB large_JJ and_CC distributed_VBN homogeneous_JJ databases_NNS that_WDT can_MD not_RB be_VB merged_VBN at_IN a_DT single_JJ location_NN ._.
Our_PRP$ distributed_VBN boosting_VBG algorithm_NN can_MD also_RB be_VB used_VBN as_IN a_DT parallel_JJ classification_NN technique_NN ,_, where_WRB a_DT massive_JJ database_NN that_WDT can_MD not_RB fit_VB into_IN main_JJ computer_NN memory_NN is_VBZ partitioned_VBN into_IN disjoint_NN subsets_NNS for_IN a_DT more_RBR efficient_JJ analysis_NN ._.
In_IN the_DT proposed_VBN method_NN ,_, at_IN each_DT boosting_VBG round_NN the_DT classifiers_NNS are_VBP first_RB learned_VBN from_IN disjoint_FW datasets_FW and_CC then_RB exchanged_VBD amongst_IN the_DT sites_NNS ._.
Finally_RB the_DT classifiers_NNS are_VBP combined_VBN into_IN a_DT weighted_JJ voting_NN ensemble_NN on_IN each_DT disjoint_JJ data_NN set_NN ._.
The_DT ensemble_NN that_WDT is_VBZ applied_VBN to_TO an_DT unseen_JJ test_NN set_NN represents_VBZ an_DT ensemble_NN of_IN ensembles_NNS built_VBN on_IN all_DT distributed_VBN sites_NNS ._.
In_IN experiments_NNS performed_VBN on_IN four_CD large_JJ data_NNS sets_VBZ the_DT proposed_JJ distributed_VBN boosting_VBG method_NN achieved_VBD classification_NN accuracy_NN comparable_JJ or_CC even_RB slightly_RB better_JJR than_IN the_DT standard_JJ boosting_VBG algorithm_NN while_IN requiring_VBG less_JJR memory_NN and_CC less_RBR computational_JJ time_NN ._.
In_IN addition_NN ,_, the_DT communication_NN overhead_NN of_IN the_DT distributed_VBN boosting_VBG algorithm_NN is_VBZ very_RB small_JJ making_VBG it_PRP a_DT viable_JJ alternative_NN to_TO the_DT standard_NN boosting_VBG for_IN large-scale_JJ databases_NNS ._.
erns_NNS highlight_VBP distributed_VBN inference_NN as_IN a_DT key_JJ issue_NN in_IN this_DT setting_NN as_RB well_RB ._.
Recent_JJ research_NN has_VBZ studied_VBN inference_NN in_IN the_DT distributed_VBN databases_NNS setting_VBG from_IN an_DT algorithmic_JJ point_NN of_IN view_NN ;_: for_IN example_NN ,_, =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT distributed_VBN boosting_VBG algorithm_NN and_CC studied_VBD its_PRP$ performance_NN empirically_RB ._.
Distributed_VBN detection_NN and_CC estimation_NN is_VBZ a_DT well-developed_JJ field_NN with_IN a_DT rich_JJ history_NN ._.
Much_JJ of_IN the_DT work_NN in_IN this_DT area_NN ha_NN
trees_NNS in_IN the_DT forest_NN ._.
Random_JJ inputs_NNS and_CC random_JJ features_NNS produce_VBP good_JJ results_NNS in_IN the_DT classification_NN ._.
For_IN distributed_VBN data_NNS classification_NN using_VBG the_DT ensemble_NN method_NN ,_, several_JJ techniques_NNS have_VBP been_VBN proposed_VBN =_JJ -_: =[_NN 20,21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Distributed_VBN classification_NN in_IN peer-to-peer_JJ networks_NNS is_VBZ also_RB studied_VBN -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
The_DT authors_NNS proposed_VBD an_DT ensemble_NN approach_NN ,_, in_IN which_WDT each_DT peer_VBP builds_VBZ its_PRP$ local_JJ classifiers_NNS on_IN the_DT local_JJ data_NNS and_CC then_RB combin_VB
trees_NNS in_IN the_DT forest_NN ._.
Random_JJ inputs_NNS and_CC random_JJ features_NNS produce_VBP good_JJ results_NNS in_IN the_DT classification_NN ._.
For_IN distributed_VBN data_NNS classification_NN using_VBG the_DT ensemble_NN method_NN ,_, several_JJ techniques_NNS have_VBP been_VBN proposed_VBN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Distributed_VBN classification_NN in_IN peer-to-peer_JJ networks_NNS is_VBZ also_RB studied_VBN -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
The_DT authors_NNS proposed_VBD an_DT ensemble_NN approach_NN ,_, in_IN which_WDT each_DT peer_VBP builds_VBZ its_PRP$ local_JJ classifiers_NNS on_IN the_DT local_JJ data_NNS and_CC then_RB
g_NN from_IN distributed_VBN data_NNS sets_NNS ,_, people_NNS have_VBP designed_VBN methods_NNS that_WDT can_MD learn_VB multiple_JJ classifiers_NNS from_IN local_JJ data_NNS sets_NNS in_IN a_DT distributed_VBN environment_NN and_CC then_RB combine_VB local_JJ classifiers_NNS into_IN a_DT global_JJ model_NN =_JJ -_: =[_NN 32_CD ,_, 33_CD ,_, 34_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, these_DT methods_NNS still_RB focus_VB on_IN learning_VBG from_IN a_DT training_NN set_VBN with_IN sufficient_JJ labeled_JJ examples_NNS ._.
Moreover_RB ,_, the_DT proposed_VBN problems_NNS and_CC solutions_NNS in_IN this_DT paper_NN are_VBP different_JJ from_IN the_DT following_JJ work_NN
t_NN ,_, our_PRP$ work_NN makes_VBZ use_NN of_IN -LRB-_-LRB- in_IN spirit_NN -RRB-_-RRB- the_DT active_JJ learning_NN paradigm_NN -LRB-_-LRB- Settles_VBZ ,_, 2009_CD -RRB-_-RRB- ._.
For_IN distributed_VBN classification_NN ,_, the_DT dominant_JJ strategy_NN -LRB-_-LRB- Predd_NNP et_FW al._FW ,_, 2006_CD ;_: McDonald_NNP et_FW al._FW ,_, 2010_CD ;_: Mann_NNP et_FW al._FW ,_, 2009_CD ;_: =_JJ -_: =_JJ Lazarevic_NNP &_CC Obradovic_NNP ,_, 2001_CD -_: =--RRB-_NN is_VBZ to_TO design_VB local_JJ classifiers_NNS that_WDT work_VBP well_RB on_IN individual_JJ nodes_NNS ._.
These_DT classifiers_NNS are_VBP then_RB communicated_VBN to_TO a_DT central_JJ server_NN ,_, and_CC then_RB aggregation_NN strategies_NNS like_IN voting_NN ,_, averaging_NN ,_, or_CC even_RB boosti_VB
ately_RB difficult_JJ examples_NNS ._.
After_IN each_DT new_JJ model_NN is_VBZ constructed_VBN ,_, the_DT dataset_NN is_VBZ reweighted_VBN using_VBG all_DT of_IN the_DT models_NNS so_RB far_RB ._.
This_DT algorithm_NN can_MD be_VB parallelized_VBN in_IN a_DT style_NN that_IN ,_, by_IN now_RB ,_, should_MD be_VB familiar_JJ =_JJ -_: =[_NN 23_CD ,_, 24_CD ,_, 46_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Each_DT processor_NN executes_VBZ the_DT sequential_JJ algorithm_NN on_IN its_PRP$ own_JJ piece_NN of_IN the_DT dataset_NN ._.
However_RB ,_, once_RB every_DT processor_NN has_VBZ built_VBN its_PRP$ model_NN ,_, these_DT are_VBP exchanged_VBN with_IN all_DT of_IN the_DT other_JJ processors_NNS ._.
The_DT reweigh_NN
atures_NNS ,_, 3_CD discrete_JJ features_NNS -LRB-_-LRB- 0_CD to_TO 255_CD -RRB-_-RRB- ,_, and_CC 44_CD binary_JJ features_NNS ._.
The_DT distribution_NN of_IN the_DT seven_CD classes_NNS in_IN the_DT data_NNS set_NN is_VBZ shown_VBN in_IN Table_NNP 1_CD ._.
In_IN our_PRP$ experiments_NNS ,_, we_PRP followed_VBD the_DT train\/test_JJS split_NN used_VBN in_IN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP randomly_RB chose_VBD 149,982_CD records_NNS for_IN the_DT training_NN set_NN ._.
The_DT remaining_VBG 431,030_CD records_NNS were_VBD used_VBN for_IN the_DT testing_NN set_NN ._.
The_DT class_NN distribution_NN in_IN the_DT training_NN set_NN and_CC testing_NN set_NN were_VBD similar_JJ ._.
2.1_CD Basi_NNS
ng_NN set_NN is_VBZ noisy_JJ ._.
Other_JJ noisy_JJ tolerant_JJ boosting_VBG algorithms_NNS include_VBP AdaBoostReg_NNP -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, AveBoost2_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, et_FW al._FW ._.
To_TO satisfy_VB the_DT requirement_NN of_IN rapid_JJ developing_VBG distributed_VBN applications_NNS ,_, Fan_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- and_CC Lazarevic_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =-[_NN 11_CD -RRB-_-RRB- proposed_VBD the_DT distributed_VBN versions_NNS of_IN boosting_VBG for_IN parallel_NN and_CC distributed_VBN data_NNS mining_NN ._.
The_DT distributed_VBN boosting_VBG algorithms_NNS put_VBD more_JJR efforts_NNS on_IN the_DT disjoint_JJ partitions_NNS of_IN the_DT data_NNS set_NN -LRB-_-LRB- d-sampling_NN
or_CC each_DT local_JJ data_NN set_NN ;_: and_CC b_LS -RRB-_-RRB- combining_VBG the_DT multiple_JJ local_JJ rule_NN sets_VBZ into_IN a_DT single_JJ global_JJ rule_NN set_NN ._.
This_DT basic_JJ two-phase_JJ scenario_NN is_VBZ often_RB used_VBN in_IN the_DT distributed_VBN data_NN mining_NN literature_NN --_: see_VB ,_, e.g._FW ,_, =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: --_: and_CC its_PRP$ basic_JJ idea_NN is_VBZ illustrated_VBN in_IN Figure_NNP 1_CD ._.
data_NNS data_NNS subset_NN 1_CD ..._: subset_NN N_NN rule_NN rule_NN set_VBN 1_CD set_NN N_NN global_JJ rule_NN set_VBN Figure_NNP 1_CD :_: generating_VBG a_DT global_JJ rule_NN set_VBN from_IN N_NN local_JJ data_NN subsets_NNS We_PRP emphas_VBP
pooling_VBG knowledge_NN in_IN the_DT form_NN of_IN an_DT ensemble_NN did_VBD increase_VB the_DT chance_NN of_IN hitting_VBG the_DT target_NN ._.
There_EX has_VBZ been_VBN other_JJ research_NN on_IN sharing_NN classifiers_NNS within_IN an_DT institution_NN or_CC among_IN different_JJ institutions_NNS =_JJ -_: =[_NN 9_CD ,_, 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, they_PRP all_DT required_VBD that_IN the_DT data_NNS be_VB from_IN the_DT same_JJ problem_NN domain_NN ,_, or_CC in_IN other_JJ words_NNS ,_, the_DT inducted_VBN classifiers_NNS are_VBP all_DT solving_VBG the_DT same_JJ classification_NN problem_NN ._.
Now_RB the_DT question_NN is_VBZ ,_, if_IN we_PRP are_VBP
ith_VB a_DT final_JJ arbiter_NN tree_NN that_WDT combines_VBZ their_PRP$ predictions_NNS -LRB-_-LRB- 10_CD -RRB-_-RRB- ;_: -LRB-_-LRB- 3_LS -RRB-_-RRB- distributed_VBN learning_NN of_IN trees_NNS by_IN boosting_VBG ,_, which_WDT operates_VBZ over_IN partitions_NNS of_IN a_DT large_JJ dataset_NN that_WDT are_VBP exchanged_VBN among_IN the_DT processors_NNS =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_JJ -_: ;_: -LRB-_-LRB- 4_LS -RRB-_-RRB- the_DT SPIES_NNS algorithm_NN ,_, which_WDT combines_VBZ the_DT AVC-group_JJ idea_NN of_IN RAINFOREST_NN with_IN effective_JJ sampling_NN of_IN the_DT training_NN data_NNS to_TO obtain_VB a_DT communication_NN -_: and_CC memory-efficient_JJ parallel_JJ tree_NN learning_NN method_NN -LRB-_-LRB- 1_CD
