An_DT iterative_JJ method_NN for_IN multi-class_JJ cost-sensitive_JJ learning_NN
Cost-sensitive_JJ learning_NN addresses_VBZ the_DT issue_NN of_IN classification_NN in_IN the_DT presence_NN of_IN varying_VBG costs_NNS associated_VBN with_IN different_JJ types_NNS of_IN misclassification_NN ._.
In_IN this_DT paper_NN ,_, we_PRP present_VBP a_DT method_NN for_IN solving_VBG multi-class_JJ cost-sensitive_JJ learning_NN problems_NNS using_VBG any_DT binary_JJ classification_NN algorithm_NN ._.
This_DT algorithm_NN is_VBZ derived_VBN using_VBG hree_CD key_JJ ideas_NNS :_: 1_LS -RRB-_-RRB- iterative_JJ weighting_NN ;_: 2_LS -RRB-_-RRB- expanding_VBG data_NNS space_NN ;_: and_CC 3_LS -RRB-_-RRB- gradient_NN boosting_VBG with_IN stochastic_JJ ensembles_NNS ._.
We_PRP establish_VBP some_DT theoretical_JJ guarantees_NNS concerning_VBG the_DT performance_NN of_IN this_DT method_NN ._.
In_IN particular_JJ ,_, we_PRP show_VBP that_IN a_DT certain_JJ variant_NN possesses_VBZ the_DT boosting_VBG property_NN ,_, given_VBN a_DT form_NN of_IN weak_JJ learning_NN assumption_NN on_IN the_DT component_NN binary_JJ classifier_NN ._.
We_PRP also_RB empirically_RB evaluate_VB the_DT performance_NN of_IN the_DT proposed_VBN method_NN using_VBG benchmark_JJ data_NNS sets_NNS and_CC verify_VB that_DT our_PRP$ method_NN generally_RB achieves_VBZ better_JJR results_NNS than_IN representative_JJ methods_NNS for_IN cost-sensitive_JJ learning_NN ,_, in_IN terms_NNS of_IN predictive_JJ performance_NN -LRB-_-LRB- cost_NN minimization_NN -RRB-_-RRB- and_CC ,_, in_IN many_JJ cases_NNS ,_, computational_JJ efficiency_NN ._.
h_NN in_IN cost-sensitive_JJ learning_NN falls_VBZ into_IN three_CD main_JJ categories_NNS ._.
The_DT first_JJ category_NN is_VBZ concerned_VBN with_IN making_VBG particular_JJ classifier_NN learners_NNS cost-sensitive_JJ ,_, including_VBG methods_NNS specific_JJ for_IN decision_NN trees_NNS =_JJ -_: =[_NN 14_CD ,_, 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, neural_JJ networks_NNS -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC support_NN vector_NN machines_NNS -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
The_DT second_JJ category_NN uses_VBZ Bayes_NNP risk_NN theory_NN to_TO assign_VB each_DT example_NN to_TO its_PRP$ lowest_JJS expected_VBN cost_NN class_NN -LRB-_-LRB- 8_CD ,_, 18_CD -RRB-_-RRB- ._.
This_DT requires_VBZ classifiers_NNS to_TO outpu_VB
earner_RB achieve_VB better_JJR weighted_JJ accuracy_NN than_IN that_DT attainable_JJ by_IN assigning_VBG all_DT examples_NNS to_TO the_DT negative_JJ class_NN ,_, as_IN we_PRP mentioned_VBD earlier_RBR ._.
4_LS ._.
EXPERIMENTAL_JJ EVALUATION_NN We_PRP use_VBP the_DT C4_NN .5_CD decision_NN tree_NN learner_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: as_IN the_DT base_NN classifier_NN learning_NN method_NN ,_, because_IN it_PRP is_VBZ a_DT standard_NN for_IN empirical_JJ comparisons_NNS and_CC it_PRP was_VBD used_VBN as_IN the_DT base_NN learner_NN by_IN Domingos_NNP for_IN the_DT MetaCost_NNP method_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
We_PRP compare_VBP our_PRP$ methods_NNS against_IN
ing_NN falls_VBZ into_IN three_CD main_JJ categories_NNS ._.
The_DT first_JJ category_NN is_VBZ concerned_VBN with_IN making_VBG particular_JJ classifier_NN learners_NNS cost-sensitive_JJ ,_, including_VBG methods_NNS specific_JJ for_IN decision_NN trees_NNS -LRB-_-LRB- 14_CD ,_, 4_CD -RRB-_-RRB- ,_, neural_JJ networks_NNS =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: and_CC support_NN vector_NN machines_NNS -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
The_DT second_JJ category_NN uses_VBZ Bayes_NNP risk_NN theory_NN to_TO assign_VB each_DT example_NN to_TO its_PRP$ lowest_JJS expected_VBN cost_NN class_NN -LRB-_-LRB- 8_CD ,_, 18_CD -RRB-_-RRB- ._.
This_DT requires_VBZ classifiers_NNS to_TO output_NN class_NN membership_NN pro_NN
d_NN ,_, because_IN it_PRP is_VBZ a_DT standard_NN for_IN empirical_JJ comparisons_NNS and_CC it_PRP was_VBD used_VBN as_IN the_DT base_NN learner_NN by_IN Domingos_NNP for_IN the_DT MetaCost_NNP method_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
We_PRP compare_VBP our_PRP$ methods_NNS against_IN three_CD representative_JJ methods_NNS :_: Bagging_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Averaging_VBG cost_NN -LRB-_-LRB- 7_CD ,_, 8_CD -RRB-_-RRB- and_CC MetaCost_NNP ._.
The_DT Averaging_VBG cost_NN method_NN was_VBD also_RB used_VBN for_IN comparison_NN in_IN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
Note_VB that_IN Bagging_NNP is_VBZ a_DT cost-insensitive_JJ learning_NN method_NN ._.
Here_RB we_PRP give_VBP a_DT brief_JJ description_NN of_IN these_DT
e_LS ,_, we_PRP assumed_VBD that_IN the_DT hypotheses_NNS output_NN by_IN a_DT cost-sensitive_JJ learner_NN is_VBZ a_DT functional_JJ hypothesis_NN h_NN ,_, i.e._FW h_NN :_: X_NN !_.
Y_NN :_: It_PRP is_VBZ also_RB possible_JJ to_TO allow_VB hypotheses_NNS that_WDT are_VBP stochastic_JJ ,_, namely_RB h_NN :_: X_NN \_FW ThetasY_FW !_.
=_SYM -_: =[_NN 0_CD ;_: 1_LS -RRB-_-RRB- -_: =_JJ -_: subject_JJ to_TO the_DT stochastic_JJ condition_NN :_: 8x_NN 2_CD X_NN X_NN y2Y_NN h_NN -LRB-_-LRB- yjx_NN -RRB-_-RRB- =_JJ 1_CD :_: With_IN stochastic_JJ hypotheses_NNS ,_, stochastic_JJ cost-sensitive_JJ learning_NN is_VBZ defined_VBN as_IN the_DT process_NN of_IN finding_VBG a_DT hypothesis_NN minimizing_VBG the_DT following_VBG
