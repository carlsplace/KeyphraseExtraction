A_DT scalable_JJ modular_JJ convex_NN solver_NN for_IN regularized_VBN risk_NN minimization_NN
A_DT wide_JJ variety_NN of_IN machine_NN learning_NN problems_NNS can_MD be_VB described_VBN as_IN minimizing_VBG a_DT regularized_VBN risk_NN functional_JJ ,_, with_IN different_JJ algorithms_NNS using_VBG different_JJ notions_NNS of_IN risk_NN and_CC different_JJ regularizers_NNS ._.
Examples_NNS include_VBP linear_JJ Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- ,_, Logistic_JJ Regression_NN ,_, Conditional_JJ Random_NNP Fields_NNP -LRB-_-LRB- CRFs_NNS -RRB-_-RRB- ,_, and_CC Lasso_NNP amongst_IN others_NNS ._.
This_DT paper_NN describes_VBZ the_DT theory_NN and_CC implementation_NN of_IN a_DT highly_RB scalable_JJ and_CC modular_JJ convex_NN solver_NN which_WDT solves_VBZ all_PDT these_DT estimation_NN problems_NNS ._.
It_PRP can_MD be_VB parallelized_VBN on_IN a_DT cluster_NN of_IN workstations_NNS ,_, allows_VBZ for_IN data-locality_NN ,_, and_CC can_MD deal_VB with_IN regularizers_NNS such_JJ as_IN l1_NN and_CC l2_NN penalties_NNS ._.
At_IN present_NN ,_, our_PRP$ solver_NN implements_VBZ 20_CD different_JJ estimation_NN problems_NNS ,_, can_MD be_VB easily_RB extended_VBN ,_, scales_NNS to_TO millions_NNS of_IN observations_NNS ,_, and_CC is_VBZ up_IN to_TO 10_CD times_NNS faster_RBR than_IN specialized_VBN solvers_NNS for_IN many_JJ applications_NNS ._.
The_DT open_JJ source_NN code_NN is_VBZ freely_RB available_JJ as_IN part_NN of_IN the_DT ELEFANT_NNP toolbox_NN ._.
that_IN ξ_NN =_JJ 1_CD ∑_CD n_NN n_NN i_FW =_JJ 1_CD ξi_NN ._.
While_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- has_VBZ a_DT huge_JJ number_NN of_IN constraints_NNS ,_, Algorithm_NN 1_CD is_VBZ a_DT cutting-plane_JJ procedure_NN that_WDT always_RB constructs_NNS a_DT solution_NN of_IN precision_NN ɛ_NN with_IN at_IN most_JJS O_NN -LRB-_-LRB- C_NN ɛ_NN -RRB-_-RRB- active_JJ constraints_NNS =_JJ -_: =[_NN 15_CD ,_, 18_CD ,_, 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT experiments_NNS from_IN Section_NN 6_CD ,_, the_DT number_NN of_IN active_JJ constraints_NNS was_VBD typically_RB around_IN 30_CD --_: independent_JJ of_IN the_DT size_NN of_IN the_DT training_NN set_NN ._.
Algorithm_NN 1_CD maintains_VBZ a_DT working_VBG set_NN of_IN m_NN constraints_NNS 〈_FW w_FW ,_, ¯_NN
ching_VBG data_NNS vectors_NNS and_CC scheduling_NN model_NN updates_NNS ,_, we_PRP opted_VBD to_TO develop_VB our_PRP$ own_JJ solver_NN implementing_VBG the_DT 1-slack_JJ cutting_VBG plane_NN algorithm_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
We_PRP briefly_RB outline_VBP our_PRP$ approach_NN here_RB using_VBG the_DT notation_NN from_IN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Consider_VB the_DT following_JJ unconstrained_JJ formulation_NN that_IN isequivalent_NN to_TO the_DT constrained_VBN problem_NN from_IN -LRB-_-LRB- 6_CD -RRB-_-RRB- :_: w_FW ∗_FW =_JJ arg_NN min_NN L_NN -LRB-_-LRB- w_NN -RRB-_-RRB- w_NN where_WRB L_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ 1_CD 2_CD |_CD |_CD w_NN |_NNP |_NNP 2_CD +_CC CR_NN -LRB-_-LRB- w_NN -RRB-_-RRB- N_NN ∑_NN R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ max_NN -LRB-_-LRB- 0_CD ,_, l_NN -LRB-_-LRB- Yn_NN ,_, H_NN -RRB-_-RRB- −_FW w_FW H_NN T_NN ∆_NN Ψ_NN -LRB-_-LRB- Xn_NN ,_,
a_DT small_JJ set_NN of_IN critical_JJ constraints_NNS ,_, which_WDT will_MD be_VB the_DT only_JJ ones_NNS to_TO be_VB ultimately_RB enforced_VBN -LRB-_-LRB- 15_CD -RRB-_-RRB- ._.
4.5_CD ._.
The_DT BMRM_NNP Algorithm_NNP We_PRP use_VBP the_DT ``_`` Bundle_NN Methods_NNS for_IN Regularized_NNP Risk_NNP Minimization_NNP ''_'' -LRB-_-LRB- BMRM_NN -RRB-_-RRB- solver_NN of_IN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT merely_RB requires_VBZ that_IN for_IN each_DT candidate_NN θ_NN ,_, we_PRP compute_VBP the_DT difference_NN in_IN gradient_NN -LRB-_-LRB- w.r.t._FW θ_FW -RRB-_-RRB- of_IN the_DT score_NN function_NN of_IN the_DT true_JJ assignments_NNS -LRB-_-LRB- x_NN same_JJ n_NN -RRB-_-RRB- ,_, and_CC the_DT most_RBS violated_VBN constraint_NN -LRB-_-LRB- k_NN viol_NN
the_DT former_JJ stopping_VBG condition_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
Theorem_NN 1_CD by_IN Teo_NN et_FW al._FW -LRB-_-LRB- 2007_CD -RRB-_-RRB- guarantees_VBZ convergence_NN of_IN the_DT CPA_NNP algorithm_NN inO_NN -LRB-_-LRB- 1_CD ε_NN -RRB-_-RRB- time_NN for_IN a_DT broad_JJ class_NN of_IN risk_NN functions_NNS :_: 2161FRANC_NN AND_CC SONNENBURG_NN Theorem_NN 1_CD -LRB-_-LRB- =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =--RRB-_NN Assume_VB that_IN ‖_FW ∂_FW R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- ‖_FW ≤_FW G_NN for_IN all_DT w_FW ∈_FW W_NN ,_, whereW_NN is_VBZ some_DT domain_NN of_IN interest_NN containing_VBG all_DT wt_JJ ′_NN for_IN t_NN ′_FW ≤_FW t._FW In_IN this_DT case_NN ,_, for_IN any_DT ε_NN -RRB-_-RRB- 0_CD and_CC C_NN -RRB-_-RRB- 0_CD ,_, Algorithm_NN 1_CD satisfies_VBZ the_DT stopping_VBG condition_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- after_IN
with_IN respect_NN to_TO w_NN -RRB-_-RRB- is_VBZ w_NN 1_CD X_NN n_NN ð_FW ^_FW ynÞ_NN :_: ð14Þ_NN N_NN n_NN Equations_NNS -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 14_CD -RRB-_-RRB- define_VBP the_DT new_JJ constraint_NN to_TO be_VB added_VBN to_TO the_DT optimization_NN problem_NN ._.
Pseudocode_NN for_IN this_DT algorithm_NN is_VBZ described_VBN in_IN Algorithm_NNP 1_CD ._.
See_VB =_JJ -_: =[_NN 38_CD -RRB-_-RRB- -_: =_SYM -_: for_IN more_JJR details_NNS ._.
Let_VB us_PRP investigate_VB the_DT complexity_NN of_IN solving_VBG -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
Using_VBG the_DT joint_JJ feature_NN map_NN as_IN in_IN -LRB-_-LRB- 6_CD -RRB-_-RRB- and_CC the_DT loss_NN as_IN in_IN -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, the_DT argument_NN in_IN -LRB-_-LRB- 11_CD -RRB-_-RRB- becomes_VBZ h_NN ðG_NN ;_: G_NN 0_CD ;_: yÞ_NN ;_: wiþ_FW ðy_FW ;_: y_NN n_NN Þ_NN X_NN X_NN yii0c_NN
._.
The_DT optimization_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 12_CD -RRB-_-RRB- is_VBZ in_IN the_DT shape_NN :_: w_FW ∗_FW =_JJ argminwf_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ λ_NN 2_CD ‖_CD w_NN ‖_NN 2_CD +_CC R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- -LRB-_-LRB- 13_CD -RRB-_-RRB- where_WRB R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- is_VBZ an_DT upper_JJ bound_VBN of_IN the_DT empirical_JJ risk_NN that_IN we_PRP want_VBP to_TO minimize_VB ._.
The_DT approach_NN described_VBN in_IN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: belongs_VBZ to_TO the_DT familiy_NN of_IN bundle_NN methods_NNS and_CC solves_VBZ the_DT general_JJ optimization_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 13_CD -RRB-_-RRB- whatever_WDT R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- provided_VBD that_IN it_PRP is_VBZ convex_NN ._.
We_PRP briefly_RB describe_VBP the_DT method_NN in_IN the_DT case_NN where_WRB R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- is_VBZ con_NN
GDs_NNS ._.
Parallel_JJ optimization_NN methods_NNS have_VBP recently_RB attracted_VBN attention_NN as_IN a_DT way_NN to_TO scale_VB up_RP machine_NN learning_NN algorithms_NNS ._.
Map-Reduce_NNP -LRB-_-LRB- Dean_NNP &_CC Ghemawat_NNP ,_, 2008_CD -RRB-_-RRB- style_NN optimization_NN methods_NNS -LRB-_-LRB- Chu_NNP et_FW al._FW ,_, 2007_CD ;_: =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =--RRB-_NN have_VBP been_VBN successful_JJ early_JJ approaches_NNS ._.
We_PRP also_RB note_VBP recent_JJ studies_NNS -LRB-_-LRB- Mann_NNP et_FW al._FW ,_, 2009_CD ;_: Zinkevich_NNP et_FW al._FW ,_, 2010_CD -RRB-_-RRB- that_WDT have_VBP parallelized_VBN SGDs_NNS without_IN using_VBG the_DT Map-Reduce_NNP framework_NN ._.
In_IN our_PRP$ experiments_NNS ,_,
ion_NN to_TO structured_JJ learning_NN -LRB-_-LRB- or_CC multi-class_JJ -RRB-_-RRB- is_VBZ discussed_VBN ._.
Recently_RB ,_, several_JJ new_JJ algorithms_NNS have_VBP been_VBN presented_VBN ,_, along_IN with_IN a_DT rate_NN of_IN convergence_NN analysis_NN -LRB-_-LRB- Joachims_NNP ,_, 2006_CD ;_: Shalev-Shwartz_NNP et_FW al._FW ,_, 2007_CD ;_: =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =_JJ -_: ;_: Tsochantaridis_NNP et_FW al._FW ,_, 2004_CD -RRB-_-RRB- ._.
All_DT of_IN these_DT algorithms_NNS are_VBP similar_JJ to_TO ours_PRP in_IN having_VBG a_DT relatively_RB low_JJ dependence_NN on_IN n_NN in_IN terms_NNS of_IN memory_NN and_CC computation_NN ._.
Among_IN these_DT ,_, Shalev-Shwartz_NNP et_FW al._FW -LRB-_-LRB- 2007_CD -RRB-_-RRB- and_CC
and_CC users_NNS with_IN an_DT acceptable_JJ memory_NN footprint_NN ._.
We_PRP achieve_VBP these_DT goals_NNS by_IN combining_VBG -LRB-_-LRB- a_DT -RRB-_-RRB- recent_JJ results_NNS in_IN optimization_NN ,_, in_IN particular_JJ the_DT application_NN of_IN bundle_NN methods_NNS to_TO convex_VB optimization_NN problems_NNS =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- b_NN -RRB-_-RRB- techniques_NNS for_IN representing_VBG functions_NNS on_IN matrices_NNS ,_, in_IN particular_JJ maximum_NN margin_NN matrix_NN factorizations_NNS -LRB-_-LRB- 10_CD ,_, 11_CD ,_, 12_CD -RRB-_-RRB- and_CC -LRB-_-LRB- c_LS -RRB-_-RRB- the_DT application_NN of_IN structured_JJ estimation_NN for_IN ranking_JJ problems_NNS ._.
We_PRP descr_VBP
exploiting_VBG this_DT technique_NN ,_, which_WDT are_VBP valid_JJ for_IN both_CC infinite_JJ and_CC finite_JJ programs_NNS ._.
One_CD based_VBN on_IN a_DT batch_NN scenario_NN ,_, inspired_VBN by_IN SVMStruct_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, and_CC one_CD based_VBN on_IN an_DT online_JJ setting_NN ,_, inspired_VBN by_IN BMRM\/Pegasos_NN =_JJ -_: =[_NN 15_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
3.1_CD A_NN Variant_JJ of_IN SVMStruct_NN The_DT work_NN of_IN -LRB-_-LRB- 16_CD ,_, 10_CD -RRB-_-RRB- on_IN SVMStruct-like_JJ optimization_NN methods_NNS can_MD be_VB used_VBN directly_RB to_TO solve_VB regularized_JJ risk_NN minimization_NN problems_NNS ._.
The_DT basic_JJ idea_NN is_VBZ to_TO compute_VB gradients_NNS of_IN
and_CC O_NN -LRB-_-LRB- log_NN -LRB-_-LRB- 1_CD \/_: ɛ_NN -RRB-_-RRB- -RRB-_-RRB- convergence_NN ,_, whenever_WRB the_DT loss_NN is_VBZ sufficiently_RB smooth_JJ ._.
An_DT important_JJ feature_NN of_IN our_PRP$ algorithm_NN is_VBZ that_IN it_PRP automatically_RB takes_VBZ advantage_NN of_IN smoothness_NN in_IN the_DT problem_NN ._.
Our_PRP$ work_NN builds_VBZ on_IN =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT describes_VBZ the_DT basic_JJ extension_NN of_IN SVMPerf_NN to_TO general_JJ convex_NN problems_NNS ._.
The_DT current_JJ paper_NN provides_VBZ a_DT -RRB-_-RRB- significantly_RB improved_VBD performance_NN bounds_NNS which_WDT match_VBP better_RBR what_WP can_MD be_VB observed_VBN in_IN practice_NN
ion_NN methods_NNS for_IN these_DT formulations_NNS ._.
Recently_RB ,_, some_DT good_JJ methods_NNS have_VBP been_VBN proposed_VBN for_IN SVMs_NNS with_IN structured_JJ outputs_NNS ,_, that_WDT also_RB specialize_VBP nicely_RB for_IN Crammer-Singer_NNP multi-class_JJ linear_JJ SVMs_NNS ._.
Teo_NNP et_FW al._FW =_SYM -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: suggested_VBD a_DT bundle_NN method_NN and_CC Joachims_NNPS et_FW al._FW -LRB-_-LRB- 13_CD -RRB-_-RRB- gave_VBD a_DT cutting_VBG plane_NN method_NN that_WDT is_VBZ very_RB close_JJ to_TO it_PRP ;_: these_DT methods_NNS can_MD be_VB viewed_VBN as_IN extensions_NNS of_IN the_DT method_NN given_VBN by_IN Joachims_NNP -LRB-_-LRB- 12_CD -RRB-_-RRB- for_IN binary_JJ line_NN
italise_NN on_IN recent_JJ advances_NNS in_IN large-margin_JJ structured_JJ estimation_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, which_WDT consist_VBP of_IN obtaining_VBG convex_NN relaxations_NNS of_IN this_DT problem_NN ._.
Without_IN going_VBG into_IN the_DT details_NNS of_IN the_DT solution_NN -LRB-_-LRB- see_VB ,_, for_IN example_NN ,_, =_JJ -_: =[_NN 15_CD ,_, 16_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, it_PRP can_MD be_VB shown_VBN that_IN a_DT convex_NN relaxation_NN of_IN this_DT problem_NN can_MD be_VB obtained_VBN ,_, which_WDT is_VBZ given_VBN by_IN min_NN θ_NN 1_CD N_NN subject_JJ to_TO N_NN ∑_FW i_FW =_JJ 1_CD ξi_NN +_CC λ_NN 2_CD ‖_FW θ_FW ‖_FW 2_CD 2_CD -LRB-_-LRB- 6a_NN -RRB-_-RRB- 〈_CD h_NN -LRB-_-LRB- S_NN i_LS ,_, U_NNP i_LS ,_, y_FW i_FW -RRB-_-RRB- −_CD h_NN -LRB-_-LRB- S_NN i_LS ,_, U_NNP i_LS ,_, y_NN -RRB-_-RRB- ,_, θ_FW 〉_FW ≥_FW ∆_NN -LRB-_-LRB- y_NN ,_, y_FW i_FW -RRB-_-RRB-
ithm_VB We_PRP propose_VBP an_DT iterative_JJ algorithm_NN to_TO solve_VB Eq_NN ._.
-LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, which_WDT is_VBZ guaranteed_VBN to_TO converge_VB to_TO a_DT global_JJ optimum_NN ._.
The_DT algorithm_NN is_VBZ closely_RB related_JJ to_TO the_DT bundle_NN method_NN -LRB-_-LRB- Hiriart-Urruty_NNP &_CC Lemarechal_NNP ,_, 1993_CD ;_: =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =--RRB-_NN ._.
The_DT optimization_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 6_CD -RRB-_-RRB- maximizes_VBZ its_PRP$ objective_JJ function_NN with_IN respect_NN to_TO two_CD variables_NNS t_NN and_CC α_NN with_IN an_DT infinite_JJ number_NN of_IN -LRB-_-LRB- quadratic_JJ -RRB-_-RRB- constraints_NNS ._.
We_PRP approach_VBP the_DT optimum_NN by_IN optimizing_VBG
exploiting_VBG this_DT technique_NN ,_, which_WDT are_VBP valid_JJ for_IN both_CC infinite_JJ and_CC finite_JJ programs_NNS ._.
One_CD based_VBN on_IN a_DT batch_NN scenario_NN ,_, inspired_VBN by_IN SVMStruct_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, and_CC one_CD based_VBN on_IN an_DT online_JJ setting_NN ,_, inspired_VBN by_IN BMRM\/Pegasos_NN =_JJ -_: =[_NN 15_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
3.1_CD A_NN Variant_JJ of_IN SVMStruct_NN The_DT work_NN of_IN -LRB-_-LRB- 16_CD ,_, 10_CD -RRB-_-RRB- on_IN SVMStruct-like_JJ optimization_NN methods_NNS can_MD be_VB used_VBN directly_RB to_TO solve_VB regularized_JJ risk_NN minimization_NN problems_NNS ._.
The_DT basic_JJ idea_NN is_VBZ to_TO compute_VB gradients_NNS of_IN
GS_NN often_RB fails_VBZ to_TO converge_VB on_IN such_JJ problems_NNS -LRB-_-LRB- Lukˇsan_NN and_CC Vlček_NN ,_, 1999_CD ;_: Haarala_NNP ,_, 2004_CD -RRB-_-RRB- ._.
Various_JJ subgradient-based_JJ approaches_NNS ,_, such_JJ as_IN subgradient_JJ descent_NN -LRB-_-LRB- Nedich_NNP and_CC Bertsekas_NNP ,_, 2000_CD -RRB-_-RRB- or_CC bundle_NN methods_NNS -LRB-_-LRB- =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =--RRB-_NN ,_, are_VBP therefore_RB preferred_VBN ._.
A_DT Quasi-Newton_NNP Approach_NNP to_TO Nonsmooth_NNP Convex_NNP Optimization_NNP Although_IN a_DT convex_NN function_NN might_MD not_RB be_VB differentiable_JJ everywhere_RB ,_, a_DT subgradient_NN always_RB exists_VBZ ._.
Let_VB w_NN be_VB a_DT point_NN wh_NN
urred_JJ loss_NN ._.
This_DT has_VBZ two_CD benefits_NNS :_: firstly_RB ,_, the_DT problem_NN has_VBZ no_DT local_JJ minima_NN ,_, and_CC secondly_RB ,_, the_DT optimization_NN problem_NN is_VBZ continuous_JJ and_CC piecewise_JJ differentiable_NN ,_, which_WDT allows_VBZ for_IN effective_JJ optimization_NN =_JJ -_: =[_NN 17_CD ,_, 19_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT setting_NN ,_, however_RB ,_, exhibits_VBZ a_DT significant_JJ problem_NN :_: the_DT looseness_NN of_IN the_DT convex_NN upper_JJ bounds_NNS can_MD sometimes_RB lead_VB to_TO poor_JJ accuracy_NN ._.
For_IN binary_JJ classification_NN ,_, -LRB-_-LRB- 2_LS -RRB-_-RRB- proposed_VBN to_TO switch_VB from_IN the_DT hinge_NN
is_VBZ an_DT upper_JJ bound_VBN of_IN the_DT empirical_JJ risk_NN that_IN we_PRP want_VBP to_TO minimize_VB ._.
Our_PRP$ algorithm_NN is_VBZ inspired_VBN by_IN a_DT recent_JJ variant_NN of_IN bundle_NN methods_NNS for_IN minimizing_VBG convex_NN regularized_VBD risk_NN in_IN machine_NN learning_NN problems_NNS -LRB-_-LRB- =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =_JJ -_: ;_: Joachims_NNP ,_, 2006_CD -RRB-_-RRB- ._.
This_DT variant_NN has_VBZ two_CD main_JJ advantages_NNS ,_, the_DT first_JJ one_CD being_VBG its_PRP$ very_RB good_JJ convergence_NN rate_NN ,_, the_DT second_JJ one_CD being_VBG its_PRP$ relevant_JJ stopping_VBG criterion_NN ,_, namely_RB the_DT gap_NN between_IN the_DT objective_NN
hinge_NN loss_NN as_IN used_VBN in_IN support_NN vector_NN machines_NNS -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
In_IN this_DT paper_NN ,_, we_PRP focus_VBP on_IN the_DT logloss_NN ._.
This_DT ℓ1-norm_NN regularized_VBD MLE_NN problem_NN yields_VBZ a_DT sparse_JJ estimate_NN by_IN setting_VBG some_DT components_NNS of_IN w_NN to_TO exact_JJ zeros_NNS =_JJ -_: =[_NN 24_CD ,_, 11_CD -RRB-_-RRB- -_: =_JJ -_: and_CC has_VBZ efficient_JJ solvers_NNS ,_, such_JJ as_IN the_DT Orthant-Wise_NNP Limited-memory_JJ Quasi-Newton_NN -LRB-_-LRB- OWL-QN_NN -RRB-_-RRB- method_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
3_LS ._.
THE_DT STATISTICAL_JJ MODEL_NN In_IN this_DT section_NN ,_, we_PRP define_VBP the_DT task_NN of_IN entity_NN relationship_NN identification_NN
sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- Quantile_JJ regression_NN -LRB-_-LRB- 27_CD -RRB-_-RRB- max_NN -LRB-_-LRB- τ_NN -LRB-_-LRB- f_FW −_FW y_FW -RRB-_-RRB- ,_, -LRB-_-LRB- 1_CD −_FW τ_FW -RRB-_-RRB- -LRB-_-LRB- y_FW −_FW f_FW -RRB-_-RRB- -RRB-_-RRB- τ_NN if_IN f_LS -RRB-_-RRB- y_NN and_CC τ_FW −_FW 1_CD otherwise_RB ɛ-insensitive_JJ -LRB-_-LRB- 41_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, |_FW f_FW −_FW y_FW |_FW −_FW ɛ_FW -RRB-_-RRB- 0_CD if_IN |_FW f_FW −_FW y_FW |_FW ≤_FW ɛ_NN and_CC sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- otherwise_RB 1_CD Huber_NNP 's_POS robust_JJ loss_NN =_JJ -_: =[_NN 31_CD -RRB-_-RRB- -_: =_SYM -_: 2_CD -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD if_IN |_FW f_FW −_FW y_FW |_FW -LRB-_-LRB- 1_CD ,_, else_RB |_CD f_FW −_FW y_FW |_FW −_FW 1_CD f_FW −_FW y_FW if_IN |_FW f_FW −_FW y_FW |_FW ≤_FW 1_CD ,_, else_JJ sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD Poisson_NN regression_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- exp_NN -LRB-_-LRB- f_LS -RRB-_-RRB- −_FW yf_FW exp_FW -LRB-_-LRB- f_LS -RRB-_-RRB- −_FW y_FW Table_NNP 2_CD :_: Vectorial_JJ loss_NN functions_NNS and_CC their_PRP$ derivatives_NNS ,_, depending_VBG o_NN
ression_NN ._.
Extensions_NNS of_IN these_DT loss_NN functions_NNS allow_VBP us_PRP to_TO handle_VB structure_NN in_IN the_DT output_NN space_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
Changing_VBG the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- to_TO the_DT sparsity_NN inducing_VBG ‖_FW w_FW ‖_NN 1_CD leads_VBZ to_TO Lasso-type_JJ estimation_NN algorithms_NNS =_JJ -_: =[_NN 30_CD ,_, 39_CD ,_, 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT kernel_NN trick_NN is_VBZ widely_RB used_VBN to_TO transform_VB many_JJ of_IN these_DT algorithms_NNS into_IN ones_NNS operating_VBG on_IN a_DT Reproducing_NNP Kernel_NNP Hilbert_NNP Space_NNP -LRB-_-LRB- RKHS_NNP -RRB-_-RRB- ._.
One_CD lifts_VBZ w_NN into_IN an_DT RKHS_NN and_CC replaces_VBZ all_DT inner_JJ product_NN comput_NN
arse_NN features_NNS -LRB-_-LRB- e.g._FW the_DT bag_NN of_IN words_NNS representation_NN of_IN a_DT document_NN -RRB-_-RRB- ._.
Second_JJ ,_, many_JJ kernels_NNS -LRB-_-LRB- e.g._FW kernels_NNS on_IN strings_NNS -LRB-_-LRB- 42_CD -RRB-_-RRB- -RRB-_-RRB- can_MD effectively_RB be_VB linearized_VBN ,_, and_CC third_JJ ,_, efficient_JJ factorization_NN methods_NNS -LRB-_-LRB- e.g._FW =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =--RRB-_NN can_MD be_VB used_VBN for_IN a_DT low_JJ rank_NN representation_NN of_IN the_DT kernel_NN matrix_NN thereby_RB effectively_RB rendering_VBG the_DT problem_NN linear_NN ._.
For_IN each_DT of_IN the_DT above_JJ estimation_NN problems_NNS specialized_VBD solversexist_NN ,_, and_CC the_DT common_JJ a_DT
which_WDT employ_VBP the_DT kernel_NN trick_NN -LRB-_-LRB- but_CC essentially_RB still_RB solve_VB -LRB-_-LRB- 1_LS -RRB-_-RRB- -RRB-_-RRB- include_VBP Support_NN Vector_NNP regression_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_JJ -_: ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponen_NN
ression_NN ._.
Extensions_NNS of_IN these_DT loss_NN functions_NNS allow_VBP us_PRP to_TO handle_VB structure_NN in_IN the_DT output_NN space_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
Changing_VBG the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- to_TO the_DT sparsity_NN inducing_VBG ‖_FW w_FW ‖_NN 1_CD leads_VBZ to_TO Lasso-type_JJ estimation_NN algorithms_NNS =_JJ -_: =[_NN 30_CD ,_, 39_CD ,_, 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT kernel_NN trick_NN is_VBZ widely_RB used_VBN to_TO transform_VB many_JJ of_IN these_DT algorithms_NNS into_IN ones_NNS operating_VBG on_IN a_DT Reproducing_NNP Kernel_NNP Hilbert_NNP Space_NNP -LRB-_-LRB- RKHS_NNP -RRB-_-RRB- ._.
One_CD lifts_VBZ w_NN into_IN an_DT RKHS_NN and_CC replaces_VBZ all_DT inner_JJ product_NN comput_NN
e_LS Support_NN Vector_NNP regression_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_JJ -_: ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_,
tleneck_NN ._.
4.2_CD Off-the-shelf_JJ Methods_NNS Since_IN our_PRP$ architecture_NN is_VBZ modular_JJ -LRB-_-LRB- see_VB figure_NN 2_CD -RRB-_-RRB- ,_, we_PRP show_VBP as_IN a_DT proof_NN of_IN concept_NN that_IN it_PRP can_MD deal_VB with_IN different_JJ types_NNS of_IN solvers_NNS ,_, such_JJ as_IN an_DT implementation_NN of_IN LBFGS_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: from_IN TAO_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ._.
There_EX are_VBP two_CD additional_JJ requirements_NNS :_: First_JJ ,_, weneed_JJ to_TO provide_VB a_DT subdifferential_NN and_CC value_NN of_IN the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- ._.
This_DT is_VBZ easily_RB achieved_VBN via_IN 1_CD ∂_CD w_NN ‖_NNP w_NNP ‖_NNP 2_CD 2_CD 2_CD =_JJ w_NN and_CC ∂_FW w_FW ‖_FW w_FW ‖_FW 1_CD ∋_CD sgn_NN w._NN
nd_IN ordinal_JJ regression_NN ,_, and_CC a_DT particular_JJ regularizer_NN Ω_NN ,_, namely_RB quadratic_JJ regularization_NN ,_, both_DT methods_NNS are_VBP equivalent_JJ ._.
The_DT advantage_NN in_IN our_PRP$ solver_NN is_VBZ the_DT use_NN of_IN efficient_JJ linear_JJ algebra_NN tools_NNS via_IN PETSc_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT modular_JJ structure_NN ,_, the_DT considerably_RB higher_JJR generality_NN in_IN both_CC loss_NN functions_NNS and_CC regularizers_NNS ,_, and_CC the_DT fact_NN that_IN data_NNS may_MD be_VB decentralized_VBN ._.
Moreover_RB ,_, our_PRP$ work_NN is_VBZ related_JJ to_TO -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, where_WRB MapReduce_NNP
deals_NNS with_IN the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- and_CC is_VBZ able_JJ to_TO query_VB the_DT loss_NN function_NN for_IN values_NNS of_IN Remp_NN -LRB-_-LRB- w_NN -RRB-_-RRB- and_CC ∂_FW wRemp_FW -LRB-_-LRB- w_NN -RRB-_-RRB- as_IN needed_VBN ._.
This_DT is_VBZ very_RB similar_JJ to_TO the_DT design_NN of_IN the_DT Toolkit_NNP for_IN Advanced_NNP Optimization_NNP -LRB-_-LRB- TAO_NNP -RRB-_-RRB- =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Depending_VBG on_IN the_DT type_NN of_IN loss_NN function_NN ,_, computing_VBG Remp_NNP can_MD be_VB very_RB costly_JJ ._.
This_DT is_VBZ particularly_RB true_JJ in_IN cases_NNS where_WRB l_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, w_NN -RRB-_-RRB- is_VBZ the_DT log-likelihood_NN of_IN an_DT intractable_JJ conditional_JJ random_JJ fields_NNS or_CC of_IN
-LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN =_JJ -_: =[_NN 38_CD ,_, 40_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN de_IN
d_FW f_FW otherwise_RB 2_CD Soft_JJ Margin_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 1_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Soft_JJ Margin_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 1_CD and_CC f_FW −_FW y_FW otherwise_RB 2_CD Exponential_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- −_CD y_NN exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- Logistic_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- -RRB-_-RRB- −_FW y_FW \/_: -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- yf_NN -RRB-_-RRB- -RRB-_-RRB- Novelty_NN -LRB-_-LRB- 32_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD f_LS -RRB-_-RRB- 0_CD if_IN f_FW ≥_FW 0_CD and_CC −_CD 1_CD otherwise_JJ Least_NNP mean_NN squares_NNS -LRB-_-LRB- 43_CD -RRB-_-RRB- -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD f_FW −_FW y_FW 1_CD 2_CD Least_NN absolute_JJ deviation_NN |_FW f_FW −_FW y_FW |_FW sgn_FW -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- Quantile_JJ regression_NN -LRB-_-LRB- 2_CD
ression_NN ._.
Extensions_NNS of_IN these_DT loss_NN functions_NNS allow_VBP us_PRP to_TO handle_VB structure_NN in_IN the_DT output_NN space_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
Changing_VBG the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- to_TO the_DT sparsity_NN inducing_VBG ‖_FW w_FW ‖_NN 1_CD leads_VBZ to_TO Lasso-type_JJ estimation_NN algorithms_NNS =_JJ -_: =[_NN 30_CD ,_, 39_CD ,_, 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT kernel_NN trick_NN is_VBZ widely_RB used_VBN to_TO transform_VB many_JJ of_IN these_DT algorithms_NNS into_IN ones_NNS operating_VBG on_IN a_DT Reproducing_NNP Kernel_NNP Hilbert_NNP Space_NNP -LRB-_-LRB- RKHS_NNP -RRB-_-RRB- ._.
One_CD lifts_VBZ w_NN into_IN an_DT RKHS_NN and_CC replaces_VBZ all_DT inner_JJ product_NN comput_NN
es_RB ,_, depending_VBG on_IN f_LS :_: =_JJ 〈_CD w_NN ,_, x_NN 〉_NN ,_, and_CC y._NN Loss_NN l_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Derivative_JJ l_NN ′_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Hinge_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 0_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_JJ Hinge_NN -LRB-_-LRB- 26_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 0_CD and_CC f_LS otherwise_RB 2_CD Soft_JJ Margin_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 1_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Soft_JJ Margin_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 1_CD and_CC f_FW −_FW y_FW otherwise_RB 2_CD Exponential_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- −_CD y_NN exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- Logistic_JJ -LRB-_-LRB- 13_CD -RRB-_-RRB- log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- -RRB-_-RRB- −_FW y_FW \/_: -LRB-_-LRB- 1_CD +_CC
ession_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS =_JJ -_: =[_NN 28_CD -RRB-_-RRB- -_: =_JJ -_: ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW -LRB-_-LRB- 9_CD
8_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW =_JJ -_: =[_NN 9_CD ,_, 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
These_DT algorithms_NNS construct_VBP the_DT Lagrange_NNP dual_JJ ,_, and_CC solve_VB for_IN the_DT Lagrange_NNP multipliers_NNS efficiently_RB ._.
Only_RB recently_RB ,_, research_NN focus_NN has_VBZ shifted_VBN back_RB to_TO solving_VBG -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT primal_JJ ,_, e.g._FW -LRB-_-LRB- 10_CD ,_, 25_CD ,_, 36_CD -RRB-_-RRB- ._.
This_DT
link_NN 2_CD ._.
The_DT time_NN reported_VBN for_IN the_DT experiments_NNS are_VBP the_DT CPU_NNP time_NN ._.
One_CD exception_NN is_VBZ for_IN parallel_NN experiments_NNS where_WRB we_PRP report_VBP the_DT CPU_NN and_CC network_NN communication_NN time_NN ._.
5.1_CD Datasets_NNPS We_PRP use_VBP the_DT datasets_NNS in_IN =_JJ -_: =[_NN 25_CD ,_, 36_CD -RRB-_-RRB- -_: =_SYM -_: for_IN classification_NN tasks_NNS ._.
For_IN regression_NN tasks_NNS ,_, we_PRP pick_VBP some_DT of_IN the_DT largest_JJS datasets_NNS in_IN Luís_NNP Torgo_NNP 's_POS website_NN 3_CD ._.
Since_IN some_DT of_IN the_DT regression_NN datasets_VBZ 2_CD http:\/\/nf.apac.edu.au\/facilities\/ac\/hardware.p_NN
ted_VBN ._.
xj_FW iTable_FW 1_CD :_: Scalar_JJ loss_NN functions_NNS and_CC their_PRP$ derivatives_NNS ,_, depending_VBG on_IN f_LS :_: =_JJ 〈_CD w_NN ,_, x_NN 〉_NN ,_, and_CC y._NN Loss_NN l_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Derivative_JJ l_NN ′_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Hinge_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 0_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Hinge_NN =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_JJ -_: max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 0_CD and_CC f_LS otherwise_RB 2_CD Soft_JJ Margin_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 1_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Soft_JJ Margin_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 1_CD and_CC f_FW −_FW y_FW otherwise_RB 2_CD Exponential_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ex_FW
a_DT matrix_NN of_IN the_DT dimensionality_NN of_IN the_DT number_NN of_IN classes_NNS ._.
Let_VB us_PRP discuss_VB the_DT following_JJ two_CD cases_NNS :_: Ontologies_NNS for_IN Structured_NNP Estimation_NNP :_: For_IN hierarchical_JJ labels_NNS ,_, e.g._FW whenever_WRB we_PRP deal_VBP with_IN an_DT ontology_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP can_MD use_VB a_DT decomposition_NN of_IN the_DT coefficient_NN vector_NN along_IN the_DT hierarchy_NN of_IN categories_NNS ._.
Let_VB d_NN denote_VB the_DT depth_NN of_IN the_DT hierarchy_NN tree_NN ,_, and_CC assume_VB that_IN each_DT leaf_NN of_IN this_DT tree_NN corresponds_VBZ to_TO a_DT label_NN ._.
W_NN
he_PRP kernel_NN trick_NN -LRB-_-LRB- but_CC essentially_RB still_RB solve_VB -LRB-_-LRB- 1_LS -RRB-_-RRB- -RRB-_-RRB- include_VBP Support_NN Vector_NNP regression_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS
8_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW =_JJ -_: =[_NN 9_CD ,_, 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
These_DT algorithms_NNS construct_VBP the_DT Lagrange_NNP dual_JJ ,_, and_CC solve_VB for_IN the_DT Lagrange_NNP multipliers_NNS efficiently_RB ._.
Only_RB recently_RB ,_, research_NN focus_NN has_VBZ shifted_VBN back_RB to_TO solving_VBG -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT primal_JJ ,_, e.g._FW -LRB-_-LRB- 10_CD ,_, 25_CD ,_, 36_CD -RRB-_-RRB- ._.
This_DT
large_JJ datasets_NNS -LRB-_-LRB- with_IN the_DT number_NN of_IN data_NNS points_NNS of_IN the_DT order_NN of_IN a_DT million_CD -RRB-_-RRB- and_CC very_RB sparse_JJ features_NNS -LRB-_-LRB- e.g._FW the_DT bag_NN of_IN words_NNS representation_NN of_IN a_DT document_NN -RRB-_-RRB- ._.
Second_JJ ,_, many_JJ kernels_NNS -LRB-_-LRB- e.g._FW kernels_NNS on_IN strings_NNS =_JJ -_: =[_NN 42_CD -RRB-_-RRB- -_: =--RRB-_NN can_MD effectively_RB be_VB linearized_VBN ,_, and_CC third_JJ ,_, efficient_JJ factorization_NN methods_NNS -LRB-_-LRB- e.g._FW -LRB-_-LRB- 18_CD -RRB-_-RRB- -RRB-_-RRB- can_MD be_VB used_VBN for_IN a_DT low_JJ rank_NN representation_NN of_IN the_DT kernel_NN matrix_NN thereby_RB effectively_RB rendering_VBG the_DT problem_NN linear_NN ._.
1_CD 2_CD ‖_CD w_NN ‖_NN 2_CD ._.
Then_RB the_DT bundle_NN method_NN produces_VBZ a_DT duality_NN gap_NN of_IN at_IN most_JJS ɛ_NN after_IN t_NN steps_NNS ,_, where_WRB t_NN ≤_NN log_NN 2_CD λRemp_NN -LRB-_-LRB- 0_CD -RRB-_-RRB- −_NN 2_CD log_NN 2_CD G_NN +_CC 8G2_NN λɛ_FW −_FW 4_CD ._.
-LRB-_-LRB- 13_CD -RRB-_-RRB- Note_VBP that_IN this_DT bound_VBN is_VBZ significantly_RB better_JJR than_IN that_DT of_IN =_JJ -_: =[_NN 40_CD ,_, 35_CD -RRB-_-RRB- -_: =_JJ -_: ,_, since_IN it_PRP only_RB depends_VBZ logarithmically_RB on_IN the_DT value_NN of_IN the_DT loss_NN and_CC offers_VBZ an_DT O_NN -LRB-_-LRB- 1_CD \/_: ɛ_NN -RRB-_-RRB- rate_NN of_IN convergence_NN rather_RB than_IN the_DT O_NN -LRB-_-LRB- 1_CD \/_: ɛ_NN 2_CD -RRB-_-RRB- rate_NN in_IN previous_JJ papers_NNS ._.
This_DT is_VBZ largely_RB due_JJ to_TO an_DT improved_JJ analysis_NN
if_IN yf_FW ≥_FW 1_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Soft_JJ Margin_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 1_CD and_CC f_FW −_FW y_FW otherwise_RB 2_CD Exponential_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- −_CD y_NN exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- Logistic_JJ -LRB-_-LRB- 13_CD -RRB-_-RRB- log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- -RRB-_-RRB- −_FW y_FW \/_: -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- yf_NN -RRB-_-RRB- -RRB-_-RRB- Novelty_NN =_JJ -_: =[_NN 32_CD -RRB-_-RRB- -_: =_JJ -_: max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD f_LS -RRB-_-RRB- 0_CD if_IN f_FW ≥_FW 0_CD and_CC −_CD 1_CD otherwise_JJ Least_NNP mean_NN squares_NNS -LRB-_-LRB- 43_CD -RRB-_-RRB- -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD f_FW −_FW y_FW 1_CD 2_CD Least_NN absolute_JJ deviation_NN |_FW f_FW −_FW y_FW |_FW sgn_FW -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- Quantile_JJ regression_NN -LRB-_-LRB- 27_CD -RRB-_-RRB- max_NN -LRB-_-LRB- τ_NN -LRB-_-LRB- f_FW −_FW y_FW -RRB-_-RRB- ,_, -LRB-_-LRB- 1_CD −_FW τ_FW -RRB-_-RRB- -LRB-_-LRB- y_FW −_FW f_FW -RRB-_-RRB- -RRB-_-RRB- τ_NN if_IN f_LS -RRB-_-RRB- y_NN and_CC
._.
Examples_NNS of_IN algorithms_NNS which_WDT employ_VBP the_DT kernel_NN trick_NN -LRB-_-LRB- but_CC essentially_RB still_RB solve_VB -LRB-_-LRB- 1_LS -RRB-_-RRB- -RRB-_-RRB- include_VBP Support_NN Vector_NNP regression_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN =_JJ -_: =[_NN 37_CD -RRB-_-RRB- -_: =_JJ -_: ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graph_NN
,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW -LRB-_-LRB- 9_CD ,_, 23_CD -RRB-_-RRB- ._.
These_DT algorithms_NNS construct_VBP the_DT Lagrange_NNP du_NNP
a_DT tools_NNS via_IN PETSc_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ,_, the_DT modular_JJ structure_NN ,_, the_DT considerably_RB higher_JJR generality_NN in_IN both_CC loss_NN functions_NNS and_CC regularizers_NNS ,_, and_CC the_DT fact_NN that_IN data_NNS may_MD be_VB decentralized_VBN ._.
Moreover_RB ,_, our_PRP$ work_NN is_VBZ related_JJ to_TO =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, where_WRB MapReduce_NNP is_VBZ used_VBN to_TO accelerate_VB machine_NN learning_NN on_IN parallel_JJ computers_NNS ._.
We_PRP use_VBP similar_JJ parallelization_NN techniques_NNS to_TO distribute_VB the_DT computation_NN of_IN values_NNS and_CC gradients_NNS of_IN the_DT empirical_JJ risk_NN Re_NNP
regularizer_NN but_CC changing_VBG the_DT loss_NN function_NN to_TO l_NN -LRB-_-LRB- xi_NN ,_, yi_NN ,_, w_NN -RRB-_-RRB- =_JJ log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_NN yi_FW 〈_FW w_NN ,_, xi_FW 〉_FW -RRB-_-RRB- -RRB-_-RRB- ,_, yields_VBZ logistic_JJ regression_NN ._.
Extensions_NNS of_IN these_DT loss_NN functions_NNS allow_VBP us_PRP to_TO handle_VB structure_NN in_IN the_DT output_NN space_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Changing_VBG the_DT regularizer_NN Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- to_TO the_DT sparsity_NN inducing_VBG ‖_FW w_FW ‖_NN 1_CD leads_VBZ to_TO Lasso-type_JJ estimation_NN algorithms_NNS -LRB-_-LRB- 30_CD ,_, 39_CD ,_, 8_CD -RRB-_-RRB- ._.
The_DT kernel_NN trick_NN is_VBZ widely_RB used_VBN to_TO transform_VB many_JJ of_IN these_DT algorithms_NNS into_IN ones_NNS ope_VBP
gression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW -LRB-_-LRB- 9_CD ,_, 23_CD -RRB-_-RRB- ._.
These_DT algorithms_NNS
y_FW ∗_FW is_VBZ the_DT argmax_NN of_IN the_DT loss_NN Softmax_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- log_NN P_NN y_FW ′_FW hP_NN exp_NN -LRB-_-LRB- fy_FW ′_FW -RRB-_-RRB- −_FW fy_FW y_FW ′_FW ey_FW ′_FW exp_FW -LRB-_-LRB- f_FW ′_FW i_FW y_NN -RRB-_-RRB- \/_: P_NN y_NN ′_CD exp_NN -LRB-_-LRB- f_FW ′_FW y_NN -RRB-_-RRB- −_FW ey_FW Multivariate_JJ Regression_NN 1_CD 2_CD -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- ⊤_CD M_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- where_WRB M_NN ≽_NN 0_CD M_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- Document_NNP Ranking_NN =_JJ -_: =[_NN 29_CD -RRB-_-RRB- -_: =_SYM -_: show_VBP that_IN a_DT large_JJ number_NN of_IN ranking_JJ scores_NNS -LRB-_-LRB- normalized_VBN discounted_JJ cumulative_JJ gain_NN ,_, mean_VB reciprocal_JJ rank_NN ,_, expected_VBN rank_NN utility_NN ,_, etc._NN -RRB-_-RRB- can_MD be_VB optimized_VBN directly_RB by_IN minimizing_VBG the_DT following_JJ loss_NN :_: l_NN -LRB-_-LRB- X_NN ,_,
of_IN predicting_VBG binary_JJ valued_VBN labels_NNS y_FW ∈_FW -LCB-_-LRB- ±_NN 1_CD -RCB-_-RRB- ,_, we_PRP may_MD set_VB Ω_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ 1_CD 2_CD ‖_CD w_NN ‖_NN 2_CD ,_, and_CC the_DT loss_NN l_NN -LRB-_-LRB- xi_NN ,_, yi_NN ,_, w_NN -RRB-_-RRB- to_TO be_VB the_DT hinge_NN loss_NN ,_, max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yi_IN 〈_CD w_NN ,_, xi_FW 〉_FW -RRB-_-RRB- ,_, which_WDT recovers_VBZ linear_JJ Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- =_JJ -_: =[_NN 25_CD ,_, 36_CD -RRB-_-RRB- -_: =_SYM -_: ._.
On_IN the_DT other_JJ hand_NN ,_, using_VBG the_DT same_JJ regularizer_NN but_CC changing_VBG the_DT loss_NN function_NN to_TO l_NN -LRB-_-LRB- xi_NN ,_, yi_NN ,_, w_NN -RRB-_-RRB- =_JJ log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_NN yi_FW 〈_FW w_NN ,_, xi_FW 〉_FW -RRB-_-RRB- -RRB-_-RRB- ,_, yields_VBZ logistic_JJ regression_NN ._.
Extensions_NNS of_IN these_DT loss_NN functions_NNS allow_VBP us_PRP to_TO h_NN
3_LS -RRB-_-RRB- log_NN -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- −_CD yf_NN -RRB-_-RRB- -RRB-_-RRB- −_FW y_FW \/_: -LRB-_-LRB- 1_CD +_CC exp_NN -LRB-_-LRB- yf_NN -RRB-_-RRB- -RRB-_-RRB- Novelty_NN -LRB-_-LRB- 32_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD f_LS -RRB-_-RRB- 0_CD if_IN f_FW ≥_FW 0_CD and_CC −_CD 1_CD otherwise_JJ Least_NNP mean_NN squares_NNS -LRB-_-LRB- 43_CD -RRB-_-RRB- -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD f_FW −_FW y_FW 1_CD 2_CD Least_NN absolute_JJ deviation_NN |_FW f_FW −_FW y_FW |_FW sgn_FW -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- Quantile_JJ regression_NN =_JJ -_: =[_NN 27_CD -RRB-_-RRB- -_: =_JJ -_: max_NN -LRB-_-LRB- τ_NN -LRB-_-LRB- f_FW −_FW y_FW -RRB-_-RRB- ,_, -LRB-_-LRB- 1_CD −_FW τ_FW -RRB-_-RRB- -LRB-_-LRB- y_FW −_FW f_FW -RRB-_-RRB- -RRB-_-RRB- τ_NN if_IN f_LS -RRB-_-RRB- y_NN and_CC τ_FW −_FW 1_CD otherwise_RB ɛ-insensitive_JJ -LRB-_-LRB- 41_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, |_FW f_FW −_FW y_FW |_FW −_FW ɛ_FW -RRB-_-RRB- 0_CD if_IN |_FW f_FW −_FW y_FW |_FW ≤_FW ɛ_NN and_CC sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- otherwise_RB 1_CD Huber_NNP 's_POS robust_JJ loss_NN -LRB-_-LRB- 31_CD -RRB-_-RRB- 2_CD -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD if_IN |_FW f_FW −_FW y_FW |_FW -LRB-_-LRB- 1_CD ,_, els_NNS
d_NN gradients_NNS of_IN l_NN can_MD be_VB computed_VBN in_IN linear_JJ time_NN ,_, once_RB f_SYM is_VBZ sorted_VBN ._.
xj_FW iTable_FW 1_CD :_: Scalar_JJ loss_NN functions_NNS and_CC their_PRP$ derivatives_NNS ,_, depending_VBG on_IN f_LS :_: =_JJ 〈_CD w_NN ,_, x_NN 〉_NN ,_, and_CC y._NN Loss_NN l_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Derivative_JJ l_NN ′_NN -LRB-_-LRB- f_FW ,_, y_NN -RRB-_-RRB- Hinge_NN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_JJ -_: max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 0_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_JJ Hinge_NN -LRB-_-LRB- 26_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, −_FW yf_FW -RRB-_-RRB- 2_CD 0_CD if_IN yf_FW ≥_FW 0_CD and_CC f_LS otherwise_RB 2_CD Soft_JJ Margin_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, 1_CD −_CD yf_NN -RRB-_-RRB- 0_CD if_IN yf_FW ≥_FW 1_CD and_CC −_CD y_NN otherwise_RB 1_CD Squared_VBD Soft_JJ Margin_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_,
3_LS -RRB-_-RRB- This_DT gives_VBZ rise_NN to_TO the_DT hope_NN that_IN if_IN we_PRP have_VBP a_DT set_VBN W_NN =_JJ -LCB-_-LRB- w1_NN ,_, ..._: ,_, wn_NN -RCB-_-RRB- of_IN locations_NNS where_WRB we_PRP compute_VBP such_PDT a_DT Taylor_NNP approximation_NN ,_, we_PRP should_MD be_VB able_JJ to_TO obtain_VB an_DT everimproving_JJ approximation_NN of_IN g_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
See_NNP Figure_NNP 2_CD for_IN an_DT illustration_NN ._.
Formally_RB ,_, we_PRP have_VBP g_NN -LRB-_-LRB- w_NN -RRB-_-RRB- ≥_FW max_FW -LRB-_-LRB- g_NN -LRB-_-LRB- ¯_CD w_NN -RRB-_-RRB- +_CC 〈_FW w_FW −_FW ¯_FW w_NN ,_, ∂_FW wg_FW -LRB-_-LRB- ¯_CD w_NN -RRB-_-RRB- 〉_NN -RRB-_-RRB- ,_, -LRB-_-LRB- 4_CD -RRB-_-RRB- ¯_FW w_FW ∈_NN W_NN which_WDT means_VBZ that_IN g_NN -LRB-_-LRB- w_NN -RRB-_-RRB- can_MD be_VB lower-bounded_JJ by_IN a_DT piecewise_JJ linear_JJ function_NN ._.
Moreover_RB ,_, the_DT appro_NN
ve_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- max_NN -LRB-_-LRB- 0_CD ,_, |_FW f_FW −_FW y_FW |_FW −_FW ɛ_FW -RRB-_-RRB- 0_CD if_IN |_FW f_FW −_FW y_FW |_FW ≤_FW ɛ_NN and_CC sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- otherwise_RB 1_CD Huber_NNP 's_POS robust_JJ loss_NN -LRB-_-LRB- 31_CD -RRB-_-RRB- 2_CD -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD if_IN |_FW f_FW −_FW y_FW |_FW -LRB-_-LRB- 1_CD ,_, else_RB |_CD f_FW −_FW y_FW |_FW −_FW 1_CD f_FW −_FW y_FW if_IN |_FW f_FW −_FW y_FW |_FW ≤_FW 1_CD ,_, else_JJ sgn_NN -LRB-_-LRB- f_FW −_FW y_NN -RRB-_-RRB- 2_CD Poisson_NN regression_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: exp_NN -LRB-_-LRB- f_LS -RRB-_-RRB- −_FW yf_FW exp_FW -LRB-_-LRB- f_LS -RRB-_-RRB- −_FW y_FW Table_NNP 2_CD :_: Vectorial_JJ loss_NN functions_NNS and_CC their_PRP$ derivatives_NNS ,_, depending_VBG on_IN the_DT vector_NN f_SYM :_: =_JJ W_NN x_NN and_CC on_IN y._NN Loss_NN Derivative_JJ Soft_JJ Margin_NN -LRB-_-LRB- 38_CD -RRB-_-RRB- maxy_FW ′_FW -LRB-_-LRB- fy_FW ′_FW −_FW fy_FW +_CC ∆_NN -LRB-_-LRB- y_NN ,_, y_FW ′_FW -RRB-_-RRB- -RRB-_-RRB- ey_FW ∗_FW −_FW ey_FW ,_, whe_NN
-LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, novelty_NN detection_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, Huber_NNP 's_POS robust_JJ regression_NN ,_, quantile_JJ regression_NN -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, ordinal_JJ regression_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, ranking_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, maximization_NN of_IN multivariate_JJ performance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN =_JJ -_: =[_NN 38_CD ,_, 40_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN de_IN
ormance_NN measures_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, structured_JJ estimation_NN -LRB-_-LRB- 38_CD ,_, 40_CD -RRB-_-RRB- ,_, Gaussian_JJ Process_VBP regression_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, conditional_JJ random_JJ fields_NNS -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, graphical_JJ models_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, exponential_JJ families_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, and_CC generalized_JJ linear_JJ models_NNS =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Traditionally_RB ,_, specialized_JJ solvers_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG the_DT kernel_NN version_NN of_IN -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT dual_JJ ,_, e.g._FW -LRB-_-LRB- 9_CD ,_, 23_CD -RRB-_-RRB- ._.
These_DT algorithms_NNS construct_VBP the_DT Lagrange_NNP dual_JJ ,_, and_CC solve_VB for_IN the_DT Lagrange_NNP multi_NNS
essary_VB that_IN individual_JJ nodes_NNS share_VBP the_DT data_NNS ,_, since_IN all_DT communication_NN revolves_VBZ around_IN sharing_VBG only_JJ values_NNS and_CC gradients_NNS of_IN Remp_NN -LRB-_-LRB- w_NN -RRB-_-RRB- ._.
•_NNP This_NNP has_VBZ the_DT added_VBN benefit_NN of_IN preserving_VBG a_DT large_JJ degree_NN of_IN privacy_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: between_IN the_DT individual_JJ database_NN owners_NNS and_CC the_DT system_NN using_VBG the_DT solver_NN ._.
At_IN every_DT step_NN the_DT data_NNS owner_NN will_MD only_RB return_VB a_DT gradient_NN which_WDT is_VBZ the_DT linear_JJ combination_NN of_IN a_DT set_NN of_IN observations_NNS ._.
Assuming_VBG tha_NN
the_DT past_JJ n_NN gradients_NNS -LRB-_-LRB- n_NN is_VBZ user_NN defined_VBN -RRB-_-RRB- ._.
LBFGS_NN is_VBZ known_VBN to_TO perform_VB well_RB on_IN continuously_RB differentiable_JJ problems_NNS ,_, such_JJ as_IN logistic_JJ regression_NN ,_, least-meansquares_JJ problems_NNS ,_, or_CC conditional_JJ random_JJ fields_NNS =_JJ -_: =[_NN 34_CD -RRB-_-RRB- -_: =_SYM -_: ._.
But_CC ,_, if_IN the_DT functions_NNS are_VBP not_RB continuously_RB differentiable_JJ -LRB-_-LRB- e.g._FW ,_, the_DT hinge_NN loss_NN and_CC its_PRP$ variants_NNS -RRB-_-RRB- then_RB LBFGS_NN may_MD fail_VB ._.
Empirically_RB ,_, we_PRP observe_VBP that_IN LBFGS_NN does_VBZ converge_VB well_RB even_RB for_IN the_DT hinge_NN losses_NNS
dual_JJ ,_, e.g._FW -LRB-_-LRB- 9_CD ,_, 23_CD -RRB-_-RRB- ._.
These_DT algorithms_NNS construct_VBP the_DT Lagrange_NNP dual_JJ ,_, and_CC solve_VB for_IN the_DT Lagrange_NNP multipliers_NNS efficiently_RB ._.
Only_RB recently_RB ,_, research_NN focus_NN has_VBZ shifted_VBN back_RB to_TO solving_VBG -LRB-_-LRB- 1_LS -RRB-_-RRB- in_IN the_DT primal_JJ ,_, e.g._FW =_JJ -_: =[_NN 10_CD ,_, 25_CD ,_, 36_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT spurt_NN in_IN research_NN interest_NN is_VBZ due_JJ to_TO three_CD main_JJ reasons_NNS :_: First_JJ ,_, many_JJ interesting_JJ problems_NNS in_IN diverse_JJ areas_NNS such_JJ as_IN text_NN classification_NN ,_, word-sense_JJ disambiguation_NN ,_, and_CC drug_NN design_NN already_RB employ_VBP
