Towards_IN scalable_JJ support_NN vector_NN machines_NNS using_VBG squashing_VBG
ed_VBN to_TO construct_VB an_DT approximate_JJ SVM_NN by_IN approximating_VBG the_DT Gram_NN matrix_NN with_IN a_DT smaller_JJR matrix_NN using_VBG either_CC low_JJ rank_NN representation_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- or_CC sampling_NN -LRB-_-LRB- 1,29_CD -RRB-_-RRB- ._.
Assuming_VBG a_DT linear_JJ kernel_NN is_VBZ used_VBN ,_, Pavlov_NNP et_FW al._FW =_SYM -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBN to_TO squash_VB the_DT original_JJ training_NN dataset_NN into_IN a_DT limited_JJ number_NN of_IN representatives_NNS and_CC construct_VB an_DT approximate_JJ SVM_NN using_VBG these_DT representatives_NNS ._.
Although_IN these_DT algorithms_NNS ,_, together_RB with_IN many_JJ o_NN
support_NN vectors_NNS after_IN some_DT straightforward_JJ extensions_NNS to_TO the_DT current_JJ work_NN ._.
Enhancing_NN the_DT SVM_NN training_NN process_NN with_IN clustering_NN or_CC similar_JJ techniques_NNS has_VBZ been_VBN examined_VBN with_IN several_JJ variations_NNS in_IN -LRB-_-LRB- 34_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 29_CD -RRB-_-RRB- ._.
Based_VBN on_IN a_DT hierarchical_JJ micro-clustering_JJ algorithm_NN ,_, Yu_FW et_FW al._FW -LRB-_-LRB- 34_CD -RRB-_-RRB- proposed_VBD a_DT scalable_JJ algorithm_NN to_TO train_VB support_NN vector_NN machines_NNS with_IN a_DT linear_JJ kernel_NN ._.
However_RB ,_, their_PRP$ algorithm_NN currently_RB wo_MD
can_MD be_VB combined_VBN with_IN fast_JJ QP_NN approaches_NNS like_IN SMO_FW etc._FW for_IN fast_JJ training_NN and_CC prediction_NN ._.
However_RB ,_, most_JJS work_NN -LRB-_-LRB- 38_CD -RRB-_-RRB- -LRB-_-LRB- 3_CD -RRB-_-RRB- -LRB-_-LRB- 46_CD -RRB-_-RRB- in_IN data_NNS squashing_VBG +_CC SVM_NNP requires_VBZ clustering_VBG the_DT data_NNS and\/or_CC linear_JJ kernels_NNS -LRB-_-LRB- 38_CD -RRB-_-RRB- -LRB-_-LRB- 46_CD -RRB-_-RRB- =_JJ -_: =[_NN 30_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Clustering_NN usually_RB needs_VBZ O_NN -LRB-_-LRB- m2_NN -RRB-_-RRB- computations_NNS and_CC high-order_JJ kernels_NNS ,_, like_IN the_DT RBF_NN kernel_NN ,_, are_VBP widely_RB used_VBN and_CC essential_JJ to_TO many_JJ successful_JJ applications_NNS ._.
Therefore_RB ,_, a_DT fast_JJ squashing_NN method_NN and_CC experi_NNS
support_NN vectors_NNS after_IN some_DT straightforward_JJ extensions_NNS to_TO the_DT current_JJ work_NN ._.
Enhancing_NN the_DT SVM_NN training_NN process_NN with_IN clustering_NN or_CC similar_JJ techniques_NNS has_VBZ been_VBN examined_VBN with_IN several_JJ variations_NNS in_IN -LRB-_-LRB- 34_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 29_CD -RRB-_-RRB- ._.
Based_VBN on_IN a_DT hierarchical_JJ micro-clustering_JJ algorithm_NN ,_, Yu_FW et_FW al._FW -LRB-_-LRB- 34_CD -RRB-_-RRB- proposed_VBD a_DT scalable_JJ algorithm_NN to_TO train_VB support_NN vector_NN machines_NNS with_IN a_DT linear_JJ kernel_NN ._.
However_RB ,_, their_PRP$ algorithm_NN currently_RB wo_MD
m_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- has_VBZ at_IN least_JJS quadratic_JJ complexity_NN in_IN the_DT number_NN of_IN data_NNS points_NNS per_IN iteration_NN ._.
There_EX has_VBZ been_VBN work_NN on_IN speeding_VBG up_RP SVM_NN training_NN includes_VBZ various_JJ forms_NNS of_IN data_NN sampling_NN ,_, boosting_VBG -LRB-_-LRB- 7_CD -RRB-_-RRB- and_CC squashing_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, in_IN our_PRP$ experience_NN handling_VBG multiclass_JJ problems_NNS is_VBZ still_RB an_DT issue_NN while_IN the_DT preprocessing_VBG steps_NNS above_IN can_MD easily_RB result_VB in_IN either_CC inaccurate_JJ classifier_NN -LRB-_-LRB- e.g._FW ,_, sampling_NN -RRB-_-RRB- or_CC are_VBP too_RB computatio_JJ
ed_VBN to_TO construct_VB an_DT approximate_JJ SVM_NN by_IN approximating_VBG the_DT Gram_NN matrix_NN with_IN a_DT smaller_JJR matrix_NN using_VBG either_CC low_JJ rank_NN representation_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- or_CC sampling_NN -LRB-_-LRB- 1,29_CD -RRB-_-RRB- ._.
Assuming_VBG a_DT linear_JJ kernel_NN is_VBZ used_VBN ,_, Pavlov_NNP et_FW al._FW =_SYM -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBN to_TO squash_VB the_DT original_JJ training_NN dataset_NN into_IN a_DT limited_JJ number_NN of_IN representatives_NNS and_CC construct_VB an_DT approximate_JJ SVM_NN using_VBG these_DT representatives_NNS ._.
Although_IN these_DT algorithms_NNS ,_, together_RB with_IN many_JJ o_NN
er_NN case_NN ,_, the_DT cluster_NN centers_NNS are_VBP used_VBN as_IN training_NN patterns_NNS ._.
The_DT idea_NN of_IN preclustering_VBG data_NNS has_VBZ been_VBN extended_VBN in_IN an_DT interesting_JJ direction_NN by_IN applying_VBG the_DT concept_NN of_IN squashing_VBG to_TO training_VBG a_DT linear_JJ SVM_NN -LRB-_-LRB- =_JJ -_: =_JJ Pavlov_NNP ,_, Chudova_NNP ,_, and_CC Smyth_NNP ,_, 200_CD -_: =_JJ -0_CD -RRB-_-RRB- ._.
For_IN training_NN ,_, the_DT SMO_NNP algorithm_NN is_VBZ used_VBN ._.
Clustering_NN is_VBZ performed_VBN using_VBG a_DT metric_NN derived_VBN from_IN the_DT likelihood_NN prole_NN of_IN the_DT data_NNS ._.
First_RB ,_, a_DT small_JJ percentage_NN of_IN the_DT original_JJ training_NN data_NNS set_VBN are_VBP rando_NN
proach_NN developed_VBD in_IN this_DT paper_NN ._.
Instead_RB of_IN performing_VBG random_JJ sampling_NN on_IN the_DT training_NN set_NN ,_, some_DT ``_`` intelligent_JJ ''_'' sampling_NN techniques_NNS have_VBP been_VBN proposed_VBN to_TO perform_VB down-sampling_JJ -LRB-_-LRB- Smola_NNP &_CC Schölkopf_NNP ,_, 2000_CD ;_: =_JJ -_: =_JJ Pavlov_NNP et_FW al._FW ,_, 2000_CD -_: =_JJ -_: ;_: Tresp_NNP ,_, 2001_CD -RRB-_-RRB- ._.
Recently_RB ,_, -LRB-_-LRB- Yu_NNP et_FW al._FW ,_, 2003_CD -RRB-_-RRB- uses_VBZ a_DT hierarchical_JJ microclustering_NN technique_NN to_TO capture_VB the_DT training_NN instances_NNS that_WDT are_VBP close_JJ to_TO the_DT decision_NN boundary_NN ._.
However_RB ,_, since_IN the_DT hierarchical_JJ mic_NN
icular_JJ data_NNS set_VBN ._.
Here_RB ,_, we_PRP are_VBP going_VBG to_TO instead_RB examine_VB how_WRB boosting_VBG may_MD be_VB used_VBN to_TO deal_VB with_IN data_NNS sets_NNS that_WDT have_VBP a_DT large_JJ number_NN of_IN examples_NNS and\/or_CC learning_VBG algorithms_NNS that_WDT are_VBP very_RB time-consuming_JJ ._.
In_IN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Pavlov_NNP et_NNP ._.
al._FW examined_VBN several_JJ methods_NNS for_IN speeding_VBG up_RP the_DT learning_NN process_NN for_IN support_NN vector_NN machines_NNS ._.
One_CD of_IN those_DT methods_NNS used_VBD boosting_VBG on_IN a_DT small_JJ subset_NN of_IN the_DT data_NNS and_CC reported_VBD good_JJ accuracie_NN
ll_NN examples_NNS within_IN the_DT margin_NN is_VBZ computationally_RB expensive_JJ ._.
Following_VBG the_DT idea_NN of_IN the_DT likelihood-based_JJ squashing_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, a_DT likelihood_NN squashing_NN method_NN was_VBD developed_VBN for_IN a_DT SVM_NN by_IN Pavlov_NN and_CC Chudova_NN =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT likelihood_NN squashing_NN method_NN assumes_VBZ a_DT probability_NN model_NN as_IN the_DT classifier_NN ._.
Examples_NNS with_IN similar_JJ probability_NN p_NN -LRB-_-LRB- xi_NN ,_, yi_FW |_FW θ_FW -RRB-_-RRB- are_VBP grouped_VBN together_RB and_CC taken_VBN as_IN a_DT weighted_JJ exemplar_NN ._.
Pavlov_NNP and_CC Chudova_NNP
support_NN vectors_NNS after_IN some_DT straightforward_JJ extensions_NNS to_TO the_DT current_JJ work_NN ._.
Enhancing_NN the_DT SVM_NN training_NN process_NN with_IN clustering_NN or_CC similar_JJ techniques_NNS has_VBZ been_VBN examined_VBN with_IN several_JJ variations_NNS in_IN -LRB-_-LRB- 34_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 29_CD -RRB-_-RRB- ._.
Based_VBN on_IN a_DT hierarchical_JJ micro-clustering_JJ algorithm_NN ,_, Yu_FW et_FW al._FW -LRB-_-LRB- 34_CD -RRB-_-RRB- proposed_VBD a_DT scalable_JJ algorithm_NN to_TO train_VB support_NN vector_NN machines_NNS with_IN a_DT linear_JJ kernel_NN ._.
However_RB ,_, their_PRP$ algorithm_NN currently_RB wo_MD
wer_NN memory_NN cost_NN and_CC yields_NNS performance_NN close_RB to_TO that_DT of_IN SMO_NN on_IN the_DT full_JJ data_NNS ._.
In_IN this_DT paper_NN we_PRP suggest_VBP another_DT method_NN for_IN scaling_VBG SVMs_NNS up_RB based_VBN on_IN squashing_NN ._.
DuMouchel_NNP et_NNP ._.
al._FW -LRB-_-LRB- 5_CD -RRB-_-RRB- and_CC Madigan_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: recently_RB introduced_VBN squashing_NN as_IN a_DT technique_NN that_WDT allows_VBZ one_CD to_TO scale_VB a_DT dataset_NN down_RP while_IN preserving_VBG its_PRP$ statistical_JJ properties_NNS ._.
In_IN particular_JJ ,_, likelihood-based_JJ squashing_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- assumes_VBZ a_DT particular_JJ st_NN
ational_JJ cost_NN is_VBZ to_TO use_VB approximation_NN methods_NNS ._.
The_DT simplest_JJS method_NN is_VBZ sampling_NN from_IN the_DT original_JJ dataset_NN and_CC using_VBG the_DT sample_NN to_TO train_VB the_DT SVM_NNP ._.
An_DT extension_NN of_IN this_DT idea_NN was_VBD explored_VBN by_IN Pavlov_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: who_WP reported_VBD results_NNS on_IN the_DT application_NN of_IN boosting_VBG to_TO training_NN SVMs_NNS ._.
Boost-SMO_JJ algorithm_NN trains_VBZ a_DT sequence_NN of_IN SVM_NN classifiers_NNS on_IN samples_NNS of_IN data_NNS so_IN that_IN each_DT subsequent_JJ classifier_NN concentrates_VBZ mostl_NN
Ms_NN ,_, has_VBZ substantially_RB lower_JJR memory_NN cost_NN and_CC yields_NNS performance_NN close_RB to_TO that_DT of_IN SMO_NN on_IN the_DT full_JJ data_NNS ._.
In_IN this_DT paper_NN we_PRP suggest_VBP another_DT method_NN for_IN scaling_VBG SVMs_NNS up_RB based_VBN on_IN squashing_NN ._.
DuMouchel_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Madigan_NNP et_NNP ._.
al._FW -LRB-_-LRB- 7_CD -RRB-_-RRB- recently_RB introduced_VBD squashing_VBG as_IN a_DT technique_NN that_WDT allows_VBZ one_CD to_TO scale_VB a_DT dataset_NN down_RP while_IN preserving_VBG its_PRP$ statistical_JJ properties_NNS ._.
In_IN particular_JJ ,_, likelihood-based_JJ squashing_NN -LRB-_-LRB- 7_CD -RRB-_-RRB-
lved_VBN in_IN solving_VBG quadratic_JJ programming_NN problem_NN arising_VBG in_IN their_PRP$ training_NN ._.
Various_JJ training_NN algorithms_NNS have_VBP been_VBN proposed_VBN to_TO speed_VB up_RP the_DT training_NN ,_, including_VBG chunking_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, Osuna_NNP 's_POS decomposition_NN method_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC Sequential_JJ Minimal_JJ Optimization_NN -LRB-_-LRB- SMO_NN -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
Although_IN these_DT algorithms_NNS accelerate_VBP the_DT training_NN ,_, they_PRP do_VBP not_RB scale_VB well_RB with_IN the_DT size_NN of_IN the_DT training_NN data_NNS ._.
Another_DT approach_NN to_TO reducing_VBG the_DT computa_NN
ing_NN Methodologies_NNS -RRB-_-RRB- :_: Artificial_NNP Intelligence_NNP --_: Learning_NNP General_NNP Terms_NNS support_VBP vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- ,_, scalability_NN ,_, squashing_NN ,_, boosting_VBG 1_CD ._.
INTRODUCTION_NN Following_VBG the_DT pioneering_JJ work_NN of_IN Vladimir_NNP Vapnik_NNP =_SYM -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, support_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- are_VBP steadily_RB gaining_VBG popularity_NN in_IN the_DT machine_NN learning_NN community_NN ._.
SVMs_NNS have_VBP been_VBN proven_VBN to_TO exhibit_VB several_JJ attractive_JJ theoretical_JJ properties_NNS including_VBG maximum_NN margin_NN
f_LS boosting_VBG to_TO training_NN SVMs_NNS ._.
Boost-SMO_JJ algorithm_NN trains_VBZ a_DT sequence_NN of_IN SVM_NN classifiers_NNS on_IN samples_NNS of_IN data_NNS so_IN that_IN each_DT subsequent_JJ classifier_NN concentrates_VBZ mostly_RB on_IN the_DT errors_NNS made_VBN by_IN the_DT previous_JJ ones_NNS =_JJ -_: =[_NN 12_CD ,_, 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
While_IN this_DT method_NN is_VBZ not_RB optimal_JJ in_IN general_JJ ,_, it_PRP allows_VBZ for_IN fast_JJ training_NN of_IN SVMs_NNS ,_, has_VBZ substantially_RB lower_JJR memory_NN cost_NN and_CC yields_NNS performance_NN close_RB to_TO that_DT of_IN SMO_NN on_IN the_DT full_JJ data_NNS ._.
In_IN this_DT paper_NN we_PRP s_VBZ
l_NN and_CC tries_VBZ to_TO preserve_VB the_DT behavior_NN of_IN the_DT likelihood_NN function_NN of_IN the_DT original_JJ data_NNS -LRB-_-LRB- referred_VBN to_TO as_IN the_DT ``_`` mother-data_NN ''_'' -RRB-_-RRB- in_IN the_DT neighborhood_NN of_IN the_DT maximum_NN likelihood_NN solution_NN ._.
A_DT recent_JJ pape_NN =_JJ -_: =_JJ r_NN by_IN Owen_NNP -LRB-_-LRB- Owe99_NN -RRB-_-RRB- -_: =_SYM -_: analyzes_VBZ why_WRB and_CC when_WRB squashing_VBG may_MD reduce_VB the_DT complexity_NN of_IN learning_VBG parameters_NNS of_IN the_DT model_NN and_CC concludes_VBZ that_IN a_DT method_NN that_WDT uses_VBZ global_JJ -LRB-_-LRB- as_IN opposed_VBN to_TO local_JJ -RRB-_-RRB- features_NNS of_IN the_DT data_NNS will_MD be_VB likely_JJ to_TO
number_NN and_CC location_NN of_IN the_DT points_NNS in_IN the_DT parameter_NN space_NN to_TO evaluate_VB the_DT likelihoods_NNS at_IN ,_, and_CC the_DT number_NN of_IN squashed_JJ data_NNS points_NNS to_TO ensure_VB a_DT sufficiently_RB good_JJ approximation_NN ._.
Various_JJ factorial_JJ designs_NNS =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: in_IN the_DT parameter_NN space_NN are_VBP suggested_VBN in_IN -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
While_IN this_DT method_NN is_VBZ universal_JJ ,_, as_IN we_PRP noted_VBD above_IN ,_, for_IN SVMs_NNS we_PRP can_MD sample_NN the_DT values_NNS of_IN w_NN from_IN the_DT prior_JJ distribution_NN in_IN equation_NN 4_CD ._.
The_DT choice_NN of_IN the_DT int_NN
on_IN between_IN the_DT classes_NNS is_VBZ equivalent_JJ to_TO the_DT minimization_NN of_IN kwk_NN 2_CD L_NN 2_CD with_IN the_DT additional_JJ constraints_NNS y_FW i_FW -LRB-_-LRB- !_.
w_NN ;_: x_NN i_LS ?_.
+_CC b_LS -RRB-_-RRB- s1_NN to_TO ensure_VB that_IN all_PDT the_DT patterns_NNS in_IN the_DT training_NN set_NN are_VBP classified_VBN correctly_RB =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, in_IN most_JJS cases_NNS perfect_JJ separation_NN is_VBZ impossible_JJ and_CC we_PRP need_VBP to_TO trade_VB errors_NNS on_IN the_DT individual_JJ training_NN patterns_NNS for_IN the_DT maximum_NN margin_NN ._.
The_DT optimization_NN problem_NN becomes_VBZ minw_NN ;_: bE_NN =_JJ minw_NN ;_: bf_NN 1_CD 2_CD
referred_VBN to_TO as_IN the_DT ``_`` mother-data_NN ''_'' -RRB-_-RRB- in_IN the_DT neighborhood_NN of_IN the_DT maximum_NN likelihood_NN solution_NN ._.
To_TO apply_VB likelihood-based_JJ squashing_NN to_TO SVMs_NNS it_PRP is_VBZ necessary_JJ to_TO have_VB a_DT probabilistic_JJ interpretation_NN -LRB-_-LRB- =_JJ -_: =_JJ see_VB ,_, e.g._FW ,_, -LRB-_-LRB- 6_CD ,_, 16_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
We_PRP use_VBP an_DT interpretation_NN of_IN the_DT SVM_NN training_NN procedure_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- as_IN a_DT problem_NN of_IN finding_VBG maximum_JJ aposteriori_NN values_NNS for_IN the_DT parameters_NNS of_IN the_DT SVM_NN ._.
We_PRP show_VBP that_IN the_DT probabilistic_JJ interpretation_NN of_IN SVM_NN tr_NN
rising_VBG in_IN their_PRP$ training_NN ._.
Various_JJ training_NN algorithms_NNS have_VBP been_VBN proposed_VBN to_TO speed_VB up_RP the_DT training_NN ,_, including_VBG chunking_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, Osuna_NNP 's_POS decomposition_NN method_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, and_CC Sequential_JJ Minimal_JJ Optimization_NN -LRB-_-LRB- SMO_NN -RRB-_-RRB- =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Although_IN these_DT algorithms_NNS accelerate_VBP the_DT training_NN ,_, they_PRP do_VBP not_RB scale_VB well_RB with_IN the_DT size_NN of_IN the_DT training_NN data_NNS ._.
Another_DT approach_NN to_TO reducing_VBG the_DT computational_JJ cost_NN is_VBZ to_TO use_VB approximation_NN methods_NNS ._.
The_DT
ffered_VBN by_IN squashSMO_NN and_CC boost-SMO_NN might_MD be_VB even_RB more_RBR significant_JJ for_IN non-linear_JJ SVMs_NNS that_WDT take_VBP much_RB longer_JJR to_TO train_VB ._.
Several_JJ authors_NNS have_VBP studied_VBN the_DT question_NN of_IN feature_NN selection_NN for_IN SVMs_NNS -LRB-_-LRB- see_VB ,_, e.g._FW =_JJ -_: =_JJ -LRB-_-LRB- BO98_NN -RRB-_-RRB- -_: =--RRB-_NN ._.
The_DT main_JJ idea_NN is_VBZ to_TO interpret_VB the_DT norm_NN of_IN w_NN as_IN a_DT norm_NN in_IN L_NN 1_CD space_NN and_CC reduce_VB the_DT training_NN task_NN to_TO linear_JJ programming_NN optimization_NN problem_NN ._.
We_PRP think_VBP that_IN squashing_NN might_MD offer_VB a_DT principled_JJ way_NN to_TO d_NN
data_NNS from_IN squashing_VBG ._.
4_CD Experiments_NNS We_PRP have_VBP run_VBN experiments_NNS on_IN four_CD datasets_NNS ,_, one_CD of_IN which_WDT is_VBZ synthetic_JJ and_CC the_DT rest_NN are_VBP publicly_RB available_JJ at_IN either_DT UCI_NNP machine_NN learning_NN -LRB-_-LRB- BM98_NN -RRB-_-RRB- or_CC UCI_NN KDD_NN repository_NN =_JJ -_: =_JJ -LRB-_-LRB- Bay99_NN -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP 6_CD evaluated_VBD the_DT performance_NN of_IN the_DT full-SMO_NN -LRB-_-LRB- SMO_NN on_IN the_DT full_JJ training_NN data_NNS -RRB-_-RRB- ,_, srs-SMO_NN -LRB-_-LRB- SMO_NN on_IN a_DT simple_JJ random_JJ sample_NN -RRB-_-RRB- ,_, squash-SMO_NN -LRB-_-LRB- SMO_NN on_IN the_DT squashed_JJ data_NNS -RRB-_-RRB- and_CC boost-SMO_NN -LRB-_-LRB- SMO_NN on_IN the_DT boosted_VBN sampl_NN
ained_VBN quadratic_JJ programming_NN problem_NN ._.
The_DT standard_JJ SMO_NN algorithm_NN solves_VBZ the_DT dual_JJ of_IN problem_NN 2_CD and_CC 3_CD by_IN decomposing_VBG it_PRP into_IN the_DT smaller_JJR problems_NNS that_WDT can_MD be_VB solved_VBN analytically_RB ._.
SMO_NN is_VBZ provably_RB optimal_JJ =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Several_JJ authors_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 16_CD ,_, 14_CD -RRB-_-RRB- -RRB-_-RRB- have_VBP shown_VBN that_IN the_DT right_JJ side_NN of_IN Equation_NN 2_CD can_MD be_VB treated_VBN as_IN a_DT log-posterior_NN on_IN the_DT parameters_NNS w_NN and_CC b_NN ,_, with_IN the_DT first_JJ term_NN corresponding_VBG to_TO a_DT prior_JJ on_IN w_NN and_CC the_DT s_NN
directly_RB on_IN the_DT weighed_VBN data_NNS from_IN squashing_VBG ._.
4_LS ._.
EXPERIMENTS_NNS We_PRP have_VBP run_VBN experiments_NNS on_IN four_CD datasets_NNS ,_, one_CD of_IN which_WDT is_VBZ synthetic_JJ and_CC the_DT rest_NN are_VBP publicly_RB available_JJ at_IN either_CC the_DT UCI_NNP machine_NN learning_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: or_CC UCI_NNP KDD_NNP repositories_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- ._.
We_PRP evaluated_VBD the_DT performance_NN of_IN the_DT full-SMO_NN -LRB-_-LRB- SMO_NN on_IN the_DT full_JJ training_NN data_NNS -RRB-_-RRB- ,_, srs-SMO_NN -LRB-_-LRB- SMO_NN on_IN a_DT simple_JJ random_JJ sample_NN -RRB-_-RRB- ,_, squash-SMO_NN -LRB-_-LRB- SMO_NN on_IN the_DT squashed_JJ data_NNS -RRB-_-RRB- and_CC boost-SMO_NN
referred_VBN to_TO as_IN the_DT ``_`` mother-data_NN ''_'' -RRB-_-RRB- in_IN the_DT neighborhood_NN of_IN the_DT maximum_NN likelihood_NN solution_NN ._.
To_TO apply_VB likelihood-based_JJ squashing_NN to_TO SVMs_NNS it_PRP is_VBZ necessary_JJ to_TO have_VB a_DT probabilistic_JJ interpretation_NN -LRB-_-LRB- =_JJ -_: =_JJ see_VB ,_, e.g._FW ,_, -LRB-_-LRB- 6_CD ,_, 16_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
We_PRP use_VBP an_DT interpretation_NN of_IN the_DT SVM_NN training_NN procedure_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- as_IN a_DT problem_NN of_IN finding_VBG maximum_JJ aposteriori_NN values_NNS for_IN the_DT parameters_NNS of_IN the_DT SVM_NN ._.
We_PRP show_VBP that_IN the_DT probabilistic_JJ interpretation_NN of_IN SVM_NN tr_NN
mum_JJ likelihood_NN solution_NN ._.
To_TO apply_VB likelihood-based_JJ squashing_NN to_TO SVMs_NNS it_PRP is_VBZ necessary_JJ to_TO have_VB a_DT probabilistic_JJ interpretation_NN -LRB-_-LRB- see_VB ,_, e.g._FW ,_, -LRB-_-LRB- 6_CD ,_, 16_CD -RRB-_-RRB- -RRB-_-RRB- ._.
We_PRP use_VBP an_DT interpretation_NN of_IN the_DT SVM_NN training_NN procedure_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: as_IN a_DT problem_NN of_IN finding_VBG maximum_JJ aposteriori_NN values_NNS for_IN the_DT parameters_NNS of_IN the_DT SVM_NN ._.
We_PRP show_VBP that_IN the_DT probabilistic_JJ interpretation_NN of_IN SVM_NN training_NN in_IN conjunction_NN with_IN likelihood-based_JJ squashing_NN allows_VBZ o_NN
