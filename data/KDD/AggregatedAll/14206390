A_DT GPU-tailored_JJ approach_NN for_IN training_NN kernelized_VBD SVMs_NNS
We_PRP present_VBP a_DT method_NN for_IN efficiently_RB training_VBG binary_JJ and_CC multiclass_JJ kernelized_VBN SVMs_NNS on_IN a_DT Graphics_NNP Processing_NNP Unit_NNP -LRB-_-LRB- GPU_NNP -RRB-_-RRB- ._.
Our_PRP$ methods_NNS apply_VBP to_TO a_DT broad_JJ range_NN of_IN kernels_NNS ,_, including_VBG the_DT popular_JJ Gaus_NNP -_: sian_JJ kernel_NN ,_, on_IN datasets_NNS as_RB large_JJ as_IN the_DT amount_NN of_IN available_JJ memory_NN on_IN the_DT graphics_NNS card_NN ._.
Our_PRP$ approach_NN is_VBZ distinguished_VBN from_IN earlier_JJR work_NN in_IN that_IN it_PRP cleanly_RB and_CC efficiently_RB handles_VBZ sparse_JJ datasets_NNS through_IN the_DT use_NN of_IN a_DT novel_JJ clustering_NN technique_NN ._.
Our_PRP$ optimization_NN algorithm_NN is_VBZ also_RB specifically_RB designed_VBN to_TO take_VB advantage_NN of_IN the_DT graphics_NNS hardware_NN ._.
This_DT leads_VBZ to_TO different_JJ algorithmic_JJ choices_NNS then_RB those_DT preferred_VBN in_IN serial_JJ implementations_NNS ._.
Our_PRP$ easy-to-use_JJ library_NN is_VBZ orders_NNS of_IN magnitude_NN faster_RBR then_RB existing_VBG CPU_NNP libraries_NNS ,_, and_CC several_JJ times_NNS faster_RBR than_IN prior_JJ GPU_NNP approaches_NNS ._.
implemented_VBN on_IN a_DT GPU_NNP ,_, and_CC present_JJ such_JJ an_DT implementation_NN for_IN both_CC binary_JJ and_CC multiclass_JJ SVMs_NNS ._.
Several_JJ authors_NNS have_VBP recently_RB proposed_VBN using_VBG GPUs_NNS for_IN kernelized_VBN SVM_NN training_NN -LRB-_-LRB- 3_CD ,_, 2_CD -RRB-_-RRB- and_CC related_JJ problems_NNS =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
These_DT previous_JJ approaches_NNS ,_, however_RB ,_, primarily_RB focused_VBN on_IN pointing_VBG out_RP the_DT advantages_NNS of_IN implementing_VBG standard_JJ algorithms_NNS on_IN graphics_NNS hardware_NN ,_, typically_RB using_VBG GPU_NNP matrix-multiplication_NN libraries_NNS ,_, an_DT
training_NN can_MD be_VB efficiently_RB implemented_VBN on_IN a_DT GPU_NNP ,_, and_CC present_JJ such_JJ an_DT implementation_NN for_IN both_CC binary_JJ and_CC multiclass_JJ SVMs_NNS ._.
Several_JJ authors_NNS have_VBP recently_RB proposed_VBN using_VBG GPUs_NNS for_IN kernelized_VBN SVM_NN training_NN =_JJ -_: =[_NN 3_CD ,_, 2_CD -RRB-_-RRB- -_: =_JJ -_: and_CC related_JJ problems_NNS -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
These_DT previous_JJ approaches_NNS ,_, however_RB ,_, primarily_RB focused_VBN on_IN pointing_VBG out_RP the_DT advantages_NNS of_IN implementing_VBG standard_JJ algorithms_NNS on_IN graphics_NNS hardware_NN ,_, typically_RB using_VBG GPU_NNP matrix-mul_JJ
ets_NN ._.
On_IN the_DT CPU_NNP ,_, taking_VBG advantage_NN of_IN sparsity_NN is_VBZ a_DT simple_JJ matter_NN ,_, and_CC sparse_JJ datasets_NNS are_VBP encountered_VBN frequently_RB enough_RB that_IN many_JJ widely-used_JJ SVM_NN solvers_NNS treat_VBP all_DT input_NN vectors_NNS as_IN sparse_NN ,_, by_IN default_NN =_JJ -_: =[_NN 9_CD ,_, 4_CD ,_, 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
On_IN the_DT GPU_NNP ,_, however_RB ,_, maximum_NN performance_NN is_VBZ only_RB achieved_VBN if_IN memory_NN accesses_NNS follow_VBP certain_JJ fairly-restrictive_JJ patterns_NNS ,_, which_WDT are_VBP difficultto_NN ensure_VB with_IN sparse_JJ data_NNS ._.
In_IN contrast_NN to_TO other_JJ GPU_NNP SVM_NNP
ter_NN convergence_NN than_IN performing_VBG fewer_JJR relatively_RB expensive_JJ ones_NNS ._.
Variants_NNS of_IN the_DT popular_JJ SMO_NNP algorithm_NN take_VB these_DT working_VBG sets_NNS to_TO be_VB as_RB small_JJ as_IN possible_JJ -LRB-_-LRB- two_CD with_IN an_DT unregularized_JJ bias_NN ,_, one_CD without_IN -RRB-_-RRB- =_JJ -_: =[_NN 11_CD ,_, 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, while_IN SVM-Light_NN ,_, by_IN default_NN ,_, uses_VBZ working_VBG sets_NNS of_IN size_NN 10_CD -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
The_DT subproblem_NN of_IN optimizing_VBG the_DT coefficients_NNS corresponding_VBG to_TO a_DT given_VBN working_NN set_NN is_VBZ again_RB a_DT quadratic_JJ program_NN -LRB-_-LRB- though_IN a_DT smaller_JJR one_CD -RRB-_-RRB- ._.
where_WRB the_DT prediction_NN is_VBZ linear_JJ in_IN the_DT input_NN representation_NN -RRB-_-RRB- and_CC ``_`` kernelized_VBN ''_'' SVMs_NNS -LRB-_-LRB- where_WRB a_DT non-linear_JJ kernel_NN defines_VBZ the_DT linear_JJ prediction_NN space_NN -RRB-_-RRB- ._.
For_IN linear_JJ SVMs_NNS ,_, stochastic_JJ methods_NNS such_JJ as_IN PEGASOS_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Stochastic_JJ Dual_JJ Coordinate_JJ Ascent_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- have_VBP recently_RB been_VBN established_VBN as_IN being_VBG effective_JJ at_IN solving_VBG extremely_RB large_JJ SVM_NN instances_NNS ,_, typically_RB in_IN less_JJR time_NN than_IN that_DT which_WDT is_VBZ required_VBN to_TO read_VB the_DT da_NN
ets_NN ._.
On_IN the_DT CPU_NNP ,_, taking_VBG advantage_NN of_IN sparsity_NN is_VBZ a_DT simple_JJ matter_NN ,_, and_CC sparse_JJ datasets_NNS are_VBP encountered_VBN frequently_RB enough_RB that_IN many_JJ widely-used_JJ SVM_NN solvers_NNS treat_VBP all_DT input_NN vectors_NNS as_IN sparse_NN ,_, by_IN default_NN =_JJ -_: =[_NN 9_CD ,_, 4_CD ,_, 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
On_IN the_DT GPU_NNP ,_, however_RB ,_, maximum_NN performance_NN is_VBZ only_RB achieved_VBN if_IN memory_NN accesses_NNS follow_VBP certain_JJ fairly-restrictive_JJ patterns_NNS ,_, which_WDT are_VBP difficultto_NN ensure_VB with_IN sparse_JJ data_NNS ._.
In_IN contrast_NN to_TO other_JJ GPU_NNP SVM_NNP
ter_NN convergence_NN than_IN performing_VBG fewer_JJR relatively_RB expensive_JJ ones_NNS ._.
Variants_NNS of_IN the_DT popular_JJ SMO_NNP algorithm_NN take_VB these_DT working_VBG sets_NNS to_TO be_VB as_RB small_JJ as_IN possible_JJ -LRB-_-LRB- two_CD with_IN an_DT unregularized_JJ bias_NN ,_, one_CD without_IN -RRB-_-RRB- =_JJ -_: =[_NN 11_CD ,_, 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, while_IN SVM-Light_NN ,_, by_IN default_NN ,_, uses_VBZ working_VBG sets_NNS of_IN size_NN 10_CD -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
The_DT subproblem_NN of_IN optimizing_VBG the_DT coefficients_NNS corresponding_VBG to_TO a_DT given_VBN working_NN set_NN is_VBZ again_RB a_DT quadratic_JJ program_NN -LRB-_-LRB- though_IN a_DT smaller_JJR one_CD -RRB-_-RRB- ._.
d_FW multiclass_FW Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- classifiers_NNS ._.
For_IN a_DT complete_JJ description_NN of_IN Support_NN Vector_NNP Machines_NNP ,_, motivating_VBG these_DT optimization_NN problems_NNS ,_, we_PRP refer_VBP the_DT reader_NN to_TO ,_, e.g._FW ,_, Schölkopf_NN and_CC Smola_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2.1_CD Binary_JJ classification_NN We_PRP consider_VBP training_VBG a_DT kernel_NN SVM_NN with_IN an_DT unregularized_JJ bias_NN term_NN ._.
Let_VB -LRB-_-LRB- x1_NN ,_, y1_NN -RRB-_-RRB- ,_, ..._: ,_, -LRB-_-LRB- xn_NN ,_, yn_NN -RRB-_-RRB- ,_, withxi_FW ∈_FW R_NN d_FW andyi_FW ∈_NN -LCB-_-LRB- ±_NN 1_CD -RCB-_-RRB- ,_, be_VB a_DT training_NN set_VBN ofnlabeled_JJ examples_NNS ,_, and_CC letK_NN :_: R_NN d_NN
pically_RB done_VBN by_IN iteratively_RB choosing_VBG a_DT subset_NN of_IN the_DT dual_JJ variables_NNS ,_, which_WDT we_PRP will_MD call_VB a_DT working_NN set_NN ,_, and_CC updating_VBG the_DT coefficients_NNS αi_IN corresponding_VBG to_TO this_DT set_NN ._.
While_IN the_DT early_JJ ``_`` chunking_NN ''_'' algorithm_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: relied_VBN on_IN choosing_VBG a_DT large_JJ working_VBG set_NN ,_, subsequent_JJ work_NN has_VBZ tended_VBN to_TO show_VB that_IN performing_VBG many_JJ computationally_RB inexpensive_JJ updates_NNS on_IN small_JJ working_VBG sets_NNS leads_VBZ to_TO faster_JJR convergence_NN than_IN performing_VBG f_SYM
ets_NN ._.
On_IN the_DT CPU_NNP ,_, taking_VBG advantage_NN of_IN sparsity_NN is_VBZ a_DT simple_JJ matter_NN ,_, and_CC sparse_JJ datasets_NNS are_VBP encountered_VBN frequently_RB enough_RB that_IN many_JJ widely-used_JJ SVM_NN solvers_NNS treat_VBP all_DT input_NN vectors_NNS as_IN sparse_NN ,_, by_IN default_NN =_JJ -_: =[_NN 9_CD ,_, 4_CD ,_, 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
On_IN the_DT GPU_NNP ,_, however_RB ,_, maximum_NN performance_NN is_VBZ only_RB achieved_VBN if_IN memory_NN accesses_NNS follow_VBP certain_JJ fairly-restrictive_JJ patterns_NNS ,_, which_WDT are_VBP difficultto_NN ensure_VB with_IN sparse_JJ data_NNS ._.
In_IN contrast_NN to_TO other_JJ GPU_NNP SVM_NNP
t_NN representation_NN -RRB-_-RRB- and_CC ``_`` kernelized_VBN ''_'' SVMs_NNS -LRB-_-LRB- where_WRB a_DT non-linear_JJ kernel_NN defines_VBZ the_DT linear_JJ prediction_NN space_NN -RRB-_-RRB- ._.
For_IN linear_JJ SVMs_NNS ,_, stochastic_JJ methods_NNS such_JJ as_IN PEGASOS_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC Stochastic_JJ Dual_JJ Coordinate_JJ Ascent_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP recently_RB been_VBN established_VBN as_IN being_VBG effective_JJ at_IN solving_VBG extremely_RB large_JJ SVM_NN instances_NNS ,_, typically_RB in_IN less_JJR time_NN than_IN that_DT which_WDT is_VBZ required_VBN to_TO read_VB the_DT data_NNS into_IN memory_NN ._.
For_IN kernel_NN SVMs_NNS ,_, most_JJS lead_NN
training_NN can_MD be_VB efficiently_RB implemented_VBN on_IN a_DT GPU_NNP ,_, and_CC present_JJ such_JJ an_DT implementation_NN for_IN both_CC binary_JJ and_CC multiclass_JJ SVMs_NNS ._.
Several_JJ authors_NNS have_VBP recently_RB proposed_VBN using_VBG GPUs_NNS for_IN kernelized_VBN SVM_NN training_NN =_JJ -_: =[_NN 3_CD ,_, 2_CD -RRB-_-RRB- -_: =_JJ -_: and_CC related_JJ problems_NNS -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
These_DT previous_JJ approaches_NNS ,_, however_RB ,_, primarily_RB focused_VBN on_IN pointing_VBG out_RP the_DT advantages_NNS of_IN implementing_VBG standard_JJ algorithms_NNS on_IN graphics_NNS hardware_NN ,_, typically_RB using_VBG GPU_NNP matrix-mul_JJ
