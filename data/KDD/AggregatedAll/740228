Applying_VBG general_JJ Bayesian_JJ techniques_NNS to_TO improve_VB TAN_NN induction_NN
ures_NNS will_MD be_VB given_VBN by_IN the_DT algorithm_NN Construct-TAN_NNP ,_, just_RB modifying_VBG the_DT step_NN where_WRB a_DT maximum_JJ spanning_NN tree_NN is_VBZ induced_VBN ,_, to_TO generate_VB a_DT set_NN containing_VBG the_DT K_NN maximum_NN spanning_NN trees_NNS by_IN using_VBG Gabow_NNP algorithm_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN order_NN to_TO calculate_VB P_NN ′_NN -LRB-_-LRB- M_NN -RRB-_-RRB- ,_, we_PRP set_VBD a_DT prior_JJ over_IN tree_NN structures_NNS that_WDT assigns_VBZ the_DT same_JJ probability_NN to_TO each_DT possible_JJ tree_NN structure_NN -LRB-_-LRB- since_IN they_PRP can_MD be_VB considered_VBN of_IN a_DT similar_JJ complexity_NN -RRB-_-RRB- ._.
We_PRP also_RB h_NN
lem_NN ._.
3.2_CD Applying_VBG the_DT multinomial_JJ sampling_NN approach_NN to_TO TAN_NN induction_NN In_IN this_DT case_NN ,_, instead_RB of_IN looking_VBG for_IN maximum_NN likelihood_NN ,_, we_PRP would_MD like_VB to_TO minimize_VB the_DT cross_JJ entropy_NN or_CC KullbackLeibler_NN divergence_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: between_IN the_DT P_NN ∗_NN S_NN and_CC the_DT probability_NN distribution_NN generated_VBN by_IN the_DT TAN_NNP -LRB-_-LRB- following_VBG 2sthe_JJ spirit_NN of_IN Statement_NN 2_CD -RRB-_-RRB- ._.
Cross_NN entropy_NN is_VBZ minimized_VBN when_WRB :_: θxi_FW |_FW Πxi_NN =_JJ P_NN ∗_NN S_NN -LRB-_-LRB- xi_FW |_FW Πxi_NN -RRB-_-RRB- =_JJ P_NN ∗_NN S_NN -LRB-_-LRB- xi_NN ,_, Πx_NN i_LS -RRB-_-RRB- P_NN ∗_NN S_NN -LRB-_-LRB- Πx_NN
Improve_VB TAN_NN Induction_NN Abstract_JJ Tree_NN Augmented_JJ Naive_JJ Bayes_NNS -LRB-_-LRB- TAN_NN -RRB-_-RRB- has_VBZ shown_VBN to_TO be_VB competitive_JJ with_IN state-of-the-art_JJ machine_NN learning_NN algorithms_VBZ =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, the_DT TAN_NNP induction_NN algorithm_NN that_WDT appears_VBZ in_IN -LRB-_-LRB- 3_CD -RRB-_-RRB- can_MD be_VB improved_VBN in_IN several_JJ ways_NNS ._.
In_IN this_DT paper_NN we_PRP identify_VBP three_CD shortcomings_NNS in_IN it_PRP and_CC introduce_VB two_CD ideas_NNS to_TO overcome_VB those_DT problems_NNS :_: the_DT multinomial_JJ sampling_NN approach_NN to_TO learning_VBG bayesian_JJ networks_NNS
of_IN LBMA_NNP to_TO TAN_NNP induction_NN in_IN 4.3_CD ._.
We_PRP empirically_RB evaluate_VBP the_DT results_NNS of_IN our_PRP$ improvements_NNS in_IN Section_NNP 5_CD ,_, to_TO finish_VB up_RP pointing_VBG some_DT conclusions_NNS in_IN Section_NN 6_CD ._.
A_DT longer_JJR version_NN of_IN this_DT paper_NN is_VBZ available_JJ =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2_CD Learning_NNP Bayesian_NNP Networks_NNP Let_NNP U_NN =_JJ -LCB-_-LRB- X1_NN ,_, ..._: ,_, Xn_NN -RCB-_-RRB- be_VB a_DT set_NN of_IN discrete_JJ random_JJ variables_NNS ._.
A_DT bayesian_JJ network_NN is_VBZ an_DT annotated_JJ directed_JJ acyclic_JJ graph_NN that_WDT encodes_VBZ a_DT joint_JJ probability_NN distribution_NN o_NN
explained_VBN in_IN Section_NNP 5_CD ._.
4_CD Local_NNP Bayesian_NNP Model_NNP Averaging_VBG The_DT second_JJ shortcoming_NN in_IN the_DT TAN_NN induction_NN algorithm_NN of_IN -LRB-_-LRB- 3_CD -RRB-_-RRB- is_VBZ that_DT uncertainty_NN in_IN model_NN selection_NN is_VBZ ignored_VBN ._.
Bayesian_JJ model_NN averaging_NN -LRB-_-LRB- BMA_NN -RRB-_-RRB- =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: provides_VBZ a_DT coherent_JJ mechanism_NN for_IN accounting_NN for_IN uncertainty_NN in_IN modelling_NN ._.
4.1_CD Bayesian_NNP Model_NNP Averaging_NNP A_NNP coherent_JJ approach_NN to_TO solve_VB the_DT classification_NN problem_NN is_VBZ calculating_VBG the_DT probability_NN of_IN each_DT
er_IN competing_VBG models_NNS ._.
In_IN order_NN to_TO handle_VB the_DT first_JJ of_IN these_DT problems_NNS ,_, we_PRP propose_VBP LBMA_NNP ,_, an_DT heuristic_NN approach_NN to_TO approximate_JJ BMA_NNP ._.
The_DT idea_NN is_VBZ similar_JJ in_IN spirit_NN to_TO the_DT Occam_NNP 's_POS Window_NNP method_NN described_VBN in_IN =_JJ -_: =[_NN 5_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
To_TO apply_VB LBMA_NNP we_PRP should_MD have_VB an_DT heuristic_NN h_NN -LRB-_-LRB- M_NN ,_, S_NN -RRB-_-RRB- such_JJ that_IN h_NN -LRB-_-LRB- M_NN ,_, S_NN -RRB-_-RRB- ≈_NN P_NN -LRB-_-LRB- M_NN |_NN S_NN -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- In_IN order_NN to_TO approximate_JJ the_DT summation_NN in_IN Equation_NN 9_CD ,_, and_CC given_VBN that_IN we_PRP have_VBP h_NN -LRB-_-LRB- M_NN ,_, S_NN -RRB-_-RRB- ,_, we_PRP define_VBP our_PRP$ set_NN of_IN interesti_NNS
rithm_NN described_VBN in_IN Section_NNP 4.3_CD ,_, we_PRP need_VBP to_TO set_VB some_DT parameters_NNS ._.
In_IN our_PRP$ experimental_JJ setting_NN ,_, we_PRP took_VBD :_: k_NN =_JJ min_NN -LRB-_-LRB- 10_CD ,_, n_NN -RRB-_-RRB- λ_NN =_JJ 10_CD -LRB-_-LRB- 17_CD -RRB-_-RRB- We_PRP tested_VBD three_CD algorithms_NNS over_IN 11_CD datasets_NNS from_IN the_DT Irvine_NNP repository_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: plus_CC our_PRP$ own_JJ credit_NN screening_NN database_NN ._.
To_TO discretize_VB continuous_JJ attributes_NNS we_PRP used_VBD equal_JJ frequency_NN discretization_NN with_IN 5_CD intervals_NNS ._.
For_IN each_DT dataset_NN and_CC algorithm_NN we_PRP evaluated_VBD accuracy_NN and_CC LogScore_NN ._.
