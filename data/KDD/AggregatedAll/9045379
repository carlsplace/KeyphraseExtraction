Structured_VBN metric_JJ learning_NN for_IN high_JJ dimensional_JJ problems_NNS
The_DT success_NN of_IN popular_JJ algorithms_NNS such_JJ as_IN k-means_NN clustering_NN or_CC nearest_JJS neighbor_NN searches_NNS depend_VBP on_IN the_DT assumption_NN that_IN the_DT underlying_VBG distance_NN functions_NNS reflect_VBP domain-specific_JJ notions_NNS of_IN similarity_NN for_IN the_DT problem_NN at_IN hand_NN ._.
The_DT distance_NN metric_JJ learning_NN problem_NN seeks_VBZ to_TO optimize_VB a_DT distance_NN function_NN subject_JJ to_TO constraints_NNS that_WDT arise_VBP from_IN fully-supervised_JJ or_CC semisupervised_JJ information_NN ._.
Several_JJ recent_JJ algorithms_NNS have_VBP been_VBN proposed_VBN to_TO learn_VB such_JJ distance_NN functions_NNS in_IN low_JJ dimensional_JJ settings_NNS ._.
One_CD major_JJ shortcoming_NN of_IN these_DT methods_NNS is_VBZ their_PRP$ failure_NN to_TO scale_VB to_TO high_JJ dimensional_JJ problems_NNS that_WDT are_VBP becoming_VBG increasingly_RB ubiquitous_JJ in_IN modern_JJ data_NNS mining_NN applications_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP present_VBP metric_JJ learning_NN algorithms_NNS that_WDT scale_VBP linearly_RB with_IN dimensionality_NN ,_, permitting_VBG efficient_JJ optimization_NN ,_, storage_NN ,_, and_CC evaluation_NN of_IN the_DT learned_VBN metric_NN ._.
This_DT is_VBZ achieved_VBN through_IN our_PRP$ main_JJ technical_JJ contribution_NN which_WDT provides_VBZ a_DT framework_NN based_VBN on_IN the_DT log-determinant_JJ matrix_NN divergence_NN which_WDT enables_VBZ efficient_JJ optimization_NN of_IN structured_JJ ,_, low-parameter_JJ Mahalanobis_NNP distances_NNS ._.
Experimentally_RB ,_, we_PRP evaluate_VBP our_PRP$ methods_NNS across_IN a_DT variety_NN of_IN high_JJ dimensional_JJ domains_NNS ,_, including_VBG text_NN ,_, statistical_JJ software_NN analysis_NN ,_, and_CC collaborative_JJ filtering_VBG ,_, showing_VBG that_IN our_PRP$ methods_NNS scale_VBP to_TO data_NNS sets_NNS with_IN tens_NNS of_IN thousands_NNS or_CC more_JJR features_NNS ._.
We_PRP show_VBP that_IN our_PRP$ learned_VBN metric_NN can_MD achieve_VB excellent_JJ quality_NN with_IN respect_NN to_TO various_JJ criteria_NNS ._.
For_IN example_NN ,_, in_IN the_DT context_NN of_IN metric_JJ learning_NN for_IN nearest_JJS neighbor_NN classification_NN ,_, we_PRP show_VBP that_IN our_PRP$ methods_NNS achieve_VBP 24_CD %_NN higher_JJR accuracy_NN over_IN the_DT baseline_NN distance_NN ._.
Additionally_RB ,_, our_PRP$ methods_NNS yield_VBP very_RB good_JJ precision_NN while_IN providing_VBG recall_NN measures_NNS up_IN to_TO 20_CD %_NN higher_JJR than_IN other_JJ baseline_NN methods_NNS such_JJ as_IN latent_JJ semantic_JJ analysis_NN ._.
eliver_JJR satisfactory_JJ results_NNS ,_, findinga_JJ good_JJ distance_NN metric_NN for_IN the_DT problem_NN at_IN hand_NN often_RB plays_VBZ a_DT very_RB crucial_JJ role_NN ._.
As_IN such_JJ ,_, metric_JJ learning_NN -LRB-_-LRB- 25_CD -RRB-_-RRB- has_VBZ received_VBN much_JJ attention_NN in_IN the_DT research_NN community_NN =_JJ -_: =[_NN 12_CD ,_, 25_CD ,_, 5_CD ,_, 22_CD ,_, 8_CD ,_, 26_CD ,_, 6_CD ,_, 7_CD ,_, 27_CD ,_, 29_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Many_JJ metric_JJ learning_NN methods_NNS have_VBP been_VBN proposed_VBN ._.
From_IN the_DT perspective_NN of_IN the_DT underlying_VBG learning_NN paradigm_NN ,_, these_DT methods_NNS can_MD be_VB grouped_VBN into_IN three_CD categories_NNS ,_, namely_RB ,_, supervised_JJ metric_JJ learning_NN ,_, uns_NNS
ngs_NNS of_IN the_DT Fifteenth_NNP Conference_NNP on_IN Computational_NNP Natural_NNP Language_NNP Learning_NNP ,_, pages_NNS 247_CD --_: 256_CD ,_, Portland_NNP ,_, Oregon_NNP ,_, USA_NNP ,_, 23_CD --_: 24_CD June_NNP 2011_CD ._.
c_NN ○_CD 2011_CD Association_NNP for_IN Computational_NNP Linguisticsalso_NNP been_VBN proposed_VBN -LRB-_-LRB- =_JJ -_: =_JJ Davis_NNP and_CC Dhillon_NNP ,_, 2008_CD -_: =--RRB-_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT new_JJ projection_NN learning_NN framework_NN ,_, Similarity_NNP Learning_NNP via_IN Siamese_NNP Neural_NNP Network_NNP -LRB-_-LRB- S2Net_NN -RRB-_-RRB- ,_, to_TO discriminatively_RB learn_VB the_DT concept_NN vector_NN representations_NNS of_IN input_NN text_NN obj_NN
ngs_NNS of_IN the_DT Fifteenth_NNP Conference_NNP on_IN Computational_NNP Natural_NNP Language_NNP Learning_NNP ,_, pages_NNS 247_CD --_: 256_CD ,_, Portland_NNP ,_, Oregon_NNP ,_, USA_NNP ,_, 23_CD --_: 24_CD June_NNP 2011_CD ._.
c_NN ○_CD 2011_CD Association_NNP for_IN Computational_NNP Linguisticsalso_NNP been_VBN proposed_VBN -LRB-_-LRB- =_JJ -_: =_JJ Davis_NNP and_CC Dhillon_NNP ,_, 2008_CD -_: =--RRB-_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT new_JJ projection_NN learning_NN framework_NN ,_, Similarity_NNP Learning_NNP via_IN Siamese_NNP Neural_NNP Network_NNP -LRB-_-LRB- S2Net_NN -RRB-_-RRB- ,_, to_TO discriminatively_RB learn_VB the_DT concept_NN vector_NN representations_NNS of_IN input_NN text_NN obj_NN
measure_NN relational_JJ similarity_NN is_VBZ two-fold_JJ ._.
First_JJ ,_, Mahalanobis_JJ distance_NN can_MD be_VB learned_VBN from_IN a_DT few_JJ data_NNS points_NNS ,_, and_CC efficient_JJ algorithms_NNS that_WDT can_MD scale_VB well_RB to_TO high-dimensional_JJ feature_NN spaces_NNS are_VBP known_VBN =_JJ -_: =[_NN 13_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Second_RB ,_, unlike_IN Euclidean_JJ distance_NN ,_, Mahalanobis_NNP distance_NN does_VBZ not_RB assume_VB that_IN features_NNS are_VBP independent_JJ ._.
This_DT is_VBZ particularly_RB important_JJ for_IN relational_JJ similarity_NN measures_NNS because_IN semantic_JJ relations_NNS ar_IN
cted_VBN ._.
For_IN the_DT case_NN when_WRB d_NN is_VBZ not_RB significantly_RB larger_JJR than_IN n_NN and_CC feature_NN space_NN vectors_NNS Φ_NN are_VBP available_JJ explicitly_RB ,_, the_DT basis_NN R_NN can_MD be_VB selected_VBN by_IN using_VBG one_CD of_IN the_DT following_JJ heuristics_NNS -LRB-_-LRB- see_VB Section_NNP 5_CD ,_, =_JJ -_: =_JJ Davis_NNP and_CC Dhillon_NNP ,_, 2008_CD -_: =_SYM -_: for_IN more_JJR details_NNS -RRB-_-RRB- :_: •_NN Using_VBG the_DT top_JJ k_NN singular_JJ vectors_NNS of_IN Φ_NN ._.
•_NNP Clustering_NNP the_DT columns_NNS of_IN Φ_NN and_CC using_VBG the_DT mean_NN vectors_NNS as_IN the_DT basis_NN R._NNP •_NNP For_IN the_DT fully-supervised_JJ case_NN ,_, if_IN the_DT number_NN of_IN classes_NNS -LRB-_-LRB- c_NN -RRB-_-RRB- is_VBZ g_NN
−_FW xj_FW -RRB-_-RRB- -LRB-_-LRB- xi_FW −_FW xj_FW -RRB-_-RRB- T_NN ť_NN -RRB-_-RRB- −_FW ¯_FW cij_FW i_FW ,_, j_FW δijλij_FW where_WRB λij_NN are_VBP dual_JJ variables_NNS with_IN λij_FW ≥_FW 0_CD ,_, δij_NN =_JJ +1_NN for_IN similarity_NN constraints_NNS and_CC δij_NN =_JJ −_NN 1_CD for_IN dissimilarity_NN constraints_NNS ._.
Using_VBG the_DT fact_NN that_IN ∇_NN A_NN log_NN |_NN A_NN |_NN =_JJ A_NN −_NN 1_CD =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT gradient_NN of_IN the_DT Lagrangian_NNP is_VBZ ,_, ∇_JJ AL_NN -LRB-_-LRB- A_NN ,_, λ_NN -RRB-_-RRB- =_JJ I_CD −_NN A_NN −_NN 1_CD +_CC X_NN i_FW ,_, j_NN δijλijUU_NN T_NN -LRB-_-LRB- xi_FW −_FW xj_FW -RRB-_-RRB- -LRB-_-LRB- xi_FW −_FW xj_FW -RRB-_-RRB- T_NN UU_NN T_NN ._.
Setting_VBG the_DT gradient_NN to_TO zero_VB and_CC solving_VBG for_IN A_DT −_NN 1_CD ,_, -LRB-_-LRB- A_DT ∗_NN -RRB-_-RRB- −_NN 1_CD =_JJ I_NN +_CC X_NN δijλijUU_NN T_NN -LRB-_-LRB- xi_FW −_FW xj_FW -RRB-_-RRB- -LRB-_-LRB- xi_FW −_FW xj_FW -RRB-_-RRB- T_NN
the_DT full-rank_JJ ITML_NN formulation_NN -LRB-_-LRB- 2.2_CD -RRB-_-RRB- ,_, we_PRP see_VBP that_IN A0_NN here_RB is_VBZ low-rank_JJ ,_, and_CC an_DT additional_JJ constraint_NN has_VBZ been_VBN added_VBN enforcing_VBG the_DT rank_NN of_IN the_DT optimal_JJ Mahalanobis_NNP matrix_NN A._NN Recent_JJ work_NN by_IN Kulis_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: considers_VBZ a_DT related_JJ problem_NN of_IN learning_VBG low-rank_JJ kernel_NN matrices_NNS subject_JJ to_TO linear_JJ constraints_NNS on_IN the_DT matrix_NN ._.
In_IN -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, the_DT LogDet_NNP divergence_NN was_VBD extended_VBN to_TO the_DT positive_JJ semi-definite_JJ cone_NN ,_, and_CC it_PRP was_VBD
can_MD be_VB constrained_VBN to_TO be_VB similar_JJ if_IN they_PRP share_VBP the_DT same_JJ class_NN label_NN and_CC dissimilar_JJ otherwise_RB ._.
One_CD class_NN of_IN distance_NN functions_NNS that_WDT has_VBZ shown_VBN good_JJ generalization_NN properties_NNS is_VBZ the_DT Mahalanobis_NNP distance_NN =_JJ -_: =[_NN 3_CD ,_, 10_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT Mahalanobis_NNP distance_NN generalizes_VBZ the_DT standard_NN squared_VBD Euclidean_JJ distance_NN commonly_RB used_VBN by_IN algorithms_NNS such_JJ as_IN the_DT k-nearest_NN neighbor_NN classifier_NN ._.
Intuitively_RB ,_, the_DT Mahalanobis_NNP distance_NN works_VBZ by_IN sca_NN
can_MD be_VB constrained_VBN to_TO be_VB similar_JJ if_IN they_PRP share_VBP the_DT same_JJ class_NN label_NN and_CC dissimilar_JJ otherwise_RB ._.
One_CD class_NN of_IN distance_NN functions_NNS that_WDT has_VBZ shown_VBN good_JJ generalization_NN properties_NNS is_VBZ the_DT Mahalanobis_NNP distance_NN =_JJ -_: =[_NN 3_CD ,_, 10_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT Mahalanobis_NNP distance_NN generalizes_VBZ the_DT standard_NN squared_VBD Euclidean_JJ distance_NN commonly_RB used_VBN by_IN algorithms_NNS such_JJ as_IN the_DT k-nearest_NN neighbor_NN classifier_NN ._.
Intuitively_RB ,_, the_DT Mahalanobis_NNP distance_NN works_VBZ by_IN sca_NN
lanobis_FW Distances_FW Term_NN frequency_NN models_NNS represent_VBP text_NN documents_NNS in_IN terms_NNS of_IN individual_JJ words_NNS and_CC their_PRP$ respective_JJ frequencies_NNS and_CC are_VBP standard_JJ representations_NNS used_VBN in_IN many_JJ text_NN analysis_NN applications_NNS =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
These_DT models_NNS typically_RB compute_VBP the_DT distance_NN between_IN two_CD examples_NNS x_NN and_CC y_NN using_VBG the_DT cosine_NN similarity_NN ,_, cos_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- =_JJ xT_NN y_NN ._.
Note_VB x_NN y_NN that_WDT when_WRB x_NN and_CC y_NN are_VBP normalized_VBN to_TO have_VB unit_NN L2_NN norm_NN ,_, the_DT cosine_NN
ubset_NN ,_, the_DT HDILR_NN method_NN outperforms_VBZ HDLR_NN for_IN low_JJ recall_NN values_NNS ,_, yet_CC it_PRP achieves_VBZ slightly_RB worse_JJR precision_NN for_IN higher_JJR recall_NN values_NNS ._.
5.3_CD Software_NNP Analysis_NNP We_PRP now_RB present_VBP results_NNS from_IN the_DT Clarify_NNP system_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: which_WDT attempt_VBP to_TO improve_VB software_NN error_NN messaging_VBG via_IN nearest_JJS neighbor_NN software_NN support_NN ._.
The_DT basis_NN of_IN the_DT Clarify_NNP system_NN lies_VBZ in_IN the_DT fact_NN that_IN modern_JJ software_NN design_NN promotes_VBZ modularity_NN and_CC abstracti_NNS
