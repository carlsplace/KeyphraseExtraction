Alpha_NN seeding_NN for_IN support_NN vector_NN machines_NNS
requirements_NNS grow_VBP linearly_RB with_IN the_DT cardinality_NN of_IN the_DT training_NN set_NN and_CC can_MD be_VB prohibitively_RB expensive_JJ for_IN large_JJ datasets_NNS ,_, unless_IN some_DT method_NN for_IN reusing_VBG previous_JJ solutions_NNS ,_, like_IN the_DT ''_'' alpha_NN seeding_NN ''_'' =_SYM -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, is_VBZ adopted_VBN ._.
5Conclusion_NN We_PRP have_VBP reviewed_VBN and_CC compared_VBN several_JJ methods_NNS for_IN selecting_VBG the_DT optimal_JJ hyperparameters_NNS of_IN a_DT SVM_NN and_CC estimating_VBG its_PRP$ generalization_NN ability_NN ._.
Both_CC classical_JJ and_CC more_RBR modern_JJ one_CD
f_FW warm-start_FW techniques_NNS when_WRB solving_VBG the_DT quadratic_JJ programming_NN associated_VBN to_TO each_DT SVM_NNP task_NN ,_, making_VBG the_DT algorithm_NN very_RB efficient_JJ even_RB compared_VBN to_TO gradient_NN descent_NN techniques_NNS coupled_VBN with_IN warm-starting_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: similar_JJ to_TO those_DT used_VBN in_IN SimpleMKL_NN ._.
Numerical_JJ experiments_NNS given_VBN in_IN the_DT sequel_NN will_MD support_VB such_PDT a_DT claim_NN ._.
Many_JJ previous_JJ works_NNS on_IN joint-sparse_JJ multi-task_JJ learning_NN have_VBP been_VBN carried_VBN to_TO in_IN a_DT linear_JJ frame_NN
sequential_JJ minimal_JJ optimization_NN algorithm_NN -LRB-_-LRB- SMO_NN -RRB-_-RRB- ._.
In_IN LIBSVM_NNP ,_, we_PRP examined_VBD several_JJ tolerances_NNS for_IN termination_NN criterion_NN :_: ε_NN =_JJ 10_CD −_NN 3_CD ,_, 10_CD −_NN 6_CD ,_, 10_CD −_CD 9_CD ._.
When_WRB we_PRP use_VBP LIBSVM_NN for_IN online-learning_NN ,_, alpha_NN seeding_NN =_JJ -_: =[_NN 11_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: sometimes_RB works_VBZ well_RB ._.
The_DT basic_JJ idea_NN of_IN alpha_NN seeding_NN is_VBZ to_TO use_VB the_DT parameters_NNS before_IN the_DT update_VBP as_IN the_DT initial_JJ parameter_NN ._.
In_IN alpha_NN seeding_NN ,_, we_PRP need_VBP to_TO take_VB care_NN of_IN the_DT fact_NN that_IN the_DT summation_NN constr_NN
argin_NN formulation_NN ._.
The_DT vertical_JJ axis_NN is_VBZ normalized_VBN di_FW erently_FW for_IN GACV_NNP ,_, VC_NNP bound_VBD ,_, approximate_JJ span_NN bound_VBD and_CC D_NN 2_CD w_NN 2_CD ._.
For_IN each_DT curve_NN ,_, ∇_NN denotes_VBZ the_DT minimum_JJ point_NN ._.
strategies_NNS ,_, such_JJ as_IN alpha_NN seeding_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: and_CC loose_JJ tolerance_NN -LRB-_-LRB- 10,11_CD -RRB-_-RRB- ,_, can_MD be_VB easily_RB carried_VBN from_IN LOO_NN to_TO k-fold_NN cross-validation_NN ._.
Thus_RB ,_, k-fold_NN cross-validation_NN is_VBZ also_RB an_DT e_SYM cient_JJ technique_NN for_IN tuning_NN SVM_NN hyperparameters_NNS ._.
sK_NN ._.
Duan_NNP et_FW al._FW \/_: Neu_NN
lidation_NN -RRB-_-RRB- for_IN each_DT leave-one-chemical-out_NN in_IN our_PRP$ experiment_NN ._.
However_RB ,_, current_JJ techniques_NNS make_VBP that_IN too_RB costly_JJ ,_, especially_RB for_IN KDA_NNP since_IN no_DT ecient_JJ model_NN selection_NN methods_NNS have_VBP yet_RB been_VBN formulated_VBN -LRB-_-LRB- see_VB =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 11_CD -RRB-_-RRB- for_IN some_DT recent_JJ methods_NNS for_IN more_RBR ecient_JJ SVM_NN model_NN selection_NN -RRB-_-RRB- ._.
Thus_RB ,_, we_PRP simply_RB selected_VBD kernels_NNS for_IN SVM_NNP and_CC KFD_NNP based_VBN on_IN which_WDT worked_VBD well_RB when_WRB randomly_RB partitioning_VBG the_DT data_NNS set_VBN into_IN trainin_NN
to_TO be_VB solved_VBN for_IN many_JJ C_NN values_NNS -LRB-_-LRB- such_JJ as_IN when_WRB tuning_NN C_NN via_IN crossvalidation_NN -RRB-_-RRB- ._.
The_DT β-seeding_NN idea_NN used_VBN here_RB is_VBZ very_RB much_RB similar_JJ to_TO the_DT idea_NN of_IN α-seeding_NN popularly_RB employed_VBN in_IN the_DT solution_NN of_IN SVM_NN duals_NNS -LRB-_-LRB- =_JJ -_: =_JJ DeCoste_NNP and_CC Wagstaff_NNP ,_, 2000_CD -_: =--RRB-_NN ._.
Eventhough_JJ full_JJ details_NNS of_IN the_DT final_JJ version_NN of_IN our_PRP$ implementation_NN of_IN L2-SVM-MFN_NN is_VBZ only_RB developed_VBN further_RB below_IN ,_, here_RB let_VB us_PRP compare_VB the_DT final_JJ implementation_NN 5_CD with_IN and_CC without_IN β-seeding_NN ._.
Table_NNP 1_CD
II_CD ._.
We_PRP begin_VBP by_IN drawing_VBG attention_NN to_TO three_CD alternative_JJ methods_NNS used_VBN to_TO speed_VB the_DT computation_NN of_IN the_DT leave-one-out_JJ error_NN for_IN C-SV_NN binary_JJ classification_NN algorithms_NNS ._.
Two_CD similar_JJ procedures_NNS ,_, described_VBN in_IN =_JJ -_: =[_NN 30_CD ,_, 52_CD -RRB-_-RRB- -_: =_JJ -_: ,_, determine_VB the_DT leave-one-out_NN bound_VBD exactly_RB ,_, but_CC achieve_VBP savings_NNS in_IN computational_JJ cost_NN by_IN initialising_VBG the_DT C-SV_NN learning_VBG algorithm_NN for_IN the_DT incomplete_JJ data_NN samples_NNS 1_CD using_VBG the_DT solution_NN obtained_VBN with_IN al_FW
Allgower_NNP and_CC Georg_NNP ,_, 1992_CD -RRB-_-RRB- ._.
The_DT closest_JJS we_PRP have_VBP seen_VBN to_TO our_PRP$ work_NN in_IN the_DT literature_NN employ_VBP similar_JJ techniques_NNS in_IN incremental_JJ learning_NN for_IN SVMs_NNS -LRB-_-LRB- Fine_NNP and_CC Scheinberg_NNP ,_, 2002_CD ;_: Cauwenberghs_NNP and_CC Poggio_NNP ,_, 2001_CD ;_: =_JJ -_: =_JJ DeCoste_NNP and_CC Wagstaff_NNP ,_, 2000_CD -_: =--RRB-_NN ._.
These_DT authors_NNS do_VBP not_RB ,_, however_RB ,_, construct_NN exact_JJ paths_NNS as_IN we_PRP do_VBP ,_, but_CC rather_RB focus_VB on_IN updating_VBG and_CC downdating_VBG the_DT solutions_NNS as_IN more_JJR -LRB-_-LRB- or_CC less_JJR -RRB-_-RRB- data_NNS arises_VBZ ._.
Diehl_NNP and_CC Cauwenberghs_NNP -LRB-_-LRB- 2003_CD -RRB-_-RRB- allow_VBP for_IN updat_NN
ent_JJ way_NN of_IN computing_VBG several_JJ Sγ_NN is_VBZ clear_JJ ._.
In_IN this_DT paragraph_NN ,_, we_PRP aim_VBP at_IN empirically_RB proving_VBG that_IN the_DT regularization_NN path_NN algorithm_NN we_PRP propose_VBP is_VBZ very_RB efficient_JJ even_RB compared_VBN to_TO a_DT warm-restart_JJ approach_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Remember_VB that_IN a_DT warm-restart_JJ approach_NN consists_VBZ of_IN using_VBG the_DT Lagrangian_JJ multipliers_NNS obtained_VBN for_IN a_DT previous_JJ value_NN of_IN λ_NN for_IN initializing_VBG the_DT QP_NN problem_NN with_IN a_DT new_JJ value_NN of_IN λ_NN ._.
DeCoste_NNP et_FW al._FW -LRB-_-LRB- 6_CD -RRB-_-RRB- has_VBZ pr_NN
are_VBP continuously_RB added_VBN into_IN an_DT already_RB large_JJ database_NN ._.
Therefore_RB ,_, how_WRB to_TO expedite_VB the_DT model_NN selection_NN process_NN has_VBZ become_VBN a_DT critical_JJ issue_NN for_IN SVM_NNP and_CC has_VBZ been_VBN addressed_VBN by_IN a_DT number_NN of_IN recent_JJ articles_NNS =_JJ -_: =[_NN 11_CD ,_, 14_CD ,_, 15_CD ,_, 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, the_DT approaches_NNS that_WDT have_VBP been_VBN proposed_VBN so_RB far_RB for_IN expediting_VBG the_DT model_NN selection_NN process_NN 1sof_JJ SVM_NN all_DT lead_VBP to_TO lower_JJR prediction_NN accuracy_NN ._.
Anyway_RB ,_, this_DT is_VBZ an_DT issue_NN that_WDT deserves_VBZ continuous_JJ in_IN
oduct_NN to_TO be_VB computed_VBN with_IN no_DT pointer-redirects_NNS -LRB-_-LRB- as_IN occur_VBP with_IN linked-list_JJ implementations_NNS -RRB-_-RRB- ,_, excellent_JJ memory_NN locality_NN ,_, and_CC using_VBG only_JJ comparison_NN and_CC increment_NN operators_NNS ._.
Alpha_NNP Seeding_NNP ._.
As_IN reported_VBN by_IN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, alpha_NN seeding_NN can_MD speed_VB up_RP the_DT training_NN time_NN of_IN that_DT is_VBZ SVMs_NNS considerably_RB ._.
The_DT main_JJ idea_NN is_VBZ that_IN starting_VBG with_IN a_DT good_JJ initial_JJ guess_NN α_FW ′_FW i_FW close_RB to_TO the_DT optimal_JJ value_NN αi_NN for_IN each_DT example_NN xi_FW dramatical_FW
the_DT minimum_NN bounding_VBG hypersphere_NN in_IN projected_VBN space_NN ._.
To_TO do_VB so_RB efficiently_RB ,_, we_PRP leverage_NN information_NN available_JJ from_IN the_DT classification_NN stage_NN ._.
Specifically_RB ,_, we_PRP use_VBP the_DT alphaseeding_NN technique_NN outlined_VBN in_IN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
•_NNP Stage_NNP 2_CD :_: Aggregate_JJ instances_NNS into_IN groups_NNS ._.
Details_NNS of_IN techniques_NNS applied_VBN -LRB-_-LRB- Membership_NN detection_NN and_CC Partitioning_NN -RRB-_-RRB- are_VBP discussed_VBN below_IN ._.
Alpha_NNP Seeding_NNP :_: The_DT learning_VBG algorithm_NN to_TO determine_VB R_NN typically_RB
Recently_RB ,_, a_DT lot_NN of_IN research_NN work_NN has_VBZ been_VBN devoted_VBN to_TO speeding_VBG up_RP the_DT LOO_NN procedure_NN so_IN that_IN it_PRP can_MD be_VB used_VBN to_TO tune_VB the_DT hyperparameters_NNS of_IN SVMs_NNS ._.
Some_DT of_IN those_DT speed-up_JJ strategies_NNS ,_, such_JJ as_IN alpha_NN seeding_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: and_CC loose_JJ tolerance_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, can_MD be_VB easily_RB carried_VBN from_IN LOO_NN to_TO k-fold_NN cross-validation_NN ._.
Thus_RB ,_, k-fold_NN cross-validation_NN is_VBZ also_RB an_DT efficient_JJ technique_NN for_IN tuning_NN SVM_NN hyperparameters_NNS ._.
Xi-Alpha_NNP Bound_VBD :_: Xi-Alp_NN
orking_VBG set_NN for_IN training_VBG a_DT new_JJ machine_NN ._.
Especially_RB ,_, when_WRB two_CD consecutive_JJ SVMs_NNS have_VBP the_DT same_JJ kernel_NN function_NN ,_, one_CD solution_NN can_MD be_VB reused_VBN directly_RB to_TO seed_NN the_DT search_NN for_IN another_DT machine_NN 's_POS solution_NN ,_, e.g._FW =_JJ -_: =[_NN 84_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN example_NN ,_, supposed_VBD that_DT training_NN the_DT first_JJ machine_NN SV_NN M1_NN with_IN kernel_NN function_NN K_NN and_CC error_NN penalty_NN C_NN =_JJ C1_NN resulted_VBD optimal_JJ solution_NN αSV_NN M1_NN ._.
To_TO train_VB anewmachineSV_NN M2_NN with_IN the_DT same_JJ kernel_NN function_NN
the_DT contemporary_JJ environment_NN ,_, how_WRB to_TO speed_VB up_RP the_DT model_NN selection_NN process_NN for_IN SVM_NNP becomes_VBZ a_DT crucial_JJ issue_NN and_CC several_JJ studies_NNS have_VBP been_VBN conducted_VBN to_TO address_VB this_DT issue_NN in_IN recent_JJ years_NNS -LRB-_-LRB- 4_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
These_DT studies_NNS share_VBP a_DT common_JJ ground_NN aimed_VBN at_IN reducing_VBG the_DT search_NN space_NN of_IN parameter_NN combinations_NNS ._.
In_IN this_DT paper_NN ,_, a_DT data_NN reduction_NN based_JJ approach_NN aimed_VBN at_IN expediting_VBG the_DT model_NN selection_NN process_NN of_IN SV_NN
e_LS -RRB-_-RRB- ._.
Note_VB that_IN this_DT is_VBZ not_RB equivalent_JJ to_TO training_VBG a_DT new_JJ SVM_NNP classifier_NN from_IN scratch_NN on_IN the_DT p_NN most_RBS recent_JJ examples_NNS ,_, because_IN each_DT successive_JJ optimization_NN problem_NN is_VBZ seeded_VBN with_IN the_DT previous_JJ hypothesis_NN w_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT hypothesis_NN may_MD contain_VB values_NNS for_IN features_NNS that_WDT do_VBP not_RB occur_VB anywhere_RB in_IN the_DT p_NN most_RBS recent_JJ examples_NNS ,_, and_CC these_DT will_MD not_RB be_VB changed_VBN ._.
This_DT allows_VBZ the_DT hypothesis_NN to_TO remember_VB rare_JJ -LRB-_-LRB- but_CC informative_JJ -RRB-_-RRB-
trained_VBN in_IN a_DT previous_JJ iteration_NN can_MD be_VB used_VBN as_IN a_DT seed_NN ,_, and_CC so_RB only_RB a_DT single_JJ model_NN needs_VBZ to_TO be_VB trained_VBN from_IN the_DT beginning_NN ._.
Note_VB that_IN unlike_IN the_DT ``_`` alphaseeding_NN ''_'' approach_NN used_VBN in_IN support_NN vector_NN machines_NNS =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT seed_NN and_CC current_JJ models_NNS need_MD not_RB be_VB trained_VBN on_IN the_DT same_JJ data_NNS and_CC may_MD have_VB completely_RB different_JJ kernel_NN functions_NNS as_IN only_RB the_DT output_NN of_IN the_DT seed_NN model_NN is_VBZ used_VBN ._.
2.2_CD ._.
Approximate_NNP Leave-One-Out_NNP Cross_NNP
e_LS -RRB-_-RRB- ._.
Note_VB that_IN this_DT is_VBZ not_RB equivalent_JJ to_TO training_VBG a_DT new_JJ SVM_NNP classifier_NN from_IN scratch_NN on_IN the_DT p_NN most_RBS recent_JJ examples_NNS ,_, because_IN each_DT successive_JJ optimization_NN problem_NN is_VBZ seeded_VBN with_IN the_DT previous_JJ hypothesis_NN w_NN =_JJ -_: =[_NN 30_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT hypothesis_NN may_MD contain_VB values_NNS for_IN features_NNS that_WDT do_VBP not_RB occur_VB anywhere_RB in_IN the_DT p_NN most_RBS recent_JJ examples_NNS ,_, and_CC these_DT will_MD not_RB be_VB changed_VBN ._.
This_DT allows_VBZ the_DT hypothesis_NN to_TO remember_VB rare_JJ -LRB-_-LRB- but_CC informative_JJ -RRB-_-RRB-
ation_NN ._.
4s3_NN .3_NN Kernel_NNP parameter_NN via_IN LOOCV_NN Another_DT natural_JJ use_NN of_IN alpha_NN seeding_NN is_VBZ for_IN sequential_JJ SVM_NNP 's_POS over_IN some_DT range_NN of_IN settings_NNS for_IN a_DT kernel_NN parameter_NN ._.
Previous_JJ work_NN with_IN Kernel_NNP Adatron_NNP SVM_NNP trainers_NNS =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: showed_VBD this_DT to_TO be_VB effective_JJ ,_, often_RB not_RB costing_VBG significantly_RB more_JJR to_TO train_VB for_IN a_DT large_JJ number_NN of_IN parameter_NN values_NNS than_IN for_IN the_DT first_JJ one_CD ._.
Based_VBN on_IN our_PRP$ experience_NN with_IN this_DT case_NN ,_, its_PRP$ effectiveness_NN see_VB
to_TO machine_NN learning_NN ._.
Recent_JJ work_NN has_VBZ established_VBN SVM_NNP 's_POS as_IN providing_VBG state-of-the-art_JJ performance_NN on_IN classification_NN and_CC regression_NN tasks_NNS across_IN a_DT variety_NN of_IN real-world_JJ applications_NNS -LRB-_-LRB- e.g._FW see_VB -LRB-_-LRB- lo_NN -RRB-_-RRB- and_CC =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
In_IN this_DT paper_NN ,_, we_PRP focus_VBP on_IN SVM_NNP 's_POS for_IN binary_JJ classification_NN -LRB-_-LRB- l_NN -RRB-_-RRB- ._.
In_IN binary_JJ classification_NN ,_, each_DT label_NN is_VBZ valued_VBN either_CC ``_`` +_CC l_NN ''_'' or_CC ``_`` -_: l_NN ''_'' ,_, indicating_VBG either_CC a_DT positive_JJ or_CC negative_JJ example_NN ,_, respectively_RB ._.
L_NN
ing_JJ state-of-the-art_JJ performance_NN on_IN classification_NN and_CC regression_NN tasks_NNS across_IN a_DT variety_NN of_IN real-world_JJ applications_NNS -LRB-_-LRB- e.g._FW see_VB -LRB-_-LRB- lo_NN -RRB-_-RRB- and_CC -LRB-_-LRB- 4_LS -RRB-_-RRB- -RRB-_-RRB- ._.
In_IN this_DT paper_NN ,_, we_PRP focus_VBP on_IN SVM_NNP 's_POS for_IN binary_JJ classification_NN =_JJ -_: =_JJ -LRB-_-LRB- l_NN -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN binary_JJ classification_NN ,_, each_DT label_NN is_VBZ valued_VBN either_CC ``_`` +_CC l_NN ''_'' or_CC ``_`` -_: l_NN ''_'' ,_, indicating_VBG either_CC a_DT positive_JJ or_CC negative_JJ example_NN ,_, respectively_RB ._.
Let_NNP XA_NNP be_VB an_DT nA_NN by_IN D_NN matrix_NN representing_VBG the_DT training_NN set_NN and_CC Xg_NN b_NN
SVM_NN training_NN ,_, by_IN seeding_VBG successive_JJ SVM_NN trainings_NNS with_IN the_DT results_NNS of_IN previous_JJ similar_JJ trainings_NNS ._.
1_CD Introduction_NN Recent_JJ progress_NN on_IN speeding_VBG up_RP the_DT training_NN time_NN for_IN support_NN vector_NN machines_NNS -LRB-_-LRB- e.g._FW -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =--RRB-_NN has_VBZ made_VBN SVM_NNP 's_POS practical_JJ now_RB for_IN training_NN sets_NNS that_WDT are_VBP fairly_RB large_JJ ._.
However_RB ,_, the_DT time_NN complexities_NNS of_IN those_DT approaches_NNS are_VBP still_RB typically_RB quadratic_JJ in_IN the_DT number_NN of_IN examples_NNS -LRB-_-LRB- N_NN -RRB-_-RRB- in_IN the_DT training_NN d_NN
