Large_JJ linear_JJ classification_NN when_WRB data_NNS can_MD not_RB fit_VB in_IN memory_NN
Recent_JJ advances_NNS in_IN linear_JJ classification_NN have_VBP shown_VBN that_IN for_IN applications_NNS such_JJ as_IN document_NN classification_NN ,_, the_DT training_NN can_MD be_VB extremely_RB efficient_JJ ._.
However_RB ,_, most_JJS of_IN the_DT existing_VBG training_NN methods_NNS are_VBP designed_VBN by_IN assuming_VBG that_IN data_NNS can_MD be_VB stored_VBN in_IN the_DT computer_NN memory_NN ._.
These_DT methods_NNS can_MD not_RB be_VB easily_RB applied_VBN to_TO data_NNS larger_JJR than_IN the_DT memory_NN capacity_NN due_JJ to_TO the_DT random_JJ access_NN to_TO the_DT disk_NN ._.
We_PRP propose_VBP and_CC analyze_VBP a_DT block_NN minimization_NN framework_NN for_IN data_NNS larger_JJR than_IN the_DT memory_NN size_NN ._.
At_IN each_DT step_NN a_DT block_NN of_IN data_NNS is_VBZ loaded_VBN from_IN the_DT disk_NN and_CC handled_VBN by_IN certain_JJ learning_NN methods_NNS ._.
We_PRP investigate_VBP two_CD implementations_NNS of_IN the_DT proposed_VBN framework_NN for_IN primal_JJ and_CC dual_JJ SVMs_NNS ,_, respectively_RB ._.
As_IN data_NNS can_MD not_RB fit_VB in_IN memory_NN ,_, many_JJ design_NN considerations_NNS are_VBP very_RB different_JJ from_IN those_DT for_IN traditional_JJ algorithms_NNS ._.
Experiments_NNS using_VBG data_NNS sets_VBZ 20_CD times_NNS larger_JJR than_IN the_DT memory_NN demonstrate_VBP the_DT effectiveness_NN of_IN the_DT proposed_VBN method_NN ._.
arallel_NN ._.
The_DT challenge_NN in_IN training_NN lies_VBZ in_IN the_DT fact_NN that_IN training_NN data_NNS sets_NNS are_VBP often_RB way_RB too_RB big_JJ to_TO be_VB fitted_VBN into_IN memory_NN ._.
Although_IN there_EX exist_VBP some_DT off-the-shelf_JJ packages_NNS that_WDT handles_VBZ memory_NN issues_NNS =_JJ -_: =[_NN 2_CD ,_, 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP found_VBD them_PRP computationally_RB expensive_JJ --_: they_PRP typically_RB need_VBP to_TO go_VB through_IN data_NNS many_JJ times_NNS ._.
To_TO resolve_VB the_DT difficulties_NNS ,_, we_PRP developed_VBD a_DT novel_NN averaged_VBD stochastic_JJ gradient_NN descent_NN -LRB-_-LRB- ASGD_NN -RRB-_-RRB- method_NN for_IN
training_NN SVMs_NNS have_VBP been_VBN evolving_VBG from_IN being_VBG scalable_JJ to_TO thousands_NNS of_IN examples_NNS to_TO being_VBG scalable_JJ to_TO millions_NNS ._.
Nevertheless_RB ,_, current_JJ state-of-the-art_JJ sequential_JJ linear_JJ SVM_NN solvers_NNS are_VBP relatively_RB slow_JJ -LRB-_-LRB- =_JJ -_: =[_NN 3_CD ,_, 19_CD -RRB-_-RRB- -_: =--RRB-_NN if_IN they_PRP are_VBP trained_VBN over_IN tens_NNS of_IN millions_NNS of_IN high-dimensional_JJ examples_NNS ._.
The_DT scalability_NN of_IN the_DT state-of-the-art_JJ sequential_JJ solvers_NNS of_IN nonlinear_JJ SVM_NN is_VBZ even_RB worse_JJR -LRB-_-LRB- -LRB-_-LRB- 15_CD -RRB-_-RRB- -RRB-_-RRB- ._.
The_DT scalability_NN of_IN sequentia_NN
the_DT size_NN of_IN the_DT training_NN data_NNS we_PRP have_VBP ._.
Therefore_RB ,_, such_JJ solvers_NNS are_VBP unsuitable_JJ for_IN our_PRP$ SVM_NNP training_NN ._.
Indeed_RB ,_, LIBLINEAR_NN recently_RB released_VBD an_DT extended_JJ version_NN that_IN explicitly_RB considered_VBN the_DT memory_NN issue_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP tested_VBD it_PRP with_IN a_DT simplified_VBN image_NN feature_NN set_NN -LRB-_-LRB- HOG_NN descriptor_NN only_RB with_IN coding_VBG dimension_NN of_IN 4,096_CD ,_, which_WDT generated_VBD 80GB_NN training_NN data_NNS -RRB-_-RRB- ._.
However_RB ,_, even_RB on_IN such_PDT a_DT small_JJ dataset_NN -LRB-_-LRB- as_IN compared_VBN to_TO our_PRP$ l_NN
