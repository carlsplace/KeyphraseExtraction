Transforming_VBG classifier_NN scores_NNS into_IN accurate_JJ multiclass_JJ probability_NN estimates_NNS
Class_NN membership_NN probability_NN estimates_NNS are_VBP important_JJ for_IN many_JJ applications_NNS of_IN data_NNS mining_NN in_IN which_WDT classification_NN outputs_NNS are_VBP combined_VBN with_IN other_JJ sources_NNS of_IN information_NN for_IN decision-making_JJ ,_, such_JJ as_IN example-dependent_JJ misclassification_NN costs_NNS ,_, the_DT outputs_NNS of_IN other_JJ classifiers_NNS ,_, or_CC domain_NN knowledge_NN ._.
Previous_JJ calibration_NN methods_NNS apply_VBP only_RB to_TO two-class_JJ problems_NNS ._.
Here_RB ,_, we_PRP show_VBP how_WRB to_TO obtain_VB accurate_JJ probability_NN estimates_NNS for_IN multiclass_JJ problems_NNS by_IN combining_VBG calibrated_VBN binary_JJ probability_NN estimates_NNS ._.
We_PRP also_RB propose_VBP a_DT new_JJ method_NN for_IN obtaining_VBG calibrated_VBN two-class_JJ probability_NN estimates_NNS that_WDT can_MD be_VB applied_VBN to_TO any_DT classifier_NN that_WDT produces_VBZ a_DT ranking_NN of_IN examples_NNS ._.
Using_VBG naive_JJ Bayes_NNS and_CC support_NN vector_NN machine_NN classifiers_NNS ,_, we_PRP give_VBP experimental_JJ results_NNS from_IN a_DT variety_NN of_IN two-class_JJ and_CC multiclass_JJ domains_NNS ,_, including_VBG direct_JJ marketing_NN ,_, text_NN categorization_NN and_CC digit_NN recognition_NN ._.
e_LS it_PRP was_VBD shown_VBN that_IN probabilities_NNS estimated_VBN by_IN these_DT algorithms_NNS are_VBP usually_RB far_RB from_IN the_DT observed_VBN ,_, true_JJ probabilities_NNS ._.
The_DT same_JJ methodology_NN was_VBD used_VBN to_TO show_VB that_IN SVM_NNP algorithms_NNS are_VBP poorly_RB calibrated_VBN -LRB-_-LRB- =_JJ -_: =_JJ Zadrozny_NNP and_CC Elkan_NNP ,_, 2002_CD -_: =-]_CD ,_, and_CC that_IN the_DT distortion_NN very_RB often_RB forms_VBZ a_DT sigmoid_JJ pattern_NN ._.
Boosting-based_JJ algorithms_NNS were_VBD shown_VBN to_TO produce_VB functions_NNS that_WDT are_VBP poorly_RB calibrated_VBN in_IN -LRB-_-LRB- Niculescu-Mizil_NNP and_CC Caruana_NNP ,_, 2005_CD -RRB-_-RRB- ._.
Although_IN the_DT
hich_VB the_DT model_NN is_VBZ used_VBN ,_, the_DT RMSE_NN -LRB-_-LRB- root_NN mean_NN squared_VBD error_NN -RRB-_-RRB- metric_NN was_VBD optimized_VBN instead_RB ._.
This_DT is_VBZ because_IN MSE_NNP tends_VBZ to_TO be_VB correlated_VBN with_IN profit_NN ,_, and_CC so_RB it_PRP can_MD be_VB used_VBN when_WRB exact_JJ costs_NNS are_VBP unavailable_JJ ._.
=_SYM -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: This_DT allows_VBZ the_DT desired_VBN misclassification_NN costs_NNS to_TO be_VB selected_VBN at_IN a_DT later_JJ time_NN by_IN varying_VBG the_DT threshold_NN for_IN classification_NN ._.
-LRB-_-LRB- 16_CD -RRB-_-RRB- 5.2_CD ._.
Calibration_NNP MSE_NNP can_MD be_VB decomposed_VBN into_IN two_CD components_NNS ,_, one_CD measuri_NN
them_PRP through_IN a_DT sigmoid_NN ._.
Platt_NNP 's_POS method_NN also_RB works_VBZ well_RB for_IN boosted_VBN trees_NNS and_CC boosted_VBD stumps_NNS ._.
A_DT sigmoid_NN ,_, however_RB ,_, might_MD not_RB be_VB the_DT correct_JJ transformation_NN for_IN all_DT learning_VBG algorithms_NNS ._.
Zadrozny_NNP and_CC Elkan_NNP =_SYM -_: =[_NN 7,8_CD -RRB-_-RRB- -_: =_SYM -_: used_VBD a_DT more_RBR general_JJ method_NN based_VBN on_IN Isotonic_JJ Regression_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- to_TO calibrate_VB predictions_NNS from_IN SVMs_NNS ,_, naive_JJ bayes_NNS ,_, boosted_VBD naive_JJ bayes_NNS ,_, and_CC decision_NN trees_NNS ._.
Isotonic_JJ Regression_NN is_VBZ more_RBR general_JJ in_IN that_IN the_DT o_NN
r_NN subset_NN of_IN trees_NNS than_IN existing_VBG approaches_NNS to_TO approximate_JJ the_DT true_JJ probability_NN ._.
5_CD Performance_NNP Evaluation_NN Reliability_NN plot_NN has_VBZ been_VBN used_VBN previously_RB to_TO measure_VB the_DT performance_NN of_IN probability_NN estimation_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
As_IN a_DT summary_NN ,_, we_PRP use_VBP the_DT x-axis_NN for_IN the_DT estimated_VBN probability_NN and_CC y-axis_NN for_IN the_DT true_JJ probability_NN ._.
The_DT probability_NN estimate_NN by_IN a_DT learner_NN is_VBZ good_JJ if_IN all_DT points_NNS are_VBP close_JJ to_TO the_DT y_NN =_JJ x_NN diagonal_JJ line_NN ._.
S_NN
edges_NNS in_IN a_DT meaningful_JJ way_NN ._.
Converting_VBG scores_NNS to_TO probabilities_NNS is_VBZ not_RB straightforward_JJ since_IN the_DT relation_NN between_IN them_PRP is_VBZ often_RB not_RB linear_JJ ._.
This_DT problemhas_VBZ been_VBN studiedfor_JJ differentkinds_NNS of_IN classifiers_NNS =_JJ -_: =[_NN 11_CD ,_, 23_CD ,_, 24_CD ,_, 25_CD -RRB-_-RRB- -_: =_JJ -_: ,_, but_CC with_IN score_NN distributions_NNS different_JJ from_IN the_DT one_NN we_PRP observe_VBP in_IN our_PRP$ case_NN ._.
In_IN Fig._NNP 15_CD ,_, we_PRP observe_VBP a_DT logarithmic_JJ shape_NN in_IN the_DT distribution_NN of_IN probabilities_NNS with_IN respect_NN to_TO scores_NNS ._.
Using_VBG a_DT curve_NN fitt_NN
._.
,_, calibration_NN -RRB-_-RRB- and_CC then_RB combine_VB these_DT probabilities_NNS by_IN averaging_VBG them_PRP ._.
This_DT approach_NN is_VBZ related_JJ to_TO our_PRP$ CSA_NNP algorithm_NN ,_, but_CC differs_VBZ in_IN that_IN we_PRP use_VBP a_DT non-parametric_JJ technique_NN called_VBN isotonic_JJ regression_NN =_JJ -_: =[_NN 39_CD -RRB-_-RRB- -_: =_SYM -_: to_TO calibrate_VB our_PRP$ scores_NNS ._.
Freund_NNP et_FW al._FW -LRB-_-LRB- 10_CD -RRB-_-RRB- describe_VBP the_DT RankBoost_NNP algorithm_NN ._.
This_DT rank-based_JJ approach_NN is_VBZ relatively_RB easy_JJ to_TO implement_VB ,_, has_VBZ a_DT strong_JJ theoretical_JJ foundation_NN ,_, and_CC has_VBZ been_VBN shown_VBN to_TO perf_VB
ation_NN ._.
Recent_JJ research_NN in_IN machine_NN learning_NN has_VBZ shown_VBN the_DT benefits_NNS of_IN calibrating_VBG predictive_JJ models_NNS ,_, which_WDT becomes_VBZ especially_RB important_JJ when_WRB probability_NN estimates_NNS are_VBP used_VBN for_IN clinical_JJ decision_NN making_NN =_JJ -_: =[_NN 140_CD ,_, 147_CD ,_, 205_CD ,_, 206_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN summary_NN ,_, the_DT quality_NN of_IN probabilistic_JJ predictive_JJ models_NNS depends_VBZ on_IN two_CD major_JJ indices_NNS :_: discrimination_NN and_CC calibration_NN ._.
Importantly_RB ,_, the_DT success_NN of_IN personalized_JJ clinical_JJ decision_NN support_NN systems_NNS de_IN
n._NN Although_IN the_DT sigmoid_NN only_RB has_VBZ two_CD degrees_NNS of_IN freedom_NN ,_, one_PRP can_MD see_VB more_RBR flexible_JJ schemes_NNS are_VBP unlikely_JJ to_TO improve_VB calibration_NN much_RB ._.
This_DT may_MD be_VB why_WRB no_DT real_JJ benefit_NN was_VBD seen_VBN using_VBG isotonic_JJ regression_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: There_EX is_VBZ one_CD data_NN set_NN ,_, LetterK_NN ,_, that_WDT shows_VBZ a_DT large_JJ difference_NN in_IN expected_VBN cost_NN ._.
This_DT is_VBZ an_DT extremely_RB imbalanced_JJ domain_NN and_CC by_IN training_VBG the_DT classifier_NN on_IN a_DT balanced_JJ data_NN set_NN ,_, the_DT black_JJ curves_NNS in_IN Figur_NNP
alibration_NN methods_NNS are_VBP :_: --_: The_DT binning_JJ averaging_NN method_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- consists_VBZ in_IN sorting_VBG the_DT examples_NNS in_IN decreasing_VBG order_NN by_IN their_PRP$ estimated_VBN probabilities_NNS and_CC dividing_VBG the_DT set_NN into_IN k_NN 1_CD Scores_NNS can_MD also_RB be_VB used_VBN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2bins_NNS -LRB-_-LRB- i.e._FW ,_, subsets_NNS of_IN equal_JJ size_NN -RRB-_-RRB- ._.
Then_RB ,_, for_IN each_DT bin_NN l_NN ,_, 1_CD ≤_CD l_NN ≤_CD k_NN ,_, the_DT corrected_JJ probability_NN estimate_NN for_IN a_DT case_NN i_FW belonging_VBG to_TO class_NN j_NN -LRB-_-LRB- p_NN ∗_NN -LRB-_-LRB- i_FW ,_, j_NN -RRB-_-RRB- -RRB-_-RRB- is_VBZ the_DT proportion_NN of_IN instances_NNS in_IN l_NN of_IN class_NN
ular_JJ class_NN ,_, we_PRP re-scale_JJ theclassifier_NN scores_NNS using_VBG isotonic_JJ regression_NN ,_, which_WDT have_VBP been_VBN successfully_RB used_VBN to_TO obtain_VB accurate_JJ class_NN membership_NN probability_NN estimates_NNS for_IN binary_JJ and_CC multiclass_JJ problems_NNS =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP then_RB combine_VBP the_DT re-scaled_JJ scores_NNS of_IN SVM_NNP and_CC FWC_NNP using_VBG a_DT weighted_JJ linear_JJ combination_NN ,_, where_WRB the_DT weights_NNS were_VBD determined_VBN empirically_RB ,_, for_IN each_DT fact_NN and_CC slot-type_JJ ._.
4.5_CD Rules_NNS In_IN addition_NN to_TO using_VBG a_DT
mon_NN are_VBP :_: binning_VBG averaging_NN ,_, isotonic_JJ regression_NN and_CC Platt_NNP 's_POS method_NN ._.
The_DT objective_NN of_IN these_DT methods_NNS -LRB-_-LRB- as_IN a_DT postprocessing_NN -RRB-_-RRB- is_VBZ to_TO transform_VB the_DT original_JJ estimated_VBN probabilities_NNS -LRB-_-LRB- scores_NNS can_MD also_RB be_VB used_VBN -LRB-_-LRB- =_JJ -_: =_JJ Zadrozny_NNP &_CC Elkan_NNP ,_, 2002_CD -_: =--RRB-_NN -RRB-_-RRB- p_NN -LRB-_-LRB- i_FW ,_, j_NN -RRB-_-RRB- into_IN calibrated_VBN probability_NN estimates_VBZ p_NN \*_NN -LRB-_-LRB- i_FW ,_, j_NN -RRB-_-RRB- ._.
It_PRP is_VBZ important_JJ to_TO remark_VB that_IN all_DT of_IN these_DT general_JJ calibration_NN methods_NNS can_MD only_RB be_VB used_VBN -LRB-_-LRB- directly_RB ,_, without_IN approaches_NNS -RRB-_-RRB- in_IN binary_JJ problems_NNS ,_,
ced_VBN classes_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
Therefore_RB ,_, before_IN computing_VBG the_DT accuracy_NN of_IN the_DT individual_JJ one-vs-rest_JJ models_NNS we_PRP also_RB convert_VBP their_PRP$ scores_NNS into_IN calibrated_VBN probability_NN scores_NNS using_VBG the_DT pairadjacent_JJ violators_NNS method_NN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN evaluating_VBG the_DT system_NN -LRB-_-LRB- see_VB Section_NNP 3.2_CD -RRB-_-RRB- we_PRP take_VBP as_IN threshold_NN the_DT calibrated_VBN score_NN of_IN 0.5_CD ,_, and_CC not_RB the_DT default_NN SVM_NN threshold_NN of_IN score_NN 0_CD ,_, i.e._FW examples_NNS with_IN calibrated_VBN scores_NNS larger_JJR or_CC equal_JJ to_TO 0_CD ._.
approach_NN ._.
Insurance_NN :_: This_DT was_VBD part_NN of_IN the_DT COIL_NN 2002_CD competition_NN and_CC proved_VBD difficult_JJ for_IN all_DT algorithms_NNS ._.
The_DT best_JJS results_NNS were_VBD only_RB a_DT few_JJ percentage_NN points_NNS better_JJR than_IN the_DT best_JJS in_IN Table_NNP IV_NNP -LRB-_-LRB- See_NNP e.g._FW =_SYM -_: =[_NN 26_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
The_DT very_RB best_RB supervised_VBN classifiers_NNS outperform_VBP the_DT outlier_NN methods_NNS on_IN most_JJS datasets_NNS tested_VBN but_CC the_DT margins_NNS are_VBP generally_RB small_JJ ._.
Remarkably_RB ,_, one_CD of_IN our_PRP$ unsupervised_JJ approaches_NNS achieved_VBD the_DT best_JJS res_NNS
of_IN all_DT predictions_NNS that_WDT are_VBP subject_JJ to_TO a_DT confidence_NN value_NN of_IN x_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- ._.
Obtaining_VBG more_RBR or_CC less_RBR well_RB calibrated_VBN probability_NN estimates_NNS has_VBZ gained_VBN a_DT lot_NN of_IN attention_NN in_IN the_DT context_NN of_IN classification_NN -LRB-_-LRB- e.g._FW =_JJ -_: =[_NN 41,42,56,57_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Here_RB ,_, in_IN the_DT unsupervised_JJ context_NN of_IN outlier_NN detection_NN ,_, we_PRP propose_VBP the_DT first_JJ approach_NN to_TO reconvert_VB plain_JJ outlier_NN scores_NNS roughly_RB into_IN probability_NN estimates_NNS as_IN a_DT motivation_NN for_IN the_DT unification_NN and_CC in_IN
._.
belonging_VBG to_TO The_DT classifier_NN is_VBZ wellcalibrated_VBN if_IN the_DT empirical_JJ class_NN membership_NN probconverges_NNS to_TO the_DT probability_NN value_NN as_IN the_DT S_NN ¨_FW ©_FW ¡_CD cents_NNS number_NN of_IN examples_NNS cents_NNS ability_NN ¡_NN classified_VBN goes_VBZ to_TO infinity_NN -LRB-_-LRB- =_JJ -_: =_JJ Zadrozny_NNP and_CC Elkan_NNP ,_, 2002_CD -_: =--RRB-_NN ._.
Intuitively_RB ,_, if_IN we_PRP consider_VBP all_PDT the_DT instances_NNS S_NN ¨_FW ©_FW ¡_CD cents_NNS to_TO which_WDT the_DT classifier_NN assigns_VBZ a_DT probability_NN S_NN ¨_FW ©_FW ¡_CD cents_NNS ._.
of_IN say_NN 0.6_CD ,_, then_RB 60_CD %_NN of_IN these_DT instances_NNS should_MD be_VB members_NNS of_IN S_NN ¨_FW ©_FW ¡_CD cents_NNS class_NN
The_DT SVM_NNP weighted_VBD sums_NNS from_IN above_RB will_MD not_RB be_VB strict_JJ probabilities_NNS between_IN 0_CD and_CC 1_CD ._.
Therefore_RB in_IN the_DT final_JJ stage_NN of_IN GleanerSRL_NN ,_, we_PRP calibrate_VBP these_DT scores_NNS into_IN proper_JJ probabilities_NNS ._.
Zadrozny_NNP and_CC Elkan_NNP =_SYM -_: =[_NN 21_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Niculescu-Mizil_NNP and_CC Caruna_NNP -LRB-_-LRB- 14_CD -RRB-_-RRB- recommend_VBP Isotonic_JJ Regression_NN for_IN large_JJ highly-skewed_JJ datasets_NNS ._.
The_DT main_JJ idea_NN behind_IN isotonic_JJ regression_NN is_VBZ to_TO transform_VB the_DT sorted_VBN list_NN of_IN SVM_NN scores_NNS into_IN monoton_NN
ier_NN ._.
Procedures_NNS for_IN calibrating_VBG classifiers_NNS have_VBP been_VBN proposed_VBN in_IN different_JJ contexts_NNS :_: In_IN weather_NN prediction_NN tasks_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- ,_, in_IN game_NN theory_NN -LRB-_-LRB- 2_CD ,_, 3_CD -RRB-_-RRB- ,_, and_CC more_RBR recently_RB in_IN the_DT context_NN of_IN pattern_NN classification_NN =_JJ -_: =[_NN 4_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Zadrozny_NNP and_CC Elkan_NNP were_VBD also_RB the_DT first_JJ to_TO notice_VB the_DT need_NN of_IN calibrating_VBG classifiers_NNS when_WRB used_VBN as_IN decision_NN making_NN aids_NNS ._.
Our_PRP$ own_JJ incentive_NN to_TO study_VB calibration_NN came_VBD from_IN applying_VBG probabilistic_JJ based_VBN
osted_VBN Naive_JJ Bayes_NNS -LRB-_-LRB- Elkan_NNP ,_, 1997_CD -RRB-_-RRB- ,_, we_PRP compared_VBD our_PRP$ reduction_NN to_TO the_DT minimum_NN expected_VBD cost_NN approach_NN after_IN calibration_NN of_IN the_DT probability_NN estimates_NNS produced_VBN by_IN the_DT learner_NN ,_, as_IN done_VBN by_IN Zadrozny_NNP and_CC Elkan_NNP -LRB-_-LRB- =_JJ -_: =_JJ Zadrozny_NNP &_CC Elkan_NNP ,_, 2002_CD -_: =--RRB-_NN ._.
We_PRP applied_VBD the_DT methods_NNS to_TO seven_CD UCI_NN multiclass_JJ datasets_NNS ._.
Since_IN these_DT datasets_NNS do_VBP not_RB have_VB costs_NNS associated_VBN with_IN them_PRP ,_, we_PRP generated_VBD artificial_JJ costs_NNS in_IN the_DT same_JJ manner_NN that_WDT was_VBD done_VBN in_IN the_DT MetaCost_NN
ifiers_NNS in_IN particular_JJ ,_, which_WDT is_VBZ known_VBN for_IN giving_VBG good_JJ predictions_NNS but_CC uncalibrated_JJ probabilities_NNS ,_, class_NN binarization_NN is_VBZ of_IN importance_NN because_IN most_JJS calibration_NN techniques_NNS operate_VBP on_IN two-class_JJ problems_NNS -LRB-_-LRB- =_JJ -_: =_JJ Zadrozny_NNP and_CC Elkan_NNP ,_, 2002_CD -_: =--RRB-_NN ._.
Finally_RB ,_, regarding_VBG computational_JJ efficiency_NN ,_, training_VBG a_DT large_JJ number_NN of_IN classifiers_NNS from_IN subsets_NNS of_IN the_DT training_NN examples_NNS may_MD be_VB cheaper_JJR than_IN training_VBG an_DT entire_JJ classifier_NN ,_, in_IN particular_JJ when_WRB the_DT b_NN
1_CD ,_, of_IN belonging_VBG to_TO class_NN ._.
The_DT classifier_NN is_VBZ wellcalibrated_VBN if_IN the_DT empirical_JJ class_NN membership_NN probability_NN S_NN converges_VBZ to_TO the_DT probability_NN value_NN S_NN as_IN the_DT number_NN of_IN examples_NNS classified_VBN goes_VBZ to_TO infinity_NN -LRB-_-LRB- =_JJ -_: =_JJ Zadrozny_NNP and_CC Elkan_NNP ,_, 2002_CD -_: =--RRB-_NN ._.
Intuitively_RB ,_, if_IN we_PRP consider_VBP all_PDT the_DT instances_NNS to_TO which_WDT the_DT classifier_NN assigns_VBZ a_DT probability_NN S_NN of_IN say_NN 0.6_CD ,_, then_RB 60_CD %_NN of_IN these_DT instances_NNS should_MD be_VB members_NNS of_IN class_NN ._.
3.2_CD Being_VBG Well_NNP Calibrated_NNP Helps_VBZ Es_NNS
decision_NN regarding_VBG ROIs_NNS of_IN different_JJ images_NNS since_IN small_JJ differences_NNS among_IN acquisition_NN environments_NNS affect_VBP the_DT absolute_JJ scale_NN of_IN the_DT distance_NN ._.
To_TO overcome_VB this_DT problem_NN ,_, researches_VBZ have_VBP been_VBN done_VBN -LRB-_-LRB- 112_CD -RRB-_-RRB- =_JJ -_: =[_NN 168_CD -RRB-_-RRB- -_: =_SYM -_: in_IN order_NN to_TO extract_VB a_DT posterior_JJ probability_NN from_IN the_DT SVM_NNP outputs_NNS but_CC this_DT interpretation_NN seems_VBZ not_RB to_TO be_VB trivial_JJ ._.
Both_DT -LRB-_-LRB- 112_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 168_CD -RRB-_-RRB- work_NN in_IN the_DT same_JJ way_NN :_: they_PRP take_VBP as_IN input_NN a_DT collection_NN of_IN the_DT mar_FW
cholesterol_NN level_NN ,_, and_CC in_IN -LRB-_-LRB- SAM97_NN -RRB-_-RRB- the_DT credit-worthiness_NN as_IN a_DT function_NN of_IN income_NN ._.
Other_JJ applications_NNS include_VBP epidemiology_NN -LRB-_-LRB- MJDP_NN +_CC 00_CD -RRB-_-RRB- ,_, microarray_NN data_NN analysis_NN -LRB-_-LRB- AHKW06_NN -RRB-_-RRB- ,_, and_CC calibration_NN of_IN classifiers_NNS =_JJ -_: =_JJ -LRB-_-LRB- ZE02_NN -RRB-_-RRB- -_: =_SYM -_: ._.
A_DT lot_NN of_IN research_NN has_VBZ concentrated_VBN on_IN finding_VBG efficient_JJ algorithms_NNS for_IN isotonic_JJ regression_NN under_IN different_JJ cost_NN functions_NNS ._.
For_IN complete_JJ orders_NNS ,_, Stout_NN -LRB-_-LRB- Sto00_NN -RRB-_-RRB- gives_VBZ algorithms_NNS to_TO find_VB the_DT optimal_JJ sol_NN
using_VBG the_DT available_JJ training_NN data_NNS by_IN applying_VBG a_DT classifier_NN learning_NN method_NN that_WDT estimates_VBZ conditional_JJ probabilities_NNS or_CC by_IN transforming_VBG the_DT outputs_NNS of_IN a_DT classifier_NN into_IN accurate_JJ probability_NN estimates_VBZ =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
55_CD One-Benefit_JJ Reduction_NN -LRB-_-LRB- Training_NN Set_NN S_NN =_JJ -LRB-_-LRB- x_NN ,_, y_NN ,_, b_NN -RRB-_-RRB- -RRB-_-RRB- 1_CD ._.
Learn_VB a_DT model_NN for_IN P_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- using_VBG S._NNP 2_CD ._.
Calculate_VB a_DT weight_NN for_IN each_DT example_NN -LRB-_-LRB- x_NN ,_, y_NN ,_, b_NN -RRB-_-RRB- :_: w_NN =_JJ b_NN P_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- 3_CD ._.
Learn_VB a_DT classifier_NN h_NN using_VBG a_DT cost-sensit_NN
be_VB interpreted_VBN as_IN prediction_NN strength_NN ._.
It_PRP has_VBZ been_VBN shown_VBN that_IN these_DT scores_NNS could_MD be_VB successfully_RB calibrated_VBN to_TO posterior_JJ class_NN probability_NN in_IN case_NN of_IN support_NN vector_NN machine_NN -LRB-_-LRB- SVM_NN -RRB-_-RRB- -LRB-_-LRB- 12_CD -RRB-_-RRB- and_CC naive_JJ Bayes_NNS =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_JJ -_: classifiers_NNS ._.
The_DT results_NNS indicate_VBP that_IN the_DT output_NN from_IN such_JJ classifiers_NNS can_MD be_VB interpreted_VBN as_IN a_DT monotonically_RB increasing_VBG function_NN of_IN the_DT posterior_JJ class_NN probabilities_NNS ._.
It_PRP follows_VBZ that_IN h_NN -LRB-_-LRB- x_NN -RRB-_-RRB- \/_: g_NN -LRB-_-LRB- x_NN -RRB-_-RRB- can_MD b_SYM
nd_IN the_DT rest_NN of_IN the_DT data_NNS as_IN the_DT negative_JJ examples_NNS ._.
Each_DT SVM_NNP outputs_VBZ a_DT score_NN s_NN that_WDT is_VBZ mapped_VBN to_TO a_DT posterior_JJ probability_NN P_NN -LRB-_-LRB- y_NN |_CD s_NNS -RRB-_-RRB- according_VBG to_TO a_DT function_NN f_SYM -LRB-_-LRB- s_NNS -RRB-_-RRB- =_JJ P_NN -LRB-_-LRB- y_NN |_CD s_NNS -RRB-_-RRB- that_WDT is_VBZ learned_VBN from_IN training_NN data_NN =_JJ -_: =_JJ -LRB-_-LRB- ZE02_NN -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT final_JJ predicted_VBN class_NN of_IN an_DT unlabeled_JJ data_NN point_NN is_VBZ given_VBN by_IN the_DT classifier_NN that_WDT produces_VBZ the_DT highest_JJS posterior_JJ probability_NN ._.
Using_VBG the_DT DWCH_NN features_VBZ with_IN a_DT one-versus-all_JJ SVM_NN ,_, the_DT authors_NNS report_NN
one_CD of_IN the_DT three_CD presented_VBN calibration_NN methods_NNS for_IN discriminative_JJ and_CC generative_JJ classifiers_NNS ,_, namely_RB a_DT GMM_NN fitting_NN of_IN the_DT scores_NNS ,_, the_DT sigmoid_JJ fitting_NN proposed_VBN by_IN Platt_NNP -LRB-_-LRB- 6_CD -RRB-_-RRB- and_CC the_DT isotonic_JJ regression_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT paper_NN is_VBZ organized_VBN as_IN follows_VBZ :_: We_PRP first_RB describe_VBP the_DT standard_JJ methods_NNS in_IN speaker_NN recognition_NN in_IN section_NN 2_CD and_CC give_VB a_DT short_JJ overview_NN of_IN kernel-based_JJ methods_NNS and_CC SVMs_NNS in_IN section_NN 3_CD ._.
Section_NN 4_CD expla_NN
rnative_JJ candidate_NN for_IN application-independent_JJ evaluation_NN ,_, namely_RB CBrier_NN ._.
This_DT cost_NN function_NN is_VBZ also_RB a_DT strictly_RB proper_JJ scoring_VBG rule_NN and_CC has_VBZ seen_VBN use_NN in_IN machine_NN learning_NN and_CC other_JJ disciplines_NNS -LRB-_-LRB- see_VB e.g._FW =_JJ -_: =_JJ Zadrozny_NNP and_CC Elkan_NNP ,_, 2002_CD -_: =_JJ -_: and_CC references_NNS therein_RB -RRB-_-RRB- ,_, and_CC particularly_RB in_IN meteorology_NN -LRB-_-LRB- Brier_NNP ,_, 1950_CD ;_: Roulston_NNP and_CC Smith_NNP ,_, 2002_CD -RRB-_-RRB- ,_, as_IN an_DT evaluation_NN objective_NN for_IN the_DT assessment_NN of_IN predictive_JJ posterior_JJ probabilities_NNS ._.
Using_VBG Corollary_NN
o_NN not_RB ._.
Fortunately_RB ,_, the_DT outputs_NNS of_IN these_DT other_JJ methods_NNS can_MD typically_RB be_VB postprocessed_VBN into_IN calibrated_VBN probabilities_NNS ._.
The_DT two_CD most_RBS common_JJ postprocessing_VBG methods_NNS for_IN calibration_NN are_VBP isotonic_JJ regression_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC fitting_JJ a_DT one-dimensional_JJ logistic_JJ regression_NN function_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
We_PRP apply_VBP the_DT latter_JJ 216_CD method_NN ,_, which_WDT is_VBZ often_RB called_VBN Platt_NNP scaling_NN ,_, to_TO SVM_NNP classifiers_NNS in_IN Section_NNP 5_CD below_IN ._.
4_LS ._.
AN_DT ILLUSTRATION_NN To_TO ill_RB
s_NNS boosting_VBG -LRB-_-LRB- 10_CD -RRB-_-RRB- ._.
In_IN addition_NN ,_, since_IN every_DT binary_JJ classifier_NN is_VBZ trained_VBN independently_RB ,_, their_PRP$ outputs_NNS may_MD be_VB on_IN different_JJ scales_NNS ,_, making_VBG it_PRP difficult_JJ to_TO compare_VB them_PRP -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
Though_IN calibration_NN techniques_NNS =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: can_MD be_VB used_VBN to_TO alleviate_VB this_DT problem_NN in_IN supervised_JJ classification_NN ,_, it_PRP is_VBZ rarely_RB used_VBN in_IN semi-supervised_JJ learning_NN due_JJ to_TO the_DT small_JJ number_NN of_IN labeled_JJ training_NN examples_NNS ._.
Moreover_RB ,_, techniques_NNS like_IN one-v_NN
ty_NN estimates_NNS ._.
On_IN the_DT other_JJ hand_NN ,_, the_DT proposed_VBN MDL-based_JJ calibration_NN mechanism_NN provides_VBZ higher_JJR calibration_NN degrees_NNS than_IN Curtailment_NN -LRB-_-LRB- 29_CD -RRB-_-RRB- -LRB-_-LRB- for_IN DTs_NNS -RRB-_-RRB- ,_, simple_JJ binning_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- -LRB-_-LRB- for_IN NB_NN -RRB-_-RRB- ,_, and_CC Isotonic_JJ Regression_NN =_JJ -_: =[_NN 30,19_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- for_IN LAC_NN -RRB-_-RRB- ._.
5.1.2_NN ._.
Results_NNS We_PRP start_VBP our_PRP$ analysis_NN by_IN evaluating_VBG each_DT classifier_NN in_IN terms_NNS of_IN its_PRP$ ability_NN for_IN estimating_VBG the_DT actual_JJ accuracy_NN ._.
Fig._NN 5_CD shows_VBZ the_DT actual_JJ accuracy_NN and_CC the_DT accuracy_NN estimates_VBZ f_SYM
scores_NNS into_IN probabilities_NNS is_VBZ non-decreasing_JJ ,_, and_CC we_PRP can_MD use_VB isotonic_JJ regression_NN to_TO learn_VB this_DT mapping_NN ._.
A_DT commonly_RB used_VBN algorithm_NN for_IN computing_VBG the_DT isotonic_JJ regression_NN is_VBZ pair-adjacent_JJ violators_NNS -LRB-_-LRB- PAV_NN -RRB-_-RRB- =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT algorithm_NN finds_VBZ the_DT stepwise-constant_JJ isotonic_JJ function_NN that_WDT best_RB fits_VBZ the_DT data_NNS according_VBG to_TO a_DT mean-squared_JJ error_NN criterion_NN ._.
PAV_NNP works_VBZ as_IN follows_VBZ ._.
Let_VB fx_FW i_FW g_NN N_NN i_LS =_JJ 1_CD be_VB the_DT training_NN examples_NNS ,_, g_NN -LRB-_-LRB- x_NN
from_IN the_DT separating_VBG hyperplane_NN are_VBP presumably_RB more_RBR likely_JJ to_TO be_VB classified_VBN correctly_RB ._.
Although_IN the_DT range_NN of_IN SVM_NN scores_NNS is_VBZ -LRB-_-LRB- \_FW Gammaa_FW ;_: a_LS -RRB-_-RRB- -LRB-_-LRB- where_WRB a_DT depends_VBZ on_IN the_DT problem_NN -RRB-_-RRB- ,_, we_PRP can_MD map_VB the_DT scores_NNS into_IN the_DT =_JJ -_: =[_NN 0_CD ;_: 1_LS -RRB-_-RRB- -_: =_JJ -_: interval_JJ by_IN re-scaling_VBG them_PRP ._.
If_IN f_LS -LRB-_-LRB- x_NN -RRB-_-RRB- is_VBZ the_DT original_JJ score_NN ,_, then_RB s_NN -LRB-_-LRB- x_NN -RRB-_-RRB- =_JJ -LRB-_-LRB- f_LS -LRB-_-LRB- x_NN -RRB-_-RRB- +_CC a_DT -RRB-_-RRB- =_JJ 2a_NN is_VBZ a_DT re-scaled_JJ score_NN between_IN 0_CD and_CC 1_CD ,_, such_JJ that_IN if_IN f_FW -LRB-_-LRB- x_NN -RRB-_-RRB- ?_.
0_CD then_RB s_NN -LRB-_-LRB- x_NN -RRB-_-RRB- ?_.
0:5_CD and_CC if_IN f_FW -LRB-_-LRB- x_NN -RRB-_-RRB- !_.
0_CD then_RB s_NN -LRB-_-LRB- x_NN -RRB-_-RRB- !_.
0:5_CD ._.
in-specific_JJ information_NN to_TO calculate_VB profit_NN we_PRP can_MD use_VB MSE_NNP to_TO evaluate_VB our_PRP$ methods_NNS ._.
5.1_CD Two-class_JJ problems_NNS The_DT first_JJ dataset_NN we_PRP use_VBP is_VBZ the_DT KDD-98_NN dataset_NN ,_, which_WDT is_VBZ available_JJ in_IN the_DT UCI_NNP KDD_NNP repository_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT dataset_NN contains_VBZ information_NN about_IN persons_NNS who_WP have_VBP made_VBN donations_NNS to_TO a_DT certain_JJ charity_NN ._.
The_DT decision-making_JJ task_NN is_VBZ to_TO choose_VB which_WDT donors_NNS to_TO request_VB a_DT new_JJ donation_NN from_IN ._.
The_DT data_NN is_VBZ divided_VBN in_IN
varying_VBG from_IN 5_CD to_TO 50_CD and_CC found_VBD that_IN PAV_NN does_VBZ slightly_RB worse_JJR than_IN binning_VBG with_IN the_DT optimal_JJ number_NN of_IN bins_NNS ._.
We_PRP also_RB applied_VBD each_DT method_NN to_TO the_DT Adult_NN dataset_NN ,_, which_WDT is_VBZ available_JJ in_IN the_DT UCI_NNP ML_NNP Repository_NNP =_SYM -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT prediction_NN task_NN is_VBZ to_TO determine_VB whether_IN a_DT person_NN makes_VBZ over_IN $_$ 50K_CD a_DT year_NN ,_, given_VBN demographic_JJ information_NN about_IN the_DT person_NN ._.
This_DT dataset_NN is_VBZ also_RB divided_VBN in_IN a_DT standard_JJ way_NN into_IN a_DT training_NN set_NN -LRB-_-LRB- 32561_CD
cores_NNS into_IN probability_NN estimates_NNS using_VBG a_DT sigmoid_JJ function_NN ._.
rate_NN class_NN membership_NN probability_NN estimates_NNS ,_, while_IN being_VBG faster_RBR ._.
The_DT same_JJ method_NN can_MD be_VB applied_VBN to_TO naive_JJ Bayes_NNS ._.
This_DT was_VBD proposed_VBN by_IN Bennett_NNP =_SYM -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: for_IN the_DT Reuters_NNP dataset_NN ._.
In_IN Figure_NNP 4_CD we_PRP show_VBP the_DT sigmoidal_JJ fit_NN to_TO the_DT naive_JJ Bayes_NNP scores_NNS for_IN the_DT Adult_NN and_CC TIC_NN datasets_NNS ._.
The_DT sigmoidal_JJ shape_NN does_VBZ not_RB appear_VB to_TO fit_VB naive_JJ Bayes_NNP scores_NNS as_RB well_RB as_IN it_PRP fi_FW
