Clustering_VBG based_VBN large_JJ margin_NN classification_NN :_: a_DT scalable_JJ approach_NN using_VBG SOCP_NN formulation_NN
This_DT paper_NN presents_VBZ a_DT novel_JJ Second_NNP Order_NNP Cone_NNP Programming_NNP -LRB-_-LRB- SOCP_NNP -RRB-_-RRB- formulation_NN for_IN large_JJ scale_NN binary_JJ classification_NN tasks_NNS ._.
Assuming_VBG that_IN the_DT class_NN conditional_JJ densities_NNS are_VBP mixture_NN distributions_NNS ,_, where_WRB each_DT component_NN of_IN the_DT mixture_NN has_VBZ a_DT spherical_JJ covariance_NN ,_, the_DT second_JJ order_NN statistics_NNS of_IN the_DT components_NNS can_MD be_VB estimated_VBN efficiently_RB using_VBG clustering_NN algorithms_NNS like_IN BIRCH_NNP ._.
For_IN each_DT cluster_NN ,_, the_DT second_JJ order_NN moments_NNS are_VBP used_VBN to_TO derive_VB a_DT second_JJ order_NN cone_NN constraint_NN via_IN a_DT Chebyshev-Cantelli_JJ inequality_NN ._.
This_DT constraint_NN ensures_VBZ that_IN any_DT data_NNS point_NN in_IN the_DT cluster_NN is_VBZ classified_VBN correctly_RB with_IN a_DT high_JJ probability_NN ._.
This_DT leads_VBZ to_TO a_DT large_JJ margin_NN SOCP_NN formulation_NN whose_WP$ size_NN depends_VBZ on_IN the_DT number_NN of_IN clusters_NNS rather_RB than_IN the_DT number_NN of_IN training_NN data_NNS points_NNS ._.
Hence_RB ,_, the_DT proposed_VBN formulation_NN scales_NNS well_RB for_IN large_JJ datasets_NNS when_WRB compared_VBN to_TO the_DT state-of-the-art_JJ classifiers_NNS ,_, Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- ._.
Experiments_NNS on_IN real_JJ world_NN and_CC synthetic_JJ datasets_NNS show_VBP that_IN the_DT proposed_VBN algorithm_NN outperforms_VBZ SVM_NN solvers_NNS in_IN terms_NNS of_IN training_NN time_NN and_CC achieves_VBZ similar_JJ accuracies_NNS ._.
-LRB-_-LRB- μ_NN ,_, κσ_NN -RRB-_-RRB- w_NN ⊤_CD x_CC −_CD b_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- Geometrically_NNP ,_, the_DT constraints_NNS in_IN -LRB-_-LRB- 11_CD -RRB-_-RRB- say_VBP that_IN all_DT points_NNS that_WDT belong_VBP to_TO B_NN -LRB-_-LRB- μ_NN ,_, κσ_NN -RRB-_-RRB- lie_NN on_IN the_DT positive_JJ half_JJ space_NN of_IN the_DT hyperplane_NN w_NN ⊤_CD x_CC −_CD b_NN =_JJ 1_CD −_FW ξ_FW ._.
This_DT geometric_JJ picture_NN -LRB-_-LRB- also_RB see_VBP =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =--RRB-_NN immediately_RB shows_VBZ that_IN all_PDT the_DT constraints_NNS -LRB-_-LRB- 11_CD -RRB-_-RRB- can_MD be_VB satisfied_VBN just_RB by_IN ensuring_VBG that_IN the_DT point_NN in_IN B_NN -LRB-_-LRB- μ_NN ,_, κσ_NN -RRB-_-RRB- whichis_VBZ nearest_JJS to_TO the_DT hyperplane_NN w_NN ⊤_CD x_CC −_CD b_NN =_JJ 1_CD −_FW ξ_FW lies_VBZ on_IN the_DT positive_JJ 676Research_NN Track_NNP Po_NNP
n_NN and_CC Σ_NN ∈_NN R_NN n_NN ×_CD n_NN ._.
Let_VB H_NN -LRB-_-LRB- w_NN ,_, b_NN -RRB-_-RRB- =_JJ -LCB-_-LRB- z_SYM |_FW w_FW ⊤_FW z_SYM -LRB-_-LRB- b_NN ,_, w_NN ,_, z_SYM ∈_CD R_NN n_NN ,_, b_NN ∈_NN R_NN -RCB-_-RRB- be_VB a_DT given_VBN half_NN space_NN ,_, with_IN w_NN ̸_NN =_JJ 0_CD ._.
Then_RB Prob_NNP -LRB-_-LRB- X_NNP ∈_NNP H_NNP -RRB-_-RRB- ≥_CD s_NN 2_CD s_NN 2_CD +_CC w_FW ⊤_FW Σw_NN where_WRB s_NN =_JJ -LRB-_-LRB- b_NN −_NN w_NN ⊤_CD μ_NN -RRB-_-RRB- +_CC ,_, -LRB-_-LRB- x_NN -RRB-_-RRB- +_CC =_JJ max_NN -LRB-_-LRB- x_NN ,_, 0_CD -RRB-_-RRB- ._.
Applying_VBG theorem_NN 1_CD -LRB-_-LRB- see_VB also_RB =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, the_DT constraints_NNS for_IN positive_JJ class_NN can_MD be_VB handled_VBN by_IN setting_VBG P_NN -LRB-_-LRB- w_FW ⊤_FW Xj_FW −_FW b_NN ≥_NN 1_CD −_NN ξj_NN -RRB-_-RRB- :_: ≥_NN -LRB-_-LRB- w_NN ⊤_NN μj_FW −_FW b_NN −_NN 1_CD +_CC ξj_NN -RRB-_-RRB- 2_CD +_CC -LRB-_-LRB- w_FW ⊤_FW μj_FW −_FW b_NN −_NN 1_CD +_CC ξj_NN -RRB-_-RRB- 2_CD +_CC +_CC w_NN ⊤_NN σ_NN 2_CD j_NN which_WDT results_VBZ in_IN the_DT constraint_NN Iw_FW ≥_FW η_FW w_FW ⊤_FW μj_FW −_FW b_NN
k_NN are_VBP the_DT mj_JJ data_NN points_NNS that_WDT belong_VBP to_TO j_VB th_DT cluster_NN ._.
As_IN the_DT formulation_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- involves_VBZ only_RB the_DT dot_NN products_NNS of_IN the_DT data_NNS points_NNS ,_, it_PRP can_MD be_VB extended_VBN to_TO arbitrary_JJ feature_NN spaces_NNS by_IN using_VBG Mercer_NNP kernels_NNS =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Assuming_VBG that_IN the_DT given_VBN dataset_NN is_VBZ linearly_RB separable_JJ ,_, one_PRP can_MD write_VB an_DT equivalent_NN of_IN the_DT hard-margin_JJ classifier_NN for_IN the_DT 2_CD The_DT same_JJ constraint_NN can_MD be_VB derived_VBN more_RBR rigorously_RB using_VBG optimization_NN theor_NN
o_NN solve_VB the_DT optimization_NN problem_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, the_DT constraints_NNS need_VBP to_TO be_VB written_VBN as_IN deterministic_JJ constraints_NNS ._.
To_TO this_DT end_NN ,_, consider_VB the_DT following_JJ multivariate_JJ generalization_NN of_IN Chebyshev-Cantelli_JJ inequality_NN =_JJ -_: =[_NN 3_CD ,_, 10_CD ,_, 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Theorem_NN 1_CD ._.
Let_VB X_NN be_VB an_DT n_NN dimensional_JJ random_JJ vector_NN ._.
The_DT mean_NN and_CC covariance_NN of_IN X_NN be_VB μ_FW ∈_FW R_NN n_NN and_CC Σ_NN ∈_NN R_NN n_NN ×_CD n_NN ._.
Let_VB H_NN -LRB-_-LRB- w_NN ,_, b_NN -RRB-_-RRB- =_JJ -LCB-_-LRB- z_SYM |_FW w_FW ⊤_FW z_SYM -LRB-_-LRB- b_NN ,_, w_NN ,_, z_SYM ∈_CD R_NN n_NN ,_, b_NN ∈_NN R_NN -RCB-_-RRB- be_VB a_DT given_VBN half_NN space_NN ,_, with_IN w_NN ̸_NN =_JJ 0_CD ._.
Then_RB Prob_NNP -LRB-_-LRB- X_NNP ∈_NNP H_NNP
size_NN of_IN the_DT optimization_NN problem_NN does_VBZ not_RB increase_VB with_IN the_DT number_NN of_IN data_NNS points_NNS ._.
Estimation_NN of_IN the_DT moments_NNS of_IN the_DT component_NN distributions_NNS is_VBZ done_VBN using_VBG an_DT efficient_JJ clustering_NN scheme_NN ,_, such_JJ as_IN BIRCH_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
BIRCH_NNP ,_, in_IN a_DT single_JJ pass_NN over_IN the_DT data_NNS constructs_NNS a_DT CF-tree_NN -LRB-_-LRB- Cluster_NN Feature_NN tree_NN -RRB-_-RRB- ,_, given_VBN a_DT limited_JJ amount_NN of_IN resources_NNS ._.
CF-tree_NN consists_VBZ of_IN the_DT sufficient_JJ statistics_NNS for_IN the_DT hierarchy_NN of_IN clusters_NNS in_IN
e_LS datasets_NNS is_VBZ a_DT challenging_JJ task_NN ,_, as_IN they_PRP may_MD not_RB fit_VB into_IN memory_NN ._.
Most_JJS of_IN the_DT existing_VBG classification_NN algorithms_NNS are_VBP not_RB attractive_JJ as_IN they_PRP perform_VBP multiple_JJ passes_NNS over_IN data_NNS ._.
Support_NN Vector_NNP Machines_NNP =_SYM -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- are_VBP one_CD of_IN the_DT most_RBS successful_JJ classifiers_NNS that_WDT achieve_VBP good_JJ generalization_NN in_IN practice_NN ._.
SVMs_NNS -LRB-_-LRB- soft-margin_JJ SVMs_NNS -RRB-_-RRB- pose_VBP the_DT classification_NN problem_NN as_IN a_DT convex_NN quadratic_JJ optimization_NN problem_NN of_IN s_NN
of_IN the_DT availability_NN of_IN efficient_JJ algorithms_NNS like_IN SMO_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC chunking_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, which_WDT solve_VBP the_DT dual_JJ of_IN the_DT SVM_NNP formulation_NN ._.
However_RB ,_, these_DT algorithms_NNS are_VBP known_VBN to_TO be_VB atleast_JJ O_NN -LRB-_-LRB- m_NN 2_CD -RRB-_-RRB- in_IN running_VBG time_NN -LRB-_-LRB- see_VB =_JJ -_: =[_NN 13_CD ,_, 16_CD -RRB-_-RRB- -_: =--RRB-_NN and_CC hence_RB not_RB scalable_JJ to_TO large_JJ datasets_NNS ._.
Clustering_NN before_IN computing_VBG the_DT classifier_NN is_VBZ an_DT interesting_JJ strategy_NN for_IN large_JJ scale_NN problems_NNS ._.
CB-SVM_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- is_VBZ an_DT iterative_JJ ,_, hierarchical_JJ clustering_NN based_VBN S_NN
tive_JJ and_CC positive_JJ spheres_NNS -LRB-_-LRB- B_NN -LRB-_-LRB- μj_NN ,_, κσj_NN -RRB-_-RRB- -RRB-_-RRB- 3_CD ._.
This_DT is_VBZ analogous_JJ to_TO the_DT case_NN of_IN SVMs_NNS ,_, where_WRB dual_JJ is_VBZ the_DT problem_NN of_IN finding_VBG distance_NN between_IN the_DT convex_NN hulls_NNS formed_VBN by_IN the_DT negative_JJ and_CC positive_JJ data_NN points_NNS =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
3_LS ._.
EXPERIMENTAL_JJ RESULTS_NNS In_IN this_DT section_NN ,_, we_PRP present_VBP experimental_JJ results_NNS on_IN synthetic_JJ and_CC real_JJ world_NN data_NNS sets_NNS ._.
The_DT results_NNS show_VBP that_IN the_DT accuracies_NNS achieved_VBN by_IN SVM_NN and_CC the_DT proposed_VBN classifier_NN are_VBP c_NN
o_NN solve_VB the_DT optimization_NN problem_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- in_IN all_DT experiments_NNS ._.
The_DT performance_NN of_IN the_DT proposed_VBN Clustering_VBG Based_VBN Classifier_NN -LRB-_-LRB- CB-SOCP_NN -RRB-_-RRB- was_VBD compared_VBN to_TO that_DT of_IN SVM_NN -LRB-_-LRB- using_VBG linear_JJ kernel_NN -RRB-_-RRB- implemented_VBN by_IN LIBSVM_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- denoted_VBN by_IN SVM_NN -RRB-_-RRB- 4_CD ._.
All_DT experiments_NNS were_VBD carried_VBN on_IN Pentium_NNP 4_CD 2.4_CD GHz_NN machines_NNS with_IN 1GB_NN memory_NN ._.
A_DT `_`` ×_NN '_'' inthe_NN tables_NNS 1,2_CD and_CC 3_CD represents_VBZ the_DT failure_NN of_IN the_DT corresponding_JJ classifier_NN to_TO complete_VB trainin_NN
o_NN solve_VB the_DT optimization_NN problem_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, the_DT constraints_NNS need_VBP to_TO be_VB written_VBN as_IN deterministic_JJ constraints_NNS ._.
To_TO this_DT end_NN ,_, consider_VB the_DT following_JJ multivariate_JJ generalization_NN of_IN Chebyshev-Cantelli_JJ inequality_NN =_JJ -_: =[_NN 3_CD ,_, 10_CD ,_, 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Theorem_NN 1_CD ._.
Let_VB X_NN be_VB an_DT n_NN dimensional_JJ random_JJ vector_NN ._.
The_DT mean_NN and_CC covariance_NN of_IN X_NN be_VB μ_FW ∈_FW R_NN n_NN and_CC Σ_NN ∈_NN R_NN n_NN ×_CD n_NN ._.
Let_VB H_NN -LRB-_-LRB- w_NN ,_, b_NN -RRB-_-RRB- =_JJ -LCB-_-LRB- z_SYM |_FW w_FW ⊤_FW z_SYM -LRB-_-LRB- b_NN ,_, w_NN ,_, z_SYM ∈_CD R_NN n_NN ,_, b_NN ∈_NN R_NN -RCB-_-RRB- be_VB a_DT given_VBN half_NN space_NN ,_, with_IN w_NN ̸_NN =_JJ 0_CD ._.
Then_RB Prob_NNP -LRB-_-LRB- X_NNP ∈_NNP H_NNP
e_LS size_NN is_VBZ not_RB dependent_JJ on_IN the_DT training_NN set_NN ._.
SVMs_NNS have_VBP emerged_VBN as_IN useful_JJ tools_NNS for_IN classification_NN in_IN practice_NN ,_, primarily_RB because_IN of_IN the_DT availability_NN of_IN efficient_JJ algorithms_NNS like_IN SMO_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC chunking_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT solve_VBP the_DT dual_JJ of_IN the_DT SVM_NNP formulation_NN ._.
However_RB ,_, these_DT algorithms_NNS are_VBP known_VBN to_TO be_VB atleast_JJ O_NN -LRB-_-LRB- m_NN 2_CD -RRB-_-RRB- in_IN running_VBG time_NN -LRB-_-LRB- see_VB -LRB-_-LRB- 13_CD ,_, 16_CD -RRB-_-RRB- -RRB-_-RRB- and_CC hence_RB not_RB scalable_JJ to_TO large_JJ datasets_NNS ._.
Clustering_NN before_IN comp_NN
optimization_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
As_IN a_DT special_JJ case_NN of_IN convex_NN non-linear_JJ optimization_NN ,_, SOCPs_NNS have_VBP gained_VBN much_JJ attention_NN in_IN recent_JJ times_NNS ._.
For_IN a_DT discussion_NN of_IN further_JJ efficient_JJ algorithms_NNS and_CC applications_NNS of_IN SOCP_NN see_VBP =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT problem_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- can_MD be_VB equivalently_RB written_VBN as_IN the_DT following_JJ convex_NN quadratic_JJ programming_NN problem_NN :_: min_NN w_NN ,_, b_NN ,_, ξ_NN j_NN 1_CD 2_CD ‖_CD w_NN ‖_NN 22_CD +_CC C_NN Pm_NN j_NN =_JJ 1_CD ξj_FW s.t._FW yj_FW -LRB-_-LRB- w_FW ⊤_FW xj_FW −_FW b_LS -RRB-_-RRB- −_NN 1_CD +_CC ξj_FW ≥_FW 0_CD ,_, ξj_FW ≥_FW 0_CD ,_, ∀_CD j_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- -LRB-_-LRB- 2_CD -RRB-_-RRB- is_VBZ the_DT famou_NN
o_NN solve_VB the_DT optimization_NN problem_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, the_DT constraints_NNS need_VBP to_TO be_VB written_VBN as_IN deterministic_JJ constraints_NNS ._.
To_TO this_DT end_NN ,_, consider_VB the_DT following_JJ multivariate_JJ generalization_NN of_IN Chebyshev-Cantelli_JJ inequality_NN =_JJ -_: =[_NN 3_CD ,_, 10_CD ,_, 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Theorem_NN 1_CD ._.
Let_VB X_NN be_VB an_DT n_NN dimensional_JJ random_JJ vector_NN ._.
The_DT mean_NN and_CC covariance_NN of_IN X_NN be_VB μ_FW ∈_FW R_NN n_NN and_CC Σ_NN ∈_NN R_NN n_NN ×_CD n_NN ._.
Let_VB H_NN -LRB-_-LRB- w_NN ,_, b_NN -RRB-_-RRB- =_JJ -LCB-_-LRB- z_SYM |_FW w_FW ⊤_FW z_SYM -LRB-_-LRB- b_NN ,_, w_NN ,_, z_SYM ∈_CD R_NN n_NN ,_, b_NN ∈_NN R_NN -RCB-_-RRB- be_VB a_DT given_VBN half_NN space_NN ,_, with_IN w_NN ̸_NN =_JJ 0_CD ._.
Then_RB Prob_NNP -LRB-_-LRB- X_NNP ∈_NNP H_NNP
well_RB for_IN large_JJ datasets_NNS ._.
In_IN all_DT cases_NNS ,_, BIRCH_NNP was_VBD used_VBN to_TO cluster_VB the_DT positive_JJ and_CC negative_JJ training_NN data_NNS points_NNS ._.
The_DT original_JJ BIRCH_NNP implementation_NN by_IN Zhang_NNP et_NNP ._.
al._FW -LRB-_-LRB- 17_CD -RRB-_-RRB- was_VBD used_VBN for_IN clustering_NN ._.
SeDuMi_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT publicly_RB available_JJ SOCP_NN solver_NN was_VBD used_VBN to_TO solve_VB the_DT optimization_NN problem_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- in_IN all_DT experiments_NNS ._.
The_DT performance_NN of_IN the_DT proposed_VBN Clustering_VBG Based_VBN Classifier_NN -LRB-_-LRB- CB-SOCP_NN -RRB-_-RRB- was_VBD compared_VBN to_TO that_DT of_IN SVM_NN
tion_NN problem_NN ,_, whose_WP$ size_NN is_VBZ not_RB dependent_JJ on_IN the_DT training_NN set_NN ._.
SVMs_NNS have_VBP emerged_VBN as_IN useful_JJ tools_NNS for_IN classification_NN in_IN practice_NN ,_, primarily_RB because_IN of_IN the_DT availability_NN of_IN efficient_JJ algorithms_NNS like_IN SMO_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: and_CC chunking_VBG -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, which_WDT solve_VBP the_DT dual_JJ of_IN the_DT SVM_NNP formulation_NN ._.
However_RB ,_, these_DT algorithms_NNS are_VBP known_VBN to_TO be_VB atleast_JJ O_NN -LRB-_-LRB- m_NN 2_CD -RRB-_-RRB- in_IN running_VBG time_NN -LRB-_-LRB- see_VB -LRB-_-LRB- 13_CD ,_, 16_CD -RRB-_-RRB- -RRB-_-RRB- and_CC hence_RB not_RB scalable_JJ to_TO large_JJ datasets_NNS ._.
Clust_NN
._.
Now_RB ,_, ‖_FW w_FW ‖_NN 2_CD -LRB-_-LRB- 11_CD -RRB-_-RRB- is_VBZ satisfied_VBN if_IN w_NN ⊤_CD x_CC ∗_CD −_CD b_NN ≥_NN 1_CD −_FW ξ_FW ._.
Thissaysthat_NN 2_CD ,_, w_FW ⊤_FW μ_FW −_FW b_NN ≥_NN 1_CD −_FW ξ_FW +_CC κσ_FW ‖_FW w_FW ‖_FW 2_CD -LRB-_-LRB- 13_CD -RRB-_-RRB- Note_VBP that_IN this_DT equation_NN is_VBZ of_IN the_DT same_JJ form_NN as_IN -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
Hence_RB ,_, geometrical_JJ interpretation_NN -LRB-_-LRB- see_VB also_RB =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =--RRB-_NN of_IN the_DT constraints_NNS of_IN -LRB-_-LRB- 10_CD -RRB-_-RRB- is_VBZ to_TO restrict_VB the_DT discriminating_VBG hyperplane_NN to_TO lie_VB such_JJ that_IN most_JJS of_IN the_DT spheres_NNS B_NN -LRB-_-LRB- μj_NN ,_, κσj_NN -RRB-_-RRB- are_VBP classified_VBN correctly_RB ._.
Figure_NN 1_CD shows_VBZ this_DT geometric_JJ picture_NN ._.
Note_VB that_IN in_IN t_NN
the_DT generalization_NN of_IN the_DT classifier_NN ._.
The_DT resulting_VBG optimization_NN turns_VBZ out_RP to_TO be_VB a_DT Second_NNP Order_NNP Cone_NNP Programming_NNP -LRB-_-LRB- SOCP_NNP -RRB-_-RRB- problem_NN ,_, which_WDT can_MD be_VB efficiently_RB solved_VBN using_VBG fast_JJ interior_NN points_NNS algorithms_VBZ =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT SOCP_NN problem_NN formulated_VBN as_RB above_RB has_VBZ k_NN linear_JJ inequalities_NNS and_CC one_CD SOC_NN constraint_NN ,_, where_WRB k_NN =_JJ k1_NN +_CC k2_NN ._.
k1_NN ,_, k2_NN are_VBP the_DT number_NN of_IN components_NNS in_IN the_DT mixture_NN model_NN of_IN the_DT i_FW th_DT class_NN ._.
Note_VB that_IN the_DT numb_JJ
