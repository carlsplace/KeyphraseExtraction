Fast_JJ coordinate_JJ descent_NN methods_NNS with_IN variable_JJ selection_NN for_IN non-negative_JJ matrix_NN factorization_NN
Nonnegative_NNP Matrix_NNP Factorization_NNP -LRB-_-LRB- NMF_NNP -RRB-_-RRB- is_VBZ an_DT effective_JJ dimension_NN reduction_NN method_NN for_IN non-negative_JJ dyadic_JJ data_NNS ,_, and_CC has_VBZ proven_VBN to_TO be_VB useful_JJ in_IN many_JJ areas_NNS ,_, such_JJ as_IN text_NN mining_NN ,_, bioinformatics_NNS and_CC image_NN processing_NN ._.
NMF_NN is_VBZ usually_RB formulated_VBN as_IN a_DT constrained_VBN non-convex_JJ optimization_NN problem_NN ,_, and_CC many_JJ algorithms_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG it_PRP ._.
Recently_RB ,_, a_DT coordinate_JJ descent_NN method_NN ,_, called_VBN FastHals_NNP ,_, has_VBZ been_VBN proposed_VBN to_TO solve_VB least_JJS squares_NNS NMF_NN and_CC is_VBZ regarded_VBN as_IN one_CD of_IN the_DT state-of-the-art_JJ techniques_NNS for_IN the_DT problem_NN ._.
In_IN this_DT paper_NN ,_, we_PRP first_RB show_VBP that_IN FastHals_NNP has_VBZ an_DT inefficiency_NN in_IN that_IN it_PRP uses_VBZ a_DT cyclic_JJ coordinate_JJ descent_NN scheme_NN and_CC thus_RB ,_, performs_VBZ unneeded_JJ descent_NN steps_NNS on_IN unimportant_JJ variables_NNS ._.
We_PRP then_RB present_VBP a_DT variable_JJ selection_NN scheme_NN that_WDT uses_VBZ the_DT gradient_NN of_IN the_DT objective_JJ function_NN to_TO arrive_VB at_IN a_DT new_JJ coordinate_JJ descent_NN method_NN ._.
Our_PRP$ new_JJ method_NN is_VBZ considerably_RB faster_RBR in_IN practice_NN and_CC we_PRP show_VBP that_IN it_PRP has_VBZ theoretical_JJ convergence_NN guarantees_NNS ._.
Moreover_RB when_WRB the_DT solution_NN is_VBZ sparse_JJ ,_, as_IN is_VBZ often_RB the_DT case_NN in_IN real_JJ applications_NNS ,_, our_PRP$ new_JJ method_NN benefits_NNS by_IN selecting_VBG important_JJ variables_NNS to_TO update_VB more_RBR often_RB ,_, thus_RB resulting_VBG in_IN higher_JJR speed_NN ._.
As_IN an_DT example_NN ,_, on_IN a_DT text_NN dataset_NN RCV1_NN ,_, our_PRP$ method_NN is_VBZ 7_CD times_NNS faster_RBR than_IN FastHals_NNP ,_, and_CC more_JJR than_IN 15_CD times_NNS faster_RBR when_WRB the_DT sparsity_NN is_VBZ increased_VBN by_IN adding_VBG an_DT L1_NN penalty_NN ._.
We_PRP also_RB develop_VBP new_JJ coordinate_JJ descent_NN methods_NNS when_WRB error_NN in_IN NMF_NN is_VBZ measured_VBN by_IN KL-divergence_NN by_IN applying_VBG the_DT Newton_NNP method_NN to_TO solve_VB the_DT one-variable_JJ sub-problems_NNS ._.
Experiments_NNS indicate_VBP that_IN our_PRP$ algorithm_NN for_IN minimizing_VBG the_DT KL-divergence_NN is_VBZ faster_RBR than_IN the_DT Lee_NNP &_CC Seung_NNP multiplicative_JJ rule_NN by_IN a_DT factor_NN of_IN 10_CD on_IN the_DT CBCL_NN image_NN dataset_NN ._.
framework_NN suggested_VBN by_IN -LRB-_-LRB- 20_CD -RRB-_-RRB- ._.
This_DT framework_NN has_VBZ been_VBN proved_VBN to_TO converge_VB to_TO stationary_JJ points_NNS by_IN -LRB-_-LRB- 6_CD -RRB-_-RRB- as_RB long_RB as_IN one_PRP can_MD solve_VB each_DT nonnegative_JJ least_JJS square_JJ sub-problem_NN exactly_RB ._.
Among_IN existing_VBG methods_NNS ,_, =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: proposes_VBZ a_DT projected_JJ gradient_NN method_NN ,_, -LRB-_-LRB- 11_CD -RRB-_-RRB- uses_VBZ an_DT active_JJ set_NN method_NN ,_, -LRB-_-LRB- 10_CD -RRB-_-RRB- applies_VBZ a_DT projected_JJ Newton_NNP method_NN ,_, and_CC -LRB-_-LRB- 12_CD -RRB-_-RRB- suggests_VBZ a_DT modified_VBN active_JJ set_NN method_NN called_VBN BlockPivot_NNP for_IN solving_VBG the_DT non-nega_JJ
t_NN algorithm_NN that_WDT has_VBZ the_DT ability_NN to_TO select_VB variables_NNS according_VBG to_TO their_PRP$ ``_`` importance_NN ''_'' ,_, thus_RB leading_VBG to_TO significant_JJ speedups_NNS ._.
Similar_JJ strategies_NNS has_VBZ been_VBN employed_VBN in_IN Lasso_NN or_CC feature_NN selection_NN problems_NNS =_JJ -_: =[_NN 17_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
To_TO illustrate_VB the_DT idea_NN ,_, we_PRP take_VBP a_DT text_NN dataset_NN ''_'' ._.
Objective_NNP Value_NNP 7.8_CD 7.6_CD 7.4_CD 7.2_CD 7_CD 6.8_CD 6.6_CD 6.4_CD x_CC 104_CD CD_NNP FastHals_NNP 6.2_CD 0_CD 2_CD 4_CD 6_CD 8_CD 10_CD Number_NN of_IN Coordinate_JJ Updates_NNS x_CC 10_CD 7_CD number_NN of_IN updates_NNS 5_CD 4_CD 3_CD 2_CD 1_CD 0_CD
processing_NN ._.
NMF_NN is_VBZ usually_RB formulated_VBN as_IN a_DT constrained_VBN non-convex_JJ optimization_NN problem_NN ,_, and_CC many_JJ algorithms_NNS have_VBP been_VBN developed_VBN for_IN solving_VBG it_PRP ._.
Recently_RB ,_, a_DT coordinate_JJ descent_NN method_NN ,_, called_VBN FastHals_NNP =_SYM -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, has_VBZ been_VBN proposed_VBN to_TO solve_VB least_JJS squares_NNS NMF_NN and_CC is_VBZ regarded_VBN as_IN one_CD of_IN the_DT state-of-the-art_JJ techniques_NNS for_IN the_DT problem_NN ._.
In_IN this_DT paper_NN ,_, we_PRP first_RB show_VBP that_IN FastHals_NNP has_VBZ an_DT inefficiency_NN in_IN that_IN it_PRP uses_VBZ
ve_NN methods_NNS ._.
To_TO test_VB their_PRP$ scalability_NN ,_, we_PRP further_RB compare_VBP their_PRP$ performances_NNS on_IN large_JJ sparse_JJ datasets_NNS ._.
We_PRP use_VBP the_DT following_JJ sparse_JJ datasets_NNS :_: 1_CD ._.
Yahoo-News_NNS -LRB-_-LRB- K-Series_NNS -RRB-_-RRB- :_: A_DT news_NN articles_NNS dataset_VBP ._.
2_CD ._.
RCV1_NN =_SYM -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: :_: An_DT archive_NN of_IN newswire_NN stories_NNS from_IN Reuters_NNP Ltd._NNP ._.
The_DT original_JJ dataset_NN has_VBZ 781,265_CD documents_NNS and_CC 47,236_CD features_NNS ._.
Following_VBG the_DT preprocessing_NN in_IN -LRB-_-LRB- 18_CD -RRB-_-RRB- ,_, we_PRP choose_VBP data_NNS from_IN 15_CD random_JJ categories_NNS and_CC eli_NNS
s_VBZ an_DT active_JJ set_NN method_NN ,_, -LRB-_-LRB- 10_CD -RRB-_-RRB- applies_VBZ a_DT projected_JJ Newton_NNP method_NN ,_, and_CC -LRB-_-LRB- 12_CD -RRB-_-RRB- suggests_VBZ a_DT modified_VBN active_JJ set_NN method_NN called_VBN BlockPivot_NNP for_IN solving_VBG the_DT non-negative_JJ least_JJS squares_NNS sub-problem_JJ ._.
Very_RB recently_RB ,_, =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: developed_VBD a_DT parallel_JJ NMF_NN solver_NN for_IN largescale_JJ web_NN data_NNS ._.
This_DT shows_VBZ a_DT practical_JJ need_NN for_IN further_RB scaling_VBG up_RP NMF_NN solvers_NNS ,_, especially_RB when_WRB the_DT matrix_NN V_NN is_VBZ sparse_JJ ,_, as_IN is_VBZ the_DT case_NN for_IN text_NN data_NNS or_CC web_NN re_NN
11\/08_CD ..._: $_$ 10.00_CD ._.
Inderjit_NNP S._NNP Dhillon_NNP Dept._NNP of_IN Computer_NNP Science_NNP University_NNP of_IN Texas_NNP at_IN Austin_NNP Austin_NNP ,_, TX_NNP 78712-1188_CD ,_, USA_NN inderjit@cs.utexas.edu_NN 1_CD ._.
INTRODUCTION_NN Non-negative_JJ matrix_NN factorization_NN -LRB-_-LRB- NMF_NN -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =[_NN 20_CD ,_, 14_CD -RRB-_-RRB- -_: =--RRB-_NN is_VBZ a_DT popular_JJ matrix_NN decomposition_NN method_NN for_IN finding_VBG non-negative_JJ representations_NNS of_IN data_NNS ._.
NMF_NN has_VBZ proved_VBN useful_JJ in_IN dimension_NN reduction_NN of_IN image_NN ,_, text_NN ,_, and_CC signal_NN data_NNS ._.
Given_VBN a_DT matrix_NN V_NN ∈_NN R_NN m_NN ×_CD n_NN ,_, V_NN
and_CC 47,236_CD features_NNS ._.
Following_VBG the_DT preprocessing_NN in_IN -LRB-_-LRB- 18_CD -RRB-_-RRB- ,_, we_PRP choose_VBP data_NNS from_IN 15_CD random_JJ categories_NNS and_CC eliminate_VB unused_JJ features_NNS ._.
However_RB ,_, our_PRP$ data_NNS is_VBZ much_RB larger_JJR than_IN the_DT one_CD used_VBN in_IN -LRB-_-LRB- 18_CD -RRB-_-RRB- ._.
3_LS ._.
MNIST_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: :_: A_DT collection_NN of_IN hand-written_JJ digits_NNS ._.
The_DT statistics_NNS of_IN the_DT datasets_NNS are_VBP summarized_VBN in_IN Table_NNP 4_CD ._.
We_PRP set_VBD k_NN according_VBG to_TO the_DT number_NN of_IN categories_NNS for_IN each_DT datasets_NNS ._.
We_PRP run_VBP GCD_NNP and_CC FastHals_NNP with_IN our_PRP$ C_NN im_NN
the_DT most_RBS commonly_RB used_VBN is_VBZ the_DT Frobenius_NNP norm_NN ,_, which_WDT leads_VBZ to_TO the_DT following_JJ minimization_NN problem_NN :_: 1_CD min_NN f_SYM -LRB-_-LRB- W_NN ,_, H_NN -RRB-_-RRB- ≡_NN W_NN ,_, H_NNP ≥_NNP 0_CD 2_CD ‖_NN V_NNP −_NNP WH_NNP ‖_NNP 2_CD F_NN =_JJ 1_CD 2_CD X_NN -LRB-_-LRB- Vij_NN −_NN -LRB-_-LRB- WH_NN -RRB-_-RRB- ij_NN -RRB-_-RRB- 2_CD -LRB-_-LRB- 1_LS -RRB-_-RRB- To_TO achieve_VB better_JJR sparsity_NN ,_, researchers_NNS -LRB-_-LRB- =_JJ -_: =[_NN 7_CD ,_, 22_CD -RRB-_-RRB- -_: =--RRB-_NN have_VBP proposed_VBN adding_VBG regularization_NN terms_NNS ,_, on_IN W_NN and_CC H_NN ,_, to_TO -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
For_IN example_NN ,_, an_DT L1-norm_JJ penalty_NN on_IN W_NN and_CC H_NN can_MD achieve_VB a_DT more_RBR sparse_JJ solution_NN :_: min_NN W_NN ,_, H_NN ≥_NN 0_CD 1_CD 2_CD ‖_NN V_NNP −_NNP WH_NNP ‖_NNP 2_CD F_NN +_CC ρ1_NN X_NN i_FW ,_, r_NN i_LS ,_, j_NN Wir_NN +_CC ρ2_NN X_NN Hrj_NN ._.
11\/08_CD ..._: $_$ 10.00_CD ._.
Inderjit_NNP S._NNP Dhillon_NNP Dept._NNP of_IN Computer_NNP Science_NNP University_NNP of_IN Texas_NNP at_IN Austin_NNP Austin_NNP ,_, TX_NNP 78712-1188_CD ,_, USA_NN inderjit@cs.utexas.edu_NN 1_CD ._.
INTRODUCTION_NN Non-negative_JJ matrix_NN factorization_NN -LRB-_-LRB- NMF_NN -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =[_NN 20_CD ,_, 14_CD -RRB-_-RRB- -_: =--RRB-_NN is_VBZ a_DT popular_JJ matrix_NN decomposition_NN method_NN for_IN finding_VBG non-negative_JJ representations_NNS of_IN data_NNS ._.
NMF_NN has_VBZ proved_VBN useful_JJ in_IN dimension_NN reduction_NN of_IN image_NN ,_, text_NN ,_, and_CC signal_NN data_NNS ._.
Given_VBN a_DT matrix_NN V_NN ∈_NN R_NN m_NN ×_CD n_NN ,_, V_NN
BCL_NN image_NN dataset_NN :_: http:\/\/cbcl.mit.edu\/cbcl\/_NN software-datasets\/FaceData2_NN ._.
html_NN 3_CD ._.
ORL_NN image_NN dataset_NN :_: http:\/\/www.cl.cam.ac.uk\/_FW research\/dtg\/attarchive_FW \/_: facedatabase_NN ._.
html_VB We_PRP follow_VBP the_DT same_JJ setting_NN as_IN in_IN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: for_IN CBCL_NN and_CC ORL_NN datasets_NNS ._.
The_DT size_NN of_IN the_DT datasets_NNS are_VBP summarized_VBN in_IN Table_NNP 2_CD ._.
To_TO ensure_VB a_DT fair_JJ comparison_NN ,_, all_DT experimental_JJ results_NNS in_IN this_DT paper_NN are_VBP the_DT average_NN of_IN 10_CD random_JJ initial_JJ points_NNS ._.
Table_NNP 2_CD
stationary_JJ point_NN of_IN -LRB-_-LRB- 3_CD -RRB-_-RRB- -LRB-_-LRB- or_CC -LRB-_-LRB- 1_LS -RRB-_-RRB- -RRB-_-RRB- ._.
With_IN the_DT condition_NN -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, the_DT one-variable_JJ sub-problems_NNS for_IN the_DT convergence_NN subsequence_NN are_VBP strictly_RB convex_VBN ._.
Then_RB the_DT proof_NN follows_VBZ the_DT proof_NN of_IN Proposition_NNP 3.3.9_CD in_IN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN KLNMF_NNP ,_, -LRB-_-LRB- 24_CD -RRB-_-RRB- is_VBZ violated_VBN only_RB when_WRB corresponding_JJ row\/columns_NNS of_IN V_NN are_VBP all_DT zero_CD -LRB-_-LRB- otherwise_RB the_DT objective_JJ value_NN will_MD be_VB infinity_NN -RRB-_-RRB- ._.
Therefore_RB ,_, zero_CD row\/columns_NNS of_IN V_NN can_MD be_VB removed_VBN by_IN preprocessing_VBG t_NN
stationary_JJ points_NNS by_IN -LRB-_-LRB- 6_CD -RRB-_-RRB- as_RB long_RB as_IN one_PRP can_MD solve_VB each_DT nonnegative_JJ least_JJS square_JJ sub-problem_NN exactly_RB ._.
Among_IN existing_VBG methods_NNS ,_, -LRB-_-LRB- 18_CD -RRB-_-RRB- proposes_VBZ a_DT projected_JJ gradient_NN method_NN ,_, -LRB-_-LRB- 11_CD -RRB-_-RRB- uses_VBZ an_DT active_JJ set_NN method_NN ,_, =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: applies_VBZ a_DT projected_JJ Newton_NNP method_NN ,_, and_CC -LRB-_-LRB- 12_CD -RRB-_-RRB- suggests_VBZ a_DT modified_VBN active_JJ set_NN method_NN called_VBN BlockPivot_NNP for_IN solving_VBG the_DT non-negative_JJ least_JJS squares_NNS sub-problem_JJ ._.
Very_RB recently_RB ,_, -LRB-_-LRB- 19_CD -RRB-_-RRB- developed_VBD a_DT parallel_JJ NM_NN
has_VBZ been_VBN proved_VBN to_TO converge_VB to_TO stationary_JJ points_NNS by_IN -LRB-_-LRB- 6_CD -RRB-_-RRB- as_RB long_RB as_IN one_PRP can_MD solve_VB each_DT nonnegative_JJ least_JJS square_JJ sub-problem_NN exactly_RB ._.
Among_IN existing_VBG methods_NNS ,_, -LRB-_-LRB- 18_CD -RRB-_-RRB- proposes_VBZ a_DT projected_JJ gradient_NN method_NN ,_, =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: uses_VBZ an_DT active_JJ set_NN method_NN ,_, -LRB-_-LRB- 10_CD -RRB-_-RRB- applies_VBZ a_DT projected_JJ Newton_NNP method_NN ,_, and_CC -LRB-_-LRB- 12_CD -RRB-_-RRB- suggests_VBZ a_DT modified_VBN active_JJ set_NN method_NN called_VBN BlockPivot_NNP for_IN solving_VBG the_DT non-negative_JJ least_JJS squares_NNS sub-problem_JJ ._.
Very_RB recent_JJ
t_NN algorithm_NN that_WDT has_VBZ the_DT ability_NN to_TO select_VB variables_NNS according_VBG to_TO their_PRP$ ``_`` importance_NN ''_'' ,_, thus_RB leading_VBG to_TO significant_JJ speedups_NNS ._.
Similar_JJ strategies_NNS has_VBZ been_VBN employed_VBN in_IN Lasso_NN or_CC feature_NN selection_NN problems_NNS =_JJ -_: =[_NN 17_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
To_TO illustrate_VB the_DT idea_NN ,_, we_PRP take_VBP a_DT text_NN dataset_NN ''_'' ._.
Objective_NNP Value_NNP 7.8_CD 7.6_CD 7.4_CD 7.2_CD 7_CD 6.8_CD 6.6_CD 6.4_CD x_CC 104_CD CD_NNP FastHals_NNP 6.2_CD 0_CD 2_CD 4_CD 6_CD 8_CD 10_CD Number_NN of_IN Coordinate_JJ Updates_NNS x_CC 10_CD 7_CD number_NN of_IN updates_NNS 5_CD 4_CD 3_CD 2_CD 1_CD 0_CD
One_CD can_MD also_RB replace_VB P_NN P_NN i_FW ,_, r_NN Wir_NN and_CC r_NN ,_, j_NN Hrj_NN by_IN Frobenius_NN norm_NN of_IN W_NN and_CC H._NN Note_VBP that_IN usually_RB m_NN ≫_CD k_NN and_CC n_NN ≫_FW k._FW Since_IN f_FW is_VBZ non-convex_JJ ,_, we_PRP can_MD only_RB hope_VB to_TO find_VB a_DT stationary_JJ point_NN of_IN f._JJ Many_JJ algorithms_NNS -LRB-_-LRB- =_JJ -_: =[_NN 15_CD ,_, 5_CD ,_, 1_CD ,_, 23_CD -RRB-_-RRB- -_: =--RRB-_NN have_VBP been_VBN proposed_VBN for_IN this_DT purpose_NN ._.
Among_IN them_PRP ,_, Lee_NNP &_CC Seung_NNP 's_POS multiplicative-update_JJ algorithm_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- has_VBZ been_VBN the_DT most_RBS common_JJ method_NN ._.
Recent_JJ methods_NNS follow_VBP the_DT Alternating_VBG Nonnegative_JJ Least_NN Squares_NNS -LRB-_-LRB-
One_CD can_MD also_RB replace_VB P_NN P_NN i_FW ,_, r_NN Wir_NN and_CC r_NN ,_, j_NN Hrj_NN by_IN Frobenius_NN norm_NN of_IN W_NN and_CC H._NN Note_VBP that_IN usually_RB m_NN ≫_CD k_NN and_CC n_NN ≫_FW k._FW Since_IN f_FW is_VBZ non-convex_JJ ,_, we_PRP can_MD only_RB hope_VB to_TO find_VB a_DT stationary_JJ point_NN of_IN f._JJ Many_JJ algorithms_NNS -LRB-_-LRB- =_JJ -_: =[_NN 15_CD ,_, 5_CD ,_, 1_CD ,_, 23_CD -RRB-_-RRB- -_: =--RRB-_NN have_VBP been_VBN proposed_VBN for_IN this_DT purpose_NN ._.
Among_IN them_PRP ,_, Lee_NNP &_CC Seung_NNP 's_POS multiplicative-update_JJ algorithm_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- has_VBZ been_VBN the_DT most_RBS common_JJ method_NN ._.
Recent_JJ methods_NNS follow_VBP the_DT Alternating_VBG Nonnegative_JJ Least_NN Squares_NNS -LRB-_-LRB-
has_VBZ been_VBN the_DT most_RBS common_JJ method_NN ._.
Recent_JJ methods_NNS follow_VBP the_DT Alternating_NNP Nonnegative_NNP Least_NNP Squares_NNPS -LRB-_-LRB- ANLS_NN -RRB-_-RRB- framework_NN suggested_VBN by_IN -LRB-_-LRB- 20_CD -RRB-_-RRB- ._.
This_DT framework_NN has_VBZ been_VBN proved_VBN to_TO converge_VB to_TO stationary_JJ points_NNS by_IN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: as_RB long_RB as_IN one_PRP can_MD solve_VB each_DT nonnegative_JJ least_JJS square_JJ sub-problem_NN exactly_RB ._.
Among_IN existing_VBG methods_NNS ,_, -LRB-_-LRB- 18_CD -RRB-_-RRB- proposes_VBZ a_DT projected_JJ gradient_NN method_NN ,_, -LRB-_-LRB- 11_CD -RRB-_-RRB- uses_VBZ an_DT active_JJ set_NN method_NN ,_, -LRB-_-LRB- 10_CD -RRB-_-RRB- applies_VBZ a_DT projected_VBN
the_DT most_RBS commonly_RB used_VBN is_VBZ the_DT Frobenius_NNP norm_NN ,_, which_WDT leads_VBZ to_TO the_DT following_JJ minimization_NN problem_NN :_: 1_CD min_NN f_SYM -LRB-_-LRB- W_NN ,_, H_NN -RRB-_-RRB- ≡_NN W_NN ,_, H_NNP ≥_NNP 0_CD 2_CD ‖_NN V_NNP −_NNP WH_NNP ‖_NNP 2_CD F_NN =_JJ 1_CD 2_CD X_NN -LRB-_-LRB- Vij_NN −_NN -LRB-_-LRB- WH_NN -RRB-_-RRB- ij_NN -RRB-_-RRB- 2_CD -LRB-_-LRB- 1_LS -RRB-_-RRB- To_TO achieve_VB better_JJR sparsity_NN ,_, researchers_NNS -LRB-_-LRB- =_JJ -_: =[_NN 7_CD ,_, 22_CD -RRB-_-RRB- -_: =--RRB-_NN have_VBP proposed_VBN adding_VBG regularization_NN terms_NNS ,_, on_IN W_NN and_CC H_NN ,_, to_TO -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
For_IN example_NN ,_, an_DT L1-norm_JJ penalty_NN on_IN W_NN and_CC H_NN can_MD achieve_VB a_DT more_RBR sparse_JJ solution_NN :_: min_NN W_NN ,_, H_NN ≥_NN 0_CD 1_CD 2_CD ‖_NN V_NNP −_NNP WH_NNP ‖_NNP 2_CD F_NN +_CC ρ1_NN X_NN i_FW ,_, r_NN i_LS ,_, j_NN Wir_NN +_CC ρ2_NN X_NN Hrj_NN ._.
One_CD can_MD also_RB replace_VB P_NN P_NN i_FW ,_, r_NN Wir_NN and_CC r_NN ,_, j_NN Hrj_NN by_IN Frobenius_NN norm_NN of_IN W_NN and_CC H._NN Note_VBP that_IN usually_RB m_NN ≫_CD k_NN and_CC n_NN ≫_FW k._FW Since_IN f_FW is_VBZ non-convex_JJ ,_, we_PRP can_MD only_RB hope_VB to_TO find_VB a_DT stationary_JJ point_NN of_IN f._JJ Many_JJ algorithms_NNS -LRB-_-LRB- =_JJ -_: =[_NN 15_CD ,_, 5_CD ,_, 1_CD ,_, 23_CD -RRB-_-RRB- -_: =--RRB-_NN have_VBP been_VBN proposed_VBN for_IN this_DT purpose_NN ._.
Among_IN them_PRP ,_, Lee_NNP &_CC Seung_NNP 's_POS multiplicative-update_JJ algorithm_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- has_VBZ been_VBN the_DT most_RBS common_JJ method_NN ._.
Recent_JJ methods_NNS follow_VBP the_DT Alternating_VBG Nonnegative_JJ Least_NN Squares_NNS -LRB-_-LRB-
One_CD can_MD also_RB replace_VB P_NN P_NN i_FW ,_, r_NN Wir_NN and_CC r_NN ,_, j_NN Hrj_NN by_IN Frobenius_NN norm_NN of_IN W_NN and_CC H._NN Note_VBP that_IN usually_RB m_NN ≫_CD k_NN and_CC n_NN ≫_FW k._FW Since_IN f_FW is_VBZ non-convex_JJ ,_, we_PRP can_MD only_RB hope_VB to_TO find_VB a_DT stationary_JJ point_NN of_IN f._JJ Many_JJ algorithms_NNS -LRB-_-LRB- =_JJ -_: =[_NN 15_CD ,_, 5_CD ,_, 1_CD ,_, 23_CD -RRB-_-RRB- -_: =--RRB-_NN have_VBP been_VBN proposed_VBN for_IN this_DT purpose_NN ._.
Among_IN them_PRP ,_, Lee_NNP &_CC Seung_NNP 's_POS multiplicative-update_JJ algorithm_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- has_VBZ been_VBN the_DT most_RBS common_JJ method_NN ._.
Recent_JJ methods_NNS follow_VBP the_DT Alternating_VBG Nonnegative_JJ Least_NN Squares_NNS -LRB-_-LRB-
solve_VB each_DT nonnegative_JJ least_JJS square_JJ sub-problem_NN exactly_RB ._.
Among_IN existing_VBG methods_NNS ,_, -LRB-_-LRB- 18_CD -RRB-_-RRB- proposes_VBZ a_DT projected_JJ gradient_NN method_NN ,_, -LRB-_-LRB- 11_CD -RRB-_-RRB- uses_VBZ an_DT active_JJ set_NN method_NN ,_, -LRB-_-LRB- 10_CD -RRB-_-RRB- applies_VBZ a_DT projected_JJ Newton_NNP method_NN ,_, and_CC =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: suggests_VBZ a_DT modified_VBN active_JJ set_NN method_NN called_VBN BlockPivot_NNP for_IN solving_VBG the_DT non-negative_JJ least_JJS squares_NNS sub-problem_JJ ._.
Very_RB recently_RB ,_, -LRB-_-LRB- 19_CD -RRB-_-RRB- developed_VBD a_DT parallel_JJ NMF_NN solver_NN for_IN largescale_JJ web_NN data_NNS ._.
This_DT shows_VBZ
