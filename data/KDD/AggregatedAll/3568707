Efficiently_RB handling_VBG feature_NN redundancy_NN in_IN high-dimensional_JJ data_NNS
High-dimensional_JJ data_NNS poses_VBZ a_DT severe_JJ challenge_NN for_IN data_NN mining_NN ._.
Feature_NN selection_NN is_VBZ a_DT frequently_RB used_VBN technique_NN in_IN pre-processing_JJ high-dimensional_JJ data_NNS for_IN successful_JJ data_NNS mining_NN ._.
Traditionally_RB ,_, feature_NN selection_NN is_VBZ focused_VBN on_IN removing_VBG irrelevant_JJ features_NNS ._.
However_RB ,_, for_IN high-dimensional_JJ data_NNS ,_, removing_VBG redundant_JJ features_NNS is_VBZ equally_RB critical_JJ ._.
In_IN this_DT paper_NN ,_, we_PRP provide_VBP a_DT study_NN of_IN feature_NN redundancy_NN in_IN high-dimensional_JJ data_NNS and_CC propose_VBP a_DT novel_JJ correlation-based_JJ approach_NN to_TO feature_VB selection_NN within_IN the_DT filter_NN model_NN ._.
The_DT extensive_JJ empirical_JJ study_NN using_VBG real-world_JJ data_NNS shows_VBZ that_IN the_DT proposed_VBN approach_NN is_VBZ efficient_JJ and_CC effective_JJ in_IN removing_VBG redundant_JJ and_CC irrelevant_JJ features_NNS ._.
reover_NN ,_, to_TO the_DT best_JJS of_IN our_PRP$ knowledge_NN ,_, the_DT issue_NN of_IN feature_NN redundance_NN has_VBZ not_RB been_VBN studied_VBN yet_RB in_IN the_DT unsupervised_JJ feature_NN selections_NNS ,_, although_IN it_PRP has_VBZ been_VBN recently_RB studied_VBN in_IN the_DT supervised_JJ learning_NN =_JJ -_: =[_NN 2,13,14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT new_JJ feature_NN selection_NN method_NN ,_, in_IN which_WDT the_DT clustering_NN and_CC the_DT feature_NN selection_NN are_VBP performed_VBN iteratively_RB ._.
A_DT new_JJ evaluation_NN index_NN is_VBZ firstly_RB introduced_VBN to_TO identify_VB the_DT mo_NN
k_NN whether_IN the_DT subset_NN is_VBZ valid_JJ -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
According_VBG to_TO evaluation_NN methods_NNS the_DT feature_NN subset_NN selection_NN can_MD classified_VBN into_IN two_CD kinds_NNS :_: filtering_VBG and_CC wrapper_NN ._.
Distance_NNP measures_NNS -LRB-_-LRB- 17_CD ,_, 18_CD -RRB-_-RRB- ,_, information_NN measures_VBZ =_JJ -_: =[_NN 19_CD ,_, 20_CD ,_, 21_CD -RRB-_-RRB- -_: =_JJ -_: ,_, correlation_NN coefficient_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- and_CC consistency_NN measures_NNS -LRB-_-LRB- 6_CD -RRB-_-RRB- are_VBP used_VBN for_IN filtering_VBG methods_NNS ._.
Wrapper_NNP refers_VBZ to_TO using_VBG a_DT classifier_NN as_IN evaluation_NN function_NN in_IN selection_NN ._.
KNN_NNP ,_, neural_JJ network_NN ,_, SVM_NN all_DT can_MD b_SYM
n_NN â€²_CD t_NN necessarily_RB provide_VBP a_DT better_JJR feature_NN set_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, different_JJ methods_NNS that_WDT take_VBP into_IN consideration_NN the_DT dependency_NN among_IN features_NNS -LRB-_-LRB- redundancy_NN -RRB-_-RRB- have_VBP been_VBN developed_VBN by_IN Peng_NNP et_FW al._FW -LRB-_-LRB- 15_CD -RRB-_-RRB- and_CC by_IN Yu_NNP and_CC Liu_NNP =_SYM -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT minimum_JJ redundancy-maximum_JJ relevance_NN feature_NN selection_NN is_VBZ a_DT robust_JJ and_CC accurate_JJ feature_NN selection_NN technique_NN that_WDT extracts_VBZ features_NNS minimally_RB redundant_JJ among_IN themselves_PRP and_CC maximally_RB relevant_JJ to_TO
select_VB the_DT feature_NN which_WDT enables_VBZ S_NN to_TO achieve_VB the_DT highest_JJS score_NN of_IN WR_NNP ,_, S_NNP or_CC WA_NNP ,_, S._NNP This_NNP technique_NN ,_, requiring_VBG only_RB a_DT complexity_NN of_IN the_DT order_NN O_NN -LRB-_-LRB- NP_NN -RRB-_-RRB- ,_, has_VBZ been_VBN applied_VBN in_IN previous_JJ feature_NN selection_NN studies_NNS =_JJ -_: =[_NN 2_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2.1_CD Analyzing_VBG the_DT Resulting_NNP Predictor_NNP Sets_NNP Predictor_NNP sets_NNS are_VBP obtained_VBN from_IN both_DT methods_NNS using_VBG only_RB the_DT training_NN set_NN ,_, thus_RB ensuring_VBG minimal_JJ bias_NN in_IN the_DT resulting_VBG accuracy_NN upon_IN the_DT test_NN set_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
Each_DT
ning_JJ process_NN ._.
Traditionally_RB ,_, the_DT feature_NN selection_NN methods_NNS have_VBP been_VBN focused_VBN on_IN removing_VBG irrelevant_JJ features_NNS ,_, but_CC in_IN problems_NNS of_IN high_JJ dimensionality_NN ,_, it_PRP is_VBZ also_RB important_JJ to_TO remove_VB redundant_JJ features_NNS =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Several_JJ effective_JJ feature_NN selection_NN methods_NNS had_VBD been_VBN proposed_VBN -LRB-_-LRB- 1_LS -RRB-_-RRB- ,_, -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 10_CD -RRB-_-RRB- ._.
However_RB the_DT increment_NN in_IN the_DT size_NN of_IN the_DT dataset_NN in_IN both_DT directions_NNS ,_, number_NN of_IN instances_NNS and_CC number_NN of_IN features_NNS be_VB
asks_VBZ ,_, improving_VBG mining_NN performance_NN like_IN predictive_JJ accuracy_NN ,_, and_CC enhancing_VBG result_NN comprehensibility_NN -LRB-_-LRB- 3_CD ,_, 5_CD ,_, 9_CD -RRB-_-RRB- ._.
Feature_NN selection_NN algorithms_NNS can_MD broadly_RB fall_VB into_IN the_DT filter_NN model_NN or_CC the_DT wrapper_NN model_NN =_JJ -_: =[_NN 4_CD ,_, 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT filter_NN model_NN relies_VBZ on_IN general_JJ characteristics_NNS of_IN the_DT training_NN data_NNS to_TO select_VB some_DT features_NNS withPermission_NN to_TO make_VB digital_JJ or_CC hard_JJ copies_NNS of_IN all_DT or_CC part_NN of_IN this_DT work_NN for_IN personal_JJ or_CC classroom_NN us_PRP
evidence_NN from_IN feature_NN selection_NN literature_NN shows_VBZ that_IN ,_, along_IN with_IN irrelevant_JJ features_NNS ,_, redundant_JJ features_NNS also_RB affect_VBP the_DT speed_NN and_CC accuracy_NN of_IN mining_NN algorithms_NNS and_CC thus_RB should_MD be_VB eliminated_VBN as_RB well_RB =_JJ -_: =[_NN 7_CD ,_, 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Therefore_RB ,_, in_IN the_DT context_NN of_IN feature_NN selection_NN for_IN high_JJ dimensional_JJ data_NNS where_WRB there_EX may_MD exist_VB many_JJ redundant_JJ features_NNS ,_, pure_JJ relevance-based_JJ feature_NN weighting_NN algorithms_NNS do_VBP not_RB meet_VB the_DT need_NN of_IN feat_NN
has_VBZ been_VBN effective_JJ in_IN removing_VBG irrelevant_JJ and_CC redundant_JJ features_NNS ,_, increasing_VBG efficiency_NN in_IN mining_NN tasks_NNS ,_, improving_VBG mining_NN performance_NN like_IN predictive_JJ accuracy_NN ,_, and_CC enhancing_VBG result_NN comprehensibility_NN =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Feature_NN selection_NN algorithms_NNS can_MD broadly_RB fall_VB into_IN the_DT filter_NN model_NN or_CC the_DT wrapper_NN model_NN -LRB-_-LRB- 4_CD ,_, 9_CD -RRB-_-RRB- ._.
The_DT filter_NN model_NN relies_VBZ on_IN general_JJ characteristics_NNS of_IN the_DT training_NN data_NNS to_TO select_VB some_DT features_NNS withP_NN
ber_NN of_IN subsets_NNS evaluated_VBN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, but_CC experiments_NNS show_VBP that_IN in_IN order_NN to_TO obtain_VB near_IN optimal_JJ results_NNS the_DT required_JJ number_NN of_IN subsets_NNS for_IN evaluation_NN is_VBZ mostly_RB at_IN least_JJS quadratic_JJ to_TO the_DT number_NN of_IN features_NNS N_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Therefore_RB ,_, with_IN at_IN least_JJS quadratic_JJ complexity_NN in_IN terms_NNS of_IN dimensionality_NN ,_, subset_NN search_NN algorithms_NNS do_VBP not_RB have_VB strong_JJ scalability_NN to_TO deal_VB with_IN high_JJ dimensional_JJ data_NNS ._.
To_TO overcome_VB the_DT problems_NNS of_IN algo_NN
has_VBZ been_VBN effective_JJ in_IN removing_VBG irrelevant_JJ and_CC redundant_JJ features_NNS ,_, increasing_VBG efficiency_NN in_IN mining_NN tasks_NNS ,_, improving_VBG mining_NN performance_NN like_IN predictive_JJ accuracy_NN ,_, and_CC enhancing_VBG result_NN comprehensibility_NN =_JJ -_: =[_NN 3_CD ,_, 5_CD ,_, 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Feature_NN selection_NN algorithms_NNS can_MD broadly_RB fall_VB into_IN the_DT filter_NN model_NN or_CC the_DT wrapper_NN model_NN -LRB-_-LRB- 4_CD ,_, 9_CD -RRB-_-RRB- ._.
The_DT filter_NN model_NN relies_VBZ on_IN general_JJ characteristics_NNS of_IN the_DT training_NN data_NNS to_TO select_VB some_DT features_NNS withP_NN
ted_VBN using_VBG Weka_NNP 's_POS implementation_NN of_IN all_PDT these_DT existing_VBG algorithms_NNS and_CC FCBF_NN is_VBZ also_RB implemented_VBN in_FW Weka_FW environment_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- ._.
All_DT together_RB 10_CD data_NNS sets_NNS are_VBP selected_VBN from_IN the_DT UCI_NNP Machine_NNP Learning_NNP Repository_NNP =_SYM -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: and_CC the_DT UCI_NNP KDD_NNP Archive_NNP -LRB-_-LRB- 1_LS -RRB-_-RRB- ._.
A_DT summary_NN of_IN data_NNS sets_NNS is_VBZ presented_VBN in_IN Table_NNP 1_CD ._.
For_IN each_DT data_NNS set_VBN ,_, we_PRP run_VBP FCBF_NN ,_, CFS-SF_NN ,_, and_CC ReliefF_NN to_TO obtain_VB a_DT set_NN of_IN selected_VBN features_NNS from_IN each_DT algorithm_NN ,_, and_CC record_NN t_NN
