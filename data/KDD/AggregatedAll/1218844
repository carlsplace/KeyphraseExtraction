Generalized_NNP additive_JJ neural_JJ networks_NNS
parameters_NNS are_VBP estimated_VBN by_IN numerically_RB optimizing_VBG some_DT suitable_JJ measure_NN of_IN fit_NN to_TO the_DT training_NN data_NNS such_JJ as_IN the_DT negative_JJ log_NN likelihood_NN ._.
2.2_CD GENERALIZED_NNP ADDITIVE_NNP MODELS_NNP Generalized_NNP additive_JJ models_NNS -LRB-_-LRB- =_JJ -_: =_JJ Hastie_NNP &_CC Tibshirani_NNP ,_, 1990_CD -_: =--RRB-_NN are_VBP a_DT compromise_NN between_IN the_DT inflexible_JJ ,_, but_CC docile_JJ ,_, linear_JJ models_NNS and_CC the_DT flexible_JJ ,_, but_CC troublesome_JJ ,_, universal_JJ approximators_NNS ._.
A_DT generalized_JJ additive_JJ model_NN -LRB-_-LRB- GAM_NN -RRB-_-RRB- has_VBZ the_DT form_NN E_NN y_NN -RRB-_-RRB- =_JJ β_NN +_CC f_LS -LRB-_-LRB- x_NN -RRB-_-RRB- +_CC f_SYM
992566_CD bwitdlr@puk.ac.za_NN Keywords_NNP Additive_NNP models_NNS ,_, neural_JJ networks_NNS ,_, logistic_JJ regression_NN ,_, partial_JJ residuals_NNS ,_, predictive_JJ modeling_NN ,_, credit_NN scoring_VBG 1_CD ._.
INTRODUCTION_NN Logistic_JJ regression_NN -LRB-_-LRB- Kleinbaum_NNP ,_, 1994_CD -RRB-_-RRB- ;_: -LRB-_-LRB- =_JJ -_: =_JJ Hosmer_NNP &_CC Lemeshow_NNP ,_, 1989_CD -_: =--RRB-_NN occupies_VBZ a_DT central_JJ position_NN in_IN the_DT field_NN of_IN credit_NN scoring_VBG -LRB-_-LRB- Thomas_NNP ,_, Edelman_NNP &_CC Crook_NNP ,_, 2002_CD -RRB-_-RRB- ;_: -LRB-_-LRB- McNab_NNP &_CC Wynn_NNP ,_, 2000_CD -RRB-_-RRB- as_IN it_PRP is_VBZ relatively_RB well_RB understood_VBN and_CC an_DT explicit_JJ formula_NN can_MD be_VB derived_VBN on_IN which_WDT c_NN
ionships_NNS between_IN the_DT target_NN and_CC input_NN variables_NNS in_IN multiple_JJ regression_NN models_NNS ._.
In_IN general_JJ ,_, there_EX are_VBP two_CD complementary_JJ approaches_NNS to_TO examining_VBG the_DT assumption_NN of_IN linearity_NN :_: informal_JJ graphical_JJ methods_NNS -LRB-_-LRB- =_JJ -_: =_JJ Cai_NNP &_CC Tsai_NNP ,_, 1999_CD -_: =--RRB-_NN and_CC formal_JJ tests_NNS ._.
Ezekiel_NNP -LRB-_-LRB- 1924_CD -RRB-_-RRB- introduced_VBD an_DT informal_JJ graphical_JJ method_NN that_WDT was_VBD termed_VBN the_DT partial_JJ residual_JJ plot_NN by_IN Larsen_NNP &_CC McCleary_NNP -LRB-_-LRB- 1972_CD -RRB-_-RRB- ._.
This_DT method_NN is_VBZ still_RB used_VBN frequently_RB ._.
The_DT visual_JJ diagn_NN
asy_NN to_TO interpret_VB graphically_RB ._.
GAMs_NNS are_VBP usually_RB presented_VBN as_IN extensions_NNS of_IN linear_JJ models_NNS ,_, but_CC can_MD also_RB be_VB presented_VBN as_IN constrained_VBN forms_NNS of_IN flexible_JJ universal_JJ approximators_NNS such_JJ as_IN pursuit_NN regression_NN -LRB-_-LRB- =_JJ -_: =_JJ Friedman_NNP &_CC Stuetzle_NNP ,_, 1981_CD -_: =--RRB-_NN and_CC artificial_JJ neural_JJ networks_NNS -LRB-_-LRB- Sarle_NNP ,_, 1994_CD -RRB-_-RRB- ._.
In_IN the_DT latter_JJ case_NN they_PRP are_VBP called_VBN GANNs_NNS ._.
An_DT example_NN of_IN a_DT GANN_NN with_IN two_CD inputs_NNS is_VBZ shown_VBN in_IN Figure_NNP 1_CD ._.
Each_DT input_NN has_VBZ a_DT direct_JJ connection_NN -LRB-_-LRB- skip_VB layer_NN -RRB-_-RRB- ,_, th_DT
uth_NN Africa_NNP +27 18 2992566_NNP bwitdlr@puk.ac.za_NNP Keywords_NNP Additive_NNP models_NNS ,_, neural_JJ networks_NNS ,_, logistic_JJ regression_NN ,_, partial_JJ residuals_NNS ,_, predictive_JJ modeling_NN ,_, credit_NN scoring_VBG 1_CD ._.
INTRODUCTION_NN Logistic_JJ regression_NN -LRB-_-LRB- =_JJ -_: =_JJ Kleinbaum_NNP ,_, 1994_CD -_: =--RRB-_NN ;_: -LRB-_-LRB- Hosmer_NNP &_CC Lemeshow_NNP ,_, 1989_CD -RRB-_-RRB- occupies_VBZ a_DT central_JJ position_NN in_IN the_DT field_NN of_IN credit_NN scoring_VBG -LRB-_-LRB- Thomas_NNP ,_, Edelman_NNP &_CC Crook_NNP ,_, 2002_CD -RRB-_-RRB- ;_: -LRB-_-LRB- McNab_NNP &_CC Wynn_NNP ,_, 2000_CD -RRB-_-RRB- as_IN it_PRP is_VBZ relatively_RB well_RB understood_VBN and_CC an_DT explicit_JJ formul_NN
LPs_NNS can_MD be_VB used_VBN to_TO simultaneously_RB estimate_VB the_DT parameters_NNS of_IN GANN_NNP models_NNS ._.
As_IN a_DT result_NN ,_, the_DT usual_JJ optimization_NN and_CC model_NN complexity_NN issues_NNS also_RB apply_VBP to_TO GANN_NNP models_NNS ._.
The_DT following_VBG set_NN of_IN instructions_NNS -LRB-_-LRB- =_JJ -_: =_JJ Potts_NNS ,_, 1999_CD -_: =--RRB-_NN ;_: -LRB-_-LRB- Potts_NNP ,_, 2000_CD -RRB-_-RRB- for_IN the_DT construction_NN of_IN a_DT GANN_NNP interactively_RB takes_VBZ advantage_NN of_IN their_PRP$ constrained_VBN form_NN to_TO simplify_VB optimization_NN and_CC model_NN selection_NN ._.
β0_RB $_$ 1_CD ._.
Construct_VB a_DT GANN_NN with_IN one_CD neuron_NN in_IN the_DT hi_UH
to_TO simultaneously_RB estimate_VB the_DT parameters_NNS of_IN GANN_NNP models_NNS ._.
As_IN a_DT result_NN ,_, the_DT usual_JJ optimization_NN and_CC model_NN complexity_NN issues_NNS also_RB apply_VBP to_TO GANN_NNP models_NNS ._.
The_DT following_VBG set_NN of_IN instructions_NNS -LRB-_-LRB- Potts_NNS ,_, 1999_CD -RRB-_-RRB- ;_: -LRB-_-LRB- =_JJ -_: =_JJ Potts_NNS ,_, 2000_CD -_: =--RRB-_NN for_IN the_DT construction_NN of_IN a_DT GANN_NNP interactively_RB takes_VBZ advantage_NN of_IN their_PRP$ constrained_VBN form_NN to_TO simplify_VB optimization_NN and_CC model_NN selection_NN ._.
β0_RB $_$ 1_CD ._.
Construct_VB a_DT GANN_NN with_IN one_CD neuron_NN in_IN the_DT hidden_JJ layer_NN and_CC
extensions_NNS of_IN linear_JJ models_NNS ,_, but_CC can_MD also_RB be_VB presented_VBN as_IN constrained_VBN forms_NNS of_IN flexible_JJ universal_JJ approximators_NNS such_JJ as_IN pursuit_NN regression_NN -LRB-_-LRB- Friedman_NNP &_CC Stuetzle_NNP ,_, 1981_CD -RRB-_-RRB- and_CC artificial_JJ neural_JJ networks_NNS -LRB-_-LRB- =_JJ -_: =_JJ Sarle_NNP ,_, 1994_CD -_: =--RRB-_NN ._.
In_IN the_DT latter_JJ case_NN they_PRP are_VBP called_VBN GANNs_NNS ._.
An_DT example_NN of_IN a_DT GANN_NN with_IN two_CD inputs_NNS is_VBZ shown_VBN in_IN Figure_NNP 1_CD ._.
Each_DT input_NN has_VBZ a_DT direct_JJ connection_NN -LRB-_-LRB- skip_VB layer_NN -RRB-_-RRB- ,_, the_DT first_JJ input_NN has_VBZ two_CD nodes_NNS in_IN the_DT hidden_JJ lay_JJ
-LRB-_-LRB- y_NN -RRB-_-RRB- -RRB-_-RRB- −_NN 1_CD g0_FW i_FW Multilayer_JJ perceptrons_NNS are_VBP the_DT most_RBS widely_RB used_VBN type_NN of_IN neural_JJ network_NN for_IN supervised_JJ prediction_NN ._.
Theoretically_RB ,_, they_PRP are_VBP universal_JJ approximators_NNS that_WDT can_MD model_VB any_DT continuous_JJ function_NN -LRB-_-LRB- =_JJ -_: =_JJ Ripley_NNP ,_, 1996_CD -_: =--RRB-_NN ._.
A_DT multilayer_JJ perceptron_NN -LRB-_-LRB- MLP_NN -RRB-_-RRB- that_WDT has_VBZ a_DT single_JJ layer_NN with_IN h_NN hidden_JJ neurons_NNS has_VBZ the_DT form_NN ¡_CD g_NN −_NN 1_CD 0_CD -LRB-_-LRB- E_NN -LRB-_-LRB- y_NN -RRB-_-RRB- -RRB-_-RRB- =_JJ w_NN +_CC w_NN tanh_NN -LRB-_-LRB- w_NN +_CC w_NN tanh_NN -LRB-_-LRB- w_NN h_NN i_LS 0_CD 0h_NN +_CC cents_NNS k_NN 1_CD j_NN =_JJ 1_CD w_NN jh_NN x_NN ji_NN -RRB-_-RRB- 01_CD +_CC cents_NNS k_NN j_NN =_JJ 1_CD w_NN j1_NN x_NN ji_NN -RRB-_-RRB- +_CC
