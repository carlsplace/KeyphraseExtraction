Grafting-light_NN :_: fast_RB ,_, incremental_JJ feature_NN selection_NN and_CC structure_NN learning_NN of_IN Markov_NNP random_JJ fields_NNS
Feature_NN selection_NN is_VBZ an_DT important_JJ task_NN in_IN order_NN to_TO achieve_VB better_JJR generalizability_NN in_IN high_JJ dimensional_JJ learning_NN ,_, and_CC structure_NN learning_NN of_IN Markov_NNP random_JJ fields_NNS -LRB-_-LRB- MRFs_NNS -RRB-_-RRB- can_MD automatically_RB discover_VB the_DT inherent_JJ structures_NNS underlying_VBG complex_JJ data_NNS ._.
Both_DT problems_NNS can_MD be_VB cast_VBN as_IN solving_VBG an_DT l1-norm_NN regularized_VBD parameter_NN estimation_NN problem_NN ._.
The_DT existing_VBG Grafting_NN method_NN can_MD avoid_VB doing_VBG inference_NN on_IN dense_JJ graphs_NNS in_IN structure_NN learning_NN by_IN incrementally_RB selecting_VBG new_JJ features_NNS ._.
However_RB ,_, Grafting_NN performs_VBZ a_DT greedy_JJ step_NN to_TO optimize_VB over_IN free_JJ parameters_NNS once_IN new_JJ features_NNS are_VBP included_VBN ._.
This_DT greedy_JJ strategy_NN results_VBZ in_IN low_JJ efficiency_NN when_WRB parameter_NN learning_NN is_VBZ itself_PRP non-trivial_JJ ,_, such_JJ as_IN in_IN MRFs_NNS ,_, in_IN which_WDT parameter_NN learning_NN depends_VBZ on_IN an_DT expensive_JJ subroutine_NN to_TO calculate_VB gradients_NNS ._.
The_DT complexity_NN of_IN calculating_VBG gradients_NNS in_IN MRFs_NNS is_VBZ typically_RB exponential_JJ to_TO the_DT size_NN of_IN maximal_JJ cliques_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP present_VBP a_DT fast_JJ algorithm_NN called_VBN Grafting-Light_NNP to_TO solve_VB the_DT l1-norm_NN regularized_VBD maximum_NN likelihood_NN estimation_NN of_IN MRFs_NNS for_IN efficient_JJ feature_NN selection_NN and_CC structure_NN learning_NN ._.
Grafting-Light_NNP iteratively_RB performs_VBZ one-step_JJ of_IN orthant-wise_JJ gradient_NN descent_NN over_IN free_JJ parameters_NNS and_CC selects_VBZ new_JJ features_NNS ._.
This_DT lazy_JJ strategy_NN is_VBZ guaranteed_VBN to_TO converge_VB to_TO the_DT global_JJ optimum_NN and_CC can_MD effectively_RB select_VB significant_JJ features_NNS ._.
On_IN both_CC synthetic_JJ and_CC real_JJ data_NNS sets_NNS ,_, we_PRP show_VBP that_IN Grafting-Light_NN is_VBZ much_RB more_RBR efficient_JJ than_IN Grafting_VBG for_IN both_DT feature_NN selection_NN and_CC structure_NN learning_NN ,_, and_CC performs_VBZ comparably_RB with_IN the_DT optimal_JJ batch_NN method_NN that_WDT directly_RB optimizes_VBZ over_IN all_PDT the_DT features_NNS for_IN feature_NN selection_NN but_CC is_VBZ much_RB more_RBR efficient_JJ and_CC accurate_JJ for_IN structure_NN learning_NN of_IN MRFs_NNS ._.
tional_JJ Markov_NNP network_NN models_NNS ._.
We_PRP can_MD improve_VB the_DT presented_VBN approach_NN in_IN several_JJ aspects_NNS ._.
First_RB ,_, to_TO further_RB speedup_VB the_DT treeRMN_NN model_NN we_PRP can_MD apply_VB efficient_JJ Markov_NNP network_NN feature_NN selection_NN methods_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_SYM -_: instead_RB of_IN systematically_RB enumerating_VBG all_DT possible_JJ feature_NN templates_NNS ._.
Second_RB ,_, as_IN we_PRP have_VBP explained_VBN at_IN the_DT end_NN of_IN section_NN 5_CD ,_, we_PRP 'd_MD like_VB to_TO apply_VB HVD_NN on_IN composite_JJ entity_NN types_NNS ._.
Third_RB ,_, we_PRP 'd_MD also_RB like_VB to_TO
ield_NN -LRB-_-LRB- F_NN title_NN ,_, p2_NN ,_, c_NN -RRB-_-RRB- Next_JJ -LRB-_-LRB- p1_NN ,_, p2_NN -RRB-_-RRB- ∧_FW InF_FW ield_NN -LRB-_-LRB- F_NN venue_NN ,_, p1_NN ,_, c_NN -RRB-_-RRB- ⇒_FW InF_FW ield_NN -LRB-_-LRB- F_NN venue_NN ,_, p2_NN ,_, c_NN -RRB-_-RRB- 5_CD Related_NNP Work_NNP Our_NNP work_NN is_VBZ related_JJ to_TO previous_JJ work_NN on_IN online_JJ feature_NN selection_NN for_IN Markov_NNP Random_NNP Fields_NNP -LRB-_-LRB- MRFs_NNS -RRB-_-RRB- =_JJ -_: =[_NN 25_CD ,_, 35_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, our_PRP$ work_NN differs_VBZ in_IN two_CD aspects_NNS ._.
First_RB ,_, this_DT previous_JJ work_NN assumes_VBZ all_PDT the_DT training_NN examples_NNS are_VBP available_JJ at_IN the_DT beginning_NN and_CC only_RB the_DT features_NNS are_VBP arriving_VBG online_NN ,_, while_IN in_IN our_PRP$ work_NN both_DT
ets_NN ._.
In_IN this_DT case_NN ,_, the_DT batch_NN method_NN Full-Opt_NNP ._.
-_: L1_NN is_VBZ optimal_JJ in_IN the_DT sense_NN of_IN obtaining_VBG the_DT best_JJS subset_NN of_IN features_NNS ,_, achieving_VBG the_DT best_JJS predictive_JJ performance_NN ,_, etc._NN ._.
Although_IN stochastic_JJ gradient_NN methods_NNS =_JJ -_: =[_NN 26_CD ,_, 25_CD -RRB-_-RRB- -_: =_SYM -_: may_MD achieve_VB better_JJR time_NN efficiency_NN than_IN Full-Opt_NNP ._.
-_: L1_NN ,_, they_PRP usually_RB select_VBP many_JJ irrelevant_JJ features_NNS because_IN of_IN the_DT approximate_JJ stochastic_JJ gradients_NNS they_PRP are_VBP using_VBG ._.
The_DT main_JJ observation_NN in_IN these_DT experi_NNS
tly_RB ,_, the_DT structure_NN learning_NN problem_NN of_IN MRFs_NNS has_VBZ been_VBN formulated_VBN as_IN a_DT feature_NN selection_NN problem_NN by_IN defining_VBG the_DT feature_NN functions_NNS f_SYM to_TO encode_VB the_DT model_NN structures_NNS and_CC performing_VBG the_DT ℓ1-regularized_JJ MLE_NN =_JJ -_: =[_NN 12_CD ,_, 27_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP consider_VBP both_CC the_DT pure_JJ feature_NN selection_NN problem_NN -LRB-_-LRB- when_WRB model_NN structures_NNS are_VBP kept_VBN fixed_JJ -RRB-_-RRB- and_CC learning_VBG structures_NNS of_IN MRFs_NNS in_IN our_PRP$ experiments_NNS ._.
Two_CD major_JJ types_NNS of_IN approaches_NNS have_VBP been_VBN used_VBN to_TO minimiz_VB
risk_NN minimization_NN ,_, i.e._FW ,_, minimizing_VBG a_DT regularized_VBN empirical_JJ risk_NN ._.
By_IN using_VBG the_DT ℓ1-norm_NN regularizer_NN ,_, irrelevant_JJ features_NNS can_MD be_VB effectively_RB discarded_VBN when_WRB the_DT minimization_NN problem_NN obtains_VBZ its_PRP$ optimum_NN =_JJ -_: =[_NN 24_CD ,_, 33_CD ,_, 34_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT approaches_NNS to_TO structure_VB learning_NN of_IN Markov_NNP random_JJ fields_NNS typically_RB use_VBP greedy_JJ local_JJ heuristic_NN search_NN that_WDT incrementally_RB changes_VBZ the_DT model_NN structure_NN by_IN adding_VBG or_CC deleting_VBG edges_NNS ._.
The_DT adding_VBG or_CC del_DT
timization_NN over_IN the_DT free_JJ parameters_NNS is_VBZ usually_RB an_DT expensive_JJ step_NN ,_, especially_RB in_IN Markov_NNP random_JJ fields_NNS ._.
In_IN MRFs_NNS ,_, finding_VBG the_DT optimal_JJ parameters_NNS requires_VBZ an_DT iterative_JJ procedure_NN ,_, such_JJ as_IN the_DT quasi-Newton_NN =_JJ -_: =[_NN 13_CD ,_, 21_CD -RRB-_-RRB- -_: =_JJ -_: or_CC stochastic_JJ gradient_NN descent_NN -LRB-_-LRB- 26_CD -RRB-_-RRB- method_NN ,_, in_IN which_WDT each_DT iteration_NN needs_VBZ to_TO compute_VB the_DT gradients_NNS ._.
Computing_NNP gradients_NNS in_IN MRFs_NNS is_VBZ computationally_RB expensive_JJ even_RB for_IN the_DT models_NNS whose_WP$ tree-width_NN is_VBZ sm_NN
rning_NN of_IN MRFs_NNS ._.
Two_CD types_NNS of_IN approaches_NNS have_VBP been_VBN successfully_RB used_VBN to_TO solve_VB the_DT ℓ1-regularized_JJ MLE_NN ,_, that_DT is_VBZ ,_, the_DT batch_NN methods_NNS -LRB-_-LRB- such_JJ as_IN the_DT OWL-QN_NN algorithm_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- and_CC the_DT ℓ1-ball_JJ projectionbased_JJ method_NN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =--RRB-_NN that_WDT directly_RB optimize_VB over_IN all_PDT the_DT candidate_NN features_NNS and_CC the_DT incremental_JJ methods_NNS -LRB-_-LRB- such_JJ as_IN Grafting_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- -RRB-_-RRB- that_WDT incrementally_RB include_VBP new_JJ features_NNS ._.
Although_IN batch_NN methods_NNS can_MD deal_VB with_IN a_DT large_JJ numbe_NN
sually_RB an_DT expensive_JJ step_NN ,_, especially_RB in_IN Markov_NNP random_JJ fields_NNS ._.
In_IN MRFs_NNS ,_, finding_VBG the_DT optimal_JJ parameters_NNS requires_VBZ an_DT iterative_JJ procedure_NN ,_, such_JJ as_IN the_DT quasi-Newton_NN -LRB-_-LRB- 13_CD ,_, 21_CD -RRB-_-RRB- or_CC stochastic_JJ gradient_NN descent_NN =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_JJ -_: method_NN ,_, in_IN which_WDT each_DT iteration_NN needs_VBZ to_TO compute_VB the_DT gradients_NNS ._.
Computing_NNP gradients_NNS in_IN MRFs_NNS is_VBZ computationally_RB expensive_JJ even_RB for_IN the_DT models_NNS whose_WP$ tree-width_NN is_VBZ small_JJ ._.
Moreover_RB ,_, our_PRP$ empirical_JJ results_NNS
risk_NN minimization_NN ,_, i.e._FW ,_, minimizing_VBG a_DT regularized_VBN empirical_JJ risk_NN ._.
By_IN using_VBG the_DT ℓ1-norm_NN regularizer_NN ,_, irrelevant_JJ features_NNS can_MD be_VB effectively_RB discarded_VBN when_WRB the_DT minimization_NN problem_NN obtains_VBZ its_PRP$ optimum_NN =_JJ -_: =[_NN 24_CD ,_, 33_CD ,_, 34_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT approaches_NNS to_TO structure_VB learning_NN of_IN Markov_NNP random_JJ fields_NNS typically_RB use_VBP greedy_JJ local_JJ heuristic_NN search_NN that_WDT incrementally_RB changes_VBZ the_DT model_NN structure_NN by_IN adding_VBG or_CC deleting_VBG edges_NNS ._.
The_DT adding_VBG or_CC del_DT
hods_NNS is_VBZ exponential_JJ to_TO the_DT size_NN of_IN maximum_NN cliques_NNS ._.
But_CC for_IN those_DT models_NNS whose_WP$ graph_NN structures_NNS contain_VBP large_JJ loops_NNS ,_, we_PRP have_VBP to_TO turn_VB to_TO approximation_NN methods_NNS ,_, either_CC deterministic_JJ variational_JJ methods_NNS =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: and_CC belief_NN propagation_NN -LRB-_-LRB- 31_CD -RRB-_-RRB- -RRB-_-RRB- ,_, or_CC stochastic_JJ Markov_NNP chain_NN Monte_NNP Carlo_NNP -LRB-_-LRB- MCMC_NNP -RRB-_-RRB- methods_NNS ._.
In_IN general_JJ ,_, the_DT accuracy_NN and_CC timeefficiency_NN of_IN these_DT approximation_NN methods_NNS depend_VBP largely_RB on_IN the_DT graph_NN structures_NNS ._.
size_NN of_IN maximum_NN cliques_NNS ._.
But_CC for_IN those_DT models_NNS whose_WP$ graph_NN structures_NNS contain_VBP large_JJ loops_NNS ,_, we_PRP have_VBP to_TO turn_VB to_TO approximation_NN methods_NNS ,_, either_CC deterministic_JJ variational_JJ methods_NNS -LRB-_-LRB- 8_CD -RRB-_-RRB- and_CC belief_NN propagation_NN =_JJ -_: =[_NN 31_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, or_CC stochastic_JJ Markov_NNP chain_NN Monte_NNP Carlo_NNP -LRB-_-LRB- MCMC_NNP -RRB-_-RRB- methods_NNS ._.
In_IN general_JJ ,_, the_DT accuracy_NN and_CC timeefficiency_NN of_IN these_DT approximation_NN methods_NNS depend_VBP largely_RB on_IN the_DT graph_NN structures_NNS ._.
For_IN dense_JJ graphs_NNS -LRB-_-LRB- e.g._FW ,_, a_DT c_NN
dom_NN fields_NNS -LRB-_-LRB- MRFs_NNS -RRB-_-RRB- are_VBP undirected_JJ graphical_JJ models_NNS and_CC have_VBP been_VBN widely_RB used_VBN in_IN an_DT ever-growing_JJ variety_NN of_IN applications_NNS ,_, including_VBG natural_JJ language_NN processing_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, data_NN mining_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, signal_NN processing_NN =_JJ -_: =[_NN 29_CD -RRB-_-RRB- -_: =_JJ -_: ,_, etc._NN ._.
Conditional_JJ random_JJ fields_NNS -LRB-_-LRB- CRFs_NNS -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- are_VBP special_JJ MRFs_NNS that_WDT are_VBP globally_RB conditioned_VBN on_IN inputs_NNS and_CC have_VBP shown_VBN great_JJ promise_NN in_IN various_JJ applications_NNS -LRB-_-LRB- 21_CD ,_, 14_CD -RRB-_-RRB- ._.
These_DT models_NNS are_VBP based_VBN on_IN composite_JJ
mation_NN methods_NNS -LRB-_-LRB- 2_CD ,_, 32_CD ,_, 3_CD -RRB-_-RRB- have_VBP been_VBN developed_VBN based_VBN on_IN the_DT ℓ1-norm_NN penalized_VBD maximum_NN likelihood_NN estimation_NN ._.
For_IN the_DT structure_NN learning_NN of_IN directed_JJ Bayesian_JJ networks_NNS ,_, the_DT structural_JJ EM_NN -LRB-_-LRB- SEM_NN -RRB-_-RRB- algorithm_NN =_JJ -_: =[_NN 4_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: has_VBZ a_DT similar_JJ procedure_NN as_IN Grafting_NNP ,_, that_DT is_VBZ ,_, alternatively_RB performing_VBG structural_JJ search_NN to_TO find_VB new_JJ model_NN structures_NNS and_CC parametric_JJ search_NN to_TO obtain_VB the_DT optimal_JJ model_NN parameters_NNS ._.
Therefore_RB ,_, SEM_NN is_VBZ g_NN
INTRODUCTION_NNP Markov_NNP random_JJ fields_NNS -LRB-_-LRB- MRFs_NNS -RRB-_-RRB- are_VBP undirected_JJ graphical_JJ models_NNS and_CC have_VBP been_VBN widely_RB used_VBN in_IN an_DT ever-growing_JJ variety_NN of_IN applications_NNS ,_, including_VBG natural_JJ language_NN processing_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, data_NN mining_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, signal_NN processing_NN -LRB-_-LRB- 29_CD -RRB-_-RRB- ,_, etc._NN ._.
Conditional_JJ random_JJ fields_NNS -LRB-_-LRB- CRFs_NNS -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- are_VBP special_JJ MRFs_NNS that_WDT are_VBP globally_RB conditioned_VBN on_IN inputs_NNS and_CC have_VBP shown_VBN great_JJ promise_NN in_IN various_JJ applications_NNS -LRB-_-LRB- 21_CD ,_, 14_CD -RRB-_-RRB- ._.
These_DT model_NN
peech_JJ tag_NN sequences_NNS -RRB-_-RRB- ._.
Therefore_RB ,_, they_PRP usually_RB have_VBP a_DT complex_JJ and_CC high-dimensional_JJ feature_NN space_NN ._.
To_TO achieve_VB better_JJR generalizability_NN and_CC interpret_VB complex_JJ data_NNS ,_, it_PRP is_VBZ desirable_JJ to_TO do_VB feature_NN selection_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: and_CC pursue_VB a_DT sparse_JJ representation_NN of_IN such_JJ models_NNS that_WDT leaves_VBZ out_RP irrelevant_JJ features_NNS ._.
Since_IN the_DT problem_NN of_IN selecting_VBG an_DT optimal_JJ subset_NN of_IN features_NNS is_VBZ NP-hard_JJ -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, a_DT popular_JJ solution_NN is_VBZ to_TO use_VB a_DT con_NN
r_NN approaches_VBZ to_TO selecting_VBG features_NNS in_IN CRFs_NNS -LRB-_-LRB- or_CC MRFs_NNS in_IN general_JJ -RRB-_-RRB- and_CC has_VBZ shown_VBN great_JJ promise_NN -LRB-_-LRB- 1_CD ,_, 16_CD -RRB-_-RRB- ._.
The_DT sparsity_NN of_IN the_DT ℓ1-norm_NN regularized_VBN MLE_NNP is_VBZ due_JJ to_TO the_DT singularity_NN of_IN the_DT ℓ1-norm_NN at_IN the_DT origin_NN =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Another_DT important_JJ problem_NN we_PRP consider_VBP is_VBZ the_DT structure_NN learning_NN of_IN MRFs_NNS ._.
As_IN the_DT variety_NN and_CC scale_NN of_IN problems_NNS increase_NN ,_, hand-crafting_JJ MRFs_NNS become_VBP less_RBR applicable_JJ ._.
Learning_VBG the_DT structures_NNS of_IN MRFs_NNS can_MD
odels_NNS and_CC have_VBP been_VBN widely_RB used_VBN in_IN an_DT ever-growing_JJ variety_NN of_IN applications_NNS ,_, including_VBG natural_JJ language_NN processing_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, data_NN mining_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, signal_NN processing_NN -LRB-_-LRB- 29_CD -RRB-_-RRB- ,_, etc._NN ._.
Conditional_JJ random_JJ fields_NNS -LRB-_-LRB- CRFs_NNS -RRB-_-RRB- =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP special_JJ MRFs_NNS that_WDT are_VBP globally_RB conditioned_VBN on_IN inputs_NNS and_CC have_VBP shown_VBN great_JJ promise_NN in_IN various_JJ applications_NNS -LRB-_-LRB- 21_CD ,_, 14_CD -RRB-_-RRB- ._.
These_DT models_NNS are_VBP based_VBN on_IN composite_JJ features_NNS that_WDT explicitly_RB exploit_VBP the_DT struct_NN
of_IN variables_NNS ,_, e.g._FW ,_, genomic_JJ microarray_NN data_NN analysis_NN -LRB-_-LRB- 30_CD -RRB-_-RRB- ._.
Feature_NN selection_NN can_MD help_VB interpret_VB complex_JJ data_NNS and_CC reduce_VB the_DT risk_NN of_IN over-fitting_NN ._.
Early_JJ approaches_NNS including_VBG the_DT Filter_NNP -LRB-_-LRB- 9_CD -RRB-_-RRB- and_CC Wrapper_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: often_RB treat_VB feature_NN selection_NN as_IN a_DT separate_JJ or_CC weakly_JJ correlated_JJ task_NN with_IN the_DT learning_NN ._.
Recently_RB ,_, feature_NN selection_NN has_VBZ been_VBN viewed_VBN as_IN an_DT integrated_JJ step_NN during_IN learning_VBG within_IN the_DT framework_NN of_IN regu_NN
with_IN a_DT model_NN whose_WP$ weights_NNS are_VBP almost_RB all_DT zeros_NNS ._.
Then_RB ,_, we_PRP iteratively_RB 1_CD The_DT composite_JJ regularizer_NN of_IN ℓ1_NN and_CC ℓ2_NN norms_NNS is_VBZ known_VBN as_IN an_DT elastic_JJ net_JJ regularizer_NN ,_, which_WDT has_VBZ nice_JJ properties_NNS as_IN discussed_VBN in_IN =_JJ -_: =[_NN 35_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Algorithm_NN 1_CD Grafting-Light_NNP Input_NNP :_: data_NNS D_NN =_JJ -LCB-_-LRB- -LRB-_-LRB- x_NN i_FW ,_, y_FW i_FW -RRB-_-RRB- -RCB-_-RRB- N_NN i_LS =_JJ 1_CD ,_, regularization_NN constant_JJ λ_NN ,_, candidate_NN feature_NN set_NN F_NN ,_, and_CC Select_NNP Unit_NNP M_NNP -LRB-_-LRB- M_NNP ≥_NNP 1_LS -RRB-_-RRB- Output_NN :_: a_DT subset_NN S_NN ⊆_NN F_NN and_CC weights_NNS w_FW Initialize_FW S_NN ←_FW ∅_FW a_DT
of_IN fk_NN over_IN the_DT sample_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- ._.
At_IN least_JJS in_IN principle_NN ,_, the_DT parameter_NN learning_NN problem_NN can_MD be_VB solved_VBN with_IN gradient_NN descent_NN methods_NNS ,_, such_JJ as_IN quasi-Newton_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, stochastic_JJ -LRB-_-LRB- 26_CD -RRB-_-RRB- ,_, or_CC exponentiated_JJ gradient_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: methods_NNS ._.
Each_DT component_NN of_IN the_DT gradient_NN is_VBZ N_NN ∑_FW ∂_FW L_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ −_CD fk_NN -LRB-_-LRB- x_NN ∂_NN wk_NN i_FW =_JJ 1_CD i_LS ,_, y_FW i_FW N_NN ∑_NN -RRB-_-RRB- +_CC Ep_NN -LRB-_-LRB- y_FW |_FW xi_FW -RRB-_-RRB- -LRB-_-LRB- fk_NN -LRB-_-LRB- x_NN i_LS =_JJ 1_CD i_LS ,_, y_NN -RRB-_-RRB- -RRB-_-RRB- ._.
-LRB-_-LRB- 1_LS -RRB-_-RRB- From_IN Eq_NN ._.
-LRB-_-LRB- 1_LS -RRB-_-RRB- ,_, we_PRP can_MD see_VB that_IN the_DT gradient_NN depends_VBZ on_IN the_DT marginal_JJ probabilities_NNS of_IN
l_NN approaches_VBZ to_TO solving_VBG the_DT ℓ1-regularization_NN problem_NN is_VBZ provided_VBN in_IN -LRB-_-LRB- 20_CD -RRB-_-RRB- ._.
Finally_RB ,_, for_IN learning_VBG structures_NNS of_IN the_DT special_JJ Gaussian_NNP Markov_NNP random_JJ fields_NNS -LRB-_-LRB- GMRFs_NNS -RRB-_-RRB- ,_, inverse_JJ covariance_NN estimation_NN methods_NNS =_JJ -_: =[_NN 2_CD ,_, 32_CD ,_, 3_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP been_VBN developed_VBN based_VBN on_IN the_DT ℓ1-norm_NN penalized_VBD maximum_NN likelihood_NN estimation_NN ._.
For_IN the_DT structure_NN learning_NN of_IN directed_JJ Bayesian_JJ networks_NNS ,_, the_DT structural_JJ EM_NN -LRB-_-LRB- SEM_NN -RRB-_-RRB- algorithm_NN -LRB-_-LRB- 4_CD ,_, 5_CD -RRB-_-RRB- has_VBZ a_DT similar_JJ proc_NN
mation_NN methods_NNS -LRB-_-LRB- 2_CD ,_, 32_CD ,_, 3_CD -RRB-_-RRB- have_VBP been_VBN developed_VBN based_VBN on_IN the_DT ℓ1-norm_NN penalized_VBD maximum_NN likelihood_NN estimation_NN ._.
For_IN the_DT structure_NN learning_NN of_IN directed_JJ Bayesian_JJ networks_NNS ,_, the_DT structural_JJ EM_NN -LRB-_-LRB- SEM_NN -RRB-_-RRB- algorithm_NN =_JJ -_: =[_NN 4_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: has_VBZ a_DT similar_JJ procedure_NN as_IN Grafting_NNP ,_, that_DT is_VBZ ,_, alternatively_RB performing_VBG structural_JJ search_NN to_TO find_VB new_JJ model_NN structures_NNS and_CC parametric_JJ search_NN to_TO obtain_VB the_DT optimal_JJ model_NN parameters_NNS ._.
Therefore_RB ,_, SEM_NN is_VBZ g_NN
ns_NNS of_IN thousands_NNS of_IN variables_NNS ,_, e.g._FW ,_, genomic_JJ microarray_NN data_NN analysis_NN -LRB-_-LRB- 30_CD -RRB-_-RRB- ._.
Feature_NN selection_NN can_MD help_VB interpret_VB complex_JJ data_NNS and_CC reduce_VB the_DT risk_NN of_IN over-fitting_NN ._.
Early_JJ approaches_NNS including_VBG the_DT Filter_NNP =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Wrapper_NNP -LRB-_-LRB- 10_CD -RRB-_-RRB- often_RB treat_VBP feature_NN selection_NN as_IN a_DT separate_JJ or_CC weakly_JJ correlated_JJ task_NN with_IN the_DT learning_NN ._.
Recently_RB ,_, feature_NN selection_NN has_VBZ been_VBN viewed_VBN as_IN an_DT integrated_JJ step_NN during_IN learning_VBG within_IN the_DT
structures_NNS underlying_VBG complex_JJ data_NNS ._.
Both_DT problems_NNS can_MD be_VB cast_VBN as_IN solving_VBG an_DT ℓ1-norm_NN regularized_VBD parameter_NN estimation_NN problem_NN ._.
To_TO solve_VB such_PDT an_DT ℓ1-regularized_JJ estimation_NN problem_NN ,_, the_DT existing_VBG Grafting_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: method_NN can_MD avoid_VB doing_VBG inference_NN on_IN dense_JJ graphs_NNS in_IN structure_NN learning_NN by_IN incrementally_RB selecting_VBG new_JJ features_NNS ._.
However_RB ,_, Grafting_NN performs_VBZ a_DT greedy_JJ step_NN of_IN optimizing_VBG over_IN free_JJ parameters_NNS once_IN new_JJ fe_NN
can_MD handle_VB a_DT large_JJ set_NN of_IN features_NNS ,_, there_EX are_VBP several_JJ scenarios_NNS in_IN which_WDT only_RB the_DT incremental_JJ methods_NNS can_MD be_VB applied_VBN ,_, such_JJ as_IN structure_NN learning_NN of_IN graphical_JJ models_NNS -LRB-_-LRB- 12_CD -RRB-_-RRB- and_CC online_JJ feature_NN selection_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Therefore_RB ,_, we_PRP focus_VBP on_IN the_DT incremental_JJ methods_NNS and_CC present_VB a_DT fast_JJ algorithm_NN for_IN selecting_VBG features_NNS and_CC learning_VBG structures_NNS of_IN Markov_NNP random_JJ fields_NNS ._.
5_CD ._.
THE_DT GRAFTING-LIGHT_JJ ALGORITHM_NN In_IN this_DT section_NN ,_, w_NN
code_VB the_DT structural_JJ dependencies_NNS among_IN random_JJ variables_NNS and_CC performing_VBG the_DT ℓ1-regularized_JJ MLE_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
We_PRP evaluate_VBP the_DT Grafting-Light_NN on_IN learning_VBG the_DT structures_NNS of_IN pairwise_JJ MRFs_NNS ._.
We_PRP use_VBP the_DT OCR_NNP data_NN set_NN =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, but_CC treat_NN characters_NNS independently_RB ._.
We_PRP get_VBP 20_CD ×_CD 20_CD binary_JJ images_NNS by_IN placing_VBG the_DT original_JJ 16_CD ×_CD 8_CD characters_NNS in_IN the_DT centers_NNS of_IN 20_CD ×_CD 20_CD black_JJ squares_NNS ._.
We_PRP compare_VBP Grafting-Light_JJ with_IN Grafting_NN and_CC Full-O_NN
RK_NNP Feature_NNP selection_NN is_VBZ an_DT important_JJ problem_NN and_CC has_VBZ become_VBN the_DT focus_NN of_IN much_JJ research_NN in_IN many_JJ areas_NNS where_WRB data_NNS samples_NNS can_MD have_VB tens_NNS of_IN thousands_NNS of_IN variables_NNS ,_, e.g._FW ,_, genomic_JJ microarray_NN data_NN analysis_NN =_JJ -_: =[_NN 30_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Feature_NN selection_NN can_MD help_VB interpret_VB complex_JJ data_NNS and_CC reduce_VB the_DT risk_NN of_IN over-fitting_NN ._.
Early_JJ approaches_NNS including_VBG the_DT Filter_NNP -LRB-_-LRB- 9_CD -RRB-_-RRB- and_CC Wrapper_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- often_RB treat_VBP feature_NN selection_NN as_IN a_DT separate_JJ or_CC weakl_JJ
ically_RB discover_VB the_DT inherent_JJ structures_NNS underlying_VBG complex_JJ data_NNS ._.
Similar_JJ as_IN in_IN feature_NN selection_NN ,_, structure_NN learning_NN of_IN MRFs_NNS can_MD be_VB cast_VBN as_IN solving_VBG an_DT ℓ1-norm_NN regularized_VBD parameter_NN estimation_NN problem_NN =_JJ -_: =[_NN 12_CD ,_, 27_CD -RRB-_-RRB- -_: =_JJ -_: ,_, where_WRB the_DT structures_NNS of_IN MRFs_NNS are_VBP encoded_VBN by_IN a_DT set_NN of_IN features_NNS ._.
However_RB ,_, solving_VBG the_DT ℓ1-norm_NN regularized_VBD estimation_NN problem_NN is_VBZ not_RB easy_JJ ,_, as_IN we_PRP explain_VBP below_IN ,_, especially_RB in_IN MRFs_NNS ,_, where_WRB the_DT inference_FW i_FW
-RRB-_-RRB- tag_NN ._.
The_DT outputs_NNS are_VBP corresponding_VBG label_NN sequences_NNS ,_, in_IN which_WDT each_DT label_NN indicates_VBZ whether_IN a_DT word_NN is_VBZ outside_IN a_DT chunk_NN -LRB-_-LRB- O_NN -RRB-_-RRB- ,_, starts_VBZ a_DT chunk_NN -LRB-_-LRB- B_NN -RRB-_-RRB- ,_, or_CC continues_VBZ a_DT chunk_NN -LRB-_-LRB- I_NN -RRB-_-RRB- ._.
We_PRP use_VBP the_DT CoNLL-2000_NN data_NNS set_VBD =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP use_VBP the_DT same_JJ method_NN as_IN in_IN -LRB-_-LRB- 21_CD -RRB-_-RRB- to_TO define_VB the_DT labels_NNS y_NN for_IN a_DT second-order_JJ Markov_NNP dependency_NN between_IN chunk_NN tags_NNS ,_, and_CC the_DT feature_NN functions_NNS that_WDT encode_VBP the_DT pairwise_JJ dependency_NN between_IN labels_NNS and_CC the_DT
0008_CD ..._: $_$ 10.00_CD ._.
1_CD ._.
INTRODUCTION_NNP Markov_NNP random_JJ fields_NNS -LRB-_-LRB- MRFs_NNS -RRB-_-RRB- are_VBP undirected_JJ graphical_JJ models_NNS and_CC have_VBP been_VBN widely_RB used_VBN in_IN an_DT ever-growing_JJ variety_NN of_IN applications_NNS ,_, including_VBG natural_JJ language_NN processing_NN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_JJ -_: ,_, data_NNS mining_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, signal_NN processing_NN -LRB-_-LRB- 29_CD -RRB-_-RRB- ,_, etc._NN ._.
Conditional_JJ random_JJ fields_NNS -LRB-_-LRB- CRFs_NNS -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- are_VBP special_JJ MRFs_NNS that_WDT are_VBP globally_RB conditioned_VBN on_IN inputs_NNS and_CC have_VBP shown_VBN great_JJ promise_NN in_IN various_JJ applications_NNS -LRB-_-LRB- 21_CD
l_NN approaches_VBZ to_TO solving_VBG the_DT ℓ1-regularization_NN problem_NN is_VBZ provided_VBN in_IN -LRB-_-LRB- 20_CD -RRB-_-RRB- ._.
Finally_RB ,_, for_IN learning_VBG structures_NNS of_IN the_DT special_JJ Gaussian_NNP Markov_NNP random_JJ fields_NNS -LRB-_-LRB- GMRFs_NNS -RRB-_-RRB- ,_, inverse_JJ covariance_NN estimation_NN methods_NNS =_JJ -_: =[_NN 2_CD ,_, 32_CD ,_, 3_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP been_VBN developed_VBN based_VBN on_IN the_DT ℓ1-norm_NN penalized_VBD maximum_NN likelihood_NN estimation_NN ._.
For_IN the_DT structure_NN learning_NN of_IN directed_JJ Bayesian_JJ networks_NNS ,_, the_DT structural_JJ EM_NN -LRB-_-LRB- SEM_NN -RRB-_-RRB- algorithm_NN -LRB-_-LRB- 4_CD ,_, 5_CD -RRB-_-RRB- has_VBZ a_DT similar_JJ proc_NN
res_NNS fixed_VBN ._.
Like_IN Grafting_NNP ,_, this_DT method_NN relies_VBZ on_IN a_DT greedy_JJ sub-step_NN of_IN fully_RB optimizing_VBG over_IN free_JJ parameters_NNS ,_, and_CC thus_RB is_VBZ inefficient_JJ for_IN MRFs_NNS and_CC may_MD under-fit_VB the_DT data_NNS ._.
Other_JJ incremental_JJ methods_NNS like_IN =_JJ -_: =[_NN 18_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: are_VBP even_RB less_RBR efficient_JJ than_IN Grafting_VBG even_RB when_WRB some_DT heuristics_NNS are_VBP used_VBN as_IN in_IN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, because_IN at_IN each_DT iteration_NN they_PRP need_VBP to_TO estimate_VB the_DT likelihood_NN gain_NN for_IN each_DT candidate_NN feature_NN ,_, which_WDT depends_VBZ on_IN a_DT
feature_NN selection_NN problem_NN ._.
ℓ1-norm_NN regularized_VBD maximum_NN likelihood_NN estimation_NN -LRB-_-LRB- MLE_NN -RRB-_-RRB- is_VBZ among_IN the_DT most_RBS popular_JJ approaches_NNS to_TO selecting_NN features_NNS in_IN CRFs_NNS -LRB-_-LRB- or_CC MRFs_NNS in_IN general_JJ -RRB-_-RRB- and_CC has_VBZ shown_VBN great_JJ promise_NN =_JJ -_: =[_NN 1_CD ,_, 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT sparsity_NN of_IN the_DT ℓ1-norm_NN regularized_VBN MLE_NNP is_VBZ due_JJ to_TO the_DT singularity_NN of_IN the_DT ℓ1-norm_NN at_IN the_DT origin_NN -LRB-_-LRB- 24_CD -RRB-_-RRB- ._.
Another_DT important_JJ problem_NN we_PRP consider_VBP is_VBZ the_DT structure_NN learning_NN of_IN MRFs_NNS ._.
As_IN the_DT variety_NN and_CC s_NN
h_NN that_WDT incrementally_RB changes_VBZ the_DT model_NN structure_NN by_IN adding_VBG or_CC deleting_VBG edges_NNS ._.
The_DT adding_VBG or_CC deleting_VBG operation_NN is_VBZ guided_VBN towards_IN an_DT improvement_NN of_IN some_DT objective_JJ function_NN ,_, such_JJ as_IN marginal_JJ likelihood_NN =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
As_IN the_DT search_NN is_VBZ local_JJ and_CC greedy_JJ ,_, the_DT learned_VBN network_NN is_VBZ -LRB-_-LRB- at_IN best_JJS -RRB-_-RRB- a_DT local_JJ optimum_NN of_IN a_DT penalized_VBN likelihood_NN score_NN ._.
Recently_RB ,_, structure_NN learning_NN of_IN MRFs_NNS has_VBZ been_VBN formulated_VBN as_IN a_DT convex_NN program_NN that_IN
is_VBZ desirable_JJ to_TO do_VB feature_NN selection_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- and_CC pursue_VB a_DT sparse_JJ representation_NN of_IN such_JJ models_NNS that_WDT leaves_VBZ out_RP irrelevant_JJ features_NNS ._.
Since_IN the_DT problem_NN of_IN selecting_VBG an_DT optimal_JJ subset_NN of_IN features_NNS is_VBZ NP-hard_NN =_JJ -_: =[_NN 28_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT popular_JJ solution_NN is_VBZ to_TO use_VB a_DT convex_NN relaxation_NN of_IN the_DT non-convex_JJ feature_NN selection_NN problem_NN ._.
ℓ1-norm_NN regularized_VBD maximum_NN likelihood_NN estimation_NN -LRB-_-LRB- MLE_NN -RRB-_-RRB- is_VBZ among_IN the_DT most_RBS popular_JJ approaches_NNS to_TO selectin_NN
