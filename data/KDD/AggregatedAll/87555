MetaCost_NNP :_: a_DT general_JJ method_NN for_IN making_VBG classifiers_NNS cost-sensitive_JJ
ins_VB a_DT set_NN of_IN weights_NNS over_IN the_DT training_NN example_NN ._.
Each_DT iteration_NN invokes_VBZ the_DT learning_VBG algorithm_NN to_TO minimize_VB the_DT weighted_JJ error_NN and_CC returns_VBZ a_DT hypothesis_NN ,_, which_WDT is_VBZ used_VBN in_IN a_DT final_JJ weighted_JJ vote_NN ._.
MetaCost_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ an_DT algorithm_NN that_WDT implements_VBZ cost-sensitive_JJ classification_NN ._.
Instead_RB of_IN modifying_VBG an_DT error_NN minimization_NN classification_NN procedure_NN ,_, it_PRP views_VBZ the_DT classifier_NN as_IN a_DT black_JJ box_NN ,_, the_DT same_JJ as_IN we_PRP do_VBP ,_, and_CC wrap_VB
tances_NNS of_IN the_DT major_JJ class_NN in_IN the_DT training_NN set_NN is_VBZ artificially_RB reduced_VBN ._.
Another_DT possible_JJ alternative_NN is_VBZ to_TO assign_VB higher_JJR error_NN costs_NNS to_TO misclassification_NN of_IN minor_JJ class_NN instances_NNS -LRB-_-LRB- Chawla_NNP et_FW al._FW ,_, 2004_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
7_CD Conclusion_NN In_IN this_DT paper_NN ,_, we_PRP have_VBP described_VBN the_DT Argument_NN Selection_NN and_CC Coercion_NN task_NN for_IN SemEval-2_NN ,_, to_TO be_VB held_VBN in_IN 2010_CD ._.
This_DT task_NN involves_VBZ the_DT identifying_VBG the_DT relation_NN between_IN a_DT predicate_NN and_CC its_PRP$
analysis_NN of_IN algorithms_NNS that_WDT produce_VBP minimum_NN costclassifiers_NNS and_CC do_VBP not_RB necessarily_RB maximize_VB the_DT objective_NN is_VBZ to_TO re-balance_VB -LRB-_-LRB- ``_`` stratify_NN ''_'' -RRB-_-RRB- the_DT training_NN set_NN given_VBN to_TO the_DT learning_NN algorithm_NN -LRB-_-LRB- Elkan_NNP ,_, 2001_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ,_, Chan_NNP and_CC Stolfo_NNP ,_, 1998_CD -RRB-_-RRB- ._.
Often_RB in_IN real-world_JJ applications_NNS ,_, the_DT misclassifications_NNS costs_NNS and_CC class_NN distributions_NNS are_VBP imprecise_JJ or_CC change_NN from_IN situation_NN to_TO situation_NN -LRB-_-LRB- Zahavi_NNP and_CC Levin_NNP ,_, 1997_CD ;_: Friedman_NNP a_DT
on_IN -LRB-_-LRB- 15_CD -RRB-_-RRB- ._.
Most_JJS of_IN the_DT research_NN addressing_VBG this_DT problem_NN can_MD be_VB classified_VBN into_IN three_CD categories_NNS ._.
One_CD consists_VBZ of_IN assigning_VBG distinct_JJ costs_NNS to_TO the_DT classification_NN errors_NNS for_IN positive_JJ and_CC negative_JJ examples_NNS =_JJ -_: =[_NN 4_CD ,_, 7_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT second_NN is_VBZ to_TO resample_VB the_DT original_JJ training_NN set_NN ,_, either_CC by_IN over-sampling_VBG the_DT minority_NN class_NN -LRB-_-LRB- 3_CD ,_, 11_CD -RRB-_-RRB- and\/or_CC undersampling_VBG the_DT majority_NN class_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- until_IN the_DT classes_NNS are_VBP approximately_RB equally_RB repre_JJ
methods_NNS measured_VBN by_IN the_DT maximum_JJ total_JJ profit_NN in_IN two_CD realworld_NN datasets_NNS -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
The_DT third_JJ approach_NN ,_, called_VBN ``_`` Relabeling_NNP ''_'' ,_, relabels_VBZ the_DT classes_NNS of_IN instances_NNS by_IN applying_VBG the_DT minimum_NN expected_VBD cost_NN criterion_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
MetaCost_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- belongs_VBZ to_TO this_DT approach_NN ._.
MetaCost_NNP uses_VBZ bagging_VBG as_IN the_DT ensemble_NN method_NN ._.
The_DT forth_RB approach_NN ,_, called_VBN ``_`` Weighting_NNP ''_'' -LRB-_-LRB- 18_CD -RRB-_-RRB- ,_, induces_VBZ costsensitivity_NN by_IN integrating_VBG the_DT instances_NNS '_POS weights_NNS direct_VBP
loan_NN for_IN a_DT customer_NN ._.
We_PRP want_VBP to_TO able_JJ to_TO structure_VB the_DT loan_NN amount_NN and_CC the_DT interest_NN rate_NN for_IN a_DT customer_NN based_VBN on_IN the_DT propensity_NN to_TO default_NN ._.
Our_PRP$ work_NN builds_VBZ upon_IN the_DT cost-sensitive_JJ learning_NN literature_NN =_JJ -_: =[_NN 5_CD ,_, 11_CD ,_, 4_CD ,_, 7_CD ,_, 12_CD -RRB-_-RRB- -_: =_JJ -_: and_CC the_DT relevant_JJ literature_NN from_IN finance_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- ._.
Stein_NNP -LRB-_-LRB- 10_CD -RRB-_-RRB- extends_VBZ a_DT cut-off_JJ score_NN based_VBN approach_NN to_TO a_DT pricing_NN approach_NN resulting_VBG in_IN a_DT more_RBR flexible_JJ and_CC profitable_JJ model_NN ._.
Using_VBG the_DT ROC_NN curve_NN quantitie_NN
he_PRP AdaCost_NNP algorithm_NN which_WDT extends_VBZ the_DT AdaBoost_NNP algorithm_NN giving_VBG weights_NNS to_TO individual_JJ training_NN examples_NNS instead_RB of_IN classes_NNS ._.
--_: Using_VBG Bayes_NNP risk_NN theory_NN to_TO assign_VB each_DT example_NN to_TO its_PRP$ lowest_JJS risk_NN class_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP 1999_CD -_: =_JJ -_: ;_: Margineantu_NNP 2002_CD ;_: Zadrozny_NNP and_CC Elkan_NNP 2001_CD -RRB-_-RRB- ._.
--_: Changing_VBG the_DT class_NN distributions_NNS in_IN the_DT training_NN set_VBD such_JJ that_IN the_DT cost-insensitive_JJ classifier_NN learned_VBN will_MD perform_VB equally_RB to_TO a_DT cost_NN sensitive_JJ classifi_NNS
heaper_JJR than_IN the_DT cost_NN of_IN misclassifying_VBG an_DT actual_JJ terrorist_NN who_WP carries_VBZ a_DT bomb_NN to_TO a_DT flight_NN ._.
Cost_NN is_VBZ not_RB necessarily_RB monetary_JJ ,_, for_IN examples_NNS ,_, it_PRP can_MD be_VB a_DT waste_NN of_IN time_NN or_CC even_RB the_DT severity_NN of_IN an_DT illness_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN our_PRP$ studies_NNS ,_, the_DT cost_NN of_IN misclassifying_VBG the_DT actual_JJ ''_'' fail_VB students_NNS ''_'' to_TO ''_'' pass_NN students_NNS ''_'' -LRB-_-LRB- so_IN we_PRP can_MD not_RB help_VB them_PRP ,_, consequently_RB ,_, they_PRP will_MD be_VB expelled_VBN from_IN the_DT university_NN -RRB-_-RRB- is_VBZ much_RB costlier_JJR than_IN the_DT
final_JJ model_NN with_IN minimum_JJ expected_JJ costs_NNS ._.
•_VB The_DT second_JJ method_NN for_IN CSL_NNP ,_, instead_RB of_IN assuming_VBG that_IN we_PRP know_VBP the_DT cost_NN ratio_NN -LRB-_-LRB- or_CC cost_NN matrix_NN -RRB-_-RRB- before_IN learning_VBG as_IN in_IN the_DT first_JJ method_NN and_CC other_JJ previous_JJ works_NNS -LRB-_-LRB- =_JJ -_: =[_NN 30_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 25_CD -RRB-_-RRB- -RRB-_-RRB- or_CC setting_VBG the_DT cost_NN ratio_NN by_IN inverting_VBG of_IN prior_JJ class_NN distributions_NNS -LRB-_-LRB- -LRB-_-LRB- 31_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 32_CD -RRB-_-RRB- -RRB-_-RRB- ,_, we_PRP treat_VBP this_DT number_NN as_IN a_DT hyperparameter_NN ,_, optimize_VB it_PRP locally_RB and_CC then_RB train_VB the_DT final_JJ models_NNS ._.
Our_PRP$
ng_NN additional_JJ methods_NNS for_IN each_DT of_IN the_DT four_CD substeps_NNS in_IN our_PRP$ approach_NN ._.
For_IN example_NN ,_, in_IN step_NN 1b_NN ,_, to_TO obtain_VB a_DT better_JJR CSS_NNP 's_POS classifier_NN ,_, we_PRP are_VBP investigating_VBG other_JJ ways_NNS ,_, such_JJ as_IN cost-sensitive_JJ classifiers_NNS -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Ting_NNP ,_, 2002_CD ;_: Turney_NNP ,_, 2000_CD -RRB-_-RRB- ,_, to_TO improve_VB the_DT performance_NN of_IN classifier_NN without_IN changing_VBG the_DT class_NN distribution_NN ._.
We_PRP are_VBP also_RB investigating_VBG the_DT features_NNS used_VBN for_IN classification_NN in_IN step_NN 1b_NN ,_, by_IN considering_VBG
;_: Liu_NNP et_FW al._FW ,_, 2009_CD -RRB-_-RRB- ._.
A_DT different_JJ branch_NN of_IN work_NN investigates_VBZ the_DT application_NN of_IN non-uniform_JJ misclassification_NN costs_NNS during_IN training_NN in_IN order_NN to_TO give_VB additional_JJ consideration_NN to_TO the_DT class_NN of_IN interest_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
There_EX has_VBZ been_VBN some_DT work_NN on_IN active_JJ learning_NN on_IN skewed_JJ data_NNS ._.
Tomanek_NNP and_CC Hahn_NNP 2009_CD investigate_VB Query_NNP By_IN Committee-based_JJ approaches_NNS to_TO sampling_NN labeled_JJ sentences_NNS for_IN the_DT task_NN of_IN named_VBN entity_NN recognit_NN
;_: Liu_NNP et_FW al._FW ,_, 2009_CD -RRB-_-RRB- ._.
A_DT different_JJ branch_NN of_IN work_NN investigates_VBZ the_DT application_NN of_IN non-uniform_JJ misclassification_NN costs_NNS during_IN training_NN in_IN order_NN to_TO give_VB additional_JJ consideration_NN to_TO the_DT class_NN of_IN interest_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
There_EX has_VBZ been_VBN some_DT work_NN on_IN active_JJ learning_NN on_IN skewed_JJ data_NNS ._.
Tomanek_NNP and_CC Hahn_NNP 2009_CD investigate_VB Query_NNP By_IN Committee-based_JJ approaches_NNS to_TO sampling_NN labeled_JJ sentences_NNS for_IN the_DT task_NN of_IN named_VBN entity_NN recognit_NN
jority_NN class_NN -LRB-_-LRB- 9_CD ,_, 16_CD -RRB-_-RRB- ._.
A_DT different_JJ branch_NN of_IN work_NN investigates_VBZ the_DT application_NN of_IN non-uniform_JJ misclassification_NN costs_NNS during_IN training_NN in_IN order_NN to_TO give_VB additional_JJ consideration_NN to_TO the_DT class_NN of_IN interest_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
There_EX has_VBZ been_VBN some_DT work_NN on_IN active_JJ learning_NN on_IN skewed_JJ data_NNS ._.
Tomanek_NN -LRB-_-LRB- 25_CD -RRB-_-RRB- investigates_VBZ Query_JJ By_IN Committee-based_JJ approaches_NNS to_TO sampling_NN labeled_JJ sentences_NNS for_IN the_DT task_NN of_IN named_VBN entity_NN recognition_NN ._.
The_DT
roup_NN --_: post-processing_JJ method_NN ,_, which_WDT mainly_RB relies_VBZ on_IN the_DT posterior_JJ probabilities_NNS produced_VBN by_IN the_DT classifiers_NNS ._.
Many_JJ papers_NNS have_VBP been_VBN published_VBN in_IN this_DT area_NN such_JJ as_IN meta_NN cost-sensitive_JJ learning_NN -LRB-_-LRB- CSL_NN -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 11_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Most_JJS of_IN them_PRP have_VBP applied_VBN CSL_NNP to_TO C4_NN .5_CD -LRB-_-LRB- -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 9_CD -RRB-_-RRB- -RRB-_-RRB- ,_, Naive_JJ Bayes_NNS -LRB-_-LRB- NB_NN -RRB-_-RRB- -LRB-_-LRB- -LRB-_-LRB- 12_CD -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC support_NN vector_NN machines_NNS -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
Since_IN the_DT methods_NNS in_IN this_DT group_NN are_VBP based_VBN on_IN posterior_JJ probabilities_NNS ,_, it_PRP is_VBZ
ction_NN on_IN attributes_NNS and_CC try_VB to_TO find_VB trees_NNS that_WDT combine_VBP high_JJ accuracy_NN with_IN low_JJ cost_NN of_IN the_DT attributes_NNS they_PRP contain_VBP -LRB-_-LRB- 11_CD -RRB-_-RRB- ;_: some_DT take_VBP different_JJ misclassification_NN costs_NNS into_IN account_NN when_WRB building_VBG the_DT tree_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ;_: some_DT do_VBP not_RB aim_VB for_IN the_DT highest_JJS accuracy_NN but_CC for_IN balanced_JJ precision-recall_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- ;_: etc._NN ._.
The_DT fact_NN that_IN researchers_NNS have_VBP developed_VBN such_JJ learning_VBG algorithms_NNS shows_VBZ that_IN users_NNS sometimes_RB do_VBP have_VB more_JJR specif_NN
observes_VBZ that_IN we_PRP should_MD expect_VB the_DT behavior_NN of_IN such_JJ algorithms_NNS to_TO be_VB more_RBR symmetric_JJ around_IN the_DT axis_NN where_WRB the_DT ratio_NN of_IN costs_NNS of_IN misclassification_NN is_VBZ unity_NN ._.
In_IN the_DT bagging_NN approach_NN taken_VBN by_IN MetaCost_NNP -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ,_, the_DT idea_NN is_VBZ to_TO resample_VB the_DT data_NNS several_JJ times_NNS and_CC apply_VB a_DT base_NN learner_NN to_TO each_DT sample_NN to_TO generate_VB alternative_JJ decision_NN trees_NNS ._.
The_DT decisions_NNS made_VBN on_IN each_DT example_NN by_IN the_DT alternative_JJ trees_NNS are_VBP combin_NN
More_RBR recently_RB ,_, as_IN research_NN into_IN cost_NN sensitivity_NN and_CC class_NN imbalance_NN have_VBP become_VBN more_RBR prevalent_JJ ,_, C4_NN .5_NN combined_VBN with_IN under-sampling_JJ or_CC over-sampling_JJ is_VBZ quickly_RB becoming_VBG the_DT accepted_VBN baseline_NN to_TO beat_VB -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Pazzani_NNP et_FW al._FW ,_, 1994_CD -RRB-_-RRB- ._.
Using_VBG our_PRP$ own_JJ performance_NN analysis_NN technique_NN ,_, called_VBN cost_NN curves_NNS -LRB-_-LRB- Drummond_NNP &_CC Holte_NNP ,_, 2000a_CD -RRB-_-RRB- ,_, discussed_VBN briefly_NN in_IN the_DT next_JJ section_NN ,_, we_PRP show_VBP that_IN undersampling_NN produces_VBZ a_DT reason_NN
ressed_VBN in_IN the_DT framework_NN of_IN cost-sensitive_JJ classification_NN and_CC S._NNP Zhang_NNP and_CC R._NNP Jarvis_NNP -LRB-_-LRB- Eds_NNP ._. -RRB-_-RRB-
:_: AI_NN 2005_CD ,_, LNAI_NN 3809_CD ,_, pp_NN ._.
133_CD --_: 142_CD ,_, 2005_CD ._.
c_NN ○_CD Springer-Verlag_NNP Berlin_NNP Heidelberg_NNP 2005134_NNP Y._NNP Yang_NNP et_FW al._FW ranking_NN =_JJ -_: =[_NN 10,5,6,12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN example_NN ,_, this_DT was_VBD the_DT central_JJ theme_NN of_IN the_DT KDDCUP-98_NN competition_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
However_RB ,_, the_DT direct_JJ marketing_NN problem_NN in_IN telecommunication_NN industry_NN pose_VBP some_DT new_JJ challenges_NNS ._.
Similar_JJ to_TO the_DT traditional_JJ pro_NN
fier_NN ,_, which_WDT mimicks_VBZ the_DT performance_NN of_IN the_DT ensemble_NN classifier_NN ._.
This_DT approach_NN has_VBZ ,_, e.g._FW ,_, been_VBN advocated_VBN in_IN the_DT MetaCost_NNP algorithm_NN ,_, in_IN which_WDT bagging_NN is_VBZ used_VBN for_IN deriving_VBG improved_VBN probability_NN estimates_VBZ =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
A_DT different_JJ approach_NN is_VBZ to_TO employ_VB stacking_VBG at_IN the_DT meta_NN level_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
Stacking_VBG generates_VBZ a_DT global_JJ classifier_NN by_IN training_VBG a_DT meta-level_JJ learner_NN for_IN combining_VBG the_DT predictions_NNS of_IN the_DT base-level_JJ classifiers_NNS
at_IN +_CC fever_NN +_CC influenza_NN ._.
Given_VBN these_DT considerations_NNS and_CC unbalanced_JJ class_NN sizes_NNS ,_, classification_NN is_VBZ done_VBN using_VBG a_DT Bayesian-network_JJ classifier_NN with_IN MetaCost_NNP ,_, a_DT mechanism_NN for_IN making_VBG classifiers_NNS costsenstive_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Structure_NN learning_NN for_IN the_DT network_NN is_VBZ performed_VBN using_VBG K2_NN hill_NN climbing_NN and_CC the_DT results_NNS are_VBP based_VBN on_IN 4-fold_JJ cross-validation_NN ._.
Recall_VB ,_, Precision_NN and_CC F-measure_NN for_IN the_DT symptoms_NNS class_NN as_IN a_DT function_NN of_IN i_FW
han_JJ other_JJ types_NNS of_IN mistakes_NNS ._.
In_IN database_NN marketing_NN ,_, for_IN example_NN ,_, the_DT cost_NN of_IN mailing_VBG a_DT letter_NN to_TO a_DT non-respondent_JJ is_VBZ very_RB small_JJ ,_, but_CC the_DT cost_NN of_IN not_RB mailing_VBG to_TO a_DT respondent_NN is_VBZ the_DT entire_JJ profit_NN lost_VBN -LRB-_-LRB- =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
While_IN such_JJ non-uniform_JJ error_NN costs_NNS can_MD be_VB measured_VBN using_VBG loss_NN functions_NNS ,_, the_DT above_JJ definition_NN does_VBZ not_RB allow_VB them_PRP to_TO be_VB taken_VBN into_IN account_NN explicitly_RB when_WRB making_VBG a_DT prediction_NN ._.
Furthermore_RB ,_, there_EX is_VBZ
two_CD main_JJ categories_NNS ._.
The_DT first_JJ category_NN aims_NNS for_IN generic_JJ procedures_NNS that_WDT can_MD make_VB arbitrary_JJ classifiers_NNS cost_NN sensitive_JJ by_IN resorting_VBG to_TO Bayes_NNP risk_NN theory_NN or_CC some_DT other_JJ cost_NN minimization_NN strategy_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT second_JJ attempts_NNS to_TO extend_VB particular_JJ algorithms_NNS so_RB as_IN to_TO produce_VB cost-sensitive_JJ generalizations_NNS ._.
Of_IN interest_NN to_TO this_DT work_NN are_VBP classifiers_NNS obtained_VBN by_IN thresholding_VBG a_DT continuous_JJ function_NN ,_, here_RB den_NN
different_JJ misclassification_NN costs_NNS -LRB-_-LRB- Breiman_NNP ,_, Friedman_NNP ,_, Olshen_NNP ,_, &_CC Stone_NNP ,_, 1984_CD ;_: Pazzani_NNP ,_, Merz_NNP ,_, Murphy_NNP ,_, Ali_NNP ,_, Hume_NNP ,_, &_CC Brunk_NNP ,_, 1994_CD ;_: Provost_NNP &_CC Buchanan_NNP ,_, 1995_CD ;_: Bradford_NNP ,_, Kunz_NNP ,_, Kohavi_NNP ,_, Brunk_NNP ,_, &_CC Brodley_NNP ,_, 1998_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Elkan_NNP ,_, 2001_CD ;_: Zadrozny_NNP ,_, Langford_NNP ,_, &_CC Abe_NNP ,_, 2003_CD ;_: Lachiche_NNP &_CC Flach_NNP ,_, 2003_CD ;_: Abe_NNP ,_, Zadrozny_NNP ,_, &_CC Langford_NNP ,_, 2004_CD ;_: Vadera_NNP ,_, 2005_CD ;_: Margineantu_NNP ,_, 2005_CD ;_: Zhu_NNP ,_, Wu_NNP ,_, Khoshgoftaar_NNP ,_, &_CC Yong_NNP ,_, 2007_CD ;_: Sheng_NNP &_CC Ling_NNP ,_, 2007_CD -RRB-_-RRB- ._.
Thes_NNS
010_CD ,_, Association_NNP for_IN the_DT Advancement_NNP of_IN Artificial_NNP Intelligence_NNP -LRB-_-LRB- www.aaai.org_NN -RRB-_-RRB- ._.
All_DT rights_NNS reserved_VBN ._.
quired_VBN ._.
Consequently_RB ,_, many_JJ of_IN the_DT training_NN examples_NNS may_MD remain_VB unlabeled_JJ ._.
Cost-sensitive_JJ learning_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP 1999_CD -_: =_JJ -_: ;_: Fan_NN et_FW al._FW 1999_CD ;_: Elkan_NNP 2001_CD ;_: Ting_NNP 2002_CD ;_: Zadrozny_NNP ,_, Langford_NNP ,_, and_CC Abe_NN 2003_CD ;_: Zhou_NNP and_CC Liu_NNP 2006b_CD ;_: 2006a_CD ;_: Masnadi-Shirazi_NNP and_CC Vasconcelos_NNP 2007_CD ;_: Lozano_NNP and_CC Abe_NNP 2008_CD -RRB-_-RRB- aims_VBZ to_TO make_VB the_DT optimal_JJ decision_NN mini_NNS
Street_NNP ,_, Second_NNP Floor_NNP -_: Press_NNP Building_NNP ,_, Chicago_NNP ,_, IL_NN 60637_CD ._.
Bianca_NNP Zadrozny_NNP ,_, John_NNP Langfords_NNP ,_, Naoki_NNP Abe_NNP Mathematical_NNP Sciences_NNP Department_NNP IBM_NNP T._NNP J._NNP Watson_NNP Research_NNP Center_NNP Yorktown_NNP Heights_NNP ,_, NY_NNP 10598_CD class_NN =_JJ -_: =[_NN 2_CD ,_, 19_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT requires_VBZ estimating_NN class_NN membership_NN probabilities_NNS and_CC ,_, in_IN the_DT case_NN where_WRB costs_NNS are_VBP nondeterministic_JJ ,_, also_RB requires_VBZ estimating_VBG expected_VBN costs_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ._.
The_DT third_JJ category_NN concerns_NNS methods_NNS for_IN conve_NN
similarity_NN scores_NNS ._.
If_IN only_RB the_DT performance_NN at_IN a_DT very_RB low_JJ false-positive_JJ rate_NN matters_NNS ,_, TWEAK_NNP can_MD still_RB be_VB easily_RB adjusted_VBN by_IN modifying_VBG the_DT loss_NN function_NN using_VBG techniques_NNS such_JJ as_IN training_NN with_IN utility_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Morik_NNP et_FW al._FW ,_, 1999_CD -RRB-_-RRB- ._.
5_CD Related_NNP Work_NNP Our_NNP term-weighting_JJ learning_NN framework_NN can_MD be_VB analogous_JJ to_TO the_DT ``_`` Siamese_NNP ''_'' architecture_NN for_IN learning_VBG jointly_RB two_CD neural_JJ networks_NNS that_WDT share_VBP the_DT same_JJ set_NN of_IN model_NN wei_NN
and_CC that_DT of_IN classifying_VBG sick_JJ people_NNS as_IN healthy_JJ are_VBP apparently_RB not_RB equal_JJ ,_, since_IN the_DT latter_NN leads_VBZ to_TO serious_JJ results_NNS ._.
Moreover_RB ,_, the_DT degree_NN of_IN seriousness_NN differs_VBZ among_IN patients_NNS ._.
Cost-sensitive_JJ learning_NN =_JJ -_: =[_NN 4_CD ,_, 3_CD ,_, 10_CD ,_, 5_CD ,_, 11_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ a_DT suitable_JJ framework_NN for_IN such_JJ cases_NNS where_WRB costs_NNS are_VBP different_JJ among_IN data_NNS ,_, and_CC the_DT amounts_NNS of_IN them_PRP are_VBP unknown_JJ at_IN the_DT stage_NN of_IN prediction_NN ._.
Wider_JJR range_NN of_IN problems_NNS can_MD be_VB treated_VBN in_IN the_DT framework_NN si_NN
le_DT customers_NNS who_WP actually_RB churn_VBP can_MD be_VB only_RB a_DT small_JJ fraction_NN of_IN the_DT customers_NNS who_WP stay_VBP ._.
In_IN the_DT past_NN ,_, many_JJ researchers_NNS have_VBP tackled_VBN the_DT direct_JJ marketing_NN problem_NN as_IN a_DT classification_NN problem_NN -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 27_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, where_WRB the_DT cost-sensitive_JJ and_CC unbalanced_JJ nature_NN of_IN the_DT problem_NN is_VBZ taken_VBN into_IN account_NN ._.
In_IN management_NN and_CC marketing_NN sciences_NNS ,_, stochastic_JJ models_NNS are_VBP used_VBN to_TO describe_VB the_DT response_NN behavior_NN of_IN custo_NN
st-sensitive_JJ learning_NN and_CC decision_NN making_NN ._.
Various_JJ authors_NNS have_VBP noted_VBN the_DT limitations_NNS of_IN classic_JJ supervised_JJ learning_NN methods_NNS when_WRB the_DT acquired_VBN rules_NNS are_VBP used_VBN for_IN cost-sensitive_JJ decision_NN making_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 13_CD ,_, 4_CD ,_, 5_CD ,_, 17_CD ,_, 8_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
A_DT number_NN of_IN cost-sensitive_JJ learning_NN methods_NNS have_VBP been_VBN developed_VBN -LRB-_-LRB- 4_CD ,_, 17_CD ,_, 6_CD -RRB-_-RRB- that_WDT have_VBP been_VBN shown_VBN to_TO be_VB superior_JJ to_TO traditional_JJ classification-based_JJ methods_NNS ._.
However_RB ,_, these_DT cost-sensitive_JJ methods_NNS onl_VBP
ROC_NN points_NNS ._.
For_IN example_NN ,_, a_DT classifier_NN that_WDT produces_VBZ probabilities_NNS of_IN an_DT example_NN being_VBG in_IN each_DT class_NN ,_, such_JJ as_IN a_DT Naive_JJ Bayes_NNP classifier_NN ,_, can_MD have_VB a_DT threshold_NN parameter_NN biasing_VBG the_DT final_JJ class_NN selection_NN =_JJ -_: =[_NN 3_CD ,_, 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Plotting_VBG all_PDT the_DT ROC_NN points_NNS that_WDT can_MD be_VB produced_VBN by_IN varying_VBG these_DT parameters_NNS produces_VBZ an_DT ROC_NN curve_NN for_IN the_DT classifier_NN ._.
Typically_RB this_DT is_VBZ a_DT discrete_JJ set_NN of_IN points_NNS ,_, including_VBG -LRB-_-LRB- 0,0_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 1,1_CD -RRB-_-RRB- ,_, which_WDT ar_VBP
work_NN has_VBZ been_VBN published_VBN on_IN either_DT problem_NN ._.
There_EX exist_VBP several_JJ dozen_NN articles_NNS in_IN which_WDT techniques_NNS for_IN cost-sensitive_JJ learning_NN are_VBP suggested_VBN -LRB-_-LRB- Turney_NNP ,_, 1996_CD -RRB-_-RRB- ,_, but_CC few_JJ studies_NNS evaluate_VBP and_CC compare_VBP them_PRP -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Pazzani_NNP ,_, Merz_NNP ,_, Murphy_NNP ,_, Ali_NNP ,_, Hume_NNP ,_, &_CC Brunk_NNP ,_, 1994_CD ;_: Provost_NNP ,_, Fawcett_NNP ,_, &_CC Kohavi_NNP ,_, 1998_CD -RRB-_-RRB- ._.
The_DT literature_NN provides_VBZ even_RB less_JJR guidance_NN in_IN situations_NNS where_WRB distributions_NNS are_VBP imprecise_JJ or_CC can_MD change_VB ._.
Given_VBN an_DT
suring_VBG the_DT relationship_NN among_IN these_DT factors_NNS ._.
Section_NN 3_CD describes_VBZ the_DT cost-sensitive_JJ version_NN of_IN the_DT RIPPER_NN rule_NN learning_NN algorithm_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- which_WDT produces_VBZ low_JJ computational_JJ cost_NN detection_NN rules_NNS ,_, a_DT MetaCost_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: procedure_NN that_WDT can_MD be_VB used_VBN to_TO minimize_VB damage_NN costs_NNS and_CC response_NN costs_NNS ,_, and_CC our_PRP$ experiments_NNS in_IN using_VBG these_DT algorithms_NNS ._.
Section_NN 4_CD describes_VBZ our_PRP$ rule_NN translation_NN system_NN ,_, and_CC the_DT experiments_NNS in_FW implemen_FW
at_IN slope_NN ._.
The_DT machine_NN learning_NN community_NN has_VBZ addressed_VBN the_DT issue_NN of_IN class_NN imbalance_NN in_IN two_CD ways_NNS ._.
One_CD is_VBZ to_TO assign_VB distinct_JJ costs_NNS to_TO training_NN examples_NNS -LRB-_-LRB- Pazzani_NNP ,_, Merz_NNP ,_, Murphy_NNP ,_, Ali_NNP ,_, Hume_NNP ,_, &_CC Brunk_NNP ,_, 1994_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
The_DT other_JJ is_VBZ to_TO re-sample_VB the_DT original_JJ dataset_NN ,_, either_CC by_IN oversampling_VBG the_DT minority_NN class_NN and\/or_CC under-sampling_JJ the_DT majority_NN class_NN -LRB-_-LRB- Kubat_NNP &_CC Matwin_NNP ,_, 1997_CD ;_: Japkowicz_NNP ,_, 2000_CD ;_: Lewis_NNP &_CC Catlett_NNP ,_, 1994_CD ;_: Lin_NNP
ation_NN -LRB-_-LRB- CSC_NNP -RRB-_-RRB- ._.
CSC_NNP is_VBZ a_DT task_NN in_IN which_WDT each_DT training_NN instance_NN has_VBZ a_DT vector_NN of_IN misclassification_NN costs_NNS associated_VBN with_IN it_PRP ,_, thus_RB rendering_VBG some_DT mistakes_NNS on_IN some_DT instances_NNS to_TO be_VB more_RBR expensive_JJ than_IN others_NNS -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
Compared_VBN to_TO a_DT standard_JJ pipeline_NN ,_, SEARN_NN is_VBZ able_JJ to_TO 49_CD Proceedings_NNP of_IN the_DT Fifteenth_NNP Conference_NNP on_IN Computational_NNP Natural_NNP Language_NNP Learning_NNP ,_, pages_NNS 49_CD --_: 57_CD ,_, Portland_NNP ,_, Oregon_NNP ,_, USA_NNP ,_, 23_CD --_: 24_CD June_NNP 2011_CD ._.
c_NN ○_NN 2011_CD
on_IN to_TO fit_VB logistic_JJ regression_NN curves_NNS to_TO the_DT outputs_NNS to_TO allow_VB for_IN precision-recall_JJ analysis_NN ._.
To_TO minimize_VB false_JJ positives_NNS ,_, while_IN tolerating_VBG false_JJ negatives_NNS ,_, DEviaNT_JJ employs_VBZ the_DT MetaCost_FW metaclassifier_FW -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ,_, which_WDT uses_VBZ bagging_VBG to_TO reclassify_VB the_DT training_NN data_NNS to_TO produce_VB a_DT single_JJ cost-sensitive_JJ classifier_NN ._.
DEviaNT_JJ sets_NNS the_DT cost_NN of_IN a_DT false_JJ positive_NN to_TO be_VB 100_CD times_NNS that_IN of_IN a_DT false_JJ negative_NN ._.
4_CD Evaluation_NN Th_NN
+_CC fever_NN +_CC influenza_NN ._.
Given_VBN these_DT considerations_NNS and_CC unbalanced_JJ class_NN sizes_NNS ,_, classification_NN is_VBZ done_VBN using_VBG a_DT Bayesian-network_JJ classifier_NN with_IN MetaCost_NNP ,_, a_DT mechanism_NN for_IN making_VBG classifiers_NNS cost-sensitive_JJ =_JJ -_: =[_NN 27_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Structure_NN learning_NN for_IN the_DT network_NN is_VBZ performed_VBN using_VBG K2_NN hill_NN climbing_NN and_CC the_DT results_NNS are_VBP based_VBN on_IN 4-fold_JJ crossvalidation_NN ._.
Recall_VB ,_, Precision_NN and_CC F-measure_NN for_IN the_DT symptoms_NNS class_NN as_IN a_DT function_NN of_IN in_IN
ensitive_JJ learning_NN and_CC decision_NN making_NN ,_, in_IN the_DT data_NNS mining_NN community_NN ._.
Various_JJ authors_NNS have_VBP noted_VBN the_DT limitations_NNS of_IN classic_JJ supervised-learning_JJ methods_NNS for_IN use_NN in_IN cost_NN sensitive_JJ decision_NN making_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 13_CD ,_, 4_CD ,_, 5_CD ,_, 17_CD ,_, 8_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, and_CC a_DT number_NN of_IN cost_NN sensitive_JJ learning_NN methods_NNS have_VBP been_VBN developed_VBN -LRB-_-LRB- 4_CD ,_, 17_CD ,_, 6_CD -RRB-_-RRB- that_IN out-perform_JJ classification_NN based_VBN methods_NNS ._.
The_DT problem_NN of_IN optimizing_VBG a_DT sequence_NN of_IN cost-sensitive_JJ decisions_NNS ,_, howev_NN
r_NN than_IN filtering_VBG them_PRP based_VBN on_IN a_DT confidence_NN score_NN ._.
Cost_NN functions_NNS have_VBP been_VBN used_VBN in_IN non-structured_JJ classification_NN settings_NNS to_TO penalize_VB certain_JJ types_NNS of_IN errors_NNS more_JJR than_IN others_NNS -LRB-_-LRB- Chan_NNP and_CC Stolfo_NNP ,_, 1998_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Kiddon_NNP and_CC Brun_NNP ,_, 2011_CD -RRB-_-RRB- ._.
The_DT goal_NN of_IN optimizing_VBG our_PRP$ structured_JJ NER_NN model_NN for_IN recall_NN is_VBZ quite_RB similar_JJ to_TO the_DT scenario_NN explored_VBN by_IN Minkov_NNP et_FW al._FW -LRB-_-LRB- 2006_CD -RRB-_-RRB- ._.
To_TO trade_VB off_RP between_IN precision_NN and_CC recall_NN ,_, they_PRP
1_CD 4_CD ,_, 7_CD -RRB-_-RRB- ._.
Another_DT is_VBZ to_TO make_VB wrappers_NNS that_WDT convert_VBP existing_VBG cost-insensitive_JJ inductive_JJ learning_NN algorithms_NNS into_IN costsensitive_JJ ones_NNS ._.
This_DT method_NN is_VBZ called_VBN cost-sensitive_JJ meta-learning_NN ,_, such_JJ as_IN MetaCost_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Costing_VBG -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, and_CC CostSensitiveClassifier_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- ._.
Section_NN 2_CD provides_VBZ a_DT more_RBR complete_JJ review_NN of_IN cost-sensitive_JJ meta-learning_JJ approaches_NNS ._.
These_DT cost-sensitive_JJ meta-learning_JJ techniques_NNS are_VBP useful_JJ because_IN
pproaches_NNS ._.
The_DT most_RBS common_JJ is_VBZ cost-sensitive_JJ learning_NN in_IN which_WDT the_DT learning_NN of_IN an_DT underlying_JJ classifier_NN is_VBZ adjusted_VBN based_VBN on_IN predetermined_JJ misclassification_NN costs_NNS ._.
One_CD of_IN these_DT approaches_NNS is_VBZ MetaCost_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
MetaCost_NNP takes_VBZ in_IN training_NN data_NNS and_CC a_DT classification_NN algorithm_NN and_CC adjusts_VBZ the_DT learning_NN by_IN taking_VBG into_IN account_NN a_DT given_VBN cost_NN matrix_NN that_WDT assigns_VBZ punishments_NNS for_IN misclassifications_NNS and_CC rewards_NNS for_IN cor_NN
ute_NN costs_NNS -RRB-_-RRB- ,_, active_JJ learning_NN costs_NNS ,_, computation_NN cost_NN ,_, human-computer_JJ interaction_NN cost_NN ,_, and_CC so_RB on_RB ._.
Most_JJS previous_JJ works_NNS on_IN cost-sensitive_JJ learning_NN only_RB consider_VBP minimizing_VBG misclassification_NN costs_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 5_CD ,_, 20_CD ,_, 6_CD ,_, 18_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
A_DT few_JJ works_NNS do_VBP attempt_NN to_TO minimize_VB the_DT sum_NN of_IN 638Research_NNP Track_NNP Paper_NNP misclassification_NN costs_NNS and_CC attribute_NN costs_NNS -LRB-_-LRB- 21_CD ,_, 4_CD ,_, 24_CD ,_, 11_CD -RRB-_-RRB- ._.
However_RB ,_, these_DT works_NNS do_VBP not_RB consider_VB data_NN acquisition_NN cost_NN ._.
That_DT
women_NNS ._.
Dealing_VBG with_IN such_JJ imbalanced_JJ data_NNS can_MD be_VB done_VBN at_IN two_CD levels_NNS -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- ._.
At_IN the_DT algorithmic_JJ level_NN ,_, assuming_VBG all_DT errors_NNS have_VBP a_DT different_JJ cost_NN is_VBZ a_DT solution_NN to_TO guide_VB the_DT data_NNS mining_NN process_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, especially_RB in_IN the_DT medical_JJ field_NN where_WRB detecting_VBG an_DT high_JJ risk_NN profile_NN is_VBZ more_RBR informative_JJ than_IN detecting_VBG a_DT low_JJ risk_NN profile_NN ._.
At_IN the_DT data_NNS level_NN ,_, sampling_NN is_VBZ another_DT solution_NN ._.
A_DT first_JJ way_NN to_TO rebalance_VB
s_NNS involved_VBN in_IN inductive_JJ concept_NN learning_NN ._.
Two_CD costs_NNS ,_, misclassification_NN and_CC test_NN ,_, are_VBP particularly_RB relevant_JJ to_TO this_DT paper_NN ._.
Several_JJ researches_VBZ have_VBP considered_VBN misclassification_NN costs_NNS but_CC not_RB test_NN costs_NNS =_JJ -_: =[_NN 9,10,20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Other_JJ papers_NNS focus_VBP on_IN the_DT cost_NN of_IN tests_NNS ,_, but_CC fail_VBP to_TO take_VB into_IN account_NN misclassification_NN costs_NNS ._.
Yang_NNP and_CC Honavar_NNP -LRB-_-LRB- 45_CD -RRB-_-RRB- offer_VBP a_DT new_JJ subset_NN feature_NN selection_NN method_NN that_WDT employs_VBZ a_DT genetic_JJ algorithm_NN as_IN
real-world_JJ data_NNS are_VBP provided_VBN ._.
Related_JJ work_NN The_DT majority_NN of_IN published_VBN cost-sensitive_JJ classification_NN algorithms_NNS assume_VBP the_DT availability_NN of_IN supervised_JJ training_NN data_NNS ,_, were_VBD all_DT instances_NNS are_VBP labeled_VBN -LRB-_-LRB- e.g._FW =_JJ -_: =[_NN 9,12,24,32,35,38_CD ,_, 39_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Some_DT work_NN considers_VBZ semi-supervised_JJ cost-sensitive_JJ classification_NN ._.
Qin_NNP et_FW al._FW -LRB-_-LRB- 29_CD -RRB-_-RRB- present_JJ costsensitive_JJ classifiers_NNS for_IN training_NN data_NNS that_WDT consists_VBZ of_IN a_DT relatively_RB small_JJ number_NN of_IN labeled_JJ instance_NN
lthough_VB not_RB explicitly_RB called_VBN reductions_NNS are_VBP effectively_RB so_RB -LRB-_-LRB- Brown_NNP et_FW al._FW ,_, 2002_CD ;_: Brown_NNP and_CC Low_NNP ,_, 1996_CD ;_: Brown_NNP and_CC Zhao_NNP ,_, 2003_CD ;_: Chaudhuri_NNP and_CC Loh_NNP ,_, 2002_CD ;_: Cossock_NNP and_CC Zhang_NNP ,_, 2006_CD ;_: Cuevas_NNP and_CC Fraiman_NNP ,_, 1997_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Steinwart_NNP et_FW al._FW ,_, 2005_CD ;_: Tasche_NNP ,_, 2001_CD -RRB-_-RRB- ._.
Two_CD key_JJ differences_NNS between_IN the_DT recent_JJ machine_NN learning_NN reductions_NNS literature_NN and_CC the_DT present_JJ paper_NN is_VBZ that_IN our_PRP$ relationships_NNS between_IN problems_NNS are_VBP -LRB-_-LRB- usually_RB -RRB-_-RRB- ex_FW
e_LS ,_, -LRB-_-LRB- 2_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 4_CD -RRB-_-RRB- discussed_VBD neural_JJ networks_NNS for_IN cost-sensitive_JJ classification_NN ;_: -LRB-_-LRB- 5_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 6_CD -RRB-_-RRB- worked_VBD on_IN cost-sensitive_JJ evolutionary_JJ algorithm_NN ;_: -LRB-_-LRB- 7_CD -RRB-_-RRB- made_VBD support_NN vector_NN machines_NNS sensitive_JJ to_TO the_DT cost_NN ;_: -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 10_CD -RRB-_-RRB- focused_VBN on_IN the_DT ensemble_NN techniques_NNS such_JJ as_IN bagging_VBG and_CC boosting_VBG ;_: decision_NN tree_NN algorithms_NNS ,_, one_CD of_IN the_DT most_RBS popular_JJ machine_NN learning_NN techniques_NNS ,_, have_VBP also_RB been_VBN studied_VBN fo_NN cost-sensitivity_NN ._.
Cu_NN
e_LS MGINI_NNP ._.
3.11_CD Cost-sensitive_JJ and_CC ROC_NNP Analysis_NNP Features_NNPS In_IN many_JJ previous_JJ sections_NNS we_PRP have_VBP seen_VBN some_DT options_NNS related_VBN to_TO costs_NNS ._.
In_IN this_DT subsection_NN we_PRP are_VBP going_VBG to_TO briefly_RB explain_VB cost-sensitive_JJ learning_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: and_CC ROC_NN analysis_NN -LRB-_-LRB- 38_CD -RRB-_-RRB- and_CC which_WDT features_VBZ SMILES_NNP provides_VBZ around_IN these_DT items_NNS ._.
Accuracy_NN -LRB-_-LRB- or_CC error_NN -RRB-_-RRB- ,_, i.e._FW ,_, percentage_NN of_IN instances_NNS that_WDT are_VBP correctly_RB classified_VBN -LRB-_-LRB- respectively_RB incorrectly_RB classified_VBN -RRB-_-RRB- has_VBZ
sensitive_JJ and_CC to_TO modify_VB the_DT class_NN distribution_NN of_IN the_DT training_NN data_NNS set_VBN to_TO obtain_VB a_DT classifier_NN that_WDT adjusts_VBZ itself_PRP to_TO a_DT specific_JJ cost_NN matrix_NN and_CC the_DT class_NN distribution_NN of_IN the_DT test_NN set_VBN if_IN known_VBN -LRB-_-LRB- 16_CD -RRB-_-RRB- -LRB-_-LRB- 8_CD -RRB-_-RRB- =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, a_DT change_NN of_IN class_NN distribution_NN is_VBZ usually_RB done_VBN by_IN stratification_NN -LRB-_-LRB- or_CC re-balancing_NN -RRB-_-RRB- ,_, i.e._FW ,_, either_CC by_IN under-sampling_NN or_CC by_IN over-sampling_NN ._.
Stratification_NN presents_VBZ some_DT problems_NNS though_IN ._.
Weiss_NNP a_DT
ost_NN of_IN predicting_VBG class_NN i_FW when_WRB the_DT true_JJ class_NN is_VBZ j._VBN From_IN the_DT previous_JJ formula_NN ,_, as_IN we_PRP will_MD see_VB ,_, we_PRP can_MD establish_VB a_DT direct_JJ threshold_NN without_IN having_VBG any_DT extra_JJ data_NNS at_IN hand_NN ._.
In_IN fact_NN ,_, some_DT existing_VBG works_NNS -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN have_VBP used_VBN the_DT previous_JJ formula_NN to_TO establish_VB a_DT threshold_NN which_WDT generates_VBZ a_DT model_NN which_WDT is_VBZ cost_NN sensitive_JJ ._.
One_CD of_IN the_DT most_RBS adequate_JJ ways_NNS to_TO establish_VB a_DT class_NN threshold_NN is_VBZ based_VBN on_IN ROC_NN analysis_NN ._.
-LRB-_-LRB- Lachi_NN
ods_NNS developed_VBD to_TO explicitly_RB use_VB costs_NNS when_WRB constructing_VBG and_CC applying_VBG a_DT prediction_NN model_NN -LRB-_-LRB- 2023_CD -RRB-_-RRB- ,_, as_RB well_RB as_IN generic_JJ methods_NNS that_WDT act_VBP as_IN wrappers_NNS around_IN any_DT data_NNS mining_NN method_NN to_TO make_VB it_PRP cost_VB sensitive_JJ =_JJ -_: =[_NN 24_CD ,_, 25_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT medical_JJ domain_NN however_RB ,_, it_PRP is_VBZ often_RB difficult_JJ to_TO determine_VB the_DT relative_JJ cost_NN of_IN misclassification_NN ._.
Most_JJS physicians_NNS would_MD argue_VB that_IN failing_VBG to_TO identify_VB acute_JJ patients_NNS -LRB-_-LRB- members_NNS of_IN small_JJ positi_NNS
hes_NNS ,_, a_DT relation_NN on_IN the_DT order_NN of_IN 500:1_CD ,_, which_WDT is_VBZ a_DT significant_JJ problem_NN for_IN the_DT learning_NN stage_NN of_IN the_DT classifier_NN ._.
Three_CD techniques_NNS were_VBD considered_VBN to_TO overcome_VB this_DT shortcoming_NN ._.
The_DT approach_NN of_IN Domingos_NNP =_SYM -_: =[_NN 37_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ called_VBN MetaCost_NNP ,_, and_CC it_PRP consists_VBZ of_IN combining_VBG several_JJ instances_NNS of_IN the_DT classifier_NN instead_RB of_IN stratification_NN -LRB-_-LRB- modify_VB the_DT proportion_NN of_IN classes_NNS in_IN the_DT training_NN data_NNS according_VBG to_TO the_DT costs_NNS -RRB-_-RRB- ._.
This_DT met_VBD
er_JJR than_IN filtering_VBG them_PRP based_VBN on_IN a_DT confidence_NN score_NN ._.
Cost_NN functions_NNS have_VBP been_VBN used_VBN in_IN nonstructured_JJ classification_NN settings_NNS to_TO penalize_VB certain_JJ types_NNS of_IN errors_NNS more_JJR than_IN others_NNS -LRB-_-LRB- Chan_NNP and_CC Stolfo_NNP ,_, 1998_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Kiddon_NNP and_CC Brun_NNP ,_, 2011_CD -RRB-_-RRB- ._.
The_DT goal_NN of_IN optimizing_VBG our_PRP$ structured_JJ NER_NN model_NN for_IN recall_NN is_VBZ quite_RB similar_JJ to_TO the_DT scenario_NN explored_VBN by_IN Minkov_NNP et_FW al._FW -LRB-_-LRB- 2006_CD -RRB-_-RRB- ,_, as_IN noted_VBN above_IN ._.
8_CD Conclusion_NN We_PRP explored_VBD the_DT pro_NN
approach_NN is_VBZ used_VBN in_IN isolation_NN ._.
The_DT problem_NN of_IN cost-sensitive_JJ classification_NN has_VBZ been_VBN discussed_VBN several_JJ times_NNS in_IN the_DT literature_NN ;_: see_VB ,_, e.g._FW ,_, -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
A_DT relatively_RB recent_JJ contribution_NN was_VBD made_VBN by_IN Domingos_NNP =_SYM -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: ,_, who_WP discusses_VBZ a_DT general_JJ method_NN for_IN making_VBG learning_NN algorithms_NNS cost8_FW Informatica_FW 25_CD page_NN xxx_NN --_: yyy_NNP H._NNP Blockeel_NNP and_CC J._NNP Struyf_NNP Figure_NNP 6_CD :_: ROC_NN diagram_NN comparing_VBG submissions_NNS to_TO the_DT Predictive_JJ Toxicology_NN Ch_NN
estimated_VBN probability_NN ._.
Instead_RB of_IN costs_NNS ,_, the_DT expected_VBN utility_NN could_MD also_RB be_VB modeled_VBN ,_, or_CC both_DT ,_, utility_NN and_CC costs_NNS ._.
While_IN in_IN supervised_JJ scenarios_NNS classifiers_NNS can_MD be_VB optimized_VBN w.r.t._IN a_DT certain_JJ cost_NN model_NN =_JJ -_: =[_NN 15_CD ,_, 56_CD -RRB-_-RRB- -_: =_JJ -_: ,_, in_IN the_DT unsupervised_JJ scenario_NN of_IN outlier_NN detection_NN ,_, the_DT assumed_JJ cost-model_NN can_MD not_RB be_VB used_VBN to_TO fit_VB or_CC train_VB the_DT algorithm_NN but_CC only_RB to_TO evaluate_VB its_PRP$ results_NNS ._.
It_PRP should_MD be_VB noted_VBN ,_, though_RB ,_, that_IN while_IN calibr_NN
learning_VBG problem_NN ,_, we_PRP must_MD allow_VB different_JJ significance_NN levels_NNS to_TO be_VB specified_VBN for_IN each_DT possible_JJ classification_NN of_IN an_DT object_NN because_IN the_DT penalty_NN of_IN misclassification_NN is_VBZ not_RB the_DT same_JJ among_IN all_DT classes_NNS =_JJ -_: =[_NN 27,28_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT problem_NN can_MD be_VB viewed_VBN as_IN a_DT conditional_JJ inference_NN ._.
We_PRP extend_VBP our_PRP$ method_NN to_TO label_VB conditional_JJ CP_NN to_TO address_VB it_PRP ,_, which_WDT can_MD also_RB be_VB seen_VBN as_IN one_CD version_NN of_IN Mondrian_JJ CP_NN -LRB-_-LRB- MCP_NN -RRB-_-RRB- -LRB-_-LRB- 3,29_CD -RRB-_-RRB- ._.
An_DT important_JJ asp_NN
assume_VB that_IN no_DT adversary_NN is_VBZ present_JJ -LRB-_-LRB- i.e._FW ,_, A_NN -LRB-_-LRB- x_NN -RRB-_-RRB- =_JJ x_NN for_IN all_DT x_NN -RRB-_-RRB- ._.
We_PRP remove_VBP this_DT restriction_NN in_IN the_DT next_JJ sections_NNS ._.
Cost-sensitive_JJ learning_NN has_VBZ been_VBN the_DT object_NN of_IN substantial_JJ study_NN in_IN the_DT KDD_NNP literature_NN =_JJ -_: =[_NN 1_CD ,_, 27_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Given_VBN a_DT classification_NN utility_NN matrix_NN UC_NN -LRB-_-LRB- yC_NN ,_, y_NN -RRB-_-RRB- ,_, the_DT Bayes_NNP optimal_JJ prediction_NN for_IN an_DT instance_NN x_NN is_VBZ the_DT class_NN yC_NN that_WDT maximizes_VBZ the_DT conditional_JJ utility_NN U_NN -LRB-_-LRB- yC_NN |_CD x_NN -RRB-_-RRB- :_: i_LS =_JJ 1_CD U_NN -LRB-_-LRB- yC_NN |_CD x_NN -RRB-_-RRB- =_JJ ∑_NN P_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- UC_NN -LRB-_-LRB- yC_NN ,_, y_NN -RRB-_-RRB- -LRB-_-LRB- 4_LS -RRB-_-RRB- y_NN
e_LS positive_JJ examples_NNS -LRB-_-LRB- percentage_NN of_IN negative_JJ examples_NNS misclassified_VBN as_IN positive_JJ -RRB-_-RRB- ._.
One_CD advantage_NN of_IN this_DT formalization_NN is_VBZ to_TO naturally_RB accommodate_VB ill-balanced_JJ distributions_NNS and_CC cost-sensitive_JJ learning_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT ROC_NNP curve_NN depicts_VBZ the_DT tradeoff_NN between_IN both_DT objectives_NNS achieved_VBN by_IN a_DT learning_NN algorithm_NN and_CC represented_VBN in_IN the_DT False_JJ Positive_JJ ,_, True_JJ Positive_JJ plane_NN -LRB-_-LRB- Fig._NN 1_CD -RRB-_-RRB- ._.
The_DT ideal_JJ hypothesis_NN corresponds_VBZ to_TO p_NN
,_, 0.00_CD ,_, 0.50_CD ,_, 0.00_CD ,_, 0.50_CD ,_, 0.00_CD -RRB-_-RRB- -LRB-_-LRB- 4_CD -RRB-_-RRB- :_: -LRB-_-LRB- 0.00_CD ,_, 1.00_CD ,_, 0.50_CD ,_, 0.00_CD ,_, 0.50_CD ,_, 0.50_CD -RRB-_-RRB- -LRB-_-LRB- 5_CD -RRB-_-RRB- :_: -LRB-_-LRB- 0.00_CD ,_, 1.00_CD ,_, 0.50_CD ,_, 0.00_CD ,_, 0.50_CD ,_, 0.00_CD -RRB-_-RRB- -LRB-_-LRB- 6_CD -RRB-_-RRB- :_: -LRB-_-LRB- 0.00_CD ,_, 0.00_CD ,_, 0.50_CD ,_, 1.00_CD ,_, 0.50_CD ,_, 0.50_CD -RRB-_-RRB- -LRB-_-LRB- 7_CD -RRB-_-RRB- :_: -LRB-_-LRB- 0.00_CD ,_, 0.00_CD ,_, 0.50_CD ,_, 1.00_CD ,_, 0.50_CD ,_, 0.00_CD -RRB-_-RRB- =_JJ -_: =-LRB-_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: :_: -LRB-_-LRB- 0.00_CD ,_, 0.00_CD ,_, 0.50_CD ,_, 0.00_CD ,_, 0.50_CD ,_, 1.00_CD -RRB-_-RRB- -LRB-_-LRB- 9_CD -RRB-_-RRB- :_: -LRB-_-LRB- 1.00_CD ,_, 0.50_CD ,_, 1.00_CD ,_, 0.50_CD ,_, 0.00_CD ,_, 0.00_CD -RRB-_-RRB- -LRB-_-LRB- 10_CD -RRB-_-RRB- :_: -LRB-_-LRB- 1.00_CD ,_, 0.50_CD ,_, 0.00_CD ,_, 0.50_CD ,_, 1.00_CD ,_, 0.00_CD -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- :_: -LRB-_-LRB- 1.00_CD ,_, 0.50_CD ,_, 0.00_CD ,_, 0.50_CD ,_, 0.00_CD ,_, 0.00_CD -RRB-_-RRB- -LRB-_-LRB- 12_CD -RRB-_-RRB- :_: -LRB-_-LRB- 0.00_CD ,_, 0.50_CD ,_, 1.00_CD ,_, 0.50_CD ,_, 0_CD ._.
approaches_NNS have_VBP previously_RB been_VBN proposed_VBN to_TO deal_VB with_IN the_DT class_NN imbalance_NN problem_NN including_VBG a_DT simple_JJ and_CC yet_RB quite_RB effective_JJ method_NN :_: re-sampling_NN -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- Lewis_NNP &_CC Gale_NNP ,_, 1994_CD -RRB-_-RRB- ,_, -LRB-_-LRB- Kubat_NNP &_CC Matwin_NNP ,_, 1997_CD -RRB-_-RRB- ,_, -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN -RRB-_-RRB- ._.
This_DT paper_NN deals_VBZ with_IN the_DT two_CD different_JJ types_NNS of_IN re-sampling_JJ approaches_NNS :_: methods_NNS that_WDT oversample_VBP the_DT small_JJ class_NN in_IN order_NN to_TO make_VB it_PRP reach_VB a_DT size_NN close_RB to_TO that_DT of_IN the_DT larger_JJR class_NN and_CC methods_NNS that_WDT
databases_NNS -LRB-_-LRB- AIS93_NN ,_, AS94_NN ,_, SA95_NN ,_, HF95_NN -RRB-_-RRB- ._.
However_RB ,_, association_NN rules_NNS neither_RB address_VBP the_DT economic_JJ value_NN of_IN transactions_NNS nor_CC produce_VBP a_DT global_JJ action_NN plan_NN for_IN decision_NN making_NN ._.
The_DT cost-sensitive_JJ classification_NN =_JJ -_: =_JJ -LRB-_-LRB- P99_NN -RRB-_-RRB- -_: =_SYM -_: assumes_VBZ an_DT error_NN metric_NN of_IN misclassification_NN and_CC minimizes_VBZ the_DT error_NN on_IN new_JJ cases_NNS ._.
No_DT such_JJ error_NN metric_NN is_VBZ given_VBN in_IN profit_NN mining_NN ._.
Rather_RB ,_, we_PRP assume_VBP that_IN customers_NNS spend_VBP some_DT money_NN on_IN recommended_JJ ite_NN
correspond_VBP to_TO the_DT correct_JJ class_NN ._.
The_DT value_NN in_IN L_NN -LRB-_-LRB- i_FW ;_: j_NN -RRB-_-RRB- gives_VBZ the_DT ``_`` loss_NN ''_'' of_IN predicting_VBG class_NN i_FW when_WRB the_DT true_JJ class_NN is_VBZ j._VBN The_DT diagonal_JJ elements_NNS ,_, L_NN -LRB-_-LRB- i_FW ;_: i_LS -RRB-_-RRB- ,_, are_VBP for_IN most_JJS tasks_NNS zero_VBP ,_, but_CC ,_, as_IN pointe_NN =_JJ -_: =d_NN out_IN in_IN -LRB-_-LRB- Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ,_, there_EX are_VBP cases_NNS when_WRB it_PRP is_VBZ reasonable_JJ to_TO assume_VB positive_JJ diagonal_JJ values_NNS ._.
For_IN example_NN ,_, Table_NNP 1_CD shows_VBZ a_DT 4-class_JJ loss_NN matrix_NN ._.
The_DT cell_NN in_IN column_NN 1_CD of_IN row_NN 3_CD gives_VBZ the_DT loss_NN -LRB-_-LRB- 4.5_CD -RRB-_-RRB- of_IN predicting_VBG class_NN 3_CD
ow_IN expected_VBN donation_NN ._.
In_IN this_DT report_NN we_PRP describe_VBP two_CD models_NNS that_IN we_PRP built_VBD for_IN solving_VBG this_DT problem_NN ._.
The_DT first_JJ uses_VBZ direct_JJ costsensitive_JJ decision-making_NN ._.
The_DT second_NN involves_VBZ using_VBG the_DT MetaCost_NN method_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP 1999_CD -_: =--RRB-_NN ._.
We_PRP first_RB describe_VBP the_DT methods_NNS ,_, then_RB we_PRP discuss_VBP the_DT results_NNS obtained_VBN on_IN a_DT training_NN dataset_NN ,_, and_CC give_VB the_DT prediction_NN of_IN the_DT expected_VBN profit_NN for_IN a_DT validation_NN dataset_NN ._.
The_DT training_NN dataset_NN has_VBZ 95412_CD re_NN
Fawcett_NNP &_CC Provost_NNP ,_, 1996_CD -RRB-_-RRB- -RRB-_-RRB- and_CC asymmetric_JJ misclassification_NN costs_NNS -LRB-_-LRB- the_DT cost_NN of_IN misclassifying_VBG an_DT example_NN from_IN one_CD class_NN is_VBZ much_RB larger_JJR than_IN the_DT cost_NN of_IN misclassifying_VBG an_DT example_NN from_IN the_DT other_JJ class_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Pazzani_NNP et_FW al._FW ,_, 1997_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Traditional_JJ learning_NN algorithms_NNS ,_, which_WDT aim_VBP to_TO maximize_VB accuracy_NN ,_, treat_NN positive_JJ and_CC negative_JJ examples_NNS as_IN equally_RB important_JJ and_CC therefore_RB do_VBP not_RB always_RB produce_VB a_DT satisfactory_JJ
=_JJ argmin_FW y2Y_FW k_NN X_NN j_NN =_JJ 1_CD P_NN -LRB-_-LRB- y_NN j_NN jx_NN -RRB-_-RRB- C_NN -LRB-_-LRB- y_NN ;_: y_FW j_FW -RRB-_-RRB- ;_: -LRB-_-LRB- 1_LS -RRB-_-RRB- This_DT gives_VBZ us_PRP a_DT cost-sensitive_JJ classification_NN procedure_NN whose_WP$ performance_NN relies_VBZ on_IN the_DT accuracy_NN of_IN the_DT computed_JJ class-probability_NN estimates_NNS ._.
MetaCost_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN is_VBZ an_DT algorithm_NN that_WDT relabels_VBZ each_DT training_NN example_NN with_IN the_DT cost-optimal_JJ label_NN -LRB-_-LRB- computed_VBN according_VBG to_TO -LRB-_-LRB- 1_LS -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC outputs_VBZ the_DT decision_NN of_IN a_DT 0\/1-loss_JJ classifier_NN trained_VBN on_IN the_DT relabeled_VBN data_NNS ._.
MetaCost_NN
is_VBZ available_JJ at_IN learning_VBG time_NN ,_, and_CC several_JJ methods_NNS have_VBP been_VBN developed_VBN to_TO incorporate_VB C_NN into_IN the_DT learning_NN process_NN -LRB-_-LRB- see_VB ,_, for_IN example_NN ,_, Knoll_NNP &_CC al._FW ,_, 1994_CD ;_: Bradford_NNP &_CC al._FW ,_, 1998_CD ;_: Kukar_NNP &_CC Kononenko_NNP ,_, 1998_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Fan_NNP &_CC al._FW ,_, 1999_CD ;_: Margineantu_NNP &_CC Dietterich_NNP ,_, 1999_CD -RRB-_-RRB- ._.
The_DT second_JJ approach_NN is_VBZ to_TO assume_VB that_DT C_NN is_VBZ not_RB available_JJ and_CC to_TO seek_VB learning_NN algorithms_NNS that_WDT produce_VBP hypotheses_NNS that_WDT have_VBP good_JJ performance_NN over_IN a_DT w_NN
Netherlands_NNP ._.
cdrummond_NN ._.
tex_NN ;_: 23\/11\/2005_CD ;_: 16:54_CD ;_: p._NN 12_CD Drummond_NNP and_CC Holte_NNP In_IN cost-sensitive_JJ learning_NN ,_, expected_VBN cost_NN under_IN a_DT range_NN of_IN cost_NN matrices_NNS has_VBZ been_VBN the_DT preferred_JJ measure_NN -LRB-_-LRB- Bradford_NNP et_FW al._FW ,_, 1998_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Margineantu_NNP and_CC Dietterich_NNP ,_, 2000_CD -RRB-_-RRB- ._.
The_DT shortcomings_NNS of_IN using_VBG accuracy_NN have_VBP been_VBN pointed_VBN out_RP by_IN others_NNS -LRB-_-LRB- Hand_NN ,_, 1997_CD ;_: Provost_NNP et_FW al._FW ,_, 1998_CD -RRB-_-RRB- ._.
The_DT most_RBS fundamental_JJ shortcoming_NN is_VBZ the_DT simple_JJ fact_NN that_IN a_DT s_NN
the_DT distribution_NN of_IN training_NN data_NNS according_VBG to_TO the_DT cost_NN matrix_NN -LRB-_-LRB- called_VBN stratification-based_JJ approaches_NNS -RRB-_-RRB- -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, or_CC based_VBN on_IN adding_VBG a_DT cost-based_JJ framework_NN on_IN top_NN of_IN existing_VBG accuracy-driven_JJ classifiers_NNS =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Our_PRP$ framework_NN uses_VBZ scores_NNS generated_VBN by_IN individual_JJ binary_JJ classifiers_NNS and_CC the_DT misclassification_NN cost_NN matrix_NN ,_, to_TO arrive_VB at_IN predictions_NNS according_VBG to_TO Bayes_NNP optimality_NN rule_NN for_IN minimum_JJ expected_JJ cost_NN ._.
In_IN
-LRB-_-LRB- used_VBN each_DT alone_RB or_CC in_IN combination_NN -RRB-_-RRB- to_TO artificially_RB balance_VB the_DT class_NN distributions_NNS -LRB-_-LRB- by_IN eliminating_VBG examples_NNS of_IN the_DT majority_NN class_NN and\/or_CC replicating_VBG examples_NNS of_IN the_DT minority_NN class_NN -RRB-_-RRB- ,_, see_VBP for_IN example_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 21_CD -RRB-_-RRB- ._.
These_DT studies_NNS on_IN classifier_NN learning_NN have_VBP also_RB criticized_VBN the_DT use_NN of_IN the_DT error_NN rate_NN as_IN performance_NN measure_NN for_IN evaluating_VBG and_CC designing_VBG classifiers_NNS ._.
Indeed_RB ,_, using_VBG error_NN rate_NN assumes_VBZ
et_FW al._FW ,_, 2003_CD -RRB-_-RRB- ,_, as_IN opposed_VBN to_TO other_JJ measures_NNS such_JJ as_IN F_NN score_NN -LRB-_-LRB- Caruana_NNP &_CC Niculescu-Mizil_NNP ,_, 2004_CD -RRB-_-RRB- ._.
The_DT ROC_NNP curve_NN also_RB shows_VBZ the_DT misclassification_NN rates_NNS achieved_VBD depending_VBG on_IN the_DT error_NN cost_NN coefficients_NNS -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
For_IN these_DT reasons_NNS ,_, -LRB-_-LRB- Bradley_NNP ,_, 1997_CD -RRB-_-RRB- argues_VBZ the_DT comparison_NN of_IN the_DT ROC_NN curves_NNS attached_VBN to_TO two_CD learning_VBG algorithms_NNS to_TO be_VB more_RBR fair_JJ and_CC informative_JJ ,_, than_IN comparing_VBG their_PRP$ misclassification_NN rates_NNS only_RB ._.
2_CD ._.
cular_JJ ,_, we_PRP measure_VBP the_DT quality_NN of_IN the_DT classifier_NN based_VBN on_IN total_JJ cost_NN ,_, which_WDT is_VBZ calculated_VBN using_VBG the_DT equation_NN below_IN -LRB-_-LRB- the_DT evaluation_NN cost_NN ratio_NN is_VBZ FPcost_NN :_: FNcost_NN -RRB-_-RRB- ._.
Total_JJ cost_NN =_JJ FP_NN \*_NN FPcost_NN +_CC FN_NN \*_NN FNcost_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: Under_IN normal_JJ circumstances_NNS ,_, the_DT evaluation_NN cost_NN ratio_NN is_VBZ passed_VBN to_TO the_DT classifier_NN induction_NN program_NN ,_, assuming_VBG that_IN it_PRP is_VBZ capable_JJ of_IN cost-sensitive_JJ learning_NN ._.
However_RB ,_, in_IN this_DT paper_NN we_PRP often_RB utilize_VBP a_DT
es_NNS -LRB-_-LRB- like_IN specificity_NN ,_, sensitivity_NN and_CC likelihood_NN ratios_NNS which_WDT are_VBP used_VBN in_IN diagnosis_NN -RRB-_-RRB- and_CC costs_VBZ matrix_NN are_VBP eventually_RB used_VBN to_TO take_VB into_IN account_NN the_DT difference_NN between_IN false_JJ positive_JJ and_CC false_JJ negative_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT additional_JJ information_NN is_VBZ essential_JJ but_CC generally_RB it_PRP focuses_VBZ exclusively_RB on_IN the_DT result_NN and_CC not_RB on_IN the_DT case_NN itself_PRP :_: This_DT is_VBZ obvious_JJ for_IN global_JJ error_NN rates_NNS -LRB-_-LRB- which_WDT are_VBP identical_JJ for_IN all_DT cases_NNS -RRB-_-RRB- ,_, bu_FW
ing_IN a_DT single_JJ classifier_NN ,_, bagging_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- can_MD be_VB used_VBN to_TO train_VB an_DT ensemble_NN of_IN classifiers_NNS to_TO improve_VB both_CC the_DT classification_NN accuracy_NN and_CC the_DT estimate_NN of_IN class_NN probabilities_NNS on_IN SU_NNP ;_: •_CD Some_DT of_IN known_JJ methods_NNS =_JJ -_: =[_NN 2,5_CD -RRB-_-RRB- -_: =_SYM -_: can_MD be_VB coupled_VBN with_IN the_DT procedure_NN from_IN Table_NNP 3_CD in_IN a_DT straightforward_JJ manner_NN to_TO produce_VB classifiers_NNS that_WDT are_VBP optimized_VBN to_TO an_DT arbitrary_JJ cost_NN matrix_NN ._.
4_CD Experimental_JJ Results_NNS We_PRP performed_VBD two_CD groups_NNS of_IN exp_NN
oint_NN using_VBG the_DT Threshold_NNP method_NN ._.
This_DT view_NN of_IN ROC_NN curve_NN plotting_VBG allows_VBZ using_VBG other_JJ methods_NNS for_IN making_VBG ML_NN algorithms_NNS cost-sensitive_JJ ._.
For_IN instance_NN ,_, one_PRP can_MD use_VB techniques_NNS as_IN Stratification_NN or_CC MetaCost_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: applied_VBN to_TO a_DT ML_NN algorithm_NN for_IN inducing_VBG a_DT set_NN of_IN classifiers_NNS for_IN a_DT range_NN of_IN class_NN and_CC cost_NN conditions_NNS ,_, and_CC then_RB linking_VBG the_DT obtained_VBN -LRB-_-LRB- FP_NN ,_, TP_NN -RRB-_-RRB- points_NNS to_TO form_VB a_DT ROC_NN curve_NN ._.
This_DT is_VBZ the_DT basis_NN of_IN the_DT method_NN
L_NN -LRB-_-LRB- cl_NN -LRB-_-LRB- l_NN -RRB-_-RRB- |_FW f_FW -RRB-_-RRB- -LRB-_-LRB- 1_LS -RRB-_-RRB- l_NN ∈_NN L_NN There_EX has_VBZ been_VBN a_DT significant_JJ volume_NN of_IN work_NN on_IN the_DT problem_NN of_IN minimizing_VBG MC_NNP costs_NNS :_: some_DT general_JJ methods_NNS are_VBP the_DT 40_CD weighted_JJ boosting_VBG algorithm_NN of_IN -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, and_CC the_DT MetaCost_NNP algorithm_NN of_IN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2.2_CD Attribute_NNP measurement_NN costs_VBZ The_DT action_NN of_IN measuring_VBG an_DT attribute_NN fi_NN is_VBZ indicated_VBN as_IN m_NN -LRB-_-LRB- fi_NN -RRB-_-RRB- ._.
This_DT action_NN may_MD incur_VB a_DT deterministic_JJ cost_NN :_: CM_NN -LRB-_-LRB- m_NN -LRB-_-LRB- fi_NN -RRB-_-RRB- -RRB-_-RRB- ._.
We_PRP assume_VBP the_DT value_NN of_IN a_DT measured_JJ attribute_NN is_VBZ c_NN
methods_NNS measured_VBN by_IN the_DT maximum_JJ total_JJ profit_NN in_IN two_CD realworld_NN datasets_NNS -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
The_DT third_JJ approach_NN ,_, called_VBN ``_`` Relabeling_NNP ''_'' ,_, relabels_VBZ the_DT classes_NNS of_IN instances_NNS by_IN applying_VBG the_DT minimum_NN expected_VBD cost_NN criterion_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
MetaCost_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- belongs_VBZ to_TO this_DT approach_NN ._.
MetaCost_NNP uses_VBZ bagging_VBG as_IN the_DT ensemble_NN method_NN ._.
The_DT forth_RB approach_NN ,_, called_VBN ``_`` Weighting_NNP ''_'' -LRB-_-LRB- 18_CD -RRB-_-RRB- ,_, induces_VBZ costsensitivity_NN by_IN integrating_VBG the_DT instances_NNS '_POS weights_NNS direct_VBP
hts_NNS ,_, such_JJ as_IN C4_NN .5_CD ,_, Bayesian_JJ classifiers_NNS ,_, as_RB well_RB as_IN a_DT GP-based_JJ classifier_NN proposed_VBN in_IN this_DT paper_NN -LRB-_-LRB- the_DT way_NN of_IN re-weighting_NN will_MD be_VB elaborated_VBN in_IN Section_NN 4_CD -RRB-_-RRB- ._.
A_DT more_RBR sophisticated_JJ method_NN called_VBN MetaCost_NNP =_SYM -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: employs_VBZ a_DT ``_`` meta-learning_JJ ''_'' procedure_NN ,_, i.e._FW bagging_NN ,_, to_TO relabel_VB the_DT classes_NNS of_IN training_NN data_NNS and_CC then_RB applies_VBZ an_DT arbitrary_JJ error-based_JJ classifier_NN directly_RB to_TO the_DT modified_VBN training_NN set_VBN to_TO generate_VB a_DT fin_NN
tive_JJ learning_NN can_MD be_VB grouped_VBN into_IN three_CD categories_NNS :_: 1_LS -RRB-_-RRB- creating_VBG particular_JJ classifiers_NNS in_IN cost-sensitive_JJ learning_NN -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 11_CD -RRB-_-RRB- -RRB-_-RRB- ,_, 2_CD -RRB-_-RRB- assigning_VBG each_DT example_NN to_TO its_PRP$ lowest_JJS expected_VBN cost_NN class_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 24_CD -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC 3_CD -RRB-_-RRB- modifying_VBG the_DT distribution_NN of_IN training_NN examples_NNS prior_RB to_TO performing_VBG learning_NN -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 1_LS -RRB-_-RRB- ,_, -LRB-_-LRB- 18_CD -RRB-_-RRB- -RRB-_-RRB- ._.
3_CD Learning_NNP of_IN SVM_NNP for_IN Ranking_NNP n_NN Assume_VB that_IN a_DT training_NN set_NN of_IN labeled_JJ data_NNS is_VBZ availabl_NN
e_LS confidence_NN of_IN a_DT rule_NN matching_VBG an_DT instance_NN can_MD be_VB used_VBN as_IN a_DT score_NN -LRB-_-LRB- Fawcett_NNP ,_, 2001_CD -RRB-_-RRB- ._.
Even_RB if_IN a_DT classifier_NN only_RB produces_VBZ a_DT class_NN label_NN ,_, an_DT aggregation_NN of_IN them_PRP may_MD be_VB used_VBN to_TO generate_VB a_DT score_NN ._.
MetaCost_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN employs_VBZ bagging_VBG to_TO generate_VB an_DT ensemble_NN of_IN discrete_JJ classifiers_NNS ,_, each_DT of_IN which_WDT produces_VBZ a_DT vote_NN ._.
The_DT set_NN of_IN votes_NNS could_MD be_VB used_VBN to_TO generate_VB a_DT score2_NN ._.
Finally_RB ,_, some_DT combination_NN of_IN scoring_VBG and_CC voting_VBG
ass_NN cost-sensitive_JJ learning_NN algorithms_NNS ._.
Using_VBG the_DT C4_NN .5_CD decision_NN tree_NN learner_NN -LRB-_-LRB- Quinlan_NNP ,_, 1993_CD -RRB-_-RRB- as_IN oracle_NN classifier_NN learner_NN ,_, we_PRP compared_VBD our_PRP$ reduction_NN to_TO the_DT cost-sensitive_JJ learning_NN algorithm_NN MetaCost_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
Using_VBG Boosted_JJ Naive_JJ Bayes_NNS -LRB-_-LRB- Elkan_NNP ,_, 1997_CD -RRB-_-RRB- ,_, we_PRP compared_VBD our_PRP$ reduction_NN to_TO the_DT minimum_NN expected_VBD cost_NN approach_NN after_IN calibration_NN of_IN the_DT probability_NN estimates_NNS produced_VBN by_IN the_DT learner_NN ,_, as_IN done_VBN by_IN Zadrozny_NNP
ong_VB a_DT single_JJ path_NN from_IN the_DT root_NN to_TO a_DT leaf_NN ._.
Indeed_RB ,_, cost-sensitive_JJ trees_NNS have_VBP been_VBN the_DT subject_NN of_IN many_JJ research_NN efforts_NNS ._.
Several_JJ works_NNS proposed_VBD learners_NNS that_WDT consider_VBP different_JJ misclassification_NN costs_NNS =_JJ -_: =[_NN 7_CD ,_, 18_CD ,_, 6_CD ,_, 9_CD ,_, 10_CD ,_, 14_CD ,_, 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
These_DT methods_NNS ,_, however_RB ,_, do_VBP not_RB consider_VB test_NN costs_NNS ._.
Other_JJ authors_NNS designed_VBD tree_NN learners_NNS that_WDT take_VBP into_IN account_NN test_NN costs_NNS ,_, such_JJ as_IN IDX_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, CSID3_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, and_CC EG2_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
These_DT methods_NNS ,_, however_RB ,_, do_VBP not_RB
posed_VBN for_IN the_DT case_NN where_WRB only_JJ data_NNS from_IN one_CD class_NN is_VBZ available_JJ ._.
With_IN regard_NN to_TO the_DT second_JJ issue_NN ,_, a_DT close_JJ connection_NN was_VBD recognized_VBN between_IN re-sampling_JJ approaches_NNS and_CC cost-sensitive_JJ approaches_NNS -LRB-_-LRB- 53_CD -RRB-_-RRB- -LRB-_-LRB- 15_CD -RRB-_-RRB- =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =-[_NN 52_CD -RRB-_-RRB- ._.
Cost-sensitive_JJ learning_NN or_CC measures_NNS assume_VBP that_IN a_DT cost-matrix_NN is_VBZ known_VBN for_IN different_JJ types_NNS of_IN errors_NNS or_CC even_RB examples_NNS ,_, which_WDT can_MD be_VB used_VBN at_IN classification_NN time_NN -LRB-_-LRB- 53_CD -RRB-_-RRB- -LRB-_-LRB- 15_CD -RRB-_-RRB- ._.
However_RB ,_, we_PRP often_RB do_VBP not_RB
he_PRP agent_NN automatically_RB trade-off_NN planning_NN and_CC learning_NN ._.
There_EX is_VBZ quite_RB some_DT literature_NN on_IN cost-sensitivity_NN and_CC learning_NN ,_, but_CC most_JJS of_IN this_DT concentrates_VBZ on_IN supervised_JJ classification_NN tasks_NNS ,_, for_IN example_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP 1999_CD -_: =--RRB-_NN ._.
Some_DT related_JJ material_NN exists_VBZ in_IN the_DT field_NN of_IN sensor_NN planning_NN -LRB-_-LRB- Koenig_NNP &_CC Liu_NNP 2000_CD -RRB-_-RRB- ,_, but_CC to_TO the_DT best_JJS of_IN our_PRP$ knowledge_NN ,_, it_PRP is_VBZ limited_VBN in_IN the_DT field_NN of_IN reinforcement_NN learning_NN ._.
The_DT lack_NN of_IN costsensitive_JJ r_NN
itive_JJ and_CC negative_JJ examples_NNS in_IN order_NN to_TO make_VB optimal_JJ cost-sensitive_JJ classifications_NNS for_IN a_DT concept-learning_JJ problem_NN ._.
Moreover_RB ,_, a_DT general_JJ method_NN to_TO make_VB a_DT learning_NN system_NN cost-sensitive_JJ is_VBZ presented_VBN in_IN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT last_JJ method_NN has_VBZ the_DT advantage_NN of_IN being_VBG applicable_JJ to_TO multi-class_JJ problems_NNS ._.
4_CD Probabilistic_NNP Classifiers_NNP Most_NNP learning_VBG algorithms_NNS can_MD be_VB adapted_VBN to_TO produce_VB probabilistic_JJ classifiers_NNS ,_, i.e._FW ,_, to_TO ind_VB
1997_CD ;_: Karwath_NNP &_CC King_NNP ,_, 2002_CD ;_: Weiss_NNP and_CC Provost_NNP ,_, 2003_CD ;_: Yan_NNP et_FW al._FW ,_, 2003_CD -RRB-_-RRB- ._.
In_IN cost-sensitive_JJ learning_NN ,_, expected_VBN cost_NN under_IN a_DT range_NN of_IN cost_NN matrices_NNS has_VBZ been_VBN the_DT preferred_JJ measure_NN -LRB-_-LRB- Bradford_NNP et_FW al._FW ,_, 1998_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Margineantu_NNP &_CC Dietterich_NNP ,_, 2000_CD -RRB-_-RRB- ._.
The_DT shortcomings_NNS of_IN using_VBG accuracy_NN have_VBP been_VBN pointed_VBN out_RP by_IN others_NNS -LRB-_-LRB- Hand_NN ,_, 1997_CD ;_: Provost_NNP et_FW al._FW ,_, 1998_CD -RRB-_-RRB- ._.
The_DT most_RBS fundamental_JJ shortcoming_NN is_VBZ the_DT simple_JJ fact_NN that_IN a_DT sin_NN
ized_VBN learning_NN methods_NNS exist_VBP to_TO tackle_VB such_JJ problems_NNS ._.
For_IN examIn_NNP Proceedings_NNP of_IN the_DT 19th_JJ Annual_JJ Innovative_JJ Applications_NNS of_IN Artificial_NNP Intelligence_NNP Conference_NNP -LRB-_-LRB- IAAI-07_NN -RRB-_-RRB- ._.
4_CD ple_NN ,_, the_DT MetaCost_NNP algorithm_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP 1999_CD -_: =--RRB-_FW relabels_FW training_NN data_NNS such_JJ that_IN a_DT classifier_NN trained_VBN to_TO minimize_VB classification_NN error_NN on_IN the_DT relabeled_VBN data_NNS will_MD minimize_VB the_DT costs_NNS of_IN misprediction_NN ._.
However_RB ,_, MetaCost_NNP and_CC other_JJ methods_NNS like_IN it_PRP are_VBP
naive_JJ Bayes_NNS naturally_RB provide_VBP those_DT probabilities_NNS while_IN other_JJ classifiers_NNS such_JJ as_IN decision_NN trees_NNS do_VBP not_RB ._.
It_PRP is_VBZ still_RB possible_JJ to_TO produce_VB ROC_NN curves_NNS for_IN any_DT type_NN of_IN classifier_NN using_VBG minor_JJ adjustments_NNS =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Area_NN under_IN the_DT ROC_NN Curve_NN -LRB-_-LRB- AUC_NN -RRB-_-RRB- is_VBZ a_DT single_JJ scalar_NN value_NN for_IN classifier_NN comparison_NN -LRB-_-LRB- 2_CD ,_, 9_CD -RRB-_-RRB- ._.
Statistically_RB speaking_NN ,_, the_DT AUC_NN of_IN a_DT classifier_NN is_VBZ the_DT probability_NN that_IN a_DT classifier_NN will_MD rank_VB a_DT randomly_RB ch_NN
modifies_VBZ particular_JJ error-minimizing_JJ classifiers_NNS cost-sensitive_JJ 4s_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- ,_, -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
The_DT second_JJ one_CD is_VBZ a_DT black-box_JJ approach_NN that_WDT converts_VBZ arbitrary_JJ error-minimizing_JJ classifiers_NNS into_IN cost-sensitive_JJ ones_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT paper_NN ,_, we_PRP utilize_VBP two_CD methods_NNS in_IN the_DT black-box_JJ approach_NN for_IN cost-sensitive_JJ learning_NN :_: costing_VBG -LRB-_-LRB- 19_CD -RRB-_-RRB- and_CC metacost_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
A_DT black-box_JJ approach_NN for_IN cost-sensitive_JJ learning_NN makes_VBZ any_DT error-minimizin_NN
._.
The_DT first_JJ attempts_NNS to_TO produce_VB generic_JJ procedures_NNS for_IN making_VBG any_DT arbitrary_JJ algorithm_NN cost_NN sensitive_JJ ,_, by_IN resorting_VBG to_TO Bayes_NNP risk_NN theory_NN or_CC some_DT other_JJ cost_NN minimizing_VBG strategy_NN -LRB-_-LRB- Zadrozny_NNP et_FW al._FW ,_, 2003_CD -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ,_, -LRB-_-LRB- Chawla_NNP et_FW al._FW ,_, 2003_CD -RRB-_-RRB- -LRB-_-LRB- Guo_NNP &_CC Viktor_NNP ,_, 2004_CD -RRB-_-RRB- ._.
The_DT second_JJ attempts_NNS to_TO extend_VB particular_JJ algorithms_NNS ,_, so_RB as_IN to_TO produce_VB cost-sensitive_JJ generalizations_NNS ._.
One_CD example_NN is_VBZ the_DT popular_JJ AdaBoost_NNP algorithm_NN ,_, whic_JJ
ational_JJ Conference_NNP on_IN Machine_NNP Learning_NNP ,_, Pittsburgh_NNP ,_, PA_NN ,_, 2006_CD ._.
Copyright_NN 2006_CD by_IN the_DT author_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- \/_: owner_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- ._.
many_JJ practical_JJ applications_NNS ._.
There_EX is_VBZ a_DT rich_JJ tradition_NN of_IN cost-sensitive_JJ learning_NN -LRB-_-LRB- Elkan_NNP ,_, 2001_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN applied_VBN to_TO independent_JJ and_CC identically_RB distributed_VBN -LRB-_-LRB- IID_NN -RRB-_-RRB- data_NNS for_IN applications_NNS such_JJ as_IN targeted_VBN marketing_NN and_CC fraud_NN &_CC intrusion_NN detection_NN ._.
These_DT approaches_NNS assume_VBP the_DT traditional_JJ machine_NN learning_NN se_FW
ifying_VBG it_PRP correctly_RB counts_VBZ as_IN λ_NN successes_NNS ._.
Spam_NN messages_NNS are_VBP treated_VBN as_IN single_JJ messages_NNS ._.
This_DT is_VBZ similar_JJ to_TO instance_NN weighting_NN during_IN training_NN ,_, and_CC leads_VBZ to_TO the_DT 17_CD We_PRP also_RB experimented_VBD with_IN MetaCost_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP 1999_CD -_: =-]_CD ,_, but_CC found_VBD it_PRP computationally_RB expensive_JJ ,_, without_IN significant_JJ gains_NNS in_IN classification_NN performance_NN ._.
This_DT agrees_VBZ with_IN Hidalgo_NNP -LRB-_-LRB- 2002_CD -RRB-_-RRB- ._.
s28_NN ·_NNP I._NNP Androutsopoulos_NNP ,_, G._NNP Paliouras_NNP ,_, and_CC E._NNP Michelakis_NNP following_VBG
ake_VB a_DT decision_NN ,_, end-users_NNS find_VBP little_JJ information_NN to_TO assess_VB the_DT relevance_NN of_IN the_DT result_NN ._.
This_DT kind_NN of_IN information_NN is_VBZ generally_RB available_JJ by_IN the_DT mean_NN of_IN error_NN rates_NNS or_CC probability_NN estimators_NNS ._.
-LRB-_-LRB- 4_LS -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN practice_NN ,_, these_DT estimators_NNS are_VBP not_RB always_RB available_JJ ,_, since_IN they_PRP are_VBP developed_VBN for_IN the_DT construction_NN of_IN the_DT tree_NN and_CC not_RB for_IN the_DT end-user_NN 's_POS need_NN -LRB-_-LRB- see_VB examples_NNS in_IN -LRB-_-LRB- 15_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 6_CD -RRB-_-RRB- -RRB-_-RRB- ._.
They_PRP are_VBP also_RB not_RB ne_VB
notation_NN introduced_VBD next_JJ ,_, a_DT consumer_NN xi_NN is_VBZ labeled_VBN as_IN profitable_JJ if_IN fˆ_NN S_NN F_NN ⋅_NN U_NN +_CC −_FW f_FW ⋅_FW U_NN −_NN C_NN ≥_NN Ψ_NN ._.
i_FW i_FW -LRB-_-LRB- 1_CD ˆ_CD i_LS -RRB-_-RRB- i_LS 9_CD This_DT reformulation_NN procedure_NN is_VBZ a_DT generalization_NN of_IN the_DT procedure_NN used_VBN by_IN MetaCost_NNP -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN for_IN cost-sensitive_JJ learning_NN ._.
See_VB the_DT Appendix_NNP for_IN a_DT comparison_NN of_IN the_DT cost-sensitive_JJ learning_NN setting_NN and_CC the_DT decision-making_JJ setting_NN ._.
9sSAAR-TSECHANSKY_NN AND_CC PROVOST_NN Active_JJ -_: Learning_NNP for_IN Decision-Mak_NNP
Fusion_NN 9_CD -LRB-_-LRB- 2008_CD -RRB-_-RRB- 120_CD --_: 133_CD 121_CD Table_NNP 1_CD Canister_NNP nodes_NNS for_IN partitions_NNS -LRB-_-LRB- each_DT time_NN step_NN -RRB-_-RRB- Type_NN Simulation_NN Partition_NN 0_CD 1_CD 2_CD 3_CD Vertical_JJ 1_CD --_: 4_CD 1640 1886 1886 1312_CD Horizontal_JJ 1_CD --_: 4_CD 1640 1640 1640 1804_CD methods_NNS as_IN in_IN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT case_NN of_IN a_DT partition_NN having_VBG zero_CD salient_JJ points_NNS ,_, a_DT single-class_JJ ``_`` classifier_NN ''_'' will_MD be_VB learned_VBN ._.
This_DT motivated_VBD an_DT adjustment_NN to_TO our_PRP$ voting_NN scheme_NN for_IN improved_VBN accuracy_NN ,_, as_IN shown_VBN in_IN Section_NNP 5_CD ._.
ensitive_JJ learning_NN ,_, among_IN which_WDT misclassification_NN costs_NNS and_CC test_NN costs_NNS are_VBP singled_VBN out_RP as_IN most_RBS important_JJ ._.
Much_JJ work_NN has_VBZ been_VBN done_VBN in_IN recent_JJ years_NNS on_IN nonuniform_JJ misclassification_NN costs_NNS -LRB-_-LRB- alone_RB -RRB-_-RRB- ,_, such_JJ as_IN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
Some_DT previous_JJ work_NN ,_, such_JJ as_IN -LRB-_-LRB- 18_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, considers_VBZ the_DT test_NN cost_NN alone_RB without_IN incorporating_VBG misclassification_NN cost_NN ,_, which_WDT is_VBZ obviously_RB an_DT oversight_NN ._.
A_DT few_JJ researchers_NNS -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 2_CD
-LRB-_-LRB- 24_CD -RRB-_-RRB- examined_VBD various_JJ methods_NNS for_IN incorporating_VBG cost_NN information_NN into_IN the_DT C4_NN .5_FW learningsalgorithm_FW ._.
Other_JJ cost-sensitive_JJ learning_NN methods_NNS that_WDT are_VBP algorithm-independent_JJ include_VBP AdaCost_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, MetaCost_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC Costing_VBG -LRB-_-LRB- 32_CD -RRB-_-RRB- ._.
In_IN addition_NN ,_, Joshi_NNP et_FW al._FW -LRB-_-LRB- 18_CD -RRB-_-RRB- discussed_VBD the_DT limitations_NNS of_IN boosting_VBG algorithms_NNS for_IN rare_JJ class_NN modeling_NN and_CC proposed_VBN PNrule_NN ,_, a_DT two-phase_JJ rule_NN induction_NN algorithm_NN ,_, to_TO handle_VB the_DT rar_NN
has_VBZ never_RB been_VBN made_VBN in_IN the_DT literature_NN ._.
Indirect_JJ methods_NNS try_VBP to_TO model_VB the_DT expected_VBN cost_NN of_IN the_DT labeling_NN and_CC assign_VB labels_NNS that_WDT minimize_VBP the_DT expected_VBN cost_NN ._.
Examples_NNS of_IN this_DT approach_NN 5sinclude_FW MetaCost_FW =_SYM -_: =[_NN 5_CD -RRB-_-RRB- and_CC d_NN -_: =_JJ -_: irect_JJ cost-sensitive_JJ decision_NN making_NN -LRB-_-LRB- 25_CD -RRB-_-RRB- -LRB-_-LRB- ``_`` direct_JJ ''_'' here_RB does_VBZ not_RB have_VB the_DT same_JJ meaning_NN -RRB-_-RRB- ._.
This_DT is_VBZ exactly_RB equivalent_JJ to_TO modeling_NN the_DT function_NN Qss_NNP ¡_NNP a_DT $_$ and_CC then_RB choosing_VBG the_DT action_NN that_WDT maximizes_VBZ
our_PRP$ mining_NN approach_NN ,_, but_CC we_PRP focus_VBP mainly_RB on_IN support_NN ._.
2.2_CD ._.
Cost-based_JJ classification_NN The_DT classification_NN model_NN discussed_VBN in_IN this_DT paper_NN can_MD be_VB used_VBN for_IN the_DT general_JJ case_NN of_IN costsensitive_JJ classification_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
In_IN this_DT section_NN ,_, we_PRP provide_VBP some_DT definitions_NNS relevant_JJ to_TO this_DT topic_NN ._.
We_PRP assume_VBP that_IN the_DT training_NN database_NN D_NN for_IN classification_NN consists_VBZ of_IN a_DT set_NN of_IN |_NN D_NN |_CD structures_NNS ,_, each_DT of_IN which_WDT is_VBZ associated_VBN with_IN
we_PRP should_MD be_VB willing_JJ to_TO ask_VB several_JJ queries_NNS for_IN information_NN so_RB additional_JJ High_JJ instances_NNS -LRB-_-LRB- each_DT worth_JJ 10_CD units_NNS -RRB-_-RRB- can_MD be_VB found_VBN ._.
To_TO train_VB this_DT model_NN ,_, we_PRP followed_VBD the_DT previous_JJ developed_JJ MetaCost_NN approach_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: and_CC refer_VB the_DT reader_NN to_TO their_PRP$ work_NN for_IN additional_JJ details_NNS in_IN how_WRB the_DT cost_NN bias_NN is_VBZ created_VBN ._.
We_PRP did_VBD find_VB that_IN the_DT MetaCost_NNP approach_NN was_VBD extremely_RB effective_JJ in_IN increasing_VBG the_DT recall_NN of_IN the_DT desired_VBN syste_NN
deal_NN with_IN imbalanced_JJ data_NNS distributions_NNS -LRB-_-LRB- when_WRB the_DT target_NN anomaly_NN class_NN involves_VBZ 1_CD %_NN or_CC 0,1_CD %_NN of_IN the_DT data_NNS -RRB-_-RRB- and_CC asymmetric_JJ error_NN costs_NNS -LRB-_-LRB- e.g._FW a_DT false_JJ negative_NN is_VBZ usually_RB more_RBR costly_JJ than_IN a_DT false_JJ positive_JJ -RRB-_-RRB- =_JJ -_: =[_NN 30_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT area_NN under_IN the_DT ROC_NN curve_NN -LRB-_-LRB- AUC_NN -RRB-_-RRB- defines_VBZ a_DT combinatorial_JJ NP-complete_JJ optimization_NN problem_NN ._.
Early_JJ approaches_NNS to_TO this_DT maximization_NN problem_NN -LRB-_-LRB- 36_CD ,_, 84_CD ,_, 34_CD -RRB-_-RRB- ,_, have_VBP considered_VBN GAbased_JJ or_CC greedy_JJ optimizatio_NN
n_NN cost_NN -LRB-_-LRB- Bayes_NN risk_NN -RRB-_-RRB- ._.
This_DT framework_NN is_VBZ appropriate_JJ when_WRB false_JJ positives_NNS and_CC false_JJ negatives_NNS carry_VBP different_JJ penalties_NNS ,_, and_CC was_VBD the_DT subject_NN of_IN most_JJS early_JJ research_NN on_IN cost-sensitive_JJ classification_NN -LRB-_-LRB- see_VB =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 12_CD -RRB-_-RRB- and_CC references_NNS therein_RB -RRB-_-RRB- ._.
The_DT second_JJ variant_NN ,_, example-dependent_JJ cost-sensitive_JJ classification_NN ,_, is_VBZ a_DT generalization_NN of_IN the_DT classdependent_JJ cost_NN problem_NN ,_, and_CC is_VBZ the_DT relevant_JJ framework_NN for_IN thi_NN
he_PRP AdaCost_NNP algorithm_NN which_WDT extends_VBZ the_DT AdaBoost_NNP algorithm_NN giving_VBG weights_NNS to_TO individual_JJ training_NN examples_NNS instead_RB of_IN classes_NNS ._.
--_: Using_VBG Bayes_NNP risk_NN theory_NN to_TO assign_VB each_DT example_NN to_TO its_PRP$ lowest_JJS risk_NN class_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP 1999_CD -_: =_JJ -_: ;_: Margineantu_NNP 2002_CD ;_: Zadrozny_NNP and_CC Elkan_NNP 2001_CD -RRB-_-RRB- ._.
--_: Changing_VBG the_DT class_NN distributions_NNS in_IN the_DT training_NN set_VBD such_JJ that_IN the_DT cost-insensitive_JJ classifier_NN learned_VBN will_MD perform_VB equally_RB to_TO a_DT cost_NN sensitive_JJ classifi_NNS
ing_NN with_IN imbalanced_JJ datasets_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, including_VBG :_: oversampling_VBG the_DT minority_NN class_NN ,_, under-sampling_JJ or_CC downsizing_VBG the_DT majority_NN class_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ,_, or_CC a_DT combination_NN of_IN both_DT -LRB-_-LRB- 5_CD ,_, 6_CD -RRB-_-RRB- ;_: building_VBG cost-sensitive_JJ classifiers_NNS =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: that_WDT penalize_VBP more_RBR heavily_RB misclassification_NN of_IN the_DT minority_NN class_NN ;_: and_CC rule-based_JJ methods_NNS that_WDT attempt_VBP to_TO learn_VB high_JJ confidence_NN rules_NNS for_IN the_DT minority_NN class_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
In_IN this_DT paper_NN we_PRP investigate_VBP another_DT
f_LS methods_NNS involves_VBZ adjusting_NN misclassification_NN costs_NNS :_: failure_NN to_TO recognize_VB a_DT positive_JJ case_NN -LRB-_-LRB- false_JJ negative_JJ -RRB-_-RRB- is_VBZ penalized_VBN more_JJR than_IN erroneously_RB classifying_VBG a_DT negative_JJ case_NN as_IN positive_JJ -LRB-_-LRB- false_JJ positive_JJ -RRB-_-RRB- =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Contrary_JJ to_TO sampling_NN approaches_NNS ,_, cost-based_JJ approaches_NNS to_TO imbalance_NN involve_VBP modifying_VBG the_DT learning_NN algorithm_NN 's_POS objective_JJ function_NN ._.
However_RB ,_, there_EX are_VBP other_JJ ways_NNS of_IN biasing_VBG the_DT inductive_JJ process_NN to_TO b_NN
large_JJ class_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- till_IN the_DT numbers_NNS of_IN samples_NNS in_IN both_DT classes_NNS are_VBP approximately_RB equal_JJ ._.
In_IN weighting_NN methods_NNS ,_, the_DT misclassification_NN costs_NNS of_IN the_DT two_CD classes_NNS are_VBP adjusted_VBN to_TO achieve_VB a_DT better_JJR performance_NN =_JJ -_: =[_NN 12_CD ,_, 13_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Japkowicz_NNP and_CC Stephen_NNP -LRB-_-LRB- 15_CD -RRB-_-RRB- present_VBP a_DT thorough_JJ survey_NN and_CC conducted_VBD a_DT comparative_JJ study_NN on_IN these_DT methods_NNS for_IN imbalanced_JJ data_NNS classification_NN problems_NNS ._.
They_PRP concluded_VBD that_IN in_IN most_JJS cases_NNS they_PRP studied_VBD ,_, t_NN
number_NN of_IN classification_NN errors_NNS -RRB-_-RRB- and_CC to_TO minimize_VB expected_JJ costs_NNS of_IN attributes_NNS ._.
Classifiers_NNS sensitive_JJ only_RB to_TO misclassification_NN costs_NNS ._.
This_DT problem_NN setting_NN assumes_VBZ that_IN all_DT data_NNS is_VBZ provided_VBN at_IN once_RB =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, therefore_RB there_EX are_VBP no_DT costs_NNS for_IN measuring_VBG attributes_NNS and_CC only_RB misclassification_NN costs_NNS matter_NN ._.
The_DT objective_NN is_VBZ to_TO minimize_VB the_DT expected_VBN misclassification_NN costs_NNS ._.
Classifiers_NNS sensitive_JJ to_TO both_DT attr_NN
ed_VBN on_IN the_DT given_VBN class_NN priors_NNS and_CC costs_NNS ._.
An_DT evolutionary_JJ approach_NN was_VBD proposed_VBN in_IN -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, attempting_VBG to_TO find_VB a_DT global_JJ solution_NN to_TO the_DT optimisation_NN problem_NN ._.
Other_JJ related_JJ approaches_NNS have_VBP been_VBN proposed_VBN in_IN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 21_CD -RRB-_-RRB- ._.
In_IN -LRB-_-LRB- 22_CD -RRB-_-RRB- the_DT theoretical_JJ extension_NN of_IN Neyman-Pearson_JJ optimisation_NN to_TO multiclass_JJ optimisation_NN was_VBD discussed_VBN ,_, allowing_VBG for_IN the_DT specification_NN of_IN any_DT element_NN in_IN the_DT confusion_NN matrix_NN ._.
In_IN -LRB-_-LRB- 23_CD -RRB-_-RRB- we_PRP
h_NN will_MD indeed_RB lead_VB to_TO minimum_JJ average_JJ risk_NN classification_NN ._.
A_DT number_NN of_IN previous_JJ methods_NNS in_IN cost-sensitive_JJ classifier_NN design_NN have_VBP sought_VBN to_TO improve_VB these_DT probability_NN estimates_NNS -LRB-_-LRB- Zadrozny_NNP &_CC Elkan_NNP ,_, 2002_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
However_RB ,_, accurate_JJ probability_NN estimation_NN is_VBZ not_RB essential_JJ in_IN classification_NN and_CC for_IN 0\/1_CD costs_NNS many_JJ classification_NN algorithms_NNS perform_VBP very_RB well_RB despite_IN their_PRP$ poor_JJ performance_NN as_IN probability_NN estimato_NN
methods_NNS specific_JJ for_IN decision_NN trees_NNS -LRB-_-LRB- 15_CD ,_, 4_CD -RRB-_-RRB- ,_, neural_JJ networks_NNS -LRB-_-LRB- 14_CD -RRB-_-RRB- and_CC support_NN vector_NN machines_NNS -LRB-_-LRB- 13_CD -RRB-_-RRB- ._.
The_DT second_JJ category_NN uses_VBZ Bayes_NNP risk_NN theory_NN to_TO assign_VB each_DT example_NN to_TO its_PRP$ lowest_JJS expected_VBN cost_NN class_NN =_JJ -_: =[_NN 8_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT requires_VBZ classifiers_NNS to_TO output_NN class_NN membership_NN probabilities_NNS and_CC sometimes_RB requires_VBZ estimating_NN costs_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- -LRB-_-LRB- when_WRB the_DT costs_NNS are_VBP unknown_JJ at_IN classification_NN time_NN -RRB-_-RRB- ._.
The_DT third_JJ category_NN concerns_NNS metho_NN
Netherlands_NNP ._.
cdrummond_NN ._.
tex_NN ;_: 23\/11\/2005_CD ;_: 16:54_CD ;_: p._NN 1s2_NN Drummond_NN and_CC Holte_NN In_IN cost-sensitive_JJ learning_NN ,_, expected_VBN cost_NN under_IN a_DT range_NN of_IN cost_NN matrices_NNS has_VBZ been_VBN the_DT preferred_JJ measure_NN -LRB-_-LRB- Bradford_NNP et_FW al._FW ,_, 1998_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Margineantu_NNP and_CC Dietterich_NNP ,_, 2000_CD -RRB-_-RRB- ._.
The_DT shortcomings_NNS of_IN using_VBG accuracy_NN have_VBP been_VBN pointed_VBN out_RP by_IN others_NNS -LRB-_-LRB- Hand_NN ,_, 1997_CD ;_: Provost_NNP et_FW al._FW ,_, 1998_CD -RRB-_-RRB- ._.
The_DT most_RBS fundamental_JJ shortcoming_NN is_VBZ the_DT simple_JJ fact_NN that_IN a_DT s_NN
y_NN -RRB-_-RRB- when_WRB the_DT correct_JJ class_NN is_VBZ y._JJ Important_JJ work_NN in_IN this_DT setting_NN includes_VBZ Breiman_NNP et_FW al._FW -LRB-_-LRB- 1984_CD -RRB-_-RRB- ,_, Pazzani_FW et_FW al._FW -LRB-_-LRB- 1994_CD -RRB-_-RRB- ,_, Fawcett_NNP and_CC Provost_NNP -LRB-_-LRB- 1997_CD -RRB-_-RRB- ,_, Bradford_NNP et_FW al._FW -LRB-_-LRB- 1998_CD -RRB-_-RRB- ,_, Domingos_NNP '_POS MetaCost_NNP algorithm_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ,_, Zadrozny_NNP and_CC Elkan_NNP -LRB-_-LRB- 2001_CD -RRB-_-RRB- ,_, and_CC Provost_NNP and_CC Fawcett_NNP -LRB-_-LRB- 2001_CD -RRB-_-RRB- ._.
A_DT few_JJ researchers_NNS in_IN machine_NN learning_NN have_VBP studied_VBN application_NN problems_NNS in_IN which_WDT there_EX is_VBZ a_DT cost_NN for_IN measuring_VBG each_DT attribute_NN -LRB-_-LRB- Norton_NNP ,_, 198_CD
be_VB designed_VBN explicitly_RB to_TO prefer_VB improved_JJ recognition_NN of_IN higher_JJR weighted_JJ objects_NNS ._.
Attempts_NNS have_VBP been_VBN made_VBN to_TO incorporate_VB misclassification_NN costs_NNS when_WRB doing_VBG more_RBR fully_RB automated_VBN machine_NN learning_NN -LRB-_-LRB- 35_CD -RRB-_-RRB- --_: =_JJ -_: =[_NN 37_CD -RRB-_-RRB- -_: =_SYM -_: ._.
B._NNP The_NNP Refinement_NNP A_NNP KGSL_NNP system_NN keeps_VBZ a_DT record_NN of_IN its_PRP$ pixel-level_JJ performance_NN for_IN the_DT training_NN images_NNS ._.
Using_VBG such_JJ information_NN ,_, the_DT system_NN can_MD tell_VB it_PRP has_VBZ weaknesses_NNS in_IN recognizing_VBG some_DT objects_NNS and_CC w_NN
ks_NNS on_IN the_DT former_JJ -LRB-_-LRB- Zadrozny_NNP &_CC Elkan_NNP 2001_CD ;_: Zadrozny_NNP ,_, Langford_NNP ,_, &_CC Abe_NNP 2002_CD ;_: Brefeld_NNP ,_, Geibel_NNP ,_, &_CC Wysotzki_NNP 2003_CD ;_: Abe_NNP ,_, Zadrozny_NNP ,_, &_CC Langford_NNP 2004_CD -RRB-_-RRB- ,_, more_JJR investigations_NNS are_VBP on_IN the_DT latter_JJ -LRB-_-LRB- Breiman_NNP et_FW al._FW 1984_CD ;_: =_JJ -_: =_JJ Domingos_NNP 1999_CD -_: =_JJ -_: ;_: Elkan_NNP 2001_CD ;_: Ting_NNP 2002_CD ;_: Drummond_NNP &_CC Holte_NNP 2003_CD ;_: Maloof_NNP 2003_CD -RRB-_-RRB- ._.
Note_VB that_IN the_DT example-dependent_JJ cost-sensitive_JJ learning_NN and_CC class-dependent_JJ cost-sensitive_JJ learning_NN are_VBP with_IN quite_RB different_JJ properties_NNS ._.
O_NN
on_IN of_IN costs_NNS into_IN learning_NN has_VBZ been_VBN regarded_VBN as_IN one_CD of_IN the_DT most_RBS relevant_JJ topics_NNS of_IN future_JJ machine_NN learning_NN research_NN ._.
During_IN the_DT past_JJ years_NNS ,_, many_JJ cost-sensitive_JJ learning_NN methods_NNS have_VBP been_VBN developed_VBN -LRB-_-LRB- 6_CD -RRB-_-RRB- =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 14_CD -RRB-_-RRB- -LRB-_-LRB- 23_CD -RRB-_-RRB- -LRB-_-LRB- 31_CD -RRB-_-RRB- ._.
However_RB ,_, although_IN there_EX are_VBP much_JJ research_NN efforts_NNS devoted_VBN to_TO making_VBG decision_NN trees_NNS cost-sensitive_JJ -LRB-_-LRB- 5_CD -RRB-_-RRB- -LRB-_-LRB- 17_CD -RRB-_-RRB- -LRB-_-LRB- 24_CD -RRB-_-RRB- -LRB-_-LRB- 33_CD -RRB-_-RRB- -LRB-_-LRB- 35_CD -RRB-_-RRB- -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, only_RB a_DT few_JJ studies_NNS discuss_VBP cost-sensitive_JJ neural_JJ networ_NN
nd_IN not_RB the_DT generation_NN of_IN models_NNS with_IN the_DT knowledge_NN extracted_VBN from_IN the_DT input_NN data_NNS ._.
Some_DT proposals_NNS of_IN data_NN mining_NN projects_NNS cost_VBP estimation_NN models_NNS have_VBP been_VBN published_VBN in_IN the_DT literature_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, but_CC either_CC are_VBP built_VBN only_RB for_IN a_DT specific_JJ kind_NN of_IN data_NN mining_NN technique_NN ,_, or_CC only_RB deal_VB with_IN a_DT certain_JJ project_NN phase_NN ._.
The_DT authors_NNS are_VBP working_VBG on_IN the_DT development_NN of_IN a_DT new_JJ parametric_JJ mathematical_JJ model_NN ,_,
hese_VB two_CD types_NNS of_IN errors_NNS are_VBP very_RB different_JJ -LRB-_-LRB- 13_CD -RRB-_-RRB- ._.
The_DT literature_NN related_JJ to_TO trained_JJ classifier_NN design_NN contains_VBZ very_RB little_JJ published_JJ material_NN on_IN general_JJ procedures_NNS of_IN achieving_VBG the_DT desired_VBN compromise_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: and_CC we_PRP consider_VBP our_PRP$ work_NN to_TO be_VB a_DT contribution_NN to_TO this_DT topic_NN ._.
We_PRP attained_VBD the_DT goal_NN of_IN finding_VBG such_JJ a_DT procedure_NN based_VBN on_IN a_DT modification_NN of_IN a_DT linear_JJ SVM_NN method_NN and_CC its_PRP$ parameters_NNS C_NN and_CC b._NN We_PRP evaluated_VBD o_NN
for_IN the_DT misclassification_NN model_NN ._.
Rather_RB than_IN making_VBG a_DT series_NN of_IN weighted_JJ classifiers_NNS ,_, which_WDT is_VBZ very_RB expensive_JJ ,_, appropriately_RB biased_VBN classification_NN techniques_NNS can_MD be_VB evolved_VBN based_VBN on_IN this_DT cost-matrix_JJ -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
Our_PRP$ likelihood-based_JJ classifier_NN can_MD be_VB easily_RB biased_VBN to_TO develop_VB a_DT cost-sensitive_JJ learning_NN scheme_NN ._.
The_DT rest_NN of_IN the_DT paper_NN is_VBZ organized_VBN as_IN follows_VBZ ._.
Section_NN 3_CD describes_VBZ the_DT design_NN principles_NNS of_IN the_DT pro_NN
ve_FW classifier_FW performance_NN when_WRB learning_VBG from_IN imbalanced_JJ datasets_NNS ._.
A_DT different_JJ approach_NN to_TO incorporating_VBG costs_NNS in_IN decision-making_NN is_VBZ to_TO define_VB fixed_JJ and_CC unequal_JJ misclassification_NN costs_NNS between_IN classes_NNS =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Cost_NN model_NN takes_VBZ the_DT form_NN of_IN a_DT cost_NN matrix_NN ,_, where_WRB the_DT cost_NN of_IN classifying_VBG a_DT sample_NN from_IN a_DT true_JJ class_NN j_NN to_TO class_NN i_FW corresponds_VBZ to_TO the_DT matrix_NN entry_NN λij_NN ._.
This_DT matrix_NN is_VBZ usually_RB expressed_VBN in_IN terms_NNS of_IN av_NN
rpus_NN to_TO another_DT ._.
The_DT paper_NN ends_VBZ with_IN perspectives_NNS for_IN further_JJ research_NN ._.
II_CD ._.
TERMS_NNP EXTRACTION_NNP MEASURES_NNP Different_NNP statistical_JJ criteria_NNS are_VBP used_VBN in_IN systems_NNS of_IN terminology_NN extraction_NN ,_, for_IN instance_NN ACABIT_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: uses_VBZ loglikelihood_NN measure_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- and_CC KEA_NN -LRB-_-LRB- 27_CD -RRB-_-RRB- uses_VBZ TF_NN x_NN IDF_NN measure_NN ._.
The_DT statistical_JJ criteria_NNS -LRB-_-LRB- value_NN of_IN the_DT measures_NNS and_CC the_DT rank_NN of_IN each_DT collocation_NN -RRB-_-RRB- used_VBN in_IN our_PRP$ approach_NN are_VBP :_: Mutual_NNP Information_NNP -LRB-_-LRB- MI_NNP
tion_NN -LRB-_-LRB- Hastie_NNP &_CC Tibshirani_NNP ,_, 1997_CD -RRB-_-RRB- -LRB-_-LRB- which_WDT ignores_VBZ the_DT costs_NNS and_CC attempts_NNS to_TO predict_VB the_DT label_NN with_IN minimal_JJ cost_NN -RRB-_-RRB- and_CC to_TO a_DT state-of-the-art_JJ multi-class_JJ cost-sensitive_JJ learning_NN algorithm_NN called_VBN MetaCost_NNP -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
We_PRP used_VBD Boosted_JJ Naive_JJ Bayes_NNS -LRB-_-LRB- Elkan_NNP ,_, 1997_CD -RRB-_-RRB- as_IN the_DT oracle_NN classifier_NN learner_NN and_CC applied_VBD the_DT methods_NNS to_TO five_CD UCI_NN multiclass_JJ datasets_NNS ._.
Since_IN these_DT datasets_NNS do_VBP not_RB have_VB costs_NNS associated_VBN with_IN them_PRP ,_, we_PRP g_NN
n_NN used_VBN in_IN machine_NN learning_VBG to_TO deal_VB with_IN class_NN imbalance_NN ._.
The_DT first_JJ consists_VBZ of_IN assigning_VBG distinct_JJ costs_NNS to_TO training_NN examples_NNS ,_, weighting_NN more_RBR heavily_RB those_DT in_IN the_DT minority_NN class_NN -LRB-_-LRB- Pazzani_FW et_FW al._FW ,_, 1994_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
The_DT second_JJ approach_NN is_VBZ to_TO re-sample_VB the_DT original_JJ dataset_NN ,_, either_CC by_IN over-sampling_VBG the_DT minority_NN class_NN and\/or_CC under-sampling_JJ the_DT majority_NN class_NN -LRB-_-LRB- Japkowicz_NNP ,_, 2000_CD ;_: Kubat_NNP and_CC Matwin_NNP ,_, 1997_CD ;_: Chawla_NNP et_FW al._FW ._.
n_NN largely_RB ignored_VBN in_IN the_DT domain_NN of_IN intrusion_NN detection_NN ._.
We_PRP are_VBP currently_RB aware_JJ of_IN two_CD other_JJ papers_NNS -LRB-_-LRB- -LRB-_-LRB- 2_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 3_CD -RRB-_-RRB- -RRB-_-RRB- dealing_VBG with_IN cost-sensitive_JJ intrusion_NN detection_NN ,_, both_DT using_VBG a_DT wrapper_NN algorithm_NN -LRB-_-LRB- MetaCost_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Weighted_JJ -LRB-_-LRB- 5_CD -RRB-_-RRB- respectively_RB -RRB-_-RRB- together_RB with_IN RIPPER_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
Although_IN both_DT papers_NNS report_VBP results_NNS on_IN the_DT KDD_NNP database_NN ,_, neither_CC does_VBZ so_RB for_IN the_DT given_VBN cost_NN matrix_NN ,_, so_RB direct_JJ comparisons_NNS between_IN the_DT statisti_NN
mparison_NN with_IN cost-sensitive_JJ classification_NN -LRB-_-LRB- CSC_NNP -RRB-_-RRB- ._.
Cost-sensitive_JJ classification_NN -LRB-_-LRB- also_RB called_VBN cost-sensitive_JJ learning_NN -RRB-_-RRB- is_VBZ another_DT approach_NN to_TO handling_VBG disparate_JJ kinds_NNS of_IN errors_NNS in_IN classification_NN -LRB-_-LRB- see_VB =_JJ -_: =[_NN 7_CD ,_, 8_CD ,_, 9_CD ,_, 10_CD -RRB-_-RRB- -_: =_JJ -_: and_CC references_NNS therein_RB -RRB-_-RRB- ._.
Following_VBG classical_JJ Bayesian_JJ decision_NN theory_NN ,_, CSC_NNP modifies_VBZ the_DT standard_JJ `_`` 0-1_NN '_'' loss_NN function_NN to_TO a_DT weighted_JJ Bayes_NNP cost_NN ._.
Cost-sensitive_JJ classifiers_NNS assume_VBP the_DT relative_JJ costs_NNS for_IN
et_FW al._FW ,_, 2003_CD -RRB-_-RRB- ,_, as_IN opposed_VBN to_TO other_JJ measures_NNS such_JJ as_IN Fscore_NNP -LRB-_-LRB- Caruana_NNP and_CC Niculescu-Mizil_NNP ,_, 2004_CD -RRB-_-RRB- ._.
The_DT ROC_NNP curve_NN also_RB shows_VBZ the_DT misclassification_NN rates_NNS achieved_VBD depending_VBG on_IN the_DT error_NN cost_NN coefficients_NNS -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =-]_CD ._.
For_IN these_DT reasons_NNS ,_, -LRB-_-LRB- Bradley_NNP ,_, 1997_CD -RRB-_-RRB- argues_VBZ the_DT comparison_NN of_IN the_DT ROC_NN curves_NNS attached_VBN to_TO two_CD learning_VBG algorithms_NNS to_TO be_VB more_RBR fair_JJ and_CC informative_JJ ,_, than_IN comparing_VBG their_PRP$ misclassification_NN rates_NNS only_RB ._.
Ac_NN
ights_NNS ,_, such_JJ as_IN C4_NN .5_CD ,_, Bayesian_JJ classifiers_NNS ,_, as_RB well_RB as_IN a_DT GP-based_JJ classifier_NN proposed_VBN in_IN this_DT paper_NN -LRB-_-LRB- theway_NN ofre-weighting_NN will_MD be_VB elaborated_VBN in_IN Section_NN 4_CD -RRB-_-RRB- ._.
A_DT more_RBR sophisticated_JJ method_NN called_VBN MetaCost_NNP =_SYM -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: employs_VBZ a_DT ``_`` meta-learning_JJ ''_'' procedure_NN ,_, i.e._FW bagging_NN ,_, to_TO relabel_VB the_DT classes_NNS of_IN training_NN data_NNS and_CC then_RB applies_VBZ an_DT arbitrary_JJ 2116_CD error-based_JJ classifier_NN directly_RB to_TO the_DT modified_VBN training_NN set_VBN to_TO generate_VB
the_DT cost_NN of_IN displaying_VBG an_DT irrelevant_JJ item_NN ._.
One_CD extension_NN to_TO the_DT standard_JJ classifier_NN learning_NN formulation_NN that_WDT has_VBZ received_VBN considerable_JJ attention_NN in_IN the_DT past_JJ few_JJ years_NNS is_VBZ the_DT cost_NN matrix_NN formulation_NN =_JJ -_: =[_NN 2_CD ,_, 4_CD ,_, 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT formulation_NN ,_, we_PRP specify_VBP a_DT cost_NN matrix_NN C_NN for_IN the_DT domain_NN in_IN which_WDT we_PRP would_MD like_VB to_TO learn_VB a_DT classifier_NN ._.
If_IN there_EX are_VBP k_NN classes_NNS ,_, the_DT cost_NN matrix_NN is_VBZ a_DT k_NN ×_CD k_NN matrix_NN of_IN real_JJ values_NNS ._.
Each_DT entry_NN C_NN -LRB-_-LRB- i_FW ,_,
w_NN imbalanced_JJ datasets_NNS oversampling_VBG has_VBZ performed_VBN satisfactorily_RB -LRB-_-LRB- Japkowicz_NNP and_CC Stephen_NNP ,_, 2002_CD ;_: Estabrooks_NNP and_CC Japkowicz_NNP ,_, 2004_CD -RRB-_-RRB- ,_, in_IN many_JJ other_JJ cases_NNS undersampling_VBG proves_VBZ to_TO be_VB superior_JJ to_TO oversampling_VBG -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Drummond_NNP and_CC Holte_NNP ,_, 2003_CD -RRB-_-RRB- ._.
2_CD The_DT main_JJ characteristics_NNS of_IN the_DT classifiers_NNS used_VBN are_VBP :_: Decision_NNP Trees_NNP -LRB-_-LRB- DT_NNP -RRB-_-RRB- :_: C4_NN .5_NN pruned_VBD decision_NN tree_NN generating_VBG algorithm_NN ,_, using_VBG the_DT C4_NN .5_CD pruning_NN algorithm_NN with_IN confiden_NN
or_CC example_NN ,_, -LRB-_-LRB- 15_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Doing_VBG this_DT well_NN is_VBZ often_RB nontrivial_JJ and_CC requires_VBZ considerable_JJ knowledge_NN of_IN the_DT algorithm_NN ._.
The_DT second_JJ approach_NN uses_VBZ Bayes_NNP risk_NN theory_NN to_TO assign_VB each_DT example_NN to_TO its_PRP$ lowest_JJS risk_NN class_NN =_JJ -_: =[_NN 14_CD ,_, 46_CD ,_, 32_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT requires_VBZ estimating_VBG conditional_JJ class_NN probabilities_NNS and_CC ,_, if_IN costs_NNS are_VBP stochastic_JJ ,_, estimating_VBG expected_VBN costs_NNS -LRB-_-LRB- 46_CD -RRB-_-RRB- ._.
The_DT third_JJ category_NN concerns_VBZ black-box_JJ reductions_NNS for_IN converting_VBG arbitrary_JJ class_NN
ing_JJ algorithm_NN ,_, including_VBG wrapper_NN methods_NNS ,_, class_NN distribution-based_JJ methods_NNS ,_, and_CC loss-based_JJ methods_NNS ._.
Other_JJ costsensitive_JJ learning_NN methods_NNS that_WDT are_VBP algorithm-independent_JJ include_VBP AdaCost_NN -LRB-_-LRB- 183_CD -RRB-_-RRB- ,_, MetaCost_NN =_JJ -_: =[_NN 177_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC costing_VBG -LRB-_-LRB- 222_CD -RRB-_-RRB- ._.
s312_NN Chapter_NN 5_CD Classification_NN :_: Alternative_JJ Techniques_NNS Extensive_JJ literature_NN is_VBZ also_RB available_JJ on_IN the_DT subject_NN of_IN multiclass_JJ learning_NN ._.
This_DT includes_VBZ the_DT works_NNS of_IN Hastie_NNP and_CC Tibshirani_NNP
t_NN cost_NN is_VBZ singled_VBN out_RP as_IN one_CD of_IN the_DT least_JJS considered_VBN ._.
In_IN particular_JJ ,_, two_CD types_NNS of_IN costs_NNS are_VBP considered_VBN :_: •_CD Misclassification_NN costs_NNS :_: these_DT are_VBP the_DT costs_NNS incurred_VBN by_IN classification_NN errors_NNS ._.
Works_NNP such_JJ as_IN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =-[_CD 5_CD -RRB-_-RRB- -LRB-_-LRB- 7_CD -RRB-_-RRB- considered_VBN classification_NN problems_NNS with_IN non-uniform_JJ misclassification_NN costs_NNS ._.
•_NNP Test_NNP costs_NNS :_: these_DT are_VBP the_DT costs_NNS incurred_VBN for_IN obtaining_VBG attribute_NN values_NNS ._.
Previous_JJ work_NN such_JJ as_IN -LRB-_-LRB- 10_CD -RRB-_-RRB- -LRB-_-LRB- 13_CD -RRB-_-RRB- considered_VBN
s._FW In_FW BOAT_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, Gehrke_NNP et_NNP al_NNP build_VB multiple_JJ bootstrapped_VBD trees_NNS in_IN memory_NN to_TO examine_VB the_DT splitting_JJ conditions_NNS of_IN a_DT coarse_JJ tree_NN ._.
There_EX has_VBZ been_VBN several_JJ advances_NNS in_IN cost-sensitive_JJ learning_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ._.
MetaCost_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: takes_VBZ advantage_NN of_IN purposeful_JJ mis-labels_NNS to_TO maximize_VB total_JJ benefits_NNS ._.
In_IN -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, Provost_NNP and_CC Fawcett_NNP study_VBD the_DT problem_NN on_IN how_WRB to_TO make_VB optimal_JJ decision_NN when_WRB cost_NN is_VBZ not_RB known_VBN precisely_RB ._.
5_CD Conclusion_NN In_IN th_DT
iterion_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
Others_NNS use_VBP the_DT receiver_NN operating_NN characteristic_NN -LRB-_-LRB- ROC_NN -RRB-_-RRB- curves_NNS -LRB-_-LRB- 3_CD ,_, 4_CD -RRB-_-RRB- to_TO measure_VB the_DT relative_JJ trade-offs_NNS between_IN true_JJ positives_NNS and_CC false_JJ positives_NNS ._.
Alternatively_RB ,_, cost-sensitive_JJ learning_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: recognizes_VBZ thatsidentifying_VBG a_DT buyer_NN as_IN a_DT non_JJ buyer_NN incurs_VBZ a_DT higher_JJR cost_NN than_IN identifying_VBG a_DT non_JJ buyer_NN as_IN a_DT buyer_NN ._.
Essentially_RB ,_, with_IN any_DT of_IN these_DT evaluation_NN criteria_NNS ,_, a_DT model_NN tends_VBZ to_TO target_NN decided_VBD cu_NN
cation_NN costs_NNS ._.
Most_JJS relational_JJ classifiers_NNS implicitly_RB assume_VBP that_IN all_DT misclassifications_NNS are_VBP equally_RB costly_JJ ._.
Within_IN the_DT machine_NN learning_NN community_NN ,_, there_EX is_VBZ a_DT rich_JJ tradition_NN of_IN cost-sensitive_JJ learning_NN =_JJ -_: =[_NN 8_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: applied_VBN to_TO independent_JJ identically_RB distributed_VBN data_NNS -LRB-_-LRB- non-relational_JJ data_NNS -RRB-_-RRB- which_WDT can_MD handle_VB varying_VBG misclassification_NN costs_NNS ._.
Our_PRP$ main_JJ contribution_NN is_VBZ to_TO develop_VB cost-sensitive_JJ relational_JJ classifiers_NNS w_NN
s_NN of_IN the_DT concerned_JJ class_NN ._.
Then_RB ,_, sampling_NN methods_NNS allow_VBP over-representing_VBG the_DT minority_NN class_NN or_CC under-representing_JJ the_DT majority_NN class_NN -LRB-_-LRB- 11_CD ,_, 14_CD -RRB-_-RRB- ._.
Some_DT wrapper_NN methods_NNS as_IN MetaCost_NNP have_VBP also_RB been_VBN proposed_VBN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
They_PRP produce_VBP several_JJ instances_NNS of_IN a_DT classifier_NN through_IN bootstrap_NN ,_, re-label_VBP each_DT example_NN by_IN votes_NNS and_CC build_VB another_DT model_NN using_VBG the_DT new_JJ labels_NNS ,_, ._.
Finally_RB ,_, the_DT methods_NNS closer_RBR to_TO the_DT one_NN proposed_VBN in_IN thi_NN
1997_CD ;_: Karwath_NNP &_CC King_NNP ,_, 2002_CD ;_: Weiss_NNP and_CC Provost_NNP ,_, 2003_CD ;_: Yan_NNP et_FW al._FW ,_, 2003_CD -RRB-_-RRB- ._.
In_IN cost-sensitive_JJ learning_NN ,_, expected_VBN cost_NN under_IN a_DT range_NN of_IN cost_NN matrices_NNS has_VBZ been_VBN the_DT preferred_JJ measure_NN -LRB-_-LRB- Bradford_NNP et_FW al._FW ,_, 1998_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Margineantu_NNP &_CC Dietterich_NNP ,_, 2000_CD -RRB-_-RRB- ._.
The_DT shortcomings_NNS of_IN using_VBG accuracy_NN have_VBP been_VBN pointed_VBN out_RP by_IN others_NNS -LRB-_-LRB- Hand_NN ,_, 1997_CD ;_: Provost_NNP et_FW al._FW ,_, 1998_CD -RRB-_-RRB- ._.
The_DT most_RBS fundamental_JJ shortcoming_NN is_VBZ the_DT simple_JJ fact_NN that_IN a_DT sin_NN
es_NNS ._.
This_DT is_VBZ referred_VBN to_TO as_IN class-imbalanced_JJ data_NNS ._.
In_IN machine_NN learning_NN ,_, this_DT problem_NN has_VBZ to_TO be_VB carefully_RB approached_VBN due_JJ to_TO a_DT possibility_NN of_IN different_JJ misclassificationscosts_NNS for_IN examples_NNS of_IN each_DT class_NN =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: and_CC a_DT significantly_RB degraded_JJ performance_NN when_WRB the_DT class_NN distribution_NN in_IN the_DT training_NN data_NNS is_VBZ heavily_RB skewed_JJ -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
There_EX are_VBP two_CD major_JJ groups_NNS of_IN techniques_NNS designed_VBN to_TO address_VB class_NN imbalance_NN ._.
The_DT
nly_RB on_IN the_DT system_NN 's_POS ROC_NN -LRB-_-LRB- Receiver_NN Operating_NN Characteristic_JJ -RRB-_-RRB- curve_NN ,_, but_CC also_RB on_IN cost_NN metrics_NNS and_CC the_DT probability_NN of_IN hostility_NN of_IN operating_NN ._.
MetaCost_NNP offers_VBZ a_DT general_JJ method_NN for_IN cost-sensitive_JJ learning_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT is_VBZ based_VBN on_IN the_DT assumption_NN that_IN costs_NNS are_VBP known_VBN in_IN advance_NN and_CC are_VBP the_DT same_JJ for_IN all_DT examples_NNS ._.
Certainly_RB previous_JJ research_NN has_VBZ been_VBN based_VBN on_IN the_DT assumption_NN that_IN misclassification_NN costs_NNS are_VBP the_DT
sumed_VBN that_IN all_DT classification66_NN errors_NNS are_VBP equally_RB costly_JJ ._.
However_RB ,_, in_IN many_JJ applications_NNS ,_, some_DT errors_NNS are_VBP more_RBR serious_JJ than_IN others_NNS ._.
Cost-sensitive_JJ learning_NN methods_NNS are_VBP needed_VBN to_TO address_VB this_DT problem_NN =_JJ -_: =[_NN 81_CD -RRB-_-RRB- -_: =_SYM -_: ._.
On_IN a_DT broader_JJR scale_NN ,_, the_DT research_NN reported_VBN in_IN this_DT dissertation_NN can_MD be_VB viewed_VBN as_IN solving_VBG a_DT more_RBR general_JJ machine_NN learning_NN problem_NN ,_, with_IN experimental_JJ validation_NN on_IN images_NNS as_IN data_NNS ._.
This_DT problem_NN concerns_NNS
t_NN project_NN should_MD be_VB invested_VBN ,_, the_DT execution_NN cost_NN ,_, profit_NN from_IN success_NN ,_, and_CC loss_NN from_IN failure_NN depend_VBP on_IN the_DT characteristics_NNS of_IN the_DT project_NN ._.
Cost-sensitive_JJ learning_NN -LRB-_-LRB- Elkan_NNP ,_, 2001_CD ;_: Bradford_NNP et_FW al._FW ,_, 1998_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Fan_NN et_FW al._FW ,_, 1999_CD ;_: Zadrozny_NNP and_CC Elkan_NNP ,_, 2001_CD ;_: Geibel_NNP et_FW al._FW ,_, 2004_CD ;_: Zadrozny_NNP et_FW al._FW ,_, 2003_CD ;_: Abe_NNP and_CC ._.
The_DT extend_VBP abstract_JJ of_IN this_DT paper_NN will_MD appear_VB in_IN the_DT proceedings_NNS of_IN the_DT Sixth_NNP SIAM_NNP International_NNP Con_NN
on_IN Fusion_NN xxx_NN -LRB-_-LRB- 2007_CD -RRB-_-RRB- xxx_NN --_: xxx_IN Table_NNP 1_CD Canister_NNP nodes_NNS for_IN partitions_NNS -LRB-_-LRB- each_DT time_NN step_NN -RRB-_-RRB- Type_NN Simulation_NN Partition_NN 0_CD 1_CD 2_CD 3_CD Vertical_JJ 1_CD --_: 4_CD 1640 1886 1886 1312_CD Horizontal_JJ 1_CD --_: 4_CD 1640 1640 1640 1804_CD methods_NNS as_IN in_IN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT case_NN of_IN a_DT partition_NN having_VBG zero_CD salient_JJ points_NNS ,_, a_DT single-class_JJ ``_`` classifier_NN ''_'' will_MD be_VB learned_VBN ._.
This_DT motivated_VBD an_DT adjustment_NN to_TO our_PRP$ voting_NN scheme_NN for_IN improved_VBN accuracy_NN ,_, as_IN shown_VBN in_IN Section_NNP 5_CD ._.
xample_NN ._.
Each_DT iteration_NN invokes_VBZ the_DT learning_VBG algorithm_NN to_TO minimize_VB the_DT weighted_JJ error_NN and_CC returns_VBZ a_DT hypothesis_NN ,_, which_WDT is_VBZ used_VBN in_IN a_DT final_JJ weighted_JJ vote_NN ._.
One_CD of_IN the_DT most_RBS similar_JJ work_NN to_TO ours_PRP is_VBZ MetaCost_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT is_VBZ an_DT algorithm_NN that_WDT implements_VBZ cost-sensitive_JJ classification_NN ._.
Instead_RB of_IN modifying_VBG an_DT error_NN minimization_NN classification_NN procedure_NN ,_, it_PRP views_VBZ the_DT classifier_NN as_IN a_DT black_JJ box_NN ,_, the_DT same_JJ as_IN we_PRP do_VBP ,_, a_DT
feature_NN is_VBZ identified_VBN by_IN its_PRP$ number_NN in_IN the_DT x-axis_NN ._.
The_DT thick_JJ line_NN corresponds_VBZ to_TO the_DT average_JJ value_NN of_IN the_DT first_JJ descriptor_NN for_IN class_NN contraction_NN ._.
ified_VBN sampling_NN -LRB-_-LRB- Chawla_NNP ,_, 2003_CD -RRB-_-RRB- and_CC cost_NN manipulation_NN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Maloof_NNP ,_, 2003_CD ;_: Ling_NNP and_CC Li_NNP ,_, 1998_CD -RRB-_-RRB- ._.
Decision_NN trees_NNS -LRB-_-LRB- Zadrozny_NNP and_CC Elkan_NNP ,_, 2001_CD -RRB-_-RRB- and_CC classifier_NN ensembles_NNS -LRB-_-LRB- Tan_NNP et_FW al._FW ,_, 2003_CD -RRB-_-RRB- have_VBP been_VBN adapted_VBN to_TO imbalanced_JJ problems_NNS too_RB ._.
Semi-supervised_JJ techniques_NNS have_VBP
ods_NNS ._.
2_CD Review_NN of_IN Previous_JJ Work_NNP Cost-sensitive_JJ learning_NN has_VBZ received_VBN an_DT extensive_JJ attention_NN in_IN recent_JJ years_NNS ._.
Much_JJ work_NN has_VBZ been_VBN done_VBN in_IN considering_VBG non-uniform_JJ misclassification_NN costs_NNS -LRB-_-LRB- alone_RB -RRB-_-RRB- ,_, such_JJ as_IN =_JJ -_: =[_NN 4_CD ,_, 5_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Those_DT works_NNS can_MD often_RB used_VBN to_TO solve_VB the_DT problem_NN of_IN learning_VBG with_IN very_RB imbalanced_JJ datasets_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ._.
Some_DT previous_JJ work_NN ,_, such_JJ as_IN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, considers_VBZ the_DT test_NN cost_NN alone_RB without_IN incorporating_VBG misclassification_NN
as_IN in_IN machine_NN learning_NN ._.
In_IN particular_JJ ,_, -LRB-_-LRB- Turney_NNP 2000_CD -RRB-_-RRB- considered_VBN the_DT following_JJ types_NNS of_IN costs_NNS in_IN machine_NN learning_NN :_: Misclassification_NN costs_NNS :_: costs_NNS incurred_VBN by_IN misclassification_NN errors_NNS ._.
Works_NNP such_JJ as_IN -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP 1999_CD -_: =_JJ -_: ,_, Elkan_NNP 2001_CD and_CC Kai_NNP 1998_CD -RRB-_-RRB- considered_VBN machine_NN learning_VBG with_IN non-uniform_JJ misclassification_NN costs_VBZ Test_NNP costs_NNS :_: costs_NNS incurred_VBN for_IN obtaining_VBG attribute_NN values_NNS ._.
Previous_JJ work_NN such_JJ as_IN -LRB-_-LRB- Nunez_NNP ,_, 1991_CD ;_: Tan_NNP 199_CD
ard_FW dataset_FW with_IN curtailed_VBN probabilities_NNS ._.
Result_NN is_VBZ normalized_VBN over_IN the_DT total_JJ benefits_NNS of_IN the_DT original_JJ ensemble_NN with_IN 256_CD classifiers_NNS are_VBP many_JJ recent_JJ publications_NNS on_IN cost-sensitive_JJ learning_NN ,_, Domingos_NNP -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP 1999_CD -_: =--RRB-_NN converts_VBZ a_DT cost-sensitive_JJ problem_NN into_IN an_DT equivalent_JJ cost-insensitive_JJ problem_NN by_IN mislabelling_NN ._.
A_DT recent_JJ workshop_NN on_IN cost-sensitive_JJ learning_NN can_MD be_VB found_VBN in_IN -LRB-_-LRB- Dietterich_NNP et_FW al._FW 2000_CD -RRB-_-RRB- ._.
Conclusion_NN Our_PRP$ s_NN
n_NN costs_NNS and_CC test_NN costs_NNS ,_, and_CC test_NN costs_NNS are_VBP normally_RB considered_VBN in_IN conjunction_NN with_IN misclassification_NN costs_NNS ._.
Much_JJ work_NN has_VBZ been_VBN done_VBN in_IN considering_VBG non-uniform_JJ misclassification_NN costs_NNS -LRB-_-LRB- alone_RB -RRB-_-RRB- ,_, such_JJ as_IN =_JJ -_: =[_NN 4_CD ,_, 5_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Those_DT works_NNS can_MD often_RB used_VBN to_TO solve_VB problem_NN of_IN learning_VBG with_IN very_RB imbalanced_JJ datasets_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ._.
Some_DT previous_JJ work_NN ,_, such_JJ as_IN -LRB-_-LRB- 10_CD ,_, 12_CD -RRB-_-RRB- ,_, consider_VBP the_DT test_NN cost_NN alone_RB without_IN incorporating_VBG misclassification_NN
the_DT features_NNS used_VBN ._.
2_CD Related_NNP Work_NNP Cost-sensitive_JJ regression_NN and_CC cost-sensitive_JJ classification_NN in_IN non-sequential_JJ decision_NN making_NN ,_, supervised_JJ learning_NN settings_NNS have_VBP been_VBN widely_RB studied_VBN ,_, for_IN example_NN in_IN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
While_IN -LRB-_-LRB- 7_CD -RRB-_-RRB- addresses_VBZ one_CD aspect_NN of_IN cost-sensitive_JJ sequential_JJ decision-making_NN ,_, it_PRP should_MD be_VB noted_VBN that_IN in_IN their_PRP$ special_JJ case_NN ,_, cost_NN is_VBZ only_RB associated_VBN with_IN actions_NNS --_: not_RB with_IN observing_VBG state_NN features_NNS --_:
different_JJ as_IN sparsity_NN is_VBZ gained_VBN by_IN only_RB including_VBG features_NNS with_IN a_DT cost_NN lower_JJR than_IN their_PRP$ value_NN ._.
Including_VBG costs_NNS of_IN features_NNS for_IN classification_NN and_CC regression_NN has_VBZ also_RB been_VBN discussed_VBN in_IN -LRB-_-LRB- Turney_NNP ,_, 2000_CD ;_: =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =_JJ -_: ;_: Goetschalckx_NNP &_CC Driessens_NNP ,_, 2007_CD -RRB-_-RRB- ._.
In_IN these_DT approaches_NNS ,_, binary_JJ tests_NNS are_VBP used_VBN in_IN a_DT decision_NN tree_NN if_IN the_DT expected_VBN information_NN gain_NN is_VBZ worth_JJ more_JJR than_IN the_DT cost_NN ._.
The_DT difference_NN with_IN this_DT paper_NN is_VBZ that_IN he_PRP
he_PRP published_VBD literature_NN ,_, there_EX are_VBP two_CD main_JJ categories_NNS in_IN which_WDT this_DT work_NN is_VBZ concentrated_VBN :_: •_CD Prediction_NN error_NN costs_NNS :_: Here_RB it_PRP is_VBZ assumed_VBN that_IN different_JJ prediction_NN errors_NNS incur_VBP different_JJ penalties_NNS -LRB-_-LRB- see_VB =_JJ -_: =[_NN 2_CD ,_, 4_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: for_IN a_DT sampling_NN of_IN recent_JJ work_NN -RRB-_-RRB- ._.
•_FW Feature_FW evaluation_NN costs_NNS :_: This_DT is_VBZ the_DT fundamental_JJ problem_NN that_IN we_PRP address_VBP ._.
Historically_NNP ,_, there_EX has_VBZ been_VBN 1_CD Def_NN ._.
Parsimonious_JJ -LRB-_-LRB- adjective_JJ -RRB-_-RRB- :_: Exhibiting_VBG the_DT quality_NN of_IN be_VB
-_: sensitive_JJ learning_NN does_VBZ not_RB consider_VB labeling_NN cost_NN ,_, assuming_VBG that_IN a_DT fixed_JJ set_NN of_IN labeled_JJ training_NN examples_NNS is_VBZ given_VBN ,_, and_CC that_IN the_DT learner_NN can_MD not_RB acquire_VB additional_JJ information_NN during_IN learning_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 7_CD ,_, 8_CD ,_, 30_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Active_JJ learning_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- focuses_VBZ on_IN the_DT problem_NN of_IN costly_JJ label_NN acquisition_NN ,_, although_IN often_RB the_DT cost_NN is_VBZ not_RB made_VBN explicit_JJ ._.
Active_JJ learning_NN -LRB-_-LRB- cf._VBP ,_, optimal_JJ experimental_JJ design_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- -RRB-_-RRB- uses_VBZ the_DT existing_VBG model_NN
acy_NN is_VBZ not_RB the_DT best_JJS metric_NN to_TO optimize_VB because_IN the_DT cost_NN of_IN misclassified_VBN examples_NNS is_VBZ highly_RB variable_JJ ._.
For_IN example_NN ,_, misclassifying_VBG a_DT single_JJ example_NN can_MD cost_VB from_IN nothing_NN to_TO upwards_NNS of_IN $_$ 2,000_CD ._.
MetaCost_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ a_DT well-known_JJ general_JJ method_NN for_IN training_VBG costsensitive_JJ classifiers_NNS ._.
In_IN our_PRP$ domain_NN ,_, MetaCost_NNP will_MD make_VB a_DT learned_VBN classifier_NN either_CC more_RBR conservative_JJ or_CC more_RBR aggressive_JJ about_IN waiting_VBG for_IN a_DT better_JJR pr_NN
using_VBG different_JJ cost_NN matrices_NNS in_IN equation_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
MetaCost_NNP is_VBZ a_DT wrapper_NN method_NN that_WDT can_MD be_VB used_VBN with_IN any_DT classification_NN algorithm_NN and_CC reduces_VBZ the_DT variance_NN of_IN the_DT probability_NN estimates_NNS by_IN bootstrapping_VBG -LRB-_-LRB- =_JJ -_: =_JJ Domingos_NNP ,_, 1999_CD -_: =--RRB-_NN ._.
MetaCost_NNP reduces_VBZ the_DT variance_NN of_IN probability_NN estimates_NNS ,_, but_CC is_VBZ not_RB designed_VBN to_TO overcome_VB systematic_JJ probability_NN estimation_NN errors_NNS ._.
4_LS ._.
The_DT Effect_NN of_IN the_DT Cost_NN Matrix_NNP on_IN the_DT Class-Decision_NNP Boundaries_NNPS I_PRP
the_DT cost_NN of_IN displaying_VBG an_DT irrelevant_JJ item_NN ._.
One_CD extension_NN to_TO the_DT standard_JJ classifier_NN learning_NN formulation_NN that_WDT has_VBZ received_VBN considerable_JJ attention_NN in_IN the_DT past_JJ few_JJ years_NNS is_VBZ the_DT cost_NN matrix_NN formulation_NN =_JJ -_: =[_NN 2_CD ,_, 4_CD ,_, 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT formulation_NN ,_, we_PRP specify_VBP a_DT cost_NN matrix_NN C_NN for_IN the_DT domain_NN in_IN which_WDT we_PRP would_MD like_VB to_TO learn_VB a_DT classifier_NN ._.
If_IN there_EX are_VBP k_NN classes_NNS ,_, the_DT cost_NN matrix_NN is_VBZ a_DT k_NN ×_CD k_NN matrix_NN of_IN real_JJ values_NNS ._.
Each_DT entry_NN C_NN -LRB-_-LRB- i_FW ,_,
For_IN classification_NN of_IN imbalanced_JJ data_NNS ,_, various_JJ techniques_NNS have_VBP been_VBN used_VBN during_IN the_DT learning_NN of_IN a_DT classifier_NN ._.
These_DT techniques_NNS include_VBP resizing_VBG training_NN sets_NNS -LRB-_-LRB- 9_CD ,_, 8_CD -RRB-_-RRB- ,_, adjusting_VBG misclassification_NN costs_NNS =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, learning_VBG rules_NNS for_IN skewed_JJ data_NNS -LRB-_-LRB- RLSD_NN -RRB-_-RRB- -LRB-_-LRB- 20_CD -RRB-_-RRB- ,_, and_CC cost_NN sensitive_JJ boosting_VBG -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
In_IN resizing_VBG training_NN sets_NNS ,_, either_CC the_DT minority_NN class_NN is_VBZ over-sampled_JJ or_CC the_DT majority_NN class_NN is_VBZ under-sampled_JJ ._.
In_IN both_DT cas_NNS
be_VB willing_JJ to_TO ask_VB several_JJ queries_NNS for_IN information_NN so_RB additional_JJ High_JJ instances_NNS -LRB-_-LRB- each_DT worth_JJ 10_CD utility_NN units_NNS -RRB-_-RRB- can_MD be_VB found_VBN ._.
To_TO train_VB this_DT model_NN ,_, we_PRP followed_VBD the_DT previously_RB developed_VBN MetaCost_NNP algorithm_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Metacost_NNP is_VBZ one_CD method_NN for_IN creating_VBG costsensitive_JJ classification_NN which_WDT allows_VBZ for_IN easily_RB setting_VBG a_DT cost_NN matrix_NN for_IN biasing_VBG the_DT classification_NN of_IN a_DT data_NN instance_NN as_IN belong_VBP to_TO one_CD category_NN versus_CC anot_NN
cing_JJ prediction_NN performance_NN ._.
2_CD ._.
RELATED_NNS WORK_VBP Recent_JJ research_NN on_IN class_NN imbalance_NN problem_NN has_VBZ focused_VBN on_IN several_JJ major_JJ groups_NNS of_IN techniques_NNS ._.
One_CD is_VBZ to_TO assign_VB distinct_JJ costs_NNS to_TO the_DT classification_NN errors_NNS =_JJ -_: =[_NN 6_CD ,_, 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT method_NN ,_, the_DT misclassification_NN penalty_NN for_IN the_DT positive_JJ class_NN is_VBZ assigned_VBN a_DT higher_JJR value_NN than_IN that_DT of_IN the_DT negative_JJ class_NN ._.
This_DT method_NN requires_VBZ tuning_NN to_TO come_VB up_RP with_IN good_JJ penalty_NN parameters_NNS f_SYM
we_PRP should_MD be_VB willing_JJ to_TO ask_VB several_JJ queries_NNS for_IN information_NN so_RB additional_JJ High_JJ instances_NNS -LRB-_-LRB- each_DT worth_JJ 10_CD units_NNS -RRB-_-RRB- can_MD be_VB found_VBN ._.
To_TO train_VB this_DT model_NN ,_, we_PRP followed_VBD the_DT previous_JJ developed_JJ MetaCost_NN approach_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: and_CC refer_VB the_DT reader_NN to_TO their_PRP$ work_NN for_IN additional_JJ details_NNS in_IN how_WRB the_DT cost_NN bias_NN is_VBZ created_VBN ._.
We_PRP did_VBD find_VB that_IN the_DT MetaCost_NNP approach_NN was_VBD extremely_RB effective_JJ in_IN increasing_VBG the_DT system_NN 's_POS ability_NN to_TO find_VB th_DT
re_IN usually_RB built_VBN by_IN sampling_NN the_DT patterns_NNS or_CC the_DT features_NNS -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
However_RB ,_, in_IN our_PRP$ application_NN it_PRP is_VBZ expected_VBN that_IN the_DT aggregation_NN of_IN strong_JJ classifiers_NNS will_MD help_VB to_TO reduce_VB more_JJR the_DT false_JJ positive_JJ errors_NNS =_JJ -_: =[_NN 18_CD ,_, 11_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
∗_NNP Universidad_NNP Pontificia_NNP de_IN Salamanca_NNP ,_, C\/Compa_FW ñía_FW 5_CD ,_, 37002_CD ,_, Salamanca_NNP ,_, Spain_NNP ._.
Emails_NNS :_: ablancogo@upsa.es,_FW mmartinmac@upsa.essIn_FW this_DT paper_NN we_PRP address_VBP the_DT problem_NN of_IN reducing_VBG the_DT false_JJ positive_JJ errors_NNS
s_NN into_IN cost-sensitive_JJ ones_NNS ._.
To_TO our_PRP$ knowledge_NN ,_, the_DT only_RB currently_RB available_JJ procedure_NN of_IN this_DT type_NN is_VBZ stratification_NN --_: changing_VBG the_DT frequency_NN of_IN classes_NNS in_IN the_DT training_NN data_NNS in_IN proportion_NN to_TO their_PRP$ cost_NN =_JJ -_: =[_NN 8_CD ,_, 9_CD ,_, 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, this_DT approach_NN has_VBZ severa_NN shortcomings_NNS ._.
It_PRP distorts_VBZ the_DT distribution_NN of_IN examples_NNS ,_, which_WDT may_MD seriously_RB affect_VB the_DT performance_NN of_IN some_DT algorithms_NNS ._.
It_PRP reduces_VBZ the_DT data_NNS available_JJ for_IN learning_NN ,_, i_FW
for_IN its_PRP$ resampling_NN scheme_NN ._.
Unlike_IN MetaCost_NNP ,_, it_PRP requires_VBZ repeating_VBG all_DT runs_VBZ every_DT time_NN the_DT cost_NN matrix_NN changes_NNS ._.
It_PRP has_VBZ only_RB been_VBN tested_VBN in_IN a_DT single_JJ domain_NN -LRB-_-LRB- credit-card_NN fraud_NN detection_NN -RRB-_-RRB- ._.
Ting_NN and_CC Zheng_NN =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP proposed_VBN a_DT cost-sensitive_JJ variant_NN of_IN boosting_VBG -LRB-_-LRB- an_DT ensemble_NN method_NN -RRB-_-RRB- for_IN decision_NN trees_NNS ._.
It_PRP is_VBZ significant_JJ here_RB because_IN it_PRP should_MD be_VB easily_RB adaptable_JJ to_TO other_JJ error-based_JJ learners_NNS ,_, and_CC like_IN MetaCo_NN
be_VB useful_JJ as_IN a_DT method_NN for_IN speeding_VBG up_RP learning_NN ._.
4_CD Related_NNP Work_NNP Cost-sensitive_JJ learning_NN is_VBZ the_DT subject_NN of_IN a_DT burgeoning_VBG literature_NN ,_, which_WDT space_NN does_VBZ not_RB allow_VB us_PRP to_TO review_VB here_RB ._.
We_PRP point_VBP the_DT reader_NN to_TO =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: for_IN a_DT brief_JJ review_NN ,_, and_CC to_TO -LRB-_-LRB- 25_CD -RRB-_-RRB- for_IN an_DT online_JJ bibliography_NN ._.
This_DT section_NN discusses_VBZ those_DT elements_NNS of_IN previous_JJ research_NN that_WDT are_VBP most_RBS closely_RB related_JJ to_TO MetaCost_NNP ._.
Chan_NNP and_CC Stolfo_NNP -LRB-_-LRB- 9_CD -RRB-_-RRB- have_VBP proposed_VBN a_DT v_LS
increase_NN in_IN cost_NN caused_VBN by_IN learning_VBG the_DT class_NN probabilities_NNS on_IN smaller_JJR samples_NNS may_MD be_VB offset_VBN or_CC exceeded_VBN by_IN the_DT reduction_NN obtained_VBN by_IN the_DT use_NN of_IN multiple_JJ models_NNS ._.
Indeed_RB ,_, this_DT idea_NN is_VBZ behind_IN Breiman_NNP 's_POS =_JJ -_: =[_NN 6_CD -RRB-_-RRB- successfu_NN -_: =_JJ -_: l_NN ``_`` pasting_NN ''_'' method_NN for_IN scaling_VBG up_RP learners_NNS ._.
At_IN the_DT same_JJ time_NN ,_, reducing_VBG resample_JJ sizes_NNS will_MD reduce_VB running_VBG time_NN ,_, by_IN a_DT factor_NN that_WDT will_MD be_VB particularly_RB significant_JJ if_IN the_DT errorbased_JJ learner_NN us_PRP
stics_NNS ,_, pattern_NN recognition_NN ,_, neura_NN networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN =_JJ -_: =[_NN 8_CD ,_, 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN
ery_NN poor_NN ._.
For_IN example_NN ,_, most_JJS decision_NN tree_NN and_CC rule_NN learners_NNS work_VBP by_IN attempting_VBG to_TO drive_VB class_NN probabilities_NNS to_TO zero_CD or_CC one_CD within_IN each_DT lea\/or_NN rule_NN ,_, and_CC the_DT resulting_VBG estimates_NNS are_VBP correspondingly_RB off_RB =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Because_IN of_IN this_DT ,_, and_CC because_IN some_DT classifiers_NNS may_MD not_RB produce_VB class_NN probabilities_NNS ,_, MetaCost_NNP allows_VBZ their_PRP$ use_NN ,_, but_CC does_VBZ not_RB require_VB it_PRP ._.
A_DT more_RBR robust_JJ and_CC generally-applicable_JJ method_NN for_IN obtaining_VBG cla_NN
et_CC ,_, like_IN k-nearest_NN neighbor_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- and_CC naive_JJ Bayes_NNS -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
In_IN its_PRP$ present_JJ form_NN ,_, MetaCost_NNP may_MD not_RB be_VB very_RB effective_JJ with_IN these_DT algorithms_NNS ,_, but_CC an_DT alternative_NN is_VBZ readily_RB suggested_VBN by_IN the_DT results_NNS of_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- and_CC =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Their_PRP$ method_NN consists_VBZ of_IN learning_VBG multiple_JJ models_NNS using_VBG different_JJ subsets_NNS of_IN the_DT attributes_NNS ,_, instead_RB of_IN different_JJ subsets_NNS of_IN the_DT examples_NNS ._.
K-nearest_NN neighbor_NN and_CC naive_JJ Bayes_NNS are_VBP unstable_JJ with_IN respec_NN
stics_NNS ,_, pattern_NN recognition_NN ,_, neura_NN networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN =_JJ -_: =[_NN 8_CD ,_, 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN
networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN =_JJ -_: =[_NN 11_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD
esearch_NN in_IN machine_NN learning_NN ,_, statistics_NNS ,_, pattern_NN recognition_NN ,_, neura_NN networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN =_JJ -_: =[_NN 20_CD ,_, 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assi_VB
be_VB useful_JJ as_IN a_DT method_NN for_IN speeding_VBG up_RP learning_NN ._.
4_CD Related_NNP Work_NNP Cost-sensitive_JJ learning_NN is_VBZ the_DT subject_NN of_IN a_DT burgeoning_VBG literature_NN ,_, which_WDT space_NN does_VBZ not_RB allow_VB us_PRP to_TO review_VB here_RB ._.
We_PRP point_VBP the_DT reader_NN to_TO =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: for_IN a_DT brief_JJ review_NN ,_, and_CC to_TO -LRB-_-LRB- 25_CD -RRB-_-RRB- for_IN an_DT online_JJ bibliography_NN ._.
This_DT section_NN discusses_VBZ those_DT elements_NNS of_IN previous_JJ research_NN that_WDT are_VBP most_RBS closely_RB related_JJ to_TO MetaCost_NNP ._.
Chan_NNP and_CC Stolfo_NNP -LRB-_-LRB- 9_CD -RRB-_-RRB- have_VBP proposed_VBN a_DT v_LS
obabilities_NNS for_IN every_DT example_NN ._.
It_PRP would_MD also_RB be_VB interesting_JJ to_TO do_VB an_DT ROC_NN analysis_NN of_IN MetaCost_NNP ,_, by_IN varying_VBG the_DT probability_NN thresholds_NNS at_IN which_WDT an_DT example_NN 's_POS relabeling_VBG changes_NNS from_IN one_CD class_NN to_TO another_DT =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT current_JJ version_NN of_IN MetaCost_NNP is_VBZ based_VBN on_IN bagging_NN ._.
An_DT alternative_JJ method_NN for_IN constructing_VBG model_NN ensembles_NNS is_VBZ boosting_VBG -LRB-_-LRB- 19_CD -RRB-_-RRB- ._.
Boosting_VBG often_RB achieves_VBZ lower_JJR error_NN rates_NNS than_IN bagging_NN ,_, and_CC using_VBG it_PRP in_IN
et_CC ,_, like_IN k-nearest_NN neighbor_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- and_CC naive_JJ Bayes_NNS -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
In_IN its_PRP$ present_JJ form_NN ,_, MetaCost_NNP may_MD not_RB be_VB very_RB effective_JJ with_IN these_DT algorithms_NNS ,_, but_CC an_DT alternative_NN is_VBZ readily_RB suggested_VBN by_IN the_DT results_NNS of_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- and_CC =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Their_PRP$ method_NN consists_VBZ of_IN learning_VBG multiple_JJ models_NNS using_VBG different_JJ subsets_NNS of_IN the_DT attributes_NNS ,_, instead_RB of_IN different_JJ subsets_NNS of_IN the_DT examples_NNS ._.
K-nearest_NN neighbor_NN and_CC naive_JJ Bayes_NNS are_VBP unstable_JJ with_IN respec_NN
fier_NN and_CC the_DT density_NN estimator_NN are_VBP the_DT same_JJ ,_, and_CC a_DT mismatch_NN between_IN probability_NN estimation_NN and_CC classification_NN stages_NNS has_VBZ indeed_RB been_VBN found_VBN to_TO hurt_VB performance_NN in_IN a_DT context_NN similar_JJ to_TO the_DT present_JJ one_CD -LRB-_-LRB- =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: ;_: see_VB oJso_NN related_JJ work_NN section_NN below_IN -RRB-_-RRB- ._.
For_IN example_NN ,_, decision_NN tree_NN and_CC rule_NN inducers_NNS are_VBP some_DT of_IN the_DT most_RBS effective_JJ learners_NNS for_IN very-high-dimensionoJ_NN domains_NNS like_IN those_DT often_RB found_VBN in_IN KDD_NNP ,_, but_CC these_DT
allows_VBZ their_PRP$ use_NN ,_, but_CC does_VBZ not_RB require_VB it_PRP ._.
A_DT more_RBR robust_JJ and_CC generally-applicable_JJ method_NN for_IN obtaining_VBG class_NN probability_NN estimates_NNS from_IN a_DT classifier_NN is_VBZ suggested_VBN by_IN recent_JJ research_NN on_IN model_NN ensembles_NNS =_JJ -_: =[_NN 10_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Many_JJ authors_NNS -LRB-_-LRB- e._NN g_NN ,_, Breiman_NNP -LRB-_-LRB- 5_CD -RRB-_-RRB- -RRB-_-RRB- have_VBP found_VBN that_IN most_JJS modern_JJ learners_NNS are_VBP highly_RB unstable_JJ ,_, in_IN that_IN applying_VBG them_PRP to_TO slightly_RB different_JJ training_NN sets_NNS tends_VBZ to_TO produce_VB very_RB different_JJ models_NNS and_CC corresp_NN
ule_NN rather_RB than_IN the_DT exception_NN has_VBZ led_VBN in_IN recent_JJ years_NNS to_TO an_DT increased_VBN interest_NN in_IN algorithms_NNS for_IN cost-sensitive_JJ classification_NN ._.
-LRB-_-LRB- Some_DT of_IN these_DT will_MD be_VB discussed_VBN in_IN the_DT section_NN on_IN related_JJ work_NN ;_: Turney_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_JJ -_: provides_VBZ an_DT online_JJ bibliography_NN on_IN the_DT topic_NN ._. -RRB-_-RRB-
Substantia_NN work_NN has_VBZ gone_VBN into_IN making_VBG individual_JJ algorithms_NNS cost-sensitive_JJ ._.
Doing_VBG this_DT for_IN ll_NN lgorithms_NNS available_JJ in_IN the_DT literature_NN would_MD be_VB a_DT very_JJ time_NN -_:
ny_IN well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN =_JJ -_: =[_NN 17_CD ,_, 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD of_IN a_DT finite_JJ number_NN of_IN classes_NNS ._.
Most_JJS of_IN the_DT currently-available_JJ
networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN =_JJ -_: =[_NN 11_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD
ny_IN well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN =_JJ -_: =[_NN 17_CD ,_, 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD of_IN a_DT finite_JJ number_NN of_IN classes_NNS ._.
Most_JJS of_IN the_DT currently-available_JJ
aining_VBG set_NN ,_, like_IN k-nearest_NN neighbor_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- and_CC naive_JJ Bayes_NNS -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
In_IN its_PRP$ present_JJ form_NN ,_, MetaCost_NNP may_MD not_RB be_VB very_RB effective_JJ with_IN these_DT algorithms_NNS ,_, but_CC an_DT alternative_NN is_VBZ readily_RB suggested_VBN by_IN the_DT results_NNS of_IN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 26_CD -RRB-_-RRB- ._.
Their_PRP$ method_NN consists_VBZ of_IN learning_VBG multiple_JJ models_NNS using_VBG different_JJ subsets_NNS of_IN the_DT attributes_NNS ,_, instead_RB of_IN different_JJ subsets_NNS of_IN the_DT examples_NNS ._.
K-nearest_NN neighbor_NN and_CC naive_JJ Bayes_NNS are_VBP unstable_JJ wi_NNS
esearch_NN in_IN machine_NN learning_NN ,_, statistics_NNS ,_, pattern_NN recognition_NN ,_, neura_NN networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN =_JJ -_: =[_NN 20_CD ,_, 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assi_VB
s_NN into_IN cost-sensitive_JJ ones_NNS ._.
To_TO our_PRP$ knowledge_NN ,_, the_DT only_RB currently_RB available_JJ procedure_NN of_IN this_DT type_NN is_VBZ stratification_NN --_: changing_VBG the_DT frequency_NN of_IN classes_NNS in_IN the_DT training_NN data_NNS in_IN proportion_NN to_TO their_PRP$ cost_NN =_JJ -_: =[_NN 8_CD ,_, 9_CD ,_, 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, this_DT approach_NN has_VBZ severa_NN shortcomings_NNS ._.
It_PRP distorts_VBZ the_DT distribution_NN of_IN examples_NNS ,_, which_WDT may_MD seriously_RB affect_VB the_DT performance_NN of_IN some_DT algorithms_NNS ._.
It_PRP reduces_VBZ the_DT data_NNS available_JJ for_IN learning_NN ,_, i_FW
sholds_NNS at_IN which_WDT an_DT example_NN 's_POS relabeling_VBG changes_NNS from_IN one_CD class_NN to_TO another_DT -LRB-_-LRB- 21_CD -RRB-_-RRB- ._.
The_DT current_JJ version_NN of_IN MetaCost_NNP is_VBZ based_VBN on_IN bagging_NN ._.
An_DT alternative_JJ method_NN for_IN constructing_VBG model_NN ensembles_NNS is_VBZ boosting_VBG =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Boosting_VBG often_RB achieves_VBZ lower_JJR error_NN rates_NNS than_IN bagging_NN ,_, and_CC using_VBG it_PRP in_IN MetaCost_NNP might_MD produce_VB corresponding_JJ reductions_NNS in_IN cost_NN ._.
6_CD Conclusion_NN KDD_NN applications_NNS have_VBP often_RB been_VBN hindered_VBN by_IN the_DT lack_NN of_IN
ecades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN -LRB-_-LRB- 11_CD ,_, 1_CD -RRB-_-RRB- ,_, linear_JJ and_CC neural_JJ classifiers_NNS =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD of_IN a_DT finite_JJ number_NN of_IN classes_NNS ._.
Most_JJS
e_LS it_PRP ._.
A_DT more_RBR robust_JJ and_CC generally-applicable_JJ method_NN for_IN obtaining_VBG class_NN probability_NN estimates_NNS from_IN a_DT classifier_NN is_VBZ suggested_VBN by_IN recent_JJ research_NN on_IN model_NN ensembles_NNS -LRB-_-LRB- 10_CD ,_, 14_CD -RRB-_-RRB- ._.
Many_JJ authors_NNS -LRB-_-LRB- e._NN g_NN ,_, Breiman_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =--RRB-_NN have_VBP found_VBN that_IN most_JJS modern_JJ learners_NNS are_VBP highly_RB unstable_JJ ,_, in_IN that_IN applying_VBG them_PRP to_TO slightly_RB different_JJ training_NN sets_NNS tends_VBZ to_TO produce_VB very_RB different_JJ models_NNS and_CC correspondingly_RB different_JJ predictions_NNS f_SYM
networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB exist_VBP ,_, including_VBG rule_NN induction_NN -LRB-_-LRB- 20_CD ,_, 12_CD -RRB-_-RRB- ,_, decision_NN tree_NN induction_NN -LRB-_-LRB- 8_CD ,_, 23_CD -RRB-_-RRB- ,_, instance-based_JJ learning_NN =_JJ -_: =[_NN 11_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, linear_JJ and_CC neural_JJ classifiers_NNS -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, Bayesian_JJ learning_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- ,_, and_CC others_NNS ._.
In_IN classification_NN problems_NNS ,_, the_DT goal_NN is_VBZ to_TO correctly_RB assign_VB examples_NNS -LRB-_-LRB- typically_RB described_VBN as_IN vectors_NNS of_IN attributes_NNS -RRB-_-RRB- to_TO one_CD
st_NN and_CC those_DT that_WDT can_MD be_VB varied_VBN without_IN substantial_JJ loss_NN ._.
Experiments_NNS on_IN a_DT larger_JJR database_NN indicate_VBP that_IN MetaCost_NN scales_NNS well_RB ._.
I_NN Introduction_NN Classification_NN is_VBZ one_CD of_IN the_DT primary_JJ tasks_NNS of_IN data_NNS mining_NN =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
It_PRP has_VBZ also_RB been_VBN a_DT subject_NN of_IN research_NN in_IN machine_NN learning_NN ,_, statistics_NNS ,_, pattern_NN recognition_NN ,_, neura_NN networks_NNS and_CC other_JJ areas_NNS for_IN several_JJ decades_NNS ._.
As_IN a_DT result_NN ,_, many_JJ well-developed_JJ approaches_NNS to_TO it_PRP now_RB
ical_JJ Evaluation_NN The_DT question_NN of_IN whether_IN MetaCost_NNP reduces_VBZ cost_NN compared_VBN to_TO the_DT error-based_JJ classifier_NN and_CC to_TO stratification_NN was_VBD studied_VBN empirically_RB using_VBG 28_CD benchmark_JJ databases_NNS from_IN the_DT UCI_NNP repository_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT C4_NN .5_CD decision_NN tree_NN learner_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- was_VBD used_VBN as_IN the_DT error-based_JJ classifier_NN because_IN of_IN its_PRP$ de_FW facto_FW role_NN as_IN a_DT standard_NN for_IN empirical_JJ comparisons_NNS ._.
The_DT C4_NN .5_CD RULES_NNS post-processor_JJ ,_, which_WDT converts_VBZ C4_NN .5_NN 's_POS d_NN
