Learning_NNP subspace_NN kernels_NNS for_IN classification_NN
Kernel_NNP methods_NNS have_VBP been_VBN applied_VBN successfully_RB in_IN many_JJ data_NNS mining_NN tasks_NNS ._.
Subspace_NN kernel_NN learning_NN was_VBD recently_RB proposed_VBN to_TO discover_VB an_DT effective_JJ low-dimensional_JJ subspace_NN of_IN a_DT kernel_NN feature_NN space_NN for_IN improved_VBN classification_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP to_TO construct_VB a_DT subspace_NN kernel_NN using_VBG the_DT Hilbert-Schmidt_JJ Independence_NN Criterion_NN -LRB-_-LRB- HSIC_NN -RRB-_-RRB- ._.
We_PRP show_VBP that_IN the_DT optimal_JJ subspace_NN kernel_NN can_MD be_VB obtained_VBN efficiently_RB by_IN solving_VBG an_DT eigenvalue_NN problem_NN ._.
One_CD limitation_NN of_IN the_DT existing_VBG subspace_NN kernel_NN learning_NN formulations_NNS is_VBZ that_IN the_DT kernel_NN learning_NN and_CC classification_NN are_VBP independent_JJ and_CC the_DT subspace_NN kernel_NN may_MD not_RB be_VB optimally_RB adapted_VBN for_IN classification_NN ._.
To_TO overcome_VB this_DT limitation_NN ,_, we_PRP propose_VBP a_DT joint_JJ optimization_NN framework_NN ,_, in_IN which_WDT we_PRP learn_VBP the_DT subspace_NN kernel_NN and_CC subsequent_JJ classifiers_NNS simultaneously_RB ._.
In_IN addition_NN ,_, we_PRP propose_VBP a_DT novel_JJ learning_NN formulation_NN that_WDT extracts_VBZ an_DT uncorrelated_JJ subspace_NN kernel_NN to_TO reduce_VB the_DT redundant_JJ information_NN in_IN a_DT subspace_NN kernel_NN ._.
Following_VBG the_DT idea_NN from_IN multiple_JJ kernel_NN learning_NN ,_, we_PRP extend_VBP the_DT proposed_VBN formulations_NNS to_TO the_DT case_NN when_WRB multiple_JJ kernels_NNS are_VBP available_JJ and_CC need_VBP to_TO be_VB combined_VBN ._.
We_PRP show_VBP that_IN the_DT integration_NN of_IN subspace_NN kernels_NNS can_MD be_VB formulated_VBN as_IN a_DT semidefinite_NN program_NN -LRB-_-LRB- SDP_NN -RRB-_-RRB- which_WDT is_VBZ computationally_RB expensive_JJ ._.
To_TO improve_VB the_DT efficiency_NN of_IN the_DT SDP_NNP formulation_NN ,_, we_PRP propose_VBP an_DT equivalent_JJ semi-infinite_JJ linear_NN program_NN -LRB-_-LRB- SILP_NN -RRB-_-RRB- formulation_NN which_WDT can_MD be_VB solved_VBN efficiently_RB by_IN the_DT column_NN generation_NN technique_NN ._.
Experimental_JJ results_NNS on_IN a_DT collection_NN of_IN benchmark_JJ data_NNS sets_NNS demonstrate_VBP the_DT effectiveness_NN of_IN the_DT proposed_VBN algorithms_NNS ._.
large_JJ data_NNS set_NN -LRB-_-LRB- a_DT large_JJ value_NN of_IN n_NN -RRB-_-RRB- due_JJ to_TO its_PRP$ positive_JJ semidefinite_NN constraints_NNS ._.
5.3_CD SILP_NNP Formulation_NNP We_PRP propose_VBP to_TO reformulate_VB the_DT maximization_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 23_CD -RRB-_-RRB- as_IN a_DT semi-infinite_JJ program_NN -LRB-_-LRB- SIP_NN -RRB-_-RRB- =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT can_MD then_RB be_VB solved_VBN more_RBR efficiently_RB ._.
The_DT SIP_NNP problem_NN refers_VBZ to_TO optimization_NN problems_NNS that_WDT maximize_VBP a_DT functional_JJ S_NN -LRB-_-LRB- a_DT -RRB-_-RRB- subject_JJ to_TO a_DT system_NN of_IN constraints_NNS on_IN a_DT ,_, i.e._FW ,_, s_NN -LRB-_-LRB- a_DT ,_, b_NN -RRB-_-RRB- ≤_NN 0_CD for_IN all_DT b_NN in_IN som_NN
of_IN a_DT certain_JJ kernel_NN feature_NN space_NN ._.
Subspace_NN kernel_NN learning_NN that_WDT finds_VBZ such_PDT a_DT lowdimensional_JJ subspace_NN for_IN effective_JJ pattern_NN discovery_NN has_VBZ received_VBN considerable_JJ attention_NN recently_RB -LRB-_-LRB- 13_CD ,_, 16_CD ,_, 17_CD ,_, 26_CD -RRB-_-RRB- ._.
In_IN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT discriminative_JJ subspace_NN kernel_NN learning_NN algorithm_NN was_VBD proposed_VBN to_TO find_VB a_DT low-dimensional_JJ subspace_NN of_IN the_DT kernel_NN feature_NN space_NN ._.
Kernel_NNP Target_NN Alignment_NN -LRB-_-LRB- KTA_NN -RRB-_-RRB- -LRB-_-LRB- 6_CD -RRB-_-RRB- was_VBD employed_VBN as_IN the_DT learning_NN criter_NN
dimensional_JJ subspace_NN of_IN a_DT certain_JJ kernel_NN feature_NN space_NN ._.
Subspace_NN kernel_NN learning_NN that_WDT finds_VBZ such_PDT a_DT lowdimensional_JJ subspace_NN for_IN effective_JJ pattern_NN discovery_NN has_VBZ received_VBN considerable_JJ attention_NN recently_RB =_JJ -_: =[_NN 13_CD ,_, 16_CD ,_, 17_CD ,_, 26_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, a_DT discriminative_JJ subspace_NN kernel_NN learning_NN algorithm_NN was_VBD proposed_VBN to_TO find_VB a_DT low-dimensional_JJ subspace_NN of_IN the_DT kernel_NN feature_NN space_NN ._.
Kernel_NNP Target_NN Alignment_NN -LRB-_-LRB- KTA_NN -RRB-_-RRB- -LRB-_-LRB- 6_CD -RRB-_-RRB- was_VBD employed_VBN as_IN the_DT learni_NNS
f_LS the_DT kernel_NN feature_NN space_NN ._.
Kernel_NNP Target_NN Alignment_NN -LRB-_-LRB- KTA_NN -RRB-_-RRB- -LRB-_-LRB- 6_CD -RRB-_-RRB- was_VBD employed_VBN as_IN the_DT learning_NN criterion_NN ,_, resulting_VBG in_IN a_DT complex_JJ nonlinear_JJ optimization_NN problem_NN ,_, for_IN which_WDT the_DT conjugate_NN gradient_NN algorithm_NN =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: was_VBD applied_VBN to_TO compute_VB a_DT locally_RB optimal_JJ solution_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP to_TO construct_VB a_DT subspace_NN kernel_NN using_VBG the_DT Hilbert-Schmidt_JJ Independence_NN Criterion_NN -LRB-_-LRB- HSIC_NN -RRB-_-RRB- recently_RB proposed_VBN for_IN measuring_VBG the_DT
elects_VBZ only_RB a_DT single_JJ best_JJS kernel_NN and_CC fails_VBZ to_TO exploit_VB such_JJ complementary_JJ information_NN ._.
We_PRP show_VBP that_IN the_DT integration_NN of_IN -LRB-_-LRB- uncorrelated_JJ -RRB-_-RRB- subspace_NN kernels_NNS can_MD be_VB formulated_VBN as_IN a_DT semidefinite_NN program_NN -LRB-_-LRB- SDP_NN -RRB-_-RRB- =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT is_VBZ computationally_RB expensive_JJ to_TO solve_VB ._.
To_TO improve_VB the_DT efficiency_NN of_IN the_DT SDP_NNP formulation_NN ,_, we_PRP propose_VBP an_DT equivalent_JJ semi-infinite_JJ linear_NN program_NN -LRB-_-LRB- SILP_NN -RRB-_-RRB- formulation_NN which_WDT can_MD be_VB solved_VBN efficientl_NN
Ky_NNP :_: Y_NN ×_NN Y_NN →_NN R_NN and_CC the_DT mapping_NN function_NN :_: Y_NN →_NN Fy_NN ._.
Assume_VB that_DT x_NN ∈_NN X_NN and_CC y_FW ∈_FW Ybe_NN drawn_VBD φKy_NN from_IN some_DT joint_JJ measure_NN pxy_NN -LRB-_-LRB- probability_NN distribution_NN -RRB-_-RRB- ,_, then_RB the_DT cross-variance_NN operator_NN Cxy_NN :_: Fy_FW →_FW Fx_NN is_VBZ defined_VBN as_IN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: :_: Cxy_NN =_JJ Exy_NN -LRB-_-LRB- -LRB-_-LRB- φKx_NN -LRB-_-LRB- x_NN -RRB-_-RRB- −_FW μx_FW -RRB-_-RRB- ⊗_NN -LRB-_-LRB- φKy_NN -LRB-_-LRB- y_NN -RRB-_-RRB- −_FW μy_FW -RRB-_-RRB- -RRB-_-RRB- ,_, -LRB-_-LRB- 4_CD -RRB-_-RRB- where_WRB ⊗_NN is_VBZ the_DT tensor_NN product_NN operator_NN ,_, μx_NN =_JJ E_NN -LRB-_-LRB- φKx_NN -LRB-_-LRB- x_NN -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC μy_NN =_JJ E_NN -LRB-_-LRB- φKy_NN -LRB-_-LRB- y_NN -RRB-_-RRB- -RRB-_-RRB- ._.
Given_VBN that_IN Fx_NN and_CC Fy_NN are_VBP separable_JJ RKHSs_NNS ,_, HSIC_NN is_VBZ then_RB defined_VBN as_IN the_DT squa_NN
lution_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP to_TO construct_VB a_DT subspace_NN kernel_NN using_VBG the_DT Hilbert-Schmidt_JJ Independence_NN Criterion_NN -LRB-_-LRB- HSIC_NN -RRB-_-RRB- recently_RB proposed_VBN for_IN measuring_VBG the_DT statistical_JJ dependence_NN of_IN random_JJ variables_NNS =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Under_IN HSIC_NNP ,_, an_DT optimal_JJ subspace_NN kernel_NN maximizes_VBZ its_PRP$ dependence_NN with_IN the_DT ideal_JJ kernel_NN constructed_VBN from_IN the_DT class_NN labels_NNS ._.
We_PRP show_VBP that_IN a_DT globally_RB optimal_JJ subspace_NN kernel_NN can_MD be_VB obtained_VBN efficiently_RB b_NN
USA_NN ._.
Copyright_NN 2008_CD ACM_NNP 978-1-60558-193-4_CD \/_: 08\/08_CD ..._: $_$ 5.00_CD ._.
1_CD ._.
INTRODUCTION_NNP Kernel_NNP methods_NNS have_VBP been_VBN applied_VBN successfully_RB in_IN various_JJ data_NNS mining_NN tasks_NNS such_JJ as_IN clustering_NN ,_, regression_NN ,_, and_CC classification_NN =_JJ -_: =[_NN 1_CD ,_, 4_CD ,_, 5_CD ,_, 18_CD ,_, 19_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
They_PRP work_VBP by_IN mapping_VBG the_DT data_NNS from_IN the_DT original_JJ input_NN space_NN to_TO a_DT high-dimensional_JJ -LRB-_-LRB- possibly_RB infinite-dimensional_JJ -RRB-_-RRB- feature_NN space_NN ._.
The_DT key_JJ fact_NN underlying_VBG the_DT success_NN of_IN kernel_NN methods_NNS is_VBZ that_IN the_DT emb_NN
solve_VB ._.
To_TO improve_VB the_DT efficiency_NN of_IN the_DT SDP_NNP formulation_NN ,_, we_PRP propose_VBP an_DT equivalent_JJ semi-infinite_JJ linear_NN program_NN -LRB-_-LRB- SILP_NN -RRB-_-RRB- formulation_NN which_WDT can_MD be_VB solved_VBN efficiently_RB using_VBG the_DT column_NN generation_NN technique_NN =_JJ -_: =[_NN 11_CD ,_, 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Experimental_JJ results_NNS on_IN a_DT collection_NN of_IN benchmark_JJ data_NNS sets_NNS demonstrate_VBP the_DT effectiveness_NN of_IN the_DT proposed_VBN algorithms_NNS ._.
The_DT remainder_NN of_IN this_DT paper_NN is_VBZ organized_VBN as_IN follows_VBZ :_: We_PRP review_VBP the_DT basics_NNS of_IN sub_NN
sification_NN with_IN SVM_NN on_IN the_DT original_JJ kernels_NNS -RRB-_-RRB- and_CC SKFE_NN algorithm_NN proposed_VBN in_IN -LRB-_-LRB- 25_CD -RRB-_-RRB- ._.
Following_VBG -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, we_PRP set_VBD the_DT subspace_NN dimension_NN as_IN the_DT number_NN of_IN classes_NNS in_IN the_DT corresponding_JJ data_NN sets_NNS ._.
LIBSVM_NN toolbox_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ used_VBN for_IN solving_VBG SVM_NNP optimization_NN problems_NNS in_IN the_DT following_JJ experiments_NNS ._.
In_IN data_NNS space_NN ,_, we_PRP employ_VBP the_DT Gaussian_JJ kernel_NN :_: K_NN -LRB-_-LRB- x_NN ,_, x_NN ′_NN -RRB-_-RRB- =_JJ exp_NN -LRB-_-LRB- −_NN ‖_CD x_CC −_CD x_CC ′_CD ‖_NN 2_CD \/_: σ_NN -RRB-_-RRB- ._.
For_IN the_DT UCI_NNP data_NNS ,_, we_PRP apply_VBP 5-fold_RB crossval_JJ
dimensional_JJ subspace_NN of_IN a_DT certain_JJ kernel_NN feature_NN space_NN ._.
Subspace_NN kernel_NN learning_NN that_WDT finds_VBZ such_PDT a_DT lowdimensional_JJ subspace_NN for_IN effective_JJ pattern_NN discovery_NN has_VBZ received_VBN considerable_JJ attention_NN recently_RB =_JJ -_: =[_NN 13_CD ,_, 16_CD ,_, 17_CD ,_, 26_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, a_DT discriminative_JJ subspace_NN kernel_NN learning_NN algorithm_NN was_VBD proposed_VBN to_TO find_VB a_DT low-dimensional_JJ subspace_NN of_IN the_DT kernel_NN feature_NN space_NN ._.
Kernel_NNP Target_NN Alignment_NN -LRB-_-LRB- KTA_NN -RRB-_-RRB- -LRB-_-LRB- 6_CD -RRB-_-RRB- was_VBD employed_VBN as_IN the_DT learni_NNS
-RRB-_-RRB- can_MD be_VB reformulated_VBN equivalently_RB as_IN follows_VBZ :_: max_NN W_NN ∈_FW Rn_FW ×_FW ℓ_FW tr_FW „_FW ``_`` W_NN T_NN -LRB-_-LRB- G_NN +_CC λI_NN -RRB-_-RRB- W_NN ''_'' −_NN 1_CD ``_`` W_NN T_NN GH_NN -LRB-_-LRB- y_NN -RRB-_-RRB- GW_NN ''_'' ``_`` ._.
-LRB-_-LRB- 8_CD -RRB-_-RRB- The_DT optimal_JJ transformation_NN W_NN ∗_NN to_TO Eq_NN ._.
-LRB-_-LRB- 8_CD -RRB-_-RRB- can_MD be_VB obtained_VBN by_IN solving_VBG an_DT eigenvalue_NN problem_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: ,_, as_IN summarized_VBN below_IN ._.
Theorem_NNP 3.1_CD ._.
Let_NNP G_NNP be_VB a_DT centered_JJ kernel_NN matrix_NN ,_, and_CC let_VB columns_NNS of_IN V_NN =_JJ -LRB-_-LRB- v1_NN ,_, ·_FW ·_FW ·_NN ,_, vℓ_NN -RRB-_-RRB- be_VB the_DT first_JJ ℓ_NN eigenvectors_NNS of_IN -LRB-_-LRB- G_NN +_CC λI_NN -RRB-_-RRB- −_NN 1_CD GH_NN -LRB-_-LRB- y_NN -RRB-_-RRB- G_NN corresponding_VBG to_TO the_DT largest_JJS ℓ_NN eigenvalues_NNS
tion_NN recently_RB -LRB-_-LRB- 13_CD ,_, 16_CD ,_, 17_CD ,_, 26_CD -RRB-_-RRB- ._.
In_IN -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, a_DT discriminative_JJ subspace_NN kernel_NN learning_NN algorithm_NN was_VBD proposed_VBN to_TO find_VB a_DT low-dimensional_JJ subspace_NN of_IN the_DT kernel_NN feature_NN space_NN ._.
Kernel_NNP Target_NN Alignment_NN -LRB-_-LRB- KTA_NN -RRB-_-RRB- =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: was_VBD employed_VBN as_IN the_DT learning_NN criterion_NN ,_, resulting_VBG in_IN a_DT complex_JJ nonlinear_JJ optimization_NN problem_NN ,_, for_IN which_WDT the_DT conjugate_NN gradient_NN algorithm_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- was_VBD applied_VBN to_TO compute_VB a_DT locally_RB optimal_JJ solution_NN ._.
In_IN th_DT
ion_NN ,_, we_PRP propose_VBP a_DT novel_JJ learning_NN formulation_NN that_WDT extracts_VBZ an_DT uncorrelated_JJ subspace_NN kernel_NN to_TO reduce_VB redundant_JJ information_NN in_IN a_DT subspace_NN kernel_NN ._.
Following_VBG the_DT idea_NN from_IN multiple_JJ kernel_NN learning_NN -LRB-_-LRB- MKL_NN -RRB-_-RRB- =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP extend_VBP the_DT proposed_VBN formulations_NNS to_TO the_DT case_NN when_WRB multiple_JJ kernels_NNS are_VBP available_JJ and_CC need_VBP to_TO be_VB combined_VBN ._.
For_IN example_NN ,_, when_WRB we_PRP employ_VBP the_DT Gaussian_JJ kernel_NN for_IN classification_NN ,_, we_PRP need_VBP to_TO estimate_VB t_NN
nes_NNS of_IN length_NN n._NN In_IN essence_NN ,_, HSIC_NN amounts_NNS to_TO computing_VBG the_DT trace_NN of_IN the_DT product_NN of_IN two_CD centered_JJ kernel_NN matrices_NNS ._.
HSIC_NN has_VBZ been_VBN applied_VBN successfully_RB in_IN clustering_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- and_CC supervised_JJ feature_NN selection_NN =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_JJ -_: tasks_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP to_TO employ_VB HSIC_NN for_IN subspace_NN kernels_NNS learning_VBG ._.
This_DT is_VBZ motivated_VBN by_IN a_DT number_NN of_IN appealing_JJ features_NNS of_IN HSIC_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- HISC_NN is_VBZ an_DT independence_NN measure_NN ;_: -LRB-_-LRB- 2_LS -RRB-_-RRB- HSIC_NN is_VBZ unbia_JJ
g_NN matrix_NN ,_, and_CC e_SYM is_VBZ the_DT vector_NN of_IN all_DT ones_NNS of_IN length_NN n._NN In_IN essence_NN ,_, HSIC_NN amounts_NNS to_TO computing_VBG the_DT trace_NN of_IN the_DT product_NN of_IN two_CD centered_JJ kernel_NN matrices_NNS ._.
HSIC_NN has_VBZ been_VBN applied_VBN successfully_RB in_IN clustering_NN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_JJ -_: and_CC supervised_JJ feature_NN selection_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- tasks_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP to_TO employ_VB HSIC_NN for_IN subspace_NN kernels_NNS learning_VBG ._.
This_DT is_VBZ motivated_VBN by_IN a_DT number_NN of_IN appealing_JJ features_NNS of_IN HSIC_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- HISC_NN is_VBZ an_DT i_FW
USA_NN ._.
Copyright_NN 2008_CD ACM_NNP 978-1-60558-193-4_CD \/_: 08\/08_CD ..._: $_$ 5.00_CD ._.
1_CD ._.
INTRODUCTION_NNP Kernel_NNP methods_NNS have_VBP been_VBN applied_VBN successfully_RB in_IN various_JJ data_NNS mining_NN tasks_NNS such_JJ as_IN clustering_NN ,_, regression_NN ,_, and_CC classification_NN =_JJ -_: =[_NN 1_CD ,_, 4_CD ,_, 5_CD ,_, 18_CD ,_, 19_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
They_PRP work_VBP by_IN mapping_VBG the_DT data_NNS from_IN the_DT original_JJ input_NN space_NN to_TO a_DT high-dimensional_JJ -LRB-_-LRB- possibly_RB infinite-dimensional_JJ -RRB-_-RRB- feature_NN space_NN ._.
The_DT key_JJ fact_NN underlying_VBG the_DT success_NN of_IN kernel_NN methods_NNS is_VBZ that_IN the_DT emb_NN
ssed_VBN as_IN a_DT linear_JJ combination_NN of_IN -LCB-_-LRB- φK_NN -LRB-_-LRB- xi_NN -RRB-_-RRB- -RCB-_-RRB- n_NN i_LS =_JJ 1_CD ,_, and_CC hence_RB Z_NN can_MD be_VB expressed_VBN as_IN :_: Z_NN =_JJ φK_NN -LRB-_-LRB- X_NN -RRB-_-RRB- W_NN ,_, -LRB-_-LRB- 1_LS -RRB-_-RRB- for_IN some_DT transformation_NN matrix_NN W_NN ∈_NN R_NN n_NN ×_FW ℓ_FW ._.
Let_VB Z_NN =_JJ UzΣzV_NN T_NN z_SYM be_VB the_DT Singular_JJ Value_NN Decomposition_NN -LRB-_-LRB- SVD_NN -RRB-_-RRB- =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: of_IN Z_NN ,_, whereUzconsists_NN of_IN orthonormal_JJ columns_NNS ,_, Vz_FW ∈_FW R_NN ℓ_FW ×_FW ℓ_FW is_VBZ orthogonal_JJ ,_, and_CC Σz_FW ∈_FW R_NN ℓ_FW ×_FW ℓ_FW is_VBZ diagonal_JJ ._.
Since_IN the_DT subspace_NN S_NN can_MD be_VB spanned_VBN by_IN -LCB-_-LRB- zi_FW -RCB-_-RRB- ℓ_FW i_FW =_JJ 1_CD ,_, the_DT columns_NNS of_IN Uz_NN form_VBP an_DT orthonormal_JJ basis_NN of_IN t_NN
USA_NN ._.
Copyright_NN 2008_CD ACM_NNP 978-1-60558-193-4_CD \/_: 08\/08_CD ..._: $_$ 5.00_CD ._.
1_CD ._.
INTRODUCTION_NNP Kernel_NNP methods_NNS have_VBP been_VBN applied_VBN successfully_RB in_IN various_JJ data_NNS mining_NN tasks_NNS such_JJ as_IN clustering_NN ,_, regression_NN ,_, and_CC classification_NN =_JJ -_: =[_NN 1_CD ,_, 4_CD ,_, 5_CD ,_, 18_CD ,_, 19_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
They_PRP work_VBP by_IN mapping_VBG the_DT data_NNS from_IN the_DT original_JJ input_NN space_NN to_TO a_DT high-dimensional_JJ -LRB-_-LRB- possibly_RB infinite-dimensional_JJ -RRB-_-RRB- feature_NN space_NN ._.
The_DT key_JJ fact_NN underlying_VBG the_DT success_NN of_IN kernel_NN methods_NNS is_VBZ that_IN the_DT emb_NN
below_IN :_: min_NN θ_NN ,_, t_NN i_LS ,_, ∀_FW i_FW subject_JJ to_TO kX_NN j_NN =_JJ 1_CD tj_NN „_NN I_CD +_CC 1_CD ξ_FW Pp_FW i_FW =_JJ 1_CD θiGi_NN Lhj_NN L_NN T_NN hj_FW tj_FW ``_`` ≽_NN 0_CD ,_, ∀_FW j_FW ,_, θ_FW ≥_FW 0_CD ,_, θ_NN T_NN r_NN =_JJ 1_CD ._.
-LRB-_-LRB- 30_CD -RRB-_-RRB- The_DT SDP_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 30_CD -RRB-_-RRB- can_MD be_VB solved_VBN by_IN standard_JJ optimization_NN solvers_NNS such_JJ as_IN SeDuMi_NN =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, it_PRP may_MD not_RB be_VB scalable_JJ to_TO large_JJ data_NNS set_NN -LRB-_-LRB- a_DT large_JJ value_NN of_IN n_NN -RRB-_-RRB- due_JJ to_TO its_PRP$ positive_JJ semidefinite_NN constraints_NNS ._.
5.3_CD SILP_NNP Formulation_NNP We_PRP propose_VBP to_TO reformulate_VB the_DT maximization_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 2_CD
USA_NN ._.
Copyright_NN 2008_CD ACM_NNP 978-1-60558-193-4_CD \/_: 08\/08_CD ..._: $_$ 5.00_CD ._.
1_CD ._.
INTRODUCTION_NNP Kernel_NNP methods_NNS have_VBP been_VBN applied_VBN successfully_RB in_IN various_JJ data_NNS mining_NN tasks_NNS such_JJ as_IN clustering_NN ,_, regression_NN ,_, and_CC classification_NN =_JJ -_: =[_NN 1_CD ,_, 4_CD ,_, 5_CD ,_, 18_CD ,_, 19_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
They_PRP work_VBP by_IN mapping_VBG the_DT data_NNS from_IN the_DT original_JJ input_NN space_NN to_TO a_DT high-dimensional_JJ -LRB-_-LRB- possibly_RB infinite-dimensional_JJ -RRB-_-RRB- feature_NN space_NN ._.
The_DT key_JJ fact_NN underlying_VBG the_DT success_NN of_IN kernel_NN methods_NNS is_VBZ that_IN the_DT emb_NN
dimensional_JJ subspace_NN of_IN a_DT certain_JJ kernel_NN feature_NN space_NN ._.
Subspace_NN kernel_NN learning_NN that_WDT finds_VBZ such_PDT a_DT lowdimensional_JJ subspace_NN for_IN effective_JJ pattern_NN discovery_NN has_VBZ received_VBN considerable_JJ attention_NN recently_RB =_JJ -_: =[_NN 13_CD ,_, 16_CD ,_, 17_CD ,_, 26_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, a_DT discriminative_JJ subspace_NN kernel_NN learning_NN algorithm_NN was_VBD proposed_VBN to_TO find_VB a_DT low-dimensional_JJ subspace_NN of_IN the_DT kernel_NN feature_NN space_NN ._.
Kernel_NNP Target_NN Alignment_NN -LRB-_-LRB- KTA_NN -RRB-_-RRB- -LRB-_-LRB- 6_CD -RRB-_-RRB- was_VBD employed_VBN as_IN the_DT learni_NNS
USA_NN ._.
Copyright_NN 2008_CD ACM_NNP 978-1-60558-193-4_CD \/_: 08\/08_CD ..._: $_$ 5.00_CD ._.
1_CD ._.
INTRODUCTION_NNP Kernel_NNP methods_NNS have_VBP been_VBN applied_VBN successfully_RB in_IN various_JJ data_NNS mining_NN tasks_NNS such_JJ as_IN clustering_NN ,_, regression_NN ,_, and_CC classification_NN =_JJ -_: =[_NN 1_CD ,_, 4_CD ,_, 5_CD ,_, 18_CD ,_, 19_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
They_PRP work_VBP by_IN mapping_VBG the_DT data_NNS from_IN the_DT original_JJ input_NN space_NN to_TO a_DT high-dimensional_JJ -LRB-_-LRB- possibly_RB infinite-dimensional_JJ -RRB-_-RRB- feature_NN space_NN ._.
The_DT key_JJ fact_NN underlying_VBG the_DT success_NN of_IN kernel_NN methods_NNS is_VBZ that_IN the_DT emb_NN
lems_NNS separately_RB -RRB-_-RRB- in_IN Figure_NNP 2_CD ._.
We_PRP can_MD observe_VB that_IN the_DT upper_JJ bound_VBN and_CC lower_JJR bound_VBN are_VBP approaching_VBG to_TO each_DT other_JJ within_IN a_DT small_JJ number_NN of_IN iterations_NNS ._.
It_PRP follows_VBZ from_IN the_DT theory_NN for_IN min-max_JJ problems_NNS in_IN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: that_IN the_DT iterative_JJ procedure_NN converges_VBZ to_TO the_DT saddle_NN point_NN of_IN the_DT optimization_NN problems_NNS ._.
Error_NN rate_NN 5.5_CD 5_CD 4.5_CD 4_CD 3.5_CD 3_CD 1_CD 2_CD 3_CD 4_CD 5_CD Iteration_NN number_NN SVM_NN joint_JJ uSVM_NN joint_JJ Error_NN rate_NN 8_CD 7_CD 6_CD 5_CD 4_CD 1_CD 2_CD 3_CD 4_CD 5_CD It_PRP
