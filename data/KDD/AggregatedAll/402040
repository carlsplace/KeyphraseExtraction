Magical_JJ thinking_NN in_IN data_NNS mining_NN :_: lessons_NNS from_IN CoIL_NN challenge_NN 2000_CD
CoIL_NN challenge_NN 2000_CD was_VBD a_DT supervised_JJ learning_NN contest_NN that_WDT attracted_VBD 43_CD entries_NNS ._.
The_DT authors_NNS of_IN 29_CD entries_NNS later_RB wrote_VBD explanations_NNS of_IN their_PRP$ work_NN ._.
This_DT paper_NN discusses_VBZ these_DT reports_NNS and_CC reaches_VBZ three_CD main_JJ conclusions_NNS ._.
First_JJ ,_, naive_JJ Bayesian_JJ classifiers_NNS remain_VBP competitive_JJ in_IN practice_NN :_: they_PRP were_VBD used_VBN by_IN both_CC the_DT winning_JJ entry_NN and_CC the_DT next_JJ best_JJS entry_NN ._.
Second_RB ,_, identifying_VBG feature_NN interactions_NNS correctly_RB is_VBZ important_JJ for_IN maximizing_VBG predictive_JJ accuracy_NN :_: this_DT was_VBD the_DT difference_NN between_IN the_DT winning_JJ classifier_NN and_CC all_DT others_NNS ._.
Third_JJ and_CC most_RBS important_JJ ,_, too_RB many_JJ researchers_NNS and_CC practitioners_NNS in_IN data_NNS mining_NN do_VBP not_RB appreciate_VB properly_RB the_DT issue_NN of_IN statistical_JJ significance_NN and_CC the_DT danger_NN of_IN overfitting_NN ._.
Given_VBN a_DT dataset_NN such_JJ as_IN the_DT one_NN for_IN the_DT CoIL_NN contest_NN ,_, it_PRP is_VBZ pointless_JJ to_TO apply_VB a_DT very_RB complicated_JJ learning_NN algorithm_NN ,_, or_CC to_TO perform_VB a_DT very_RB time-consuming_JJ model_NN search_NN ._.
In_IN either_DT ease_NN ,_, one_CD is_VBZ likely_JJ to_TO overfit_VB the_DT training_NN data_NNS and_CC to_TO fool_VB oneself_NN in_IN estimating_VBG predictive_JJ accuracy_NN and_CC in_IN discovering_VBG useful_JJ correlations_NNS ._.
t_NN receive_VBP the_DT test_NN set_NN targets_NNS nor_CC were_VBD they_PRP informed_VBD of_IN the_DT CoIL_NNP Challenge_NNP or_CC any_DT of_IN the_DT results_NNS ._.
In_IN contrast_NN ,_, the_DT second_JJ group_NN of_IN students_NNS read_VBP a_DT paper_NN written_VBN by_IN the_DT winner_NN of_IN the_DT prediction_NN task_NN -LRB-_-LRB- =_JJ -_: =_JJ Elkan_NNP ,_, 2001_CD -_: =--RRB-_NN ._.
Both_DT groups_NNS compete_VBP very_RB well_RB with_IN thes182_FW P._FW VAN_NNP DER_NNP PUTTEN_NNP AND_NNP M._NNP VAN_NNP SOMEREN_NNP Figure_NNP 1_CD ._.
Histogram_NN of_IN prediction_NN task_NN performance_NN for_IN CoIL_NNP Challenge_NNP participants_NNS and_CC two_CD reference_NN groups_NNS of_IN studen_NN
makes_VBZ the_DT naive_JJ assumption_NN more_RBR realistic_JJ ._.
In_IN fact_NN ,_, NBC_NNP can_MD be_VB even_RB very_RB competitive_JJ ,_, when_WRB trained_VBN with_IN a_DT carefully_RB designed_VBN feature_NN set_NN ._.
For_IN instance_NN ,_, the_DT CoIL_NN challenge_NN -LRB-_-LRB- 74_CD -RRB-_-RRB- was_VBD won_VBN by_IN a_DT NBC_NNP entry_NN =_JJ -_: =[_NN 41_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT outperformed_VBD more_RBR complicated_JJ models_NNS such_JJ as_IN SVMs_NNS or_CC neural_JJ networks_NNS ._.
The_DT data_NNS set_VBN of_IN the_DT competition_NN was_VBD characterized_VBN by_IN several_JJ correlated_JJ features_NNS and_CC noisy_JJ data_NNS ;_: a_DT later_JJ analysis_NN of_IN the_DT c_NN
chers_NNS fail_VBP to_TO create_VB good_JJ data_NNS mining_NN models_NNS because_IN they_PRP do_VBP not_RB place_VB enough_JJ importance_NN on_IN statistical_JJ significance_NN ,_, and_CC therefore_RB overfit_VBP the_DT model_NN to_TO the_DT specific_JJ dataset_NN that_IN they_PRP have_VBP available_JJ -LRB-_-LRB- =_JJ -_: =_JJ Elkan_NNP 2001_CD -_: =--RRB-_NN ._.
Existing_VBG methods_NNS guard_NN against_IN this_DT by_IN measuring_VBG how_WRB well_RB the_DT model_NN created_VBN on_IN part_NN of_IN the_DT data_NNS -LRB-_-LRB- the_DT training_NN set_NN -RRB-_-RRB- fits_VBZ a_DT second_JJ set_NN -LRB-_-LRB- validation_NN and\/or_CC testing_NN sets_NNS -RRB-_-RRB- ._.
Splitting_VBG the_DT data_NNS into_IN multip_NN
no_DT feature_NN noise_NN -LRB-_-LRB- 26_CD -RRB-_-RRB- ._.
Similarly_RB ,_, NBayes_NN ,_, which_WDT assumes_VBZ independence_NN among_IN all_DT data_NNS features_NNS ,_, is_VBZ suitable_JJ because_IN it_PRP has_VBZ been_VBN shown_VBN to_TO outperform_VB other_JJ much_RB more_RBR complicated_JJ machine_NN learning_NN methods_NNS =_JJ -_: =[_NN 27_CD -RRB-_-RRB- -_: =_SYM -_: ._.
SVM_NN is_VBZ chosen_VBN as_IN a_DT baseline_NN due_JJ to_TO its_PRP$ generalization_NN ability_NN and_CC popularity_NN in_IN medical_JJ classification_NN research_NN -LRB-_-LRB- 28_CD ,_, 29_CD -RRB-_-RRB- ._.
The_DT effectiveness_NN of_IN considering_VBG feature_NN dependencies_NNS under_IN BN-NC_NN would_MD be_VB w_NN
hat_NN we_PRP may_MD find_VB patterns_NNS that_WDT do_VBP not_RB exist_VB -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, or_CC greatly_RB overestimate_VB the_DT significance_NN of_IN a_DT pattern_NN because_IN of_IN a_DT failure_NN to_TO understand_VB the_DT role_NN of_IN parameter_NN searching_VBG in_IN the_DT data_NNS mining_NN process_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN ,_, as_IN we_PRP will_MD show_VB ,_, it_PRP can_MD be_VB very_RB difficult_JJ to_TO compare_VB the_DT results_NNS across_IN methods_NNS or_CC even_RB to_TO reproduce_VB the_DT results_NNS of_IN heavily_RB parameterized_VBN algorithms_NNS ._.
Permission_NN to_TO make_VB digital_JJ or_CC hard_JJ
eedings_NNS ._.
This_DT work_NN is_VBZ supported_VBN in_IN part_NN by_IN grants_NNS NSF_FW CAREER_FW IIS-0447773_NN ,_, and_CC NSF_NN DBI-0321756_NN ._.
spattern_NN because_IN of_IN a_DT failure_NN to_TO understand_VB the_DT role_NN of_IN parameter_NN searching_VBG in_IN the_DT data_NNS mining_NN process_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN ,_, as_IN we_PRP will_MD show_VB ,_, it_PRP can_MD be_VB very_RB difficult_JJ to_TO compare_VB the_DT results_NNS across_IN methods_NNS or_CC even_RB to_TO reproduce_VB the_DT results_NNS of_IN heavily_RB parameterized_VBN algorithms_NNS ._.
Data_NN mining_NN algorithm_NN should_MD ideall_VB
tree_NN induction_NN algorithms_NNS ,_, fuzzy_JJ clustering_NN and_CC rule_NN discovery_NN ,_, support_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- ,_, logistic_JJ regression_NN ,_, boosting_VBG and_CC bagging_VBG ,_, and_CC more_JJR -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
The_DT best_JJS technique_NN for_IN prediction_NN reported_VBN in_IN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 12_CD -RRB-_-RRB- is_VBZ the_DT Naive_JJ Bayesian_JJ learning_NN ,_, provided_VBN 800_CD predictions_NNS made_VBN ,_, which_WDT gives_VBZ a_DT hit_NN rate_NN about_IN 15.2_CD %_NN ._.
Predictors_NNS based_VBN on_IN the_DT backpropagation_NN MLP_NN networks_NNS show_VBP accuracy_NN rate_NN about_IN 71_CD %_NN and_CC hit_NN r_NN
he_PRP least_JJS common_JJ ._.
entries_NNS were_VBD then_RB chosen_VBN from_IN the_DT interval_NN -LRB-_-LRB- 0_CD ,_, 1000_CD -RRB-_-RRB- ,_, which_WDT often_RB leads_VBZ to_TO cost_NN matrices_NNS in_IN which_WDT the_DT correct_JJ label_NN is_VBZ not_RB the_DT least_JJS costly_JJ one_CD ._.
Besides_IN being_VBG unreasonable_JJ -LRB-_-LRB- see_VB Elkan_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, these_DT cost_NN matrices_NNS can_MD give_VB an_DT unfair_JJ advantage_NN to_TO cost-sensitive_JJ methods_NNS over_IN cost-insensitive_JJ ones_NNS ._.
We_PRP therefore_RB set_VBD the_DT diagonal_JJ entries_NNS to_TO be_VB identically_RB zero_CD ,_, which_WDT is_VBZ consistent_JJ with_IN our_PRP$ nor_CC
d_NN to_TO the_DT processing_NN of_IN a_DT -LRB-_-LRB- typically_RB large_JJ -RRB-_-RRB- data_NNS set_VBN with_IN a_DT view_NN to_TO summarising_VBG the_DT data_NNS into_IN a_DT more_RBR usable_JJ form_NN and\/or_CC gaining_VBG insights_NNS concerning_VBG patterns_NNS in_IN the_DT data_NNS that_WDT are_VBP statistically_RB reliable_JJ =_JJ -_: =[_NN 52_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Data_NN mining_NN comprises_VBZ a_DT variety_NN of_IN tools_NNS ,_, including_VBG those_DT drawn_VBN from_IN statistics_NNS ,_, machine_NN learning_NN and_CC natural_JJ computing_NN ._.
Such_JJ approaches_NNS ,_, could_MD ,_, therefore_RB ,_, be_VB be_VB distributed_VBN amongst_IN those_DT sections_NNS c_NN
d_NN to_TO the_DT processing_NN of_IN a_DT -LRB-_-LRB- typically_RB large_JJ -RRB-_-RRB- data_NNS set_VBN with_IN a_DT view_NN to_TO summarising_VBG the_DT data_NNS into_IN a_DT more_RBR usable_JJ form_NN and\/or_CC gaining_VBG insights_NNS concerning_VBG patterns_NNS in_IN the_DT data_NNS that_WDT are_VBP statistically_RB reliable_JJ =_JJ -_: =[_NN 58_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Data_NN mining_NN comprises_VBZ a_DT variety_NN of_IN tools_NNS ,_, including_VBG those_DT drawn_VBN from_IN statistics_NNS ,_, machine_NN learning_NN and_CC natural_JJ computing_NN ._.
Such_JJ approaches_NNS ,_, could_MD ,_, therefore_RB ,_, be_VB be_VB distributed_VBN amongst_IN those_DT sections_NNS c_NN
an_DT appropriate_JJ prior_JJ -LRB-_-LRB- any_DT one_NN will_MD do_VB in_IN the_DT limit_NN -RRB-_-RRB- --_: this_DT turns_VBZ out_RP to_TO be_VB somewhat_RB problematic_JJ in_IN practice_NN when_WRB dealing_VBG with_IN continuous_JJ variables_NNS ,_, though_IN not_RB insurmountable_JJ -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
3sperformance_NN ._.
Elkan_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: ,_, for_IN example_NN ,_, describes_VBZ the_DT winning_JJ entry_NN in_IN a_DT data_NN mining_NN contest_NN based_VBN on_IN naive_JJ Bayesian_JJ learning_NN ,_, modified_VBN -LRB-_-LRB- manually_RB by_IN the_DT author_NN -RRB-_-RRB- to_TO incorporate_VB a_DT few_JJ key_JJ dependencies_NNS between_IN variables_NNS and_CC to_TO d_NN
ecause_NN of_IN the_DT incomplete_JJ of_IN knowledge_NN ,_, it_PRP is_VBZ a_DT PAC_NN learning_NN process_NN ._.
However_RB ,_, in_IN some_DT real-problems_NNS ,_, we_PRP are_VBP not_RB sure_JJ whether_IN the_DT learning_NN rules_NNS exist_VBP or_CC not_RB ._.
So_RB ,_, we_PRP have_VBP to_TO avoid_VB the_DT magical_JJ thinking_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: that_IN ,_, there_EX is_VBZ a_DT perfect_JJ rule_NN which_WDT can_MD cover_VB all_DT positive_JJ examples_NNS ._.
b_NN -RRB-_-RRB- ._.
Another_DT important_JJ problem_NN involved_VBN in_IN real-word_JJ data_NNS mining_NN ,_, that_DT is_VBZ ,_, whether_IN the_DT database_NN contains_VBZ the_DT useful_JJ or_CC appropriate_JJ
l_NN their_PRP$ entries_NNS satisfy_VBP a_DT domain_NN expert_NN ._.
Such_JJ challenges_NNS can_MD be_VB a_DT very_RB useful_JJ source_NN of_IN feedback_NN to_TO the_DT research_NN community_NN ,_, provided_VBN that_IN thorough_JJ analysis_NN of_IN results_NNS has_VBZ been_VBN performed_VBN -LRB-_-LRB- for_IN example_NN ,_, =_JJ -_: =_JJ Elkan_NNP ,_, 2001_CD -_: =_JJ -_: ,_, describes_VBZ lessons_NNS from_IN the_DT CoIL_NN Challenge_NNP 2000_CD -RRB-_-RRB- ._.
s6_NNP N._NNP LAVRAČ_NNP ,_, H._NNP MOTODA_NNP ,_, AND_NNP T._NNP FAWCETT_NNP With_IN this_DT background_NN in_IN mind_NN ,_, a_DT workshop_NN on_IN Data_NNP Mining_NNP Lessons_NNP Learned_NNP -LRB-_-LRB- DMLL2002_NN -RRB-_-RRB- was_VBD organized_VBN at_IN the_DT Ninete_NN
winning_VBG entry_NN in_IN a_DT data_NN mining_NN contest_NN based_VBN on_IN naive_JJ Bayesian_JJ learning_NN ,_, modified_VBN -LRB-_-LRB- manually_RB by_IN the_DT author_NN -RRB-_-RRB- to_TO incorporate_VB a_DT few_JJ key_JJ dependencies_NNS between_IN variables_NNS and_CC to_TO discard_VB irrelevant_JJ variables_NNS =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT case_NN ,_, the_DT pre-representational_JJ problem_NN has_VBZ been_VBN solved_VBN directly_RB and_CC completely_RB by_IN the_DT human_JJ encoding_VBG the_DT task_NN ._.
A_DT more_RBR general_JJ approach_NN to_TO adaptive_JJ optimization_NN ,_, given_VBN a_DT solution_NN space_NN define_VB
itioner_NN of_IN magic_NN does_VBZ not_RB unlearn_VB his_PRP$ magical_JJ view_NN of_IN events_NNS when_WRB the_DT magic_NN does_VBZ not_RB work_VB ._.
In_IN fact_NN ,_, the_DT propositions_NNS which_WDT govern_VBP punctuation_NN have_VBP the_DT general_JJ characteristic_NN of_IN being_VBG self-validating_JJ &_CC q_NN =_JJ -_: =_JJ uot_NN ;_: -LRB-_-LRB- 1_LS -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN any_DT culture_NN ,_, humans_NNS have_VBP a_DT certain_JJ set_NN of_IN expectations_NNS that_IN they_PRP use_VBP to_TO explain_VB the_DT results_NNS of_IN their_PRP$ actions_NNS ._.
When_WRB something_NN surprising_JJ happens_VBZ ,_, rather_RB than_IN question_VB the_DT expectations_NNS ,_, people_NNS typi_VBP
ater_NN wrote_VBD reports_NNS explaining_VBG their_PRP$ methods_NNS and_CC results_NNS ._.
The_DT authors_NNS of_IN these_DT reports_NNS appear_VBP to_TO be_VB data_NN mining_NN practitioners_NNS or_CC researchers_NNS ,_, as_IN opposed_VBN to_TO students_NNS ._.
The_DT reports_NNS have_VBP been_VBN published_VBN by_IN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT CoIL_NN contest_NN was_VBD quite_RB similar_JJ to_TO the_DT competitions_NNS organized_VBN in_IN conjunction_NN with_IN the_DT KDD_NNP conference_NN in_IN recent_JJ years_NNS ,_, and_CC to_TO other_JJ data_NNS mining_NN competitions_NNS ._.
The_DT contest_NN task_NN was_VBD to_TO learn_VB a_DT classi_NN
1_CD yields_NNS an_DT improved_JJ estimate_NN of_IN the_DT true_JJ standard_JJ deviation_NN of_IN the_DT parent_NN population_NN that_WDT will_MD not_RB systematically_RB be_VB too_RB small_JJ ._.
Technically_NNP ,_, using_VBG 1_CD n_NN 1_CD gives_VBZ the_DT minimum_JJ variance_NN unbiased_JJ estimator_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- ._.
Man_NN -_: =_JJ -_: y_NN contest_NN submissions_NNS reveal_VBP basic_JJ misunderstandings_NNS about_IN the_DT issue_NN of_IN overfitting_NN ._.
For_IN example_NN ,_, one_CD team_NN wrote_VBD that_IN they_PRP used_VBD ''_'' :_: :_: :_: evolutionary_JJ search_NN for_IN choosing_VBG the_DT predictive_JJ features_NNS ._.
T_NN
ustering_NN methods_NNS are_VBP false_JJ ._.
The_DT k-means_NN algorithm_NN ,_, for_IN example_NN ,_, can_MD handle_VB datasets_NNS with_IN millions_NNS of_IN records_NNS and_CC hundreds_NNS of_IN dimensions_NNS ,_, where_WRB no_DT two_CD records_NNS are_VBP identical_JJ ,_, in_IN effectively_RB linear_JJ time_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Of_IN course_NN the_DT k-means_NN algorithm_NN is_VBZ not_RB a_DT panacea_NN :_: it_PRP assumes_VBZ that_IN all_DT features_NNS are_VBP numerical_JJ and_CC a_DT Euclidean_JJ distance_NN metric_NN ,_, and_CC no_DT universally_RB good_JJ method_NN is_VBZ known_VBN for_IN relaxing_VBG these_DT assumptions_NNS ._.
to_TO use_VB to_TO organize_VB our_PRP$ perceptions_NNS ._.
Whatever_WDT these_DT principles_NNS are_VBP ,_, if_IN we_PRP have_VBP learned_VBN them_PRP ,_, it_PRP is_VBZ because_IN they_PRP appeared_VBD to_TO be_VB useful_JJ in_IN the_DT past_NN ._.
As_IN pointed_VBN out_RP in_IN a_DT different_JJ context_NN by_IN Thomas_NNP Kuhn_NNP =_SYM -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, practitioners_NNS of_IN science_NN do_VBP not_RB unlearn_VB their_PRP$ scientific_JJ worldview_NN when_WRB science_NN can_MD not_RB explain_VB a_DT certain_JJ phenomenon_NN ._.
Instead_RB ,_, they_PRP either_RB ignore_VBP the_DT phenomenon_NN ,_, or_CC they_PRP redouble_VBP their_PRP$ efforts_NNS to_TO und_VB
is_VBZ sensitive_JJ to_TO unbalanced_JJ data_NNS ,_, and_CC probability_NN estimates_NNS at_IN leaves_NNS are_VBP smoothed_VBN ,_, then_RB decision_NN trees_NNS can_MD be_VB fully_RB competitive_JJ with_IN naive_JJ Bayesian_JJ classifiers_NNS on_IN commercial_JJ response_NN prediction_NN tasks_NNS =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
9_CD ._.
CONCLUSIONS_NNS In_IN summary_NN ,_, there_EX are_VBP three_CD main_JJ lessons_NNS to_TO be_VB learned_VBN from_IN the_DT CoIL_NN data_NNS mining_NN contest_NN ._.
The_DT first_JJ two_CD lessons_NNS are_VBP technical_JJ ,_, one_CD positive_JJ and_CC one_CD negative_JJ ._.
The_DT positive_JJ lesson_NN is_VBZ th_DT
possible_JJ to_TO compare_VB two_CD different_JJ learning_VBG methods_NNS with_IN the_DT same_JJ training_NN and_CC test_NN datasets_NNS in_IN a_DT way_NN that_WDT is_VBZ more_RBR sensitive_JJ than_IN the_DT simple_JJ binomial_JJ calculation_NN above_IN ,_, using_VBG McNemar_NNP 's_POS hypothesis_NN test_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Let_VB A_NN and_CC B_NN designate_VBP two_CD learning_VBG algorithms_NNS and_CC let_VB n_NN 10_CD be_VB the_DT number_NN of_IN test_NN examples_NNS classified_VBN correctly_RB by_IN A_NN but_CC incorrectly_RB by_IN B._NNP Similarly_RB ,_, let_VB n_NN 01_CD be_VB the_DT number_NN classified_VBN incorrectly_RB by_IN
eatures_NNS were_VBD discretized_VBN in_IN advance_NN ,_, the_DT CoIL_NN competition_NN could_MD not_RB serve_VB as_IN a_DT test_NN of_IN discretization_NN methods_NNS ._.
The_DT predictive_JJ accuracy_NN of_IN a_DT naive_JJ Bayesian_JJ classifier_NN can_MD often_RB be_VB improved_VBN by_IN boosting_VBG =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC by_IN adding_VBG new_JJ attributes_NNS derived_VBN from_IN combinations_NNS of_IN existing_VBG attributes_NNS ._.
Both_DT boosting_VBG and_CC derived_VBN attributes_NNS are_VBP ways_NNS of_IN relaxing_VBG the_DT conditional_JJ independence_NN assumptions_NNS that_WDT constitute_VBP the_DT
making_VBG the_DT offer_NN ._.
Therefore_RB ,_, the_DT aim_NN of_IN data_NNS mining_NN should_MD be_VB to_TO estimate_VB the_DT probability_NN that_IN a_DT customer_NN would_MD accept_VB an_DT offer_NN ,_, and_CC also_RB the_DT costs_NNS and_CC benefits_NNS of_IN the_DT customer_NN accepting_VBG or_CC declining_VBG =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Second_RB ,_, a_DT customer_NN should_MD not_RB be_VB offered_VBN an_DT insurance_NN policy_NN just_RB because_IN he_PRP or_CC she_PRP resembles_VBZ other_JJ customers_NNS who_WP have_VBP the_DT same_JJ type_NN of_IN policy_NN ._.
The_DT characteristics_NNS that_WDT predict_VBP who_WP is_VBZ most_RBS likely_JJ to_TO
also_RB methodologies_NNS for_IN data_NN mining_NN ._.
It_PRP is_VBZ noteworthy_JJ that_IN none_NN of_IN the_DT reports_NNS written_VBN by_IN CoIL_NN contest_NN participants_NNS mention_VBP using_VBG any_DT part_NN of_IN the_DT CRISP-DM_NN European_JJ standard_JJ methodology_NN for_IN data_NNS mining_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
8_CD ._.
CHOICE_NN OF_IN LEARNING_VBG METHOD_NN Although_IN it_PRP is_VBZ difficult_JJ to_TO say_VB with_IN certainty_NN that_IN one_CD learning_NN method_NN gives_VBZ more_RBR accurate_JJ classifiers_NNS than_IN another_DT ,_, it_PRP is_VBZ possible_JJ to_TO say_VB with_IN certainty_NN that_IN for_IN pract_NN
fic_JJ thinking_NN ._.
Being_VBG primed_VBN to_TO see_VB patterns_NNS in_IN small_JJ datasets_NNS is_VBZ an_DT innate_JJ characteristic_NN of_IN humans_NNS and_CC perhaps_RB other_JJ animals_NNS also_RB ,_, and_CC this_DT characteristic_NN is_VBZ often_RB useful_JJ for_IN success_NN in_IN everyday_JJ life_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Moreover_RB ,_, the_DT starting_VBG point_NN of_IN scientific_JJ thinking_NN is_VBZ often_RB a_DT type_NN of_IN magical_JJ thinking_NN :_: scientists_NNS commonly_RB posit_VBP hypotheses_NNS based_VBN on_IN a_DT low_JJ number_NN of_IN observations_NNS ._.
These_DT hypotheses_NNS are_VBP frequently_RB u_NN
