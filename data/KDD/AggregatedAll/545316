Data_NN mining_NN in_IN metric_JJ space_NN :_: an_DT empirical_JJ analysis_NN of_IN supervised_JJ learning_NN performance_NN criteria_NNS
Many_JJ criteria_NNS can_MD be_VB used_VBN to_TO evaluate_VB the_DT performance_NN of_IN supervised_JJ learning_NN ._.
Different_JJ criteria_NNS are_VBP appropriate_JJ in_IN different_JJ settings_NNS ,_, and_CC it_PRP is_VBZ not_RB always_RB clear_JJ which_WDT criteria_NNS to_TO use_VB ._.
A_DT further_JJ complication_NN is_VBZ that_IN learning_VBG methods_NNS that_WDT perform_VBP well_RB on_IN one_CD criterion_NN may_MD not_RB perform_VB well_RB on_IN other_JJ criteria_NNS ._.
For_IN example_NN ,_, SVMs_NNS and_CC boosting_VBG are_VBP designed_VBN to_TO optimize_VB accuracy_NN ,_, whereas_IN neural_JJ nets_NNS typically_RB optimize_VBP squared_VBN error_NN or_CC cross_NN entropy_NN ._.
We_PRP conducted_VBD an_DT empirical_JJ study_NN using_VBG a_DT variety_NN of_IN learning_VBG methods_NNS -LRB-_-LRB- SVMs_NNS ,_, neural_JJ nets_NNS ,_, k-nearest_NN neighbor_NN ,_, bagged_VBN and_CC boosted_VBN trees_NNS ,_, and_CC boosted_VBD stumps_NNS -RRB-_-RRB- to_TO compare_VB nine_CD boolean_JJ classification_NN performance_NN metrics_NNS :_: Accuracy_NN ,_, Lift_NN ,_, F-Score_NN ,_, Area_NNP under_IN the_DT ROC_NNP Curve_NNP ,_, Average_NNP Precision_NNP ,_, Precision\/Recall_NNP Break-Even_NNP Point_NNP ,_, Squared_NNP Error_NNP ,_, Cross_NNP Entropy_NNP ,_, and_CC Probability_NNP Calibration_NNP ._.
Multidimensional_JJ scaling_NN -LRB-_-LRB- MDS_NN -RRB-_-RRB- shows_VBZ that_IN these_DT metrics_NNS span_VBP a_DT low_JJ dimensional_JJ manifold_NN ._.
The_DT three_CD metrics_NNS that_WDT are_VBP appropriate_JJ when_WRB predictions_NNS are_VBP interpreted_VBN as_IN probabilities_NNS :_: squared_VBD error_NN ,_, cross_NN entropy_NN ,_, and_CC calibration_NN ,_, lay_VBD in_IN one_CD part_NN of_IN metric_JJ space_NN far_RB away_RB from_IN metrics_NNS that_WDT depend_VBP on_IN the_DT relative_JJ order_NN of_IN the_DT predicted_VBN values_NNS :_: ROC_NN area_NN ,_, average_JJ precision_NN ,_, break-even_JJ point_NN ,_, and_CC lift_NN ._.
In_IN between_IN them_PRP fall_VBP two_CD metrics_NNS that_WDT depend_VBP on_IN comparing_VBG predictions_NNS to_TO a_DT threshold_NN :_: accuracy_NN and_CC F-score_NN ._.
As_IN expected_VBN ,_, maximum_NN margin_NN methods_NNS such_JJ as_IN SVMs_NNS and_CC boosted_VBN trees_NNS have_VBP excellent_JJ performance_NN on_IN metrics_NNS like_IN accuracy_NN ,_, but_CC perform_VBP poorly_RB on_IN probability_NN metrics_NNS such_JJ as_IN squared_VBN error_NN ._.
What_WP was_VBD not_RB expected_VBN was_VBD that_IN the_DT margin_NN methods_NNS have_VBP excellent_JJ performance_NN on_IN ordering_VBG metrics_NNS such_JJ as_IN ROC_NN area_NN and_CC average_JJ precision_NN ._.
We_PRP introduce_VBP a_DT new_JJ metric_NN ,_, SAR_NNP ,_, that_IN combines_NNS squared_VBD error_NN ,_, accuracy_NN ,_, and_CC ROC_NNP area_NN into_IN one_CD metric_NN ._.
MDS_NN and_CC correlation_NN analysis_NN shows_VBZ that_IN SAR_NN is_VBZ centrally_RB located_JJ and_CC correlates_VBZ well_RB with_IN other_JJ metrics_NNS ,_, suggesting_VBG that_IN it_PRP is_VBZ a_DT good_JJ general_JJ purpose_NN metric_NN to_TO use_VB when_WRB more_RBR specific_JJ criteria_NNS are_VBP not_RB known_VBN ._.
etrics_NNS ._.
The_DT five_CD metrics_NNS are_VBP :_: accuracy_NN ,_, AUC_NN ,_, F-measure_NN ,_, Root_NN Mean_NN Squared_VBD error_NN -LRB-_-LRB- RMS_NNP -RRB-_-RRB- ,_, and_CC Mean_NN Cross_NN Entropy_NN -LRB-_-LRB- MXE_NN -RRB-_-RRB- ._.
Definitions_NNS of_IN these_DT metrics_NNS can_MD be_VB found_VBN in_IN Appendix_NNP ._.
Caruana_NNP and_CC Niculescu-Mizil_NNP =_SYM -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: suggested_VBD that_IN when_WRB the_DT goal_NN metric_NN is_VBZ unknown_JJ during_IN model_NN construction_NN and_CC model_NN selection_NN ,_, one_PRP could_MD use_VB the_DT average_NN of_IN several_JJ different_JJ metrics_NNS as_IN the_DT goal_NN metric_NN ._.
Here_RB we_PRP use_VBP the_DT average_NN of_IN the_DT
class_NN ̂yi_NN as_IN output_NN by_IN the_DT classifier_NN ,_, which_WDT is_VBZ probabilistic_JJ ,_, and_CC the_DT actual_JJ class_NN label_NN yi_NN determined_VBN by_IN a_DT human_JJ ,_, which_WDT is_VBZ discrete_JJ ._.
RMSE_NNP has_VBZ been_VBN shown_VBN to_TO be_VB an_DT appropriate_JJ ,_, wellcalibrated_JJ metric_JJ -LRB-_-LRB- =_JJ -_: =_JJ Caruana_NNP &_CC Niculescu-Mizil_NNP ,_, 2004_CD -_: =--RRB-_NN ._.
Further_RB motivating_VBG its_PRP$ use_NN here_RB is_VBZ that_IN ,_, unlike_IN traditional_JJ binary_JJ classification_NN accuracy_NN -LRB-_-LRB- ACC_NN -RRB-_-RRB- ,_, RMSE_NN captures_VBZ prediction_NN confidence_NN :_: correct_JJ predictions_NNS of_IN higher_JJR confidence_NN will_MD result_VB in_IN higher_JJR
tion_NN of_IN ,_, the_DT available_JJ configurations_NNS ;_: namely_RB algorithm_NN classification_NN performance_NN -LRB-_-LRB- ACP_NN -RRB-_-RRB- -LRB-_-LRB- assessing_VBG accuracy_NN -RRB-_-RRB- and_CC algorithm_NN sensitivity_NN -LRB-_-LRB- AS_JJ -RRB-_-RRB- -LRB-_-LRB- assessing_VBG robustness_NN -RRB-_-RRB- ._.
Finally_RB ,_, mean_VB cross_JJ entropy_NN -LRB-_-LRB- MXE_NN -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =_JJ Caruana_NNP &_CC Niculescu-Mizil_NNP ,_, 2004_CD -_: =--RRB-_NN is_VBZ used_VBN to_TO assess_VB the_DT performance_NN attribute_NN in_IN terms_NNS of_IN accuracy_NN in_IN the_DT probabilistic_JJ setting_NN when_WRB interested_VBN in_IN predicting_VBG the_DT probability_NN that_IN an_DT instance_NN belongs_VBZ to_TO a_DT particular_JJ class_NN and_CC similar_JJ
ared_JJ error_NN -LRB-_-LRB- RMSE_NN -RRB-_-RRB- ,_, represent_VBP three_CD most_RBS often_RB used_VBN metrics_NNS ,_, which_WDT represent_VBP 8_CD http:\/\/www.cs.toronto.edu\/˜delve\/data\/datasets.html16_NN threshold_NN metric_NN ,_, probability_NN metric_NN and_CC rank_NN metric_NN ,_, respectively_RB =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN our_PRP$ paper_NN ,_, we_PRP will_MD use_VB the_DT three_CD performance_NN metrics_NNS for_IN binary_JJ classification_NN problems_NNS ._.
The_DT procedure_NN of_IN parameter_NN optimization_NN follows_VBZ Rätsch_NNP 's_POS methodology_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, which_WDT trains_VBZ the_DT algorithm_NN with_IN
umber_NN of_IN true_JJ positives_NNS in_IN the_DT top_JJ r_NN positions_NNS and_CC NT_NN P_NN is_VBZ the_DT total_JJ number_NN of_IN true_JJ categories_NNS of_IN instance_NN x._NN maxF_NN 1_CD is_VBZ thus_RB defined_VBN as_IN maxF_NN 1_CD =_JJ maxr_NN F_NN 1_CD -LRB-_-LRB- r_NN -RRB-_-RRB- ._.
Precision_NN Recall_VB Break_VB Even_JJ Point_NN -LRB-_-LRB- PRBEP_NN -RRB-_-RRB- =_JJ -_: =[_NN 2_CD ,_, 30_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ commonly_RB used_VBN for_IN measuring_VBG the_DT performance_NN of_IN text_NN categorization_NN ._.
A_DT text_NN categorization_NN algorithm_NN typically_RB exhibits_VBZ some_DT trade-offs_NNS between_IN precision_NN and_CC recall_NN ._.
For_IN instance_NN ,_, an_DT algorithm_NN that_IN
-LRB-_-LRB- 30_CD -RRB-_-RRB- is_VBZ also_RB a_DT theoretical_JJ work_NN on_IN the_DT features_NNS a_DT metric_NN should_MD have_VB and_CC proposing_VBG new_JJ ones_NNS ._.
4However_NN ,_, empirical_JJ studies_NNS have_VBP been_VBN scarce_JJ and_CC limited_JJ in_IN the_DT literature_NN ._.
The_DT only_JJ exception_NN to_TO this_DT is_VBZ =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, independent_JJ and_CC simultaneous_JJ to_TO a_DT preliminary_JJ work_NN of_IN us_PRP -LRB-_-LRB- 19_CD -RRB-_-RRB- ._.
Caruana_NNP &_CC Niculescu-Mizil_NNP 's_POS work_NN analyses_NNS the_DT behavior_NN of_IN several_JJ performance_NN measures_NNS against_IN a_DT great_JJ number_NN of_IN machine_NN learning_NN tech_NN
graphs_NNS are_VBP best_RB printed_VBN in_IN color_NN ._.
three_CD times_NNS more_RBR event-related_JJ queries_NNS in_IN this_DT set_NN of_IN queries_NNS than_IN in_IN a_DT 0.01-fraction_JJ random_JJ sample_NN of_IN the_DT queries_NNS ._.
Lift_VB measures_NNS the_DT precision_NN at_IN a_DT given_VBN threshold_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, albeit_RB scaled_VBD such_JJ that_IN it_PRP can_MD be_VB larger_JJR than_IN 1_CD ._.
Table_NNP 3_CD shows_VBZ the_DT average_JJ precision_NN and_CC lift_NN obtained_VBN by_IN the_DT system_NN when_WRB attempting_VBG to_TO identify_VB term-matched_JJ queries_NNS in_IN the_DT popular_JJ queries_NNS ._.
The_DT lift_NN
2_CD n_NN ·_NN c_NN Although_IN originally_RB MSE_NNP is_VBZ not_RB a_DT calibration_NN measure_NN ,_, it_PRP was_VBD decomposed_VBN in_IN -LRB-_-LRB- 7_CD -RRB-_-RRB- in_IN terms_NNS of_IN calibration_NN loss_NN and_CC refinement_NN loss_NN ._.
--_: A_DT calibration_NN measure_NN based_VBN on_IN overlapping_VBG binning_NN is_VBZ CalBin_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT is_VBZ defined_VBN as_IN follows_VBZ ._.
For_IN each_DT class_NN ,_, we_PRP must_MD order_VB all_DT cases_NNS by_IN predicted_VBN p_NN -LRB-_-LRB- i_FW ,_, j_NN -RRB-_-RRB- ,_, giving_VBG new_JJ indices_NNS i_FW ∗_FW ._.
Take_VB the_DT 100_CD first_JJ elements_NNS -LRB-_-LRB- i_FW ∗_FW from_IN 1_CD to_TO 100_CD -RRB-_-RRB- as_IN the_DT first_JJ bin_NN ._.
Calculate_VB the_DT per_IN
erformance_NN measures_NNS ,_, aiming_VBG to_TO find_VB one_CD that_WDT performs_VBZ well_RB for_IN the_DT application_NN specific_JJ performance_NN measure_NN after_IN post-processing_VBG the_DT resulting_VBG model_NN -LRB-_-LRB- e.g._FW -LRB-_-LRB- Lewis_NNP ,_, 2001_CD ;_: Yang_NNP ,_, 2001_CD ;_: Abe_NNP et_FW al._FW ,_, 2004_CD ;_: =_JJ -_: =_JJ Caruana_NNP &_CC Niculescu-Mizil_NNP ,_, 2004_CD -_: =--RRB-_NN -RRB-_-RRB- ._.
However_RB ,_, in_IN particular_JJ for_IN non-linear_JJ performance_NN measures_NNS like_IN F1score_NN or_CC PRBEP_NN ,_, the_DT relationship_NN to_TO tractable_JJ measures_NNS is_VBZ at_IN best_JJS approximate_JJ and_CC requires_VBZ extensive_JJ search_NN via_IN cross-validation_NN ._.
s\/classifier\/learning_NN algorithm_NN ._.
Therefore_RB ,_, the_DT Area_NNP Under_IN the_DT ROC_NN curve_NN -LRB-_-LRB- AUC_NN -RRB-_-RRB- does_VBZ not_RB depend_VB on_IN the_DT imbalance_NN of_IN the_DT training_NN set_VBN -LRB-_-LRB- Kolcz_NNP et_FW al._FW ,_, 2003_CD -RRB-_-RRB- ,_, as_IN opposed_VBN to_TO other_JJ measures_NNS such_JJ as_IN F_NN score_NN -LRB-_-LRB- =_JJ -_: =_JJ Caruana_NNP &_CC Niculescu-Mizil_NNP ,_, 2004_CD -_: =--RRB-_NN ._.
The_DT ROC_NNP curve_NN also_RB shows_VBZ the_DT misclassification_NN rates_NNS achieved_VBD depending_VBG on_IN the_DT error_NN cost_NN coefficients_NNS -LRB-_-LRB- Domingos_NNP ,_, 1999_CD -RRB-_-RRB- ._.
For_IN these_DT reasons_NNS ,_, -LRB-_-LRB- Bradley_NNP ,_, 1997_CD -RRB-_-RRB- argues_VBZ the_DT comparison_NN of_IN the_DT ROC_NN curves_NNS a_DT
hile_VB these_DT ranking_JJ tasks_NNS were_VBD originally_RB dealt_VBN with_IN by_IN learning_VBG a_DT classifier_NN ,_, it_PRP has_VBZ recently_RB been_VBN shown_VBN that_IN the_DT error_NN rate_NN of_IN a_DT classifier_NN is_VBZ lowly_RB correlated_VBN to_TO its_PRP$ Area_NNP Under_IN the_DT ROC_NN Curve_NN -LRB-_-LRB- AUC_NN -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =_JJ Caruana_NNP &_CC Niculescu-Mizil_NNP ,_, 2004_CD -_: =_JJ -_: ;_: Cortes_NNP &_CC Mohri_NNP ,_, 2004_CD -RRB-_-RRB- ,_, a_DT measure_NN of_IN the_DT ranking_JJ performance_NN of_IN a_DT function_NN ,_, equal_JJ to_TO the_DT probability_NN on_IN a_DT given_VBN set_NN that_IN a_DT positive_JJ instance_NN has_VBZ a_DT greater_JJR score_NN than_IN a_DT negative_JJ one_CD ._.
This_DT observation_NN
are_VBP applied_VBN to_TO the_DT dataset_NN as_IN a_DT preprocessing_VBG step_NN before_IN feeding_VBG it_PRP into_IN the_DT learning_NN algorithms_NNS ._.
Finally_RB ,_, we_PRP use_VBP the_DT average_NN squared_VBD error_NN as_IN our_PRP$ last_JJ performance_NN comparison_NN metric_NN ._.
Recent_JJ research_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: shows_VBZ that_IN this_DT metric_NN is_VBZ remarkably_RB robust_JJ and_CC hassAttribute_NN name_NN Information_NNP gain_NN Gain_NNP Ratio_NNP Chi-Square_NNP SVM_NNP feature_NN Avg_NN ._.
Rank_NNP Attribute_NNP Eval_NNP ._.
evaluator_NN Sum_NN of_IN Papers_NNP 3_CD 4_CD 3_CD 4_CD 3_CD Sum_NN of_IN Neighbors_NNS 1_CD 3_CD
Caruana_NNP et_FW al._FW have_VBP empirically_RB shown_VBN ,_, on_IN a_DT large_JJ number_NN of_IN experiments_NNS ,_, that_IN standard_JJ optimisation_NN criteria_NNS for_IN classification_NN are_VBP lowly_RB correlated_VBN to_TO the_DT classifier_NN performance_NN in_IN terms_NNS of_IN the_DT AUC_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN conclusion_NN ,_, a_DT good_JJ summariser_NN should_MD have_VB high_JJ ordering_VBG performance_NN ,_, which_WDT can_MD ,_, for_IN example_NN ,_, be_VB measured_VBN via_IN the_DT AUC_NN ,_, while_IN the_DT optimisation_NN of_IN the_DT standard_JJ classification_NN criterion_NN yields_VBZ a_DT poss_NN
on_IN show_NN that_IN the_DT supervised_JJ method_NN outperforms_VBZ the_DT semi_NN supervised_JJ method_NN ._.
Indeed_RB our_PRP$ algorithm_NN does_VBZ not_RB optimize_VB directly_RB this_DT criterion_NN ._.
Moreover_RB even_RB if_IN AUC_NN and_CC average_JJ precision_NN can_MD be_VB correlated_VBN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT AUC_NN is_VBZ more_RBR related_JJ to_TO negative_JJ examples_NNS than_IN to_TO positive_JJ examples_NNS ._.
They_PRP are_VBP indeed_RB very_RB few_JJ positive_JJ documents_NNS for_IN each_DT query_NN ._.
In_IN fact_NN we_PRP could_MD use_VB directly_RB the_DT average_JJ precision_NN as_IN supervised_VBN
database_NN proteins_NNS from_IN 150_CD SCOP_NN domains_NNS ,_, we_PRP measure_VBP classification_NN performance_NN using_VBG 10_CD fold_JJ cross-validation_NN ._.
To_TO evaluate_VB the_DT accuracy_NN ,_, we_PRP use_VBP Precision_NN and_CC Recall_VB in_IN the_DT context_NN of_IN machine_NN learning_NN =_JJ -_: =[_NN 101_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT have_VBP different_JJ meanings_NNS from_IN the_DT Precision_NNP and_CC Recall_VB in_IN the_DT information_NN retrieval_NN area_NN ._.
Given_VBN nr_IN SCOP_NN domains_NNS in_IN the_DT testing_NN data_NNS ,_, N_NN d_NN P_NN denotes_VBZ the_DT number_NN of_IN testing_NN proteins_NNS that_WDT are_VBP classi_NNS
ionally_RB ,_, learning_VBG algorithms_NNS for_IN predictive_JJ models_NNS have_VBP focused_VBN on_IN improving_VBG prediction_NN quality_NN ,_, e.g._FW ,_, measured_VBN by_IN accuracy_NN ,_, root_NN mean_NN squared_VBD error_NN -LRB-_-LRB- RMSE_NN -RRB-_-RRB- ,_, area_NN under_IN the_DT ROC_NN curve_NN and_CC other_JJ metrics_NNS =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Research_NN in_IN data_NN mining_NN also_RB considered_VBN model_NN building_NN time_NN ,_, i.e._FW ,_, to_TO improve_VB the_DT time_NN it_PRP takes_VBZ to_TO learn_VB predictive_JJ models_NNS for_IN large_JJ data_NNS sets_NNS ._.
However_RB ,_, there_EX is_VBZ another_DT aspect_NN of_IN a_DT predictive_JJ model_NN
0,1_CD -RRB-_-RRB- ,_, with_IN no_DT false_JJ positive_JJ and_CC 100_CD %_NN true_JJ positive_JJ examples_NNS ._.
ROC_NN curve_NN has_VBZ no_DT sensitivities_NNS to_TO the_DT ratio_NN of_IN positives_NNS and_CC negatives_NNS examples_NNS -LRB-_-LRB- 15_CD -RRB-_-RRB- as_IN opposed_VBN to_TO other_JJ accuracy_NN measures_NNS such_JJ as_IN Fscore_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
One_CD advantage_NN of_IN ROC_NN curves_NNS is_VBZ to_TO naturally_RB accommodate_VB ill-balanced_JJ distributions_NNS and_CC costsensitive_JJ learning_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
The_DT area_NN under_IN the_DT ROC_NN curve_NN -LRB-_-LRB- AUC_NN -RRB-_-RRB- is_VBZ thus_RB viewed_VBN as_IN a_DT global_JJ measure_NN of_IN the_DT learni_NNS
more_RBR direct_JJ meaning_NN ._.
Categorization_NN Based_VBN on_IN the_DT Predictive_NNP Performance_NNP Measure_NN There_EX are_VBP several_JJ alternatives_NNS for_IN measuring_VBG the_DT predictive_JJ performance_NN of_IN a_DT classification_NN algorithm_NN ._.
A_DT recent_JJ paper_NN -LRB-_-LRB- =_JJ -_: =_JJ Caruana_NNP &_CC Niculescu-Mizil_NNP ,_, 2004_CD -_: =--RRB-_NN discusses_VBZ 9_CD different_JJ measures_NNS of_IN predictive_JJ performance_NN in_IN the_DT context_NN of_IN standard_JJ ,_, flat_JJ classification_NN problems_NNS ._.
A_DT discussion_NN of_IN conventional_JJ measures_NNS of_IN predictive_JJ performance_NN for_IN flat_JJ classifica_NN
I_PRP 10.1109_CD \/_: ICDM_NN .2007.106_CD Seventh_NNP IEEE_NNP International_NNP Conference_NNP on_IN Data_NNP Mining_NNP 123_CD to_TO express_VB a_DT well-calibrated_JJ classifier_NN :_: the_DT proper_JJ class_NN occurrence_NN rate_NN is_VBZ mapped_VBN correctly_RB for_IN each_DT unseen_JJ example_NN ._.
=_SYM -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: further_RB suggests_VBZ that_IN any_DT reasonable_JJ performance_NN metric_NN should_MD be_VB optimized_VBN by_IN this_DT one_CD true_JJ model_NN and_CC no_DT other_JJ model_NN should_MD yield_VB better_JJR performance_NN ._.
Unfortunately_RB ,_, this_DT task_NN makes_VBZ several_JJ fundament_NN
nd_NN orthologous_JJ protein_NN pairs_NNS ._.
The_DT ROCR_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- package_NN for_IN the_DT statistical_JJ computing_NN environment_NN R_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- was_VBD used_VBN for_IN calculating_VBG receiver_NN operating_NN characteristics_NNS -LRB-_-LRB- ROC_NN -RRB-_-RRB- curves_NNS and_CC the_DT calibration_NN error_NN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: for_IN the_DT classification_NN task_NN ._.
The_DT analysis_NN shows_VBZ that_IN the_DT rfunSim_NN score_NN achieves_VBZ a_DT better_JJR calibration_NN error_NN than_IN the_DT funSim_NN score_NN ._.
The_DT detailed_JJ analysis_NN with_IN examples_NNS of_IN protein_NN pairs_NNS can_MD be_VB found_VBN in_IN
forms_NNS of_IN ranking_NN correspond_VBP to_TO the_DT bipartite_JJ feedback_NN case_NN studied_VBN in_IN -LRB-_-LRB- 10_CD -RRB-_-RRB- and_CC we_PRP hence_RB refer_VBP to_TO them_PRP as_IN the_DT bipartite_JJ ranking_NN problems_NNS ._.
sification_NN algorithms_NNS indicate_VBP that_IN ,_, as_IN in_IN the_DT supervised_JJ case_NN =_JJ -_: =[_NN 7_CD ,_, 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT error_NN rate_NN of_IN a_DT semi-supervised_JJ classification_NN function_NN is_VBZ not_RB necessarily_RB a_DT good_JJ indicator_NN of_IN the_DT accuracy_NN of_IN the_DT ranking_JJ function_NN derived_VBN from_IN it_PRP ._.
In_IN the_DT remainder_NN of_IN the_DT paper_NN ,_, we_PRP discuss_VBP ,_, in_IN
have_VB zero_CD mean_NN and_CC unit_NN standard_JJ deviation_NN ._.
The_DT ERR_NNP ,_, the_DT AUC_NN ,_, and_CC the_DT RMSE_NN represent_VBP three_CD most_RBS often_RB used_VBN metrics_NNS ,_, which_WDT represent_VBP threshold_NN metric_NN ,_, probability_NN metric_NN ,_, and_CC rank_NN metric_NN ,_, respectively_RB =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN our_PRP$ paper_NN ,_, we_PRP will_MD use_VB the_DT three_CD performance_NN metrics_NNS for_IN binary_JJ classification_NN problems_NNS ._.
The_DT procedure_NN of_IN parameter_NN optimization_NN follows_VBZ Rätsch_NNP 's_POS methodology_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, which_WDT trains_VBZ the_DT algorithm_NN with_IN
ures_VBZ ,_, accompanied_VBN by_IN an_DT interpretation_NN of_IN the_DT results_NNS ,_, should_MD result_VB in_IN little_JJ extra_JJ reviewing_VBG load_NN ._.
In_IN fact_NN ,_, current_JJ research_NN suggests_VBZ we_PRP may_MD be_VB able_JJ to_TO summarize_VB various_JJ measures_NNS into_IN a_DT single_JJ one_CD -LRB-_-LRB- =_JJ -_: =_JJ Caruana_NNP and_CC Niculescu-Mizil_NNP ,_, 2004_CD -_: =_JJ -_: ;_: Huang_NNP and_CC Ling_NNP ,_, 2006_CD -RRB-_-RRB- ,_, simplifying_VBG the_DT tables_NNS once_RB again_RB ._.
Reform_NNP :_: For_IN applications_NNS ,_, a_DT measure_NN of_IN performance_NN is_VBZ needed_VBN to_TO compare_VB algorithms_NNS ,_, but_CC it_PRP is_VBZ not_RB the_DT only_JJ thing_NN needed_VBN ._.
Users_NNS are_VBP often_RB rel_NN
ne_IN True_NNP Model_NNP for_IN data_NNS is_VBZ within_IN the_DT set_NN of_IN Turing_NN machines_NNS ,_, then_RB it_PRP is_VBZ possible_JJ to_TO express_VB a_DT well-calibrated_JJ classifier_NN :_: the_DT proper_JJ class_NN occurrence_NN rate_NN is_VBZ mapped_VBN correctly_RB for_IN each_DT unseen_JJ example_NN ._.
=_SYM -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: further_RB suggests_VBZ that_IN any_DT reasonable_JJ performance_NN metric_NN should_MD be_VB optimized_VBN by_IN this_DT one_CD true_JJ model_NN and_CC no_DT other_JJ model_NN should_MD yield_VB better_JJR performance_NN ._.
Received_VBN October_NNP 29_CD ,_, 2007_CD Revised_VBN December_NNP 24_CD ,_, 2_CD
e_LS a_DT hybrid_NN metric_NN that_WDT is_VBZ defined_VBN as_IN the_DT equally-weighted_JJ mean_JJ performance_NN under_IN RMS_NNP ,_, ROC_NNP and_CC ACC_NNP -LRB-_-LRB- called_VBN ``_`` ALL_NN ''_'' -RRB-_-RRB- ._.
We_PRP follow_VBP the_DT definitions_NNS of_IN these_DT performance_NN metrics_NNS found_VBN in_IN Caruana_NNP and_CC Niculescu_NNP =_SYM -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: ,_, since_IN they_PRP are_VBP implemented_VBN in_IN the_DT PERF_NN code_NN that_WDT was_VBD made_VBN available_JJ by_IN Caruana_NNP in_IN connection_NN with_IN the_DT KDD_NNP Cup_NNP 2004.4_CD Skalak_NNP ,_, Niculescu-Mizil_NNP ,_, Caruana_NNP We_PRP have_VBP also_RB adopted_VBN the_DT same_JJ conventions_NNS as_IN to_TO
ift_NN would_MD be_VB 2.0_CD ,_, meaning_VBG the_DT top_JJ 50_CD %_NN are_VBP twice_RB as_RB likely_JJ to_TO respond_VB compared_VBN to_TO the_DT overall_JJ average_NN ._.
Along_IN with_IN lift_NN ,_, the_DT area_NN under_IN a_DT roc_NN curve_NN -LRB-_-LRB- AUC_NN -RRB-_-RRB- has_VBZ also_RB become_VBN a_DT common_JJ way_NN to_TO evaluate_VB models_NNS =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, lift_NN and_CC AUC_NN both_DT measure_NN the_DT same_JJ quantity_NN ,_, which_WDT is_VBZ the_DT difference_NN between_IN using_VBG a_DT model_NN and_CC not_RB using_VBG a_DT model_NN ._.
Therefore_RB ,_, the_DT results_NNS from_IN using_VBG lift_NN or_CC AUC_NN should_MD be_VB very_RB similar_JJ ._.
One_CD s_NN
