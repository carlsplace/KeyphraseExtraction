Canonicalization_NN of_IN database_NN records_NNS using_VBG adaptive_JJ similarity_NN measures_NNS
It_PRP is_VBZ becoming_VBG increasingly_RB common_JJ to_TO construct_NN databases_NNS from_IN information_NN automatically_RB culled_VBD from_IN many_JJ heterogeneous_JJ sources_NNS ._.
For_IN example_NN ,_, a_DT research_NN publication_NN database_NN can_MD be_VB constructed_VBN by_IN automatically_RB extracting_VBG titles_NNS ,_, authors_NNS ,_, and_CC conference_NN information_NN from_IN online_JJ papers_NNS ._.
A_DT common_JJ difficulty_NN in_IN consolidating_VBG data_NNS from_IN multiple_JJ sources_NNS is_VBZ that_IN records_NNS are_VBP referenced_VBN in_IN a_DT variety_NN of_IN ways_NNS -LRB-_-LRB- e.g._FW abbreviations_NNS ,_, aliases_NNS ,_, and_CC misspellings_NNS -RRB-_-RRB- ._.
Therefore_RB ,_, it_PRP can_MD be_VB difficult_JJ to_TO construct_VB a_DT single_JJ ,_, standard_JJ representation_NN to_TO present_VB to_TO the_DT user_NN ._.
We_PRP refer_VBP to_TO the_DT task_NN of_IN constructing_VBG this_DT representation_NN as_IN canonicalization_NN ._.
Despite_IN its_PRP$ importance_NN ,_, there_EX is_VBZ little_JJ existing_VBG work_NN on_IN canonicalization_NN ._.
In_IN this_DT paper_NN ,_, we_PRP explore_VBP the_DT use_NN of_IN edit_NN distance_NN measures_NNS to_TO construct_VB a_DT canonical_JJ representation_NN that_WDT is_VBZ ``_`` central_JJ ''_'' in_IN the_DT sense_NN that_IN it_PRP is_VBZ most_RBS similar_JJ to_TO each_DT of_IN the_DT disparate_JJ records_NNS ._.
This_DT approach_NN reduces_VBZ the_DT impact_NN of_IN noisy_JJ records_NNS on_IN the_DT canonical_JJ representation_NN ._.
Furthermore_RB ,_, because_IN the_DT user_NN may_MD prefer_VB different_JJ styles_NNS of_IN canonicalization_NN ,_, we_PRP show_VBP how_WRB different_JJ edit_NN distance_NN costs_NNS can_MD result_VB in_IN different_JJ forms_NNS of_IN canonicalization_NN ._.
For_IN example_NN ,_, reducing_VBG the_DT cost_NN of_IN character_NN deletions_NNS can_MD result_VB in_IN representations_NNS that_WDT favor_VBP abbreviated_JJ forms_NNS over_IN expanded_JJ forms_NNS -LRB-_-LRB- e.g._FW KDD_NNP versus_CC Conference_NNP on_IN Knowledge_NNP Discovery_NNP and_CC Data_NNP Mining_NNP -RRB-_-RRB- ._.
We_PRP describe_VBP how_WRB to_TO learn_VB these_DT costs_NNS from_IN a_DT small_JJ amount_NN of_IN manually_RB annotated_JJ data_NNS using_VBG stochastic_JJ hill-climbing_NN ._.
Additionally_RB ,_, we_PRP investigate_VBP feature-based_JJ methods_NNS to_TO learn_VB ranking_JJ preferences_NNS over_IN canonicalizations_NNS ._.
These_DT approaches_NNS can_MD incorporate_VB arbitrary_JJ textual_JJ evidence_NN to_TO select_VB a_DT canonical_JJ record_NN ._.
We_PRP evaluate_VBP our_PRP$ approach_NN on_IN a_DT real-world_JJ publications_NNS database_NN and_CC show_VBP that_IN our_PRP$ learning_NN method_NN results_VBZ in_IN a_DT canonicalization_NN solution_NN that_WDT is_VBZ robust_JJ to_TO errors_NNS and_CC easily_RB customizable_JJ to_TO user_NN preferences_NNS ._.
nable_JJ string_NN edit-distance_NN between_IN attribute_NN values_NNS to_TO find_VB strings_NNS for_IN each_DT attribute_NN with_IN the_DT least_JJS distance_NN to_TO other_JJ values_NNS in_IN the_DT cluster_NN ._.
This_DT differs_VBZ from_IN the_DT system_NN presented_VBN in_IN Culotta_NNP et_FW al._FW =_SYM -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: :_: our_PRP$ system_NN finds_VBZ canonical_JJ values_NNS for_IN each_DT attribute_NN separately_RB ,_, whereas_IN in_IN -LRB-_-LRB- 8_CD -RRB-_-RRB- the_DT canonical_JJ entity_NN must_MD match_VB exactly_RB one_CD of_IN the_DT existing_VBG mentions_VBZ ._.
Let_NNP D_NNP :_: vi_LS ×_FW vj_FW ↦_FW →_FW R_NN +_CC be_VB the_DT string_NN edit_FW distan_FW
fines_NNS a_DT string_NN in_IN terms_NNS of_IN the_DT string_NN edit_VB operations_NNS required_VBN to_TO create_VB it_PRP ._.
This_DT work_NN is_VBZ extended_VBN and_CC applied_VBN successfully_RB to_TO record_NN deduplication_NN by_IN Bilenko_NNP and_CC Mooney_NNP -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
Recently_RB ,_, Culotta_NNP et_FW al._FW =_SYM -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: describe_VBP several_JJ methods_NNS for_IN canonicalization_NN of_IN database_NN records_NNS that_WDT are_VBP robust_JJ to_TO noisy_JJ data_NNS and_CC customizable_JJ to_TO user_NN preferences_NNS -LRB-_-LRB- e.g._FW ,_, a_DT preference_NN for_IN acronyms_NNS versus_CC full_JJ words_NNS -RRB-_-RRB- ._.
2.3_CD Schema_NNP Ma_NNP
over_IN Λ_NN with_IN fixed_JJ mean_NN and_CC variance_NN to_TO mitigate_VB over-fitting_JJ ._.
We_PRP find_VBP the_DT setting_NN of_IN Λ_NN that_WDT minimizes_VBZ Equation_NN 5_CD using_VBG limited-memory_JJ BFGS_NN ,_, a_DT gradient_NN ascent_NN method_NN with_IN a_DT second-order_JJ approximation_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
5.3.2_FW MIRA_FW MIRA_NN -LRB-_-LRB- Margin_NN Infused_VBN Relaxed_JJ Algorithm_NN -RRB-_-RRB- is_VBZ a_DT relaxed_VBN ,_, online_JJ maximum_NN margin_NN training_NN algorithm_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
It_PRP iteratively_RB cycles_NNS through_IN the_DT training_NN set_NN and_CC updates_NNS the_DT parameter_NN vector_NN with_IN tw_NN
xtend_VB Zhu_NNP and_CC Unger_NNP 's_POS work_NN by_IN using_VBG a_DT conditional_JJ random_JJ field_NN to_TO learn_VB the_DT costs_NNS of_IN a_DT variety_NN of_IN flexible_JJ edit_NN distance_NN operations_NNS ._.
However_RB ,_, they_PRP do_VBP not_RB explore_VB canonicalization_NN ._.
Ristad_NN and_CC Yianilos_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: learn_VB a_DT probability_NN distribution_NN over_IN atomic_JJ string_NN edit_NN operations_NNS -LRB-_-LRB- insertion_NN ,_, deletion_NN ,_, substitution_NN -RRB-_-RRB- and_CC define_VB a_DT stochastic_JJ transducer_NN that_WDT defines_VBZ the_DT probability_NN of_IN a_DT string_NN as_IN either_CC the_DT Viter_NN
of_IN all_DT possible_JJ sequences_NNS of_IN edit_NN operations_NNS required_VBN to_TO produce_VB that_DT string_NN ._.
The_DT parameters_NNS of_IN this_DT generative_JJ model_NN are_VBP learned_VBN using_VBG the_DT expectation_NN maximization_NN -LRB-_-LRB- EM_NN -RRB-_-RRB- algorithm_NN ._.
Bilenko_NN and_CC Mooney_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: present_VB a_DT method_NN to_TO learn_VB edit_NN distance_NN based_VBN similarity_NN measures_NNS of_IN each_DT attribute_NN between_IN records_NNS in_IN order_NN to_TO perform_VB deduplication_NN ._.
They_PRP extend_VBP the_DT work_NN of_IN Ristad_NNP and_CC Yianilos_NNP -LRB-_-LRB- 12_CD -RRB-_-RRB- by_IN accommodatin_NN
defining_VBG the_DT distance_NN between_IN two_CD strings_NNS ._.
A_DT natural_JJ choice_NN is_VBZ the_DT Levenshtein_NNP distance_NN :_: the_DT number_NN of_IN character_NN insertions_NNS ,_, deletions_NNS ,_, and_CC replacements_NNS required_VBN to_TO transform_VB one_CD string_NN into_IN another_DT =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT recursive_JJ definition_NN of_IN the_DT Levenshtein_NNP distance_NN for_IN strings_NNS s_NN n_NN and_CC t_NN m_NN with_IN length_NN n_NN and_CC m_NN is_VBZ the_DT following_NN :_: D_NN -LRB-_-LRB- s_NN n_NN ,_, t_NN m_NN -RRB-_-RRB- =_JJ min_NN 8_CD -RRB-_-RRB- -LRB-_-LRB- -RRB-_-RRB- :_: cr_NN -LRB-_-LRB- sn_NN ,_, tm_NN -RRB-_-RRB- +_CC D_NN -LRB-_-LRB- s_NN n_NN −_NN 1_CD ,_, t_NN m_NN −_NN 1_CD -RRB-_-RRB- ci_NN +_CC D_NN -LRB-_-LRB- s_NN n_NN −_NN 1_CD ,_, t_NN m_NN -RRB-_-RRB- cd_NN
e_LS database_NN ._.
This_DT can_MD be_VB accomplished_VBN by_IN storing_VBG the_DT top_JJ n_NN most_RBS confident_JJ extraction_NN results_NNS -LRB-_-LRB- along_IN with_IN corresponding_JJ probabilities_NNS or_CC confidence_NN measures_NNS -RRB-_-RRB- for_IN each_DT desired_VBN record_NN ._.
Gupta_NNP and_CC Sarawagi_NNP =_SYM -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: leverage_NN confidence_NN value_NN outputs_NNS from_IN the_DT extraction_NN models_NNS to_TO improve_VB query_NN results_NNS on_IN databases_NNS containing_VBG uncertainty_NN ._.
Fundamentally_RB the_DT problem_NN is_VBZ canonicalization_NN because_IN the_DT system_NN is_VBZ faced_VBN wi_IN
xtual_JJ evidence_NN into_IN a_DT discriminative_JJ model_NN of_IN canonicalization_NN ._.
Canonicalization_NNP has_VBZ also_RB been_VBN implicitly_RB considered_VBN in_IN deduplication_NN research_NN ._.
For_IN example_NN ,_, Milch_NNP et_FW al._FW -LRB-_-LRB- 11_CD -RRB-_-RRB- and_CC McCallum_NN and_CC Wellner_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: propose_VBP deduplication_NN models_NNS containing_VBG variables_NNS for_IN canonical_JJ attributes_NNS of_IN a_DT record_NN ._.
The_DT variables_NNS are_VBP used_VBN to_TO help_VB deduplication_NN ,_, although_IN the_DT accuracy_NN of_IN the_DT resulting_VBG canonical_JJ records_NNS is_VBZ not_RB o_NN
plete_RB ,_, it_PRP erroneously_RB includes_VBZ the_DT editor_NN field_NN from_IN record_NN -LRB-_-LRB- c_NN -RRB-_-RRB- ,_, which_WDT is_VBZ not_RB truly_RB a_DT duplicate_VB ._.
To_TO address_VB this_DT issue_NN ,_, it_PRP may_MD be_VB useful_JJ to_TO consider_VB measures_NNS of_IN field_NN compatibility_NN ,_, as_IN in_IN Wick_NNP et_FW al._FW =_SYM -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
4.3_CD Record_NNP Generation_NNP The_DT record_NN generation_NN approach_NN to_TO canonicalization_NN is_VBZ an_DT extension_NN of_IN the_DT record_NN merging_JJ approach_NN that_WDT may_MD also_RB propose_VB field_NN values_NNS that_WDT do_VBP not_RB explicitly_RB exist_VB in_IN any_DT of_IN the_DT
utomatically_RB extracted_VBN records_NNS ._.
One_CD approach_NN when_WRB consolidating_VBG extractions_NNS from_IN various_JJ sources_NNS is_VBZ to_TO perform_VB some_DT type_NN of_IN weighted_JJ voting_NN to_TO determine_VB which_WDT facts_NNS should_MD be_VB inserted_VBN in_IN the_DT database_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Another_DT approach_NN is_VBZ to_TO store_VB the_DT uncertainty_NN of_IN these_DT extractions_NNS directly_RB in_IN the_DT database_NN ._.
This_DT can_MD be_VB accomplished_VBN by_IN storing_VBG the_DT top_JJ n_NN most_RBS confident_JJ extraction_NN results_NNS -LRB-_-LRB- along_IN with_IN corresponding_VBG
odels_NNS -RRB-_-RRB- and_CC other_JJ sources_NNS of_IN textual_JJ evidence_NN into_IN a_DT discriminative_JJ model_NN of_IN canonicalization_NN ._.
Canonicalization_NNP has_VBZ also_RB been_VBN implicitly_RB considered_VBN in_IN deduplication_NN research_NN ._.
For_IN example_NN ,_, Milch_NNP et_FW al._FW =_SYM -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: and_CC McCallum_NNP and_CC Wellner_NNP -LRB-_-LRB- 10_CD -RRB-_-RRB- propose_VBP deduplication_NN models_NNS containing_VBG variables_NNS for_IN canonical_JJ attributes_NNS of_IN a_DT record_NN ._.
The_DT variables_NNS are_VBP used_VBN to_TO help_VB deduplication_NN ,_, although_IN the_DT accuracy_NN of_IN the_DT result_NN
been_VBN present_JJ in_IN many_JJ application_NN and_CC research_NN areas_NNS ._.
In_IN this_DT section_NN we_PRP review_VBP several_JJ of_IN these_DT applications_NNS as_RB well_RB as_IN related_JJ work_NN in_IN the_DT area_NN of_IN learning_VBG string_NN edit_NN distance_NN costs_NNS ._.
Tejada_NNP et_FW al._FW =_SYM -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: devise_VB a_DT system_NN to_TO automatically_RB extract_VB and_CC consolidate_VB information_NN from_IN multiple_JJ sources_NNS into_IN a_DT unified_JJ database_NN ._.
When_WRB a_DT user_NN queries_VBZ this_DT database_NN ,_, multiple_JJ representations_NNS of_IN an_DT attribute_NN are_VBP ine_NN
5_CD using_VBG limited-memory_JJ BFGS_NN ,_, a_DT gradient_NN ascent_NN method_NN with_IN a_DT second-order_JJ approximation_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
5.3.2_FW MIRA_FW MIRA_NN -LRB-_-LRB- Margin_NN Infused_VBN Relaxed_JJ Algorithm_NN -RRB-_-RRB- is_VBZ a_DT relaxed_VBN ,_, online_JJ maximum_NN margin_NN training_NN algorithm_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
It_PRP iteratively_RB cycles_NNS through_IN the_DT training_NN set_NN and_CC updates_NNS the_DT parameter_NN vector_NN with_IN two_CD constraints_NNS :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- the_DT true_JJ canonical_JJ record_NN must_MD have_VB a_DT higher_JJR score_NN than_IN any_DT other_JJ record_NN by_IN a_DT given_VBN margin_NN ,_,
efficiently_RB using_VBG the_DT Hildreth_NN and_CC D’esopo_NN method_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
To_TO improve_VB the_DT stability_NN of_IN this_DT online_NN method_NN ,_, we_PRP average_VBP the_DT parameter_NN vectors_NNS from_IN each_DT update_VBP at_IN the_DT end_NN of_IN training_NN ,_, as_IN in_IN voted_VBN perceptron_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
6_CD ._.
EXPERIMENTS_NNS 6.1_CD Data_NNPS We_PRP collect_VBP 3,683_CD citations_NNS to_TO 100_CD distinct_JJ papers_NNS from_IN Rexa_NNP ,_, an_DT online_JJ publications_NNS search_NN engine_NN ._.
3_CD These_DT citations_NNS were_VBD automatically_RB extracted_VBN from_IN the_DT headers_NNS of_IN research_NN
ditionally_RB ,_, we_PRP enable_VBP the_DT user_NN to_TO express_VB preferences_NNS independent_JJ of_IN the_DT data_NNS source_NN ._.
Other_JJ work_NN has_VBZ focused_VBN on_IN learning_VBG the_DT parameters_NNS of_IN string_NN edit_NN distance_NN with_IN encouraging_JJ results_NNS ._.
Zhu_NNP and_CC Unger_NNP =_SYM -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: apply_VB string_NN edit_NN distances_NNS to_TO the_DT task_NN of_IN merging_JJ database_NN records_NNS ._.
They_PRP observe_VBP that_IN parameters_NNS can_MD not_RB be_VB optimized_VBN individually_RB due_JJ to_TO the_DT complex_JJ interaction_NN between_IN various_JJ edit_NN cost_NN weights_NNS on_IN
of_IN genetic_JJ approaches_NNS we_PRP propose_VBP learning_VBG the_DT edit_NN costs_NNS using_VBG either_CC stochastic_JJ search_NN ,_, or_CC an_DT exhaustive_JJ search_NN over_IN a_DT relatively_RB small_JJ discrete_JJ space_NN of_IN possible_JJ parameter_NN settings_NNS ._.
McCallum_NNP et_FW al._FW =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: also_RB use_VBP a_DT discriminatively_RB learned_VBN edit_NN distance_NN to_TO perform_VB record_NN deduplication_NN ._.
They_PRP extend_VBP Zhu_NNP and_CC Unger_NNP 's_POS work_NN by_IN using_VBG a_DT conditional_JJ random_JJ field_NN to_TO learn_VB the_DT costs_NNS of_IN a_DT variety_NN of_IN flexible_JJ edi_NN
rameter_NN update_VBP is_VBZ a_DT quadratic_JJ program_NN with_IN constraint_NN size_NN equal_JJ to_TO the_DT number_NN of_IN noncanonical_JJ records_NNS in_IN the_DT training_NN example_NN ._.
This_DT QP_NN can_MD bessolved_VB efficiently_RB using_VBG the_DT Hildreth_NN and_CC D’esopo_NN method_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
To_TO improve_VB the_DT stability_NN of_IN this_DT online_NN method_NN ,_, we_PRP average_VBP the_DT parameter_NN vectors_NNS from_IN each_DT update_VBP at_IN the_DT end_NN of_IN training_NN ,_, as_IN in_IN voted_VBN perceptron_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ._.
6_CD ._.
EXPERIMENTS_NNS 6.1_CD Data_NNPS We_PRP collect_VBP 3,683_CD citatio_NN
