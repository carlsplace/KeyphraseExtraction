Knowledge_NN transfer_NN via_IN multiple_JJ model_NN local_JJ structure_NN mapping_NN
The_DT effectiveness_NN of_IN knowledge_NN transfer_NN using_VBG classification_NN algorithms_NNS depends_VBZ on_IN the_DT difference_NN between_IN the_DT distribution_NN that_WDT generates_VBZ the_DT training_NN examples_NNS and_CC the_DT one_NN from_IN which_WDT test_NN examples_NNS are_VBP to_TO be_VB drawn_VBN ._.
The_DT task_NN can_MD be_VB especially_RB difficult_JJ when_WRB the_DT training_NN examples_NNS are_VBP from_IN one_CD or_CC several_JJ domains_NNS different_JJ from_IN the_DT test_NN domain_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT locally_RB weighted_JJ ensemble_NN framework_NN to_TO combine_VB multiple_JJ models_NNS for_IN transfer_NN learning_NN ,_, where_WRB the_DT weights_NNS are_VBP dynamically_RB assigned_VBN according_VBG to_TO a_DT model_NN 's_POS predictive_JJ power_NN on_IN each_DT test_NN example_NN ._.
It_PRP can_MD integrate_VB the_DT advantages_NNS of_IN various_JJ learning_VBG algorithms_NNS and_CC the_DT labeled_JJ information_NN from_IN multiple_JJ training_NN domains_NNS into_IN one_CD unified_JJ classification_NN model_NN ,_, which_WDT can_MD then_RB be_VB applied_VBN on_IN a_DT different_JJ domain_NN ._.
Importantly_RB ,_, different_JJ from_IN many_JJ previously_RB proposed_VBN methods_NNS ,_, none_NN of_IN the_DT base_NN learning_NN method_NN is_VBZ required_VBN to_TO be_VB specifically_RB designed_VBN for_IN transfer_NN learning_NN ._.
We_PRP show_VBP the_DT optimality_NN of_IN a_DT locally_RB weighted_JJ ensemble_NN framework_NN as_IN a_DT general_JJ approach_NN to_TO combine_VB multiple_JJ models_NNS for_IN domain_NN transfer_NN ._.
We_PRP then_RB propose_VBP an_DT implementation_NN of_IN the_DT local_JJ weight_NN assignments_NNS by_IN mapping_VBG the_DT structures_NNS of_IN a_DT model_NN onto_IN the_DT structures_NNS of_IN the_DT test_NN domain_NN ,_, and_CC then_RB weighting_NN each_DT model_NN locally_RB according_VBG to_TO its_PRP$ consistency_NN with_IN the_DT neighborhood_NN structure_NN around_IN the_DT test_NN example_NN ._.
Experimental_JJ results_NNS on_IN text_NN classification_NN ,_, spam_NN filtering_VBG and_CC intrusion_NN detection_NN data_NNS sets_NNS demonstrate_VBP significant_JJ improvements_NNS in_IN classification_NN accuracy_NN gained_VBN by_IN the_DT framework_NN ._.
On_IN a_DT transfer_NN learning_VBG task_NN of_IN newsgroup_NN message_NN categorization_NN ,_, the_DT proposed_VBN locally_RB weighted_JJ ensemble_NN framework_NN achieves_VBZ 97_CD %_NN accuracy_NN when_WRB the_DT best_JJS single_JJ model_NN predicts_VBZ correctly_RB only_RB on_IN 73_CD %_NN of_IN the_DT test_NN examples_NNS ._.
In_IN summary_NN ,_, the_DT improvement_NN in_IN accuracy_NN is_VBZ over_IN 10_CD %_NN and_CC up_IN to_TO 30_CD %_NN across_IN different_JJ problems_NNS ._.
s_VBZ the_DT boosting_VBG weight_NN formula_NN as_IN the_DT re-weighting_JJ scheme_NN ._.
Some_DT other_JJ methods_NNS base_VBP on_IN dimension_NN reduction_NN ,_, which_WDT usually_RB map_VBP data_NNS to_TO a_DT new_JJ representation_NN facilitating_VBG domain_NN transfer_NN -LRB-_-LRB- -LRB-_-LRB- 10_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Recently_RB ,_, =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: proposes_VBZ a_DT locally_RB weighted_JJ ensemble_NN framework_NN to_TO combine_VB multiple_JJ models_NNS for_IN transfer_NN learning_NN by_IN dynamically_RB assigning_VBG weights_NNS of_IN a_DT model_NN according_VBG to_TO a_DT model_NN 's_POS predictive_JJ power_NN on_IN each_DT test_NN exampl_NN
sumes_NNS that_IN conditional_JJ probabilities_NNS rt_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- andrs_NNS -LRB-_-LRB- y_FW |_FW x_LS -RRB-_-RRB- aresimilar_NN in_IN regions_NNS of_IN the_DT latent_JJ space_NN where_WRB marginal_JJ distribution_NN qt_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC qs_NN -LRB-_-LRB- x_NN -RRB-_-RRB- of_IN corresponding_JJ examples_NNS are_VBP close_JJ ._.
Other_JJ works_NNS ,_, such_JJ as_IN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, assumes_VBZ q_NN -LRB-_-LRB- x_NN -RRB-_-RRB- is_VBZ related_JJ to_TO r_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- ._.
They_PRP both_DT implicitly_RB assume_VB that_IN marginal_JJ distribution_NN and_CC conditional_JJ probability_NN are_VBP directly_RB related_JJ ._.
In_IN summary_NN ,_, either_CC of_IN the_DT following_NN is_VBZ assumed_VBN to_TO be_VB true_JJ
s_NN to_TO raw_JJ data_NNS ,_, but_CC instead_RB use_VBP prediction_NN results_NNS from_IN multiple_JJ models_NNS as_IN input_NN ._.
Some_DT other_JJ types_NNS of_IN information_NN combination_NN have_VBP also_RB drawn_VBN researchers_NNS '_POS attention_NN ,_, such_JJ as_IN transfer_NN learning_NN ensemble_NN =_JJ -_: =[_NN 10_CD ,_, 19_CD -RRB-_-RRB- -_: =_JJ -_: ,_, webpages_NNS classification_NN based_VBN on_IN content_NN and_CC link_NN information_NN -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, label_NN inference_NN from_IN two_CD unlabeled_JJ data_NNS sources_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, and_CC ensemble_NN of_IN relational_JJ classifiers_NNS -LRB-_-LRB- 23_CD -RRB-_-RRB- ._.
However_RB ,_, all_PDT these_DT methods_NNS only_RB
aches_NNS for_IN transfer_NN learning_NN have_VBP been_VBN proposed_VBN ._.
Given_VBN multiple_JJ weak_JJ classifiers_NNS that_WDT are_VBP trained_VBN in_IN different_JJ domains_NNS ,_, these_DT are_VBP combined_VBN with_IN weights_NNS based_VBN on_IN relatedness_NN to_TO the_DT target_NN domains_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
An_DT AdaBoost_NNP algorithm_NN was_VBD modified_VBN so_RB as_IN to_TO be_VB better_JJR fit_NN for_IN transfer_NN learning_NN by_IN less_JJR weighting_NN of_IN useless_JJ source_NN data_NNS -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
While_IN boosting_VBG adjusts_VBZ the_DT weights_NNS of_IN data_NNS or_CC classifiers_NNS adaptively_RB ,_, ba_NN
s_NN to_TO task_NN parameters_NNS ._.
This_DT method_NN not_RB only_RB models_NNS the_DT relationships_NNS between_IN tasks_NNS explicitly_RB ,_, but_CC also_RB gives_VBZ an_DT algorithm_NN for_IN the_DT informed_JJ use_NN of_IN several_JJ source_NN tasks_NNS in_IN transfer_NN learning_NN ._.
Gao_NNP et_FW al._FW =_SYM -_: =[_NN 32_CD -RRB-_-RRB- -_: =_SYM -_: propose_VBP that_IN task_NN similarity_NN can_MD be_VB a_DT local_JJ measure_NN rather_RB than_IN a_DT global_JJ measure_NN ._.
They_PRP estimate_VBP similarities_NNS in_IN the_DT neighborhood_NN of_IN each_DT test_NN example_NN individually_RB ._.
These_DT local_JJ consistency_NN estimates_VBZ b_NN
transfer_NN method_NN of_IN Wu_NNP and_CC Dietterich_NNP -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, which_WDT uses_VBZ auxiliary_JJ data_NNS in_IN SVMs_NNS to_TO both_DT constrain_VB the_DT learning_NN and_CC identify_VB support_NN vectors_NNS ._.
Ensembles_NNS have_VBP been_VBN used_VBN previously_RB for_IN transfer_NN by_IN Gao_NNP et_FW al._FW =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, who_WP developed_VBD a_DT locally_RB weighted_JJ ensemble_NN that_IN weights_NNS each_DT member_NN classifier_NN differently_RB depending_VBG on_IN the_DT data_NNS region_NN ._.
Like_IN Gao_NNP et_FW al._FW 's_POS method_NN ,_, TransferBoost_NNP can_MD utilize_VB base_NN algorithms_NNS not_RB inhere_VBP
ature_NN ,_, which_WDT makes_VBZ it_PRP more_RBR flexible_JJ compared_VBN with_IN parametric_JJ models_NNS ._.
The_DT proposed_VBN transfer_NN learning_NN framework_NN is_VBZ fundamentally_RB different_JJ from_IN existing_VBG graph-based_JJ methods_NNS ._.
For_IN example_NN ,_, the_DT authors_NNS of_IN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT locally_RB weighted_JJ ensemble_NN framework_NN to_TO combine_VB multiple_JJ models_NNS for_IN transfer_NN learning_NN ,_, where_WRB the_DT weights_NNS of_IN different_JJ models_NNS are_VBP approximated_VBN using_VBG a_DT graph-based_JJ approach_NN ;_: the_DT authors_NNS of_IN -LRB-_-LRB- 1_CD
adaption_NN -RRB-_-RRB- has_VBZ been_VBN a_DT very_RB active_JJ research_NN area_NN in_IN recent_JJ years_NNS ._.
Representative_JJ work_NN includes_VBZ multi-label_JJ text_NN classification_NN -LRB-_-LRB- 30_CD ,_, 13_CD -RRB-_-RRB- ,_, crossdomain_NN sentiment_NN prediction_NN -LRB-_-LRB- 4_CD ,_, 6_CD ,_, 15_CD -RRB-_-RRB- ,_, intrusion_NN detection_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, verb_NN argument_NN classification_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, cross-lingual_JJ classification_NN -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, and_CC cross-domain_JJ relation_NN extraction_NN -LRB-_-LRB- 29_CD -RRB-_-RRB- ._.
In_IN all_DT of_IN these_DT scenarios_NNS ,_, the_DT features_NNS are_VBP given_VBN as_IN the_DT input_NN of_IN their_PRP$ algorithms_NNS -LRB-_-LRB- e_SYM
from_IN different_JJ models_NNS Mi_NNP ,_, because_IN a_DT good_JJ Mi_NN with_IN low_JJ test_NN error_NN implies_VBZ that_IN the_DT training_NN and_CC testing_NN data_NNS share_VBP similar_JJ data_NNS distributions_NNS ._.
Thus_RB ,_, we_PRP employ_VBP the_DT Locally_RB Weighted_JJ Ensemble_NN -LRB-_-LRB- LWE_NN -RRB-_-RRB- scheme_NN =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: to_TO dynamically_RB estimate_VB combination_NN weights_NNS for_IN every_DT test_NN example_NN by_IN checking_VBG the_DT consistency_NN of_IN data_NNS structures_NNS between_IN the_DT model_NN predictions_NNS -LRB-_-LRB- near_IN the_DT test_NN example_NN -RRB-_-RRB- and_CC the_DT real_JJ data_NNS distribution_NN
nce_NN to_TO our_PRP$ work_NN are_VBP domain_JJ adaptation_NN techniques_NNS specifically_RB developed_VBD for_IN text_NN and_CC sentiment_NN classification_NN -LRB-_-LRB- e.g._FW ,_, Blitzer_NNP ,_, McDonald_NNP ,_, &_CC Pereira_NNP ,_, 2006_CD ;_: Finn_NNP &_CC Kushmerick_NNP ,_, 2006_CD ;_: Blitzer_NNP et_FW al._FW ,_, 2007_CD ;_: =_JJ -_: =_JJ Gao_NN ,_, Fan_NN ,_, Jiang_NNP ,_, &_CC Han_NNP ,_, 2008_CD -_: =_JJ -_: ;_: Ling_NNP ,_, Dai_NNP ,_, Xue_NNP ,_, Yang_NNP ,_, &_CC Yu_NNP ,_, 2008_CD ;_: Tan_NNP ,_, Cheng_NNP ,_, Wang_NNP ,_, &_CC Xu_NNP ,_, 2009_CD -RRB-_-RRB- ._.
It_PRP is_VBZ worth_JJ noting_VBG that_IN our_PRP$ domain_NN adaptation_NN setting_NN is_VBZ different_JJ from_IN the_DT traditional_JJ setting_NN ._.
Traditionally_RB ,_, sophisticated_JJ classif_NN
other_JJ methods_NNS that_WDT learn_VBP model_NN parameters_NNS to_TO reduce_VB marginal_JJ probability_NN differences_NNS -LRB-_-LRB- 10_CD ,_, 11_CD -RRB-_-RRB- ._.
Similarly_RB ,_, several_JJ algorithms_NNS have_VBP been_VBN developed_VBN in_IN the_DT past_NN to_TO combine_VB knowledge_NN from_IN multiple_JJ sources_NNS =_JJ -_: =[_NN 12_CD ,_, 13_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Most_JJS of_IN these_DT methods_NNS measure_VBP the_DT distribution_NN difference_NN between_IN each_DT source_NN and_CC target_NN domain_NN data_NNS ,_, independently_RB ,_, based_VBN on_IN marginal_JJ or_CC conditional_JJ probability_NN differences_NNS and_CC combine_VB the_DT hypothes_NNS
we_PRP introduce_VBP five_CD traditional_JJ classifiers_NNS ,_, including_VBG Naive_JJ Bayes_NNS -LRB-_-LRB- NB_NN -RRB-_-RRB- ,_, SVM_NN ,_, C4_NN .5_NN ,_, K-NN_NN and_CC NNge_NN -LRB-_-LRB- Ng_NN -RRB-_-RRB- ,_, and_CC three_CD state-of-theart_JJ transfer_NN learning_NN methods_NNS :_: TrAdaBoost_NNP -LRB-_-LRB- TA_NNP -RRB-_-RRB- -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, LatentMap_NN -LRB-_-LRB- LM_NN -RRB-_-RRB- -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC LWE_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Amongthem_NNP ,_, TrAdaBoostisbasedoninstancesweighting_NNP ,_, LatentMapTable_NNP 2_CD :_: Dataset_NNP for_IN Algorithm_NNP and_CC Parameters_NNP Selection_NN Data_NNP Set_NNP |_NNP S_NNP |_FW |_FW T_NN |_CD Description_NN Red-White_NN -LRB-_-LRB- RW_NN -RRB-_-RRB- 1599_CD 4998_CD physicochemical_JJ White-Red_JJ -LRB-_-LRB- WR_NN -RRB-_-RRB- 49_CD
s_VBZ another_DT widely_RB used_VBN data_NNS set_VBN for_IN evaluating_VBG learning_NN algorithms_NNS ._.
It_PRP contains_VBZ five_CD top_JJ categories_NNS and_CC many_JJ subcategories_NNS ._.
For_IN easy_JJ comparison_NN ,_, we_PRP use_VBP a_DT preprocessed_JJ version_NN adopted_VBN in_IN previous_JJ work_NN -LRB-_-LRB- =_JJ -_: =_JJ Gao_NNP et_FW al._FW 2008_CD -_: =_JJ -_: ;_: Zhuang_NNP et_FW al._FW 2010_CD -RRB-_-RRB- ,_, which_WDT contains_VBZ three_CD cross-domain_JJ data_NNS sets_VBZ orgs_NNS vs_CC people_NNS ,_, orgs_NNS vs_CC place_NN and_CC people_NNS vs_CC place_NN ._.
We_PRP use_VBP Accuracy_NNP as_IN the_DT evaluation_NN criteria_NNS ,_, as_IN it_PRP is_VBZ widely_RB adopted_VBN in_IN the_DT literat_NN
n._NN With_IN the_DT new_JJ feature_NN representation_NN ,_, the_DT performance_NN of_IN the_DT target_NN task_NN is_VBZ expected_VBN to_TO improve_VB significantly_RB ._.
A_DT third_JJ case_NN can_MD be_VB referred_VBN to_TO as_IN parameter-transfer_JJ approach_NN -LRB-_-LRB- 45_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 46_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 47_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 48_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 49_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT assumes_VBZ that_IN the_DT source_NN tasks_NNS and_CC the_DT target_NN tasks_NNS share_VBP some_DT parameters_NNS or_CC prior_JJ distributions_NNS of_IN the_DT hyperparameters_NNS of_IN the_DT models_NNS ._.
The_DT transferred_VBN knowledge_NN is_VBZ encoded_VBN into_IN the_DT shared_JJ param_NN
._.
found_VBN in_IN a_DT subspace_NN -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- -RRB-_-RRB- or_CC a_DT projected_VBN feature_NN space_NN where_WRB the_DT different_JJ tasks_NNS are_VBP similar_JJ to_TO each_DT other_JJ -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 2_CD -RRB-_-RRB- -RRB-_-RRB- ._.
There_EX are_VBP also_RB some_DT other_JJ solutions_NNS like_IN model-combination_NN based_VBN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, transfer_NN across_IN similar_JJ learning_NN parameters_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC so_RB on_RB ._.
Different_JJ from_IN these_DT works_NNS ,_, we_PRP mainly_RB study_VBD the_DT problem_NN to_TO transfer_VB knowledge_NN across_IN tasks_NNS having_VBG different_JJ class_NN labels_NNS ._.
One_CD im_NN
ith_JJ different_JJ learning_NN algorithms_NNS ._.
In_IN particular_JJ ,_, since_IN most_JJS data_NNS sets_NNS are_VBP high-dimensional_JJ ,_, the_DT following_NN commonly_RB used_VBN algorithms_NNS are_VBP appropriate_JJ choices_NNS :_: 1_LS -RRB-_-RRB- Winnow_NN -LRB-_-LRB- WNN_NN -RRB-_-RRB- from_IN learning_VBG package_NN SNoW_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, 2_LS -RRB-_-RRB- Logistic_JJ Regression_NN -LRB-_-LRB- LR_NN -RRB-_-RRB- implemented_VBN in_IN BBR_NN package_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: and_CC 3_LS -RRB-_-RRB- Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- implemented_VBN in_IN LibSVM_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
When_WRB we_PRP only_RB have_VBP a_DT single_JJ source_NN domain_NN in_IN the_DT training_NN ,_, three_CD single_JJ cl_NN
t_NN Process_VB prior_JJ is_VBZ used_VBN to_TO couple_VB the_DT parameters_NNS of_IN several_JJ models_NNS from_IN the_DT same_JJ parameterized_JJ family_NN of_IN dis-tributions_NNS ._.
-LRB-_-LRB- 10_CD -RRB-_-RRB- extends_VBZ the_DT boosting_VBG method_NN to_TO perform_VB transfer_NN learning_NN ._.
Bennett_NNP et_FW al._FW =_SYM -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT methodology_NN for_IN building_VBG a_DT meta-classifier_NN which_WDT combines_VBZ multiple_JJ distinct_JJ classifiers_NNS through_IN the_DT use_NN of_IN reliability_NN indicators_NNS ._.
The_DT proposed_VBN weighted_JJ ensemble_NN provides_VBZ a_DT more_RBR general_JJ fr_NN
arning_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
By_IN combining_VBG decisions_NNS from_IN individual_JJ classifiers_NNS ,_, ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_NNS -LRB-_-LRB- 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX h_NN
ain_NN ._.
Since_IN semi-supervised_JJ learning_NN -LRB-_-LRB- transductive_JJ learning_NN -RRB-_-RRB- is_VBZ closely_RB related_JJ to_TO the_DT problem_NN ,_, we_PRP compare_VBP the_DT proposed_VBN method_NN with_IN Transductive_JJ Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- TSVM_NNP -RRB-_-RRB- implemented_VBN in_IN SVM_NN light_NN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Furthermore_RB ,_, in_IN the_DT proposed_VBN framework_NN ,_, the_DT two_CD main_JJ steps_NNS are_VBP ,_, predicting_VBG labels_NNS using_VBG weighted_JJ classifiers_NNS if_IN the_DT classifiers_NNS are_VBP sufficiently_RB accurate_JJ in_IN terms_NNS of_IN alignment_NN with_IN clustering_NN struct_NN
ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_VBZ =_JJ -_: =[_NN 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN p_NN
ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_VBZ =_JJ -_: =[_NN 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN p_NN
mble_JJ methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN problem_NN ._.
In_IN =_JJ -_: =[_NN 11_CD ,_, 26_CD -RRB-_-RRB- -_: =_JJ -_: ,_, it_PRP is_VBZ assumed_VBN that_IN the_DT training_NN and_CC the_DT test_NN examples_NNS are_VBP generated_VBN from_IN a_DT mixture_NN of_IN different_JJ models_NNS ,_, and_CC the_DT test_NN distribution_NN has_VBZ different_JJ mixture_NN coefficients_NNS than_IN the_DT training_NN distribution_NN ._.
I_PRP
x_NN -RRB-_-RRB- and_CC multiple_JJ output_NN variables_NNS ,_, so_IN the_DT basic_JJ setting_NN is_VBZ different_JJ from_IN our_PRP$ problem_NN ._.
The_DT ``_`` clustering_NN ''_'' assumption_NN in_IN our_PRP$ work_NN is_VBZ exploited_VBN in_IN some_DT transfer_NN learning_NN and_CC semi-supervised_JJ learning_NN works_VBZ =_JJ -_: =[_NN 9_CD ,_, 28_CD -RRB-_-RRB- -_: =_JJ -_: ,_, where_WRB clustering_NN structure_NN is_VBZ utilized_VBN in_IN smoothing_VBG predictions_NNS among_IN neighbors_NNS ._.
Our_PRP$ paper_NN differs_VBZ from_IN these_DT papers_NNS by_IN utilizing_VBG the_DT assumption_NN in_IN weighting_NN different_JJ models_NNS locally_RB to_TO combine_VB all_DT
rithms_NNS are_VBP appropriate_JJ choices_NNS :_: 1_LS -RRB-_-RRB- Winnow_NN -LRB-_-LRB- WNN_NN -RRB-_-RRB- from_IN learning_VBG package_NN SNoW_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, 2_LS -RRB-_-RRB- Logistic_JJ Regression_NN -LRB-_-LRB- LR_NN -RRB-_-RRB- implemented_VBN in_IN BBR_NN package_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: and_CC 3_LS -RRB-_-RRB- Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- implemented_VBN in_IN LibSVM_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
When_WRB we_PRP only_RB have_VBP a_DT single_JJ source_NN domain_NN in_IN the_DT training_NN ,_, three_CD single_JJ classifiers_NNS are_VBP trained_VBN using_VBG the_DT above_JJ learning_NN algorithms_NNS and_CC combined_VBN according_VBG to_TO the_DT proposed_JJ weighted_JJ ensemble_NN framework_NN ._.
iminative_JJ models_NNS ,_, and_CC 2_LS -RRB-_-RRB- the_DT method_NN does_VBZ not_RB depend_VB on_IN specific_JJ applications_NNS and_CC makes_VBZ no_DT assumption_NN about_IN the_DT form_NN of_IN distributions_NNS generating_VBG the_DT training_NN or_CC the_DT test_NN data_NNS ._.
Multi-task_JJ learning_NN -LRB-_-LRB- MTL_NN -RRB-_-RRB- =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT learns_VBZ several_JJ related_JJ tasks_NNS at_IN the_DT same_JJ time_NN with_IN a_DT shared_JJ representation_NN ,_, considers_VBZ single_JJ P_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC multiple_JJ output_NN variables_NNS ,_, so_IN the_DT basic_JJ setting_NN is_VBZ different_JJ from_IN our_PRP$ problem_NN ._.
The_DT ``_`` clust_NN
it_PRP is_VBZ assumed_VBN that_IN the_DT training_NN and_CC the_DT test_NN examples_NNS are_VBP generated_VBN from_IN a_DT mixture_NN of_IN different_JJ models_NNS ,_, and_CC the_DT test_NN distribution_NN has_VBZ different_JJ mixture_NN coefficients_NNS than_IN the_DT training_NN distribution_NN ._.
In_IN =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT Dirichlet_NNP Process_VB prior_JJ is_VBZ used_VBN to_TO couple_VB the_DT parameters_NNS of_IN several_JJ models_NNS from_IN the_DT same_JJ parameterized_JJ family_NN of_IN dis-tributions_NNS ._.
-LRB-_-LRB- 10_CD -RRB-_-RRB- extends_VBZ the_DT boosting_VBG method_NN to_TO perform_VB transfer_NN learning_NN ._.
Ben_NNP
data_NNS sets_NNS are_VBP high-dimensional_JJ ,_, the_DT following_NN commonly_RB used_VBN algorithms_NNS are_VBP appropriate_JJ choices_NNS :_: 1_LS -RRB-_-RRB- Winnow_NN -LRB-_-LRB- WNN_NN -RRB-_-RRB- from_IN learning_VBG package_NN SNoW_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, 2_LS -RRB-_-RRB- Logistic_JJ Regression_NN -LRB-_-LRB- LR_NN -RRB-_-RRB- implemented_VBN in_IN BBR_NN package_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: ;_: and_CC 3_LS -RRB-_-RRB- Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- implemented_VBN in_IN LibSVM_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
When_WRB we_PRP only_RB have_VBP a_DT single_JJ source_NN domain_NN in_IN the_DT training_NN ,_, three_CD single_JJ classifiers_NNS are_VBP trained_VBN using_VBG the_DT above_JJ learning_NN algorithms_NNS and_CC
is_VBZ assumed_VBN that_IN the_DT two_CD distributions_NNS differ_VBP only_RB in_IN P_NN -LRB-_-LRB- x_NN -RRB-_-RRB- but_CC not_RB in_IN P_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- ,_, the_DT problem_NN is_VBZ referred_VBN to_TO as_IN covariate_NN shift_NN -LRB-_-LRB- 25_CD ,_, 18_CD -RRB-_-RRB- or_CC sample_NN selection_NN bias_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
The_DT instance_NN weighting_NN approaches_VBZ =_JJ -_: =[_NN 25_CD ,_, 18_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: try_VB to_TO re-weight_VB each_DT training_NN example_NN with_IN Ptest_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC maximize_VB the_DT re-weighted_JJ log_NN likelihood_NN ._.
Ptrain_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN of_IN work_NN tries_VBZ to_TO change_VB the_DT representation_NN of_IN the_DT observation_NN x_NN hoping_VBG that_IN th_DT
ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_VBZ =_JJ -_: =[_NN 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN p_NN
ngle_JJ source_NN of_IN information_NN and_CC try_VB to_TO learn_VB a_DT global_JJ single_JJ model_NN that_WDT adapts_VBZ well_RB to_TO the_DT test_NN set_NN ._.
Constructing_VBG a_DT good_JJ ensemble_NN of_IN classifiers_NNS has_VBZ been_VBN an_DT active_JJ research_NN area_NN in_IN supervised_JJ learning_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
By_IN combining_VBG decisions_NNS from_IN individual_JJ classifiers_NNS ,_, ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, baggi_NN
mble_JJ methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN problem_NN ._.
In_IN =_JJ -_: =[_NN 11_CD ,_, 26_CD -RRB-_-RRB- -_: =_JJ -_: ,_, it_PRP is_VBZ assumed_VBN that_IN the_DT training_NN and_CC the_DT test_NN examples_NNS are_VBP generated_VBN from_IN a_DT mixture_NN of_IN different_JJ models_NNS ,_, and_CC the_DT test_NN distribution_NN has_VBZ different_JJ mixture_NN coefficients_NNS than_IN the_DT training_NN distribution_NN ._.
I_PRP
d_NN test_NN distributions_NNS started_VBD gaining_VBG much_JJ attention_NN very_RB recently_RB ._.
When_WRB it_PRP is_VBZ assumed_VBN that_IN the_DT two_CD distributions_NNS differ_VBP only_RB in_IN P_NN -LRB-_-LRB- x_NN -RRB-_-RRB- but_CC not_RB in_IN P_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- ,_, the_DT problem_NN is_VBZ referred_VBN to_TO as_RB covariate_VB shift_NN =_JJ -_: =[_NN 25_CD ,_, 18_CD -RRB-_-RRB- -_: =_JJ -_: or_CC sample_NN selection_NN bias_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
The_DT instance_NN weighting_NN approaches_NNS -LRB-_-LRB- 25_CD ,_, 18_CD ,_, 5_CD -RRB-_-RRB- try_VBP to_TO re-weight_VB each_DT training_NN example_NN with_IN Ptest_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC maximize_VB the_DT re-weighted_JJ log_NN likelihood_NN ._.
Ptrain_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN o_NN
n_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN of_IN work_NN tries_VBZ to_TO change_VB the_DT representation_NN of_IN the_DT observation_NN x_NN hoping_VBG that_IN the_DT distributions_NNS of_IN the_DT training_NN and_CC the_DT test_NN examples_NNS will_MD become_VB very_RB similar_JJ after_IN the_DT transformation_NN =_JJ -_: =[_NN 3_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
-LRB-_-LRB- 22_CD -RRB-_-RRB- transforms_VBZ the_DT model_NN learned_VBD from_IN the_DT training_NN examples_NNS into_IN a_DT Bayesian_NNP prior_RB to_TO be_VB applied_VBN to_TO the_DT learning_NN process_NN on_IN the_DT test_NN domain_NN ._.
The_DT major_JJ difference_NN between_IN our_PRP$ work_NN and_CC these_DT studies_NNS i_LS
ng_NN much_JJ attention_NN very_RB recently_RB ._.
When_WRB it_PRP is_VBZ assumed_VBN that_IN the_DT two_CD distributions_NNS differ_VBP only_RB in_IN P_NN -LRB-_-LRB- x_NN -RRB-_-RRB- but_CC not_RB in_IN P_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- ,_, the_DT problem_NN is_VBZ referred_VBN to_TO as_IN covariate_NN shift_NN -LRB-_-LRB- 25_CD ,_, 18_CD -RRB-_-RRB- or_CC sample_NN selection_NN bias_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT instance_NN weighting_NN approaches_NNS -LRB-_-LRB- 25_CD ,_, 18_CD ,_, 5_CD -RRB-_-RRB- try_VBP to_TO re-weight_VB each_DT training_NN example_NN with_IN Ptest_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC maximize_VB the_DT re-weighted_JJ log_NN likelihood_NN ._.
Ptrain_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN of_IN work_NN tries_VBZ to_TO change_VB the_DT rep_NN
n_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN of_IN work_NN tries_VBZ to_TO change_VB the_DT representation_NN of_IN the_DT observation_NN x_NN hoping_VBG that_IN the_DT distributions_NNS of_IN the_DT training_NN and_CC the_DT test_NN examples_NNS will_MD become_VB very_RB similar_JJ after_IN the_DT transformation_NN =_JJ -_: =[_NN 3_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
-LRB-_-LRB- 22_CD -RRB-_-RRB- transforms_VBZ the_DT model_NN learned_VBD from_IN the_DT training_NN examples_NNS into_IN a_DT Bayesian_NNP prior_RB to_TO be_VB applied_VBN to_TO the_DT learning_NN process_NN on_IN the_DT test_NN domain_NN ._.
The_DT major_JJ difference_NN between_IN our_PRP$ work_NN and_CC these_DT studies_NNS i_LS
her_PRP$ line_NN of_IN work_NN tries_VBZ to_TO change_VB the_DT representation_NN of_IN the_DT observation_NN x_NN hoping_VBG that_IN the_DT distributions_NNS of_IN the_DT training_NN and_CC the_DT test_NN examples_NNS will_MD become_VB very_RB similar_JJ after_IN the_DT transformation_NN -LRB-_-LRB- 3_CD ,_, 24_CD -RRB-_-RRB- ._.
=_SYM -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: transforms_VBZ the_DT model_NN learned_VBD from_IN the_DT training_NN examples_NNS into_IN a_DT Bayesian_NNP prior_RB to_TO be_VB applied_VBN to_TO the_DT learning_NN process_NN on_IN the_DT test_NN domain_NN ._.
The_DT major_JJ difference_NN between_IN our_PRP$ work_NN and_CC these_DT studies_NNS is_VBZ that_IN
x_NN where_WRB the_DT ij_NN entry_NN is_VBZ model_NN Mi_NNP 's_POS predicted_VBN P_NN -LRB-_-LRB- y_NN =_JJ j_FW |_FW x_NN ,_, Mi_NN -RRB-_-RRB- ,_, i.e._FW ,_, h_NN i_FW j._FW Then_RB the_DT output_NN of_IN the_DT model_NN averaging_NN framework_NN for_IN x_NN is_VBZ a_DT vector_NN h_NN e_SYM =_JJ Hw_NN ._.
Note_VB that_DT w_NN satisfies_VBZ the_DT constraints_NNS that_WDT wi_VBP ∈_NN =_JJ -_: =[_NN 0_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: and_CC ∑_CD k_NN wi_NN =_JJ 1_CD ,_, and_CC thus_RB the_DT output_NN vector_NN hi_UH i_FW =_JJ 1_CD from_IN a_DT single_JJ model_NN Mi_NNP is_VBZ a_DT special_JJ case_NN of_IN h_NN e_SYM when_WRB wi_NN =_JJ 1_CD and_CC other_JJ weights_NNS are_VBP zero_CD ._.
But_CC we_PRP wish_VBP to_TO find_VB a_DT weight_NN vector_NN w_NN which_WDT minimizes_VBZ the_DT dist_NN
erent_JJ mixture_NN coefficients_NNS than_IN the_DT training_NN distribution_NN ._.
In_IN -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, a_DT Dirichlet_NNP Process_VB prior_JJ is_VBZ used_VBN to_TO couple_VB the_DT parameters_NNS of_IN several_JJ models_NNS from_IN the_DT same_JJ parameterized_JJ family_NN of_IN dis-tributions_NNS ._.
=_SYM -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: extends_VBZ the_DT boosting_VBG method_NN to_TO perform_VB transfer_NN learning_NN ._.
Bennett_NNP et_FW al._FW -LRB-_-LRB- 4_CD -RRB-_-RRB- proposed_VBD a_DT methodology_NN for_IN building_VBG a_DT meta-classifier_NN which_WDT combines_VBZ multiple_JJ distinct_JJ classifiers_NNS through_IN the_DT use_NN of_IN reli_NN
d_NN test_NN distributions_NNS started_VBD gaining_VBG much_JJ attention_NN very_RB recently_RB ._.
When_WRB it_PRP is_VBZ assumed_VBN that_IN the_DT two_CD distributions_NNS differ_VBP only_RB in_IN P_NN -LRB-_-LRB- x_NN -RRB-_-RRB- but_CC not_RB in_IN P_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- ,_, the_DT problem_NN is_VBZ referred_VBN to_TO as_RB covariate_VB shift_NN =_JJ -_: =[_NN 25_CD ,_, 18_CD -RRB-_-RRB- -_: =_JJ -_: or_CC sample_NN selection_NN bias_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
The_DT instance_NN weighting_NN approaches_NNS -LRB-_-LRB- 25_CD ,_, 18_CD ,_, 5_CD -RRB-_-RRB- try_VBP to_TO re-weight_VB each_DT training_NN example_NN with_IN Ptest_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC maximize_VB the_DT re-weighted_JJ log_NN likelihood_NN ._.
Ptrain_NN -LRB-_-LRB- x_NN -RRB-_-RRB- Another_DT line_NN o_NN
ategy_NN is_VBZ to_TO split_VB the_DT sub-categories_NNS among_IN the_DT training_NN and_CC the_DT test_NN sets_VBZ so_RB that_IN the_DT distributions_NNS of_IN the_DT two_CD sets_NNS are_VBP similar_JJ but_CC not_RB exactly_RB the_DT same_JJ ._.
The_DT tasks_NNS are_VBP generated_VBN in_IN the_DT same_JJ way_NN as_IN in_IN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: and_CC more_JJR details_NNS can_MD be_VB found_VBN there_RB ._.
Intrusion_NN detection_NN ._.
The_DT KDD_NNP cup_NN ’99_CD data_NNS set_NN consists_VBZ of_IN a_DT series_NN of_IN TCP_NNP connection_NN records_NNS for_IN a_DT local_JJ area_NN network_NN ._.
Each_DT example_NN in_IN the_DT data_NNS set_NN corresponds_VBZ to_TO
acy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_NNS -LRB-_-LRB- 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB =_JJ -_: =[_NN 1_CD ,_, 19_CD -RRB-_-RRB- -_: =_JJ -_: ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN problem_NN ._.
In_IN -LRB-_-LRB- 11_CD ,_, 26_CD -RRB-_-RRB- ,_, it_PRP is_VBZ assumed_VBN that_IN the_DT training_NN a_DT
ensembles_NNS can_MD usually_RB reduce_VB variance_NN and_CC achieve_VB higher_JJR accuracy_NN than_IN individual_JJ classifiers_NNS ._.
Such_JJ methods_NNS include_VBP Bayesian_JJ averaging_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, bagging_NN ,_, boosting_VBG and_CC many_JJ variants_NNS of_IN ensemble_NN approaches_VBZ =_JJ -_: =[_NN 2_CD ,_, 27_CD ,_, 13_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Some_DT ensemble_NN methods_NNS assign_VBP weights_NNS locally_RB -LRB-_-LRB- 1_CD ,_, 19_CD -RRB-_-RRB- ,_, but_CC such_JJ weights_NNS are_VBP determined_VBN based_VBN on_IN training_NN data_NNS only_RB ._.
There_EX has_VBZ not_RB been_VBN much_JJ work_NN on_IN ensemble_NN methods_NNS to_TO address_VB the_DT transfer_NN learning_NN p_NN
thods_NNS where_WRB the_DT model_NN weights_NNS are_VBP set_VBN the_DT same_JJ for_IN all_PDT the_DT test_NN examples_NNS ._.
Suppose_VB there_EX are_VBP k_NN models_NNS ,_, then_RB each_DT model_NN will_MD have_VB a_DT weight_NN 1_CD at_IN every_DT test_NN k_NN example_NN ._.
We_PRP use_VBP the_DT clustering_NN package_NN CLUTO_NN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT is_VBZ designed_VBN for_IN high-dimensional_JJ data_NNS clustering_NN ,_, to_TO cluster_VB the_DT test_NN set_NN ._.
Again_RB ,_, other_JJ clustering_NN algorithms_NNS could_MD be_VB used_VBN as_RB long_RB as_IN the_DT ``_`` clustering_NN ''_'' assumption_NN is_VBZ satisfied_VBN ._.
We_PRP compare_VBP with_IN
