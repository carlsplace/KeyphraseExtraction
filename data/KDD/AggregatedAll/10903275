Learning_NNP incoherent_JJ sparse_JJ and_CC low-rank_JJ patterns_NNS from_IN multiple_JJ tasks_NNS
We_PRP consider_VBP the_DT problem_NN of_IN learning_VBG incoherent_JJ sparse_JJ and_CC low-rank_JJ patterns_NNS from_IN multiple_JJ tasks_NNS ._.
Our_PRP$ approach_NN is_VBZ based_VBN on_IN a_DT linear_JJ multi-task_JJ learning_NN formulation_NN ,_, in_IN which_WDT the_DT sparse_JJ and_CC low-rank_JJ patterns_NNS are_VBP induced_VBN by_IN a_DT cardinality_NN regularization_NN term_NN and_CC a_DT low-rank_JJ constraint_NN ,_, respectively_RB ._.
This_DT formulation_NN is_VBZ non-convex_JJ ;_: we_PRP convert_VBP it_PRP into_IN its_PRP$ convex_NN surrogate_NN ,_, which_WDT can_MD be_VB routinely_RB solved_VBN via_IN semidefinite_JJ programming_NN for_IN small-size_JJ problems_NNS ._.
We_PRP propose_VBP to_TO employ_VB the_DT general_JJ projected_JJ gradient_NN scheme_NN to_TO efficiently_RB solve_VB such_PDT a_DT convex_NN surrogate_NN ;_: however_RB ,_, in_IN the_DT optimization_NN formulation_NN ,_, the_DT objective_JJ function_NN is_VBZ non-differentiable_JJ and_CC the_DT feasible_JJ domain_NN is_VBZ non-trivial_JJ ._.
We_PRP present_VBP the_DT procedures_NNS for_IN computing_VBG the_DT projected_VBN gradient_NN and_CC ensuring_VBG the_DT global_JJ convergence_NN of_IN the_DT projected_VBN gradient_NN scheme_NN ._.
The_DT computation_NN of_IN projected_VBN gradient_NN involves_VBZ a_DT constrained_VBN optimization_NN problem_NN ;_: we_PRP show_VBP that_IN the_DT optimal_JJ solution_NN to_TO such_PDT a_DT problem_NN can_MD be_VB obtained_VBN via_IN solving_VBG an_DT unconstrained_JJ optimization_NN subproblem_NN and_CC an_DT Euclidean_JJ projection_NN subproblem_NN ._.
In_IN addition_NN ,_, we_PRP present_VBP two_CD projected_JJ gradient_NN algorithms_NNS and_CC discuss_VBP their_PRP$ rates_NNS of_IN convergence_NN ._.
Experimental_JJ results_NNS on_IN benchmark_JJ data_NNS sets_NNS demonstrate_VBP the_DT effectiveness_NN of_IN the_DT proposed_VBN multi-task_JJ learning_NN formulation_NN and_CC the_DT efficiency_NN of_IN the_DT proposed_VBN projected_JJ gradient_NN algorithms_NNS ._.
l1_NN 8000_CD 120_CD 80_CD multimedia_NNS MediaMill2_NN 8000 120 100_CD multimedia_NNS References_NNS 7929_CD 26397_CD 15_CD text_NN Science_NNP 6345_CD 24002_CD 22_CD text_NN We_PRP employ_VBP six_CD benchmark_JJ data_NNS sets_NNS in_IN our_PRP$ experiments_NNS ._.
One_CD of_IN them_PRP is_VBZ AR_NN Face_NNP Data_NNP =_SYM -_: =[_NN 24_CD -RRB-_-RRB- -_: =_JJ -_: :_: we_PRP use_VBP its_PRP$ subset_NN consisting_VBG of_IN 1400_CD face_NN images_NNS corresponding_VBG to_TO 100_CD persons_NNS ._.
Another_DT three_CD are_VBP LIBSVM_NNP multi-label_JJ data_NN sets_NNS 2_CD :_: for_IN Scene_NN and_CC Yeast_NN ,_, we_PRP use_VBP the_DT entire_JJ data_NN sets_NNS ;_: for_IN MediaMill_NNP ,_, we_PRP
d_NN into_IN the_DT multi-task_JJ learning_NN domain_NN -LRB-_-LRB- 1_CD ,_, 4_CD ,_, 20_CD ,_, 27_CD ,_, 28_CD -RRB-_-RRB- to_TO capture_VB the_DT task_NN relationship_NN via_IN a_DT shared_JJ low-rank_JJ structure_NN of_IN the_DT model_NN parameters_NNS ,_, resulting_VBG in_IN a_DT tractable_JJ convex_NN optimization_NN problem_NN =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN many_JJ real-world_JJ applications_NNS ,_, the_DT underlying_VBG predictive_JJ classifiers_NNS may_MD lie_VB in_IN a_DT hypothesis_NN space_NN of_IN some_DT low-rank_JJ structure_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, in_IN which_WDT the_DT multiple_JJ learning_NN tasks_NNS can_MD be_VB coupled_VBN using_VBG a_DT set_VBN o_NN
ate_VBD ._.
The_DT proposed_VBN algorithm_NN for_IN solving_VBG the_DT formulation_NN in_IN Eq_NN ._.
-LRB-_-LRB- 5_CD -RRB-_-RRB- finds_VBZ a_DT globally_RB optimal_JJ solution_NN and_CC achieves_VBZ the_DT optimal_JJ convergence_NN rate_NN among_IN all_DT first-order_JJ methods_NNS ._.
Note_VB that_IN recent_JJ works_NNS in_IN =_JJ -_: =[_NN 12_CD ,_, 14_CD ,_, 37_CD -RRB-_-RRB- -_: =_SYM -_: consider_VB the_DT problem_NN of_IN decomposing_VBG a_DT given_VBN matrix_NN into_IN its_PRP$ underlying_JJ sparse_JJ component_NN and_CC low-rank_JJ component_NN in_IN a_DT different_JJ setting_NN :_: they_PRP study_VBP the_DT theoretical_JJ condition_NN under_IN which_WDT such_JJ two_CD compon_NN
+_CC Q_NNP ,_, -LRB-_-LRB- 1_LS -RRB-_-RRB- as_IN illustrated_VBN in_IN Figure_NNP 1_CD ._.
The_DT ℓ_NN 0_CD -_: norm_NN -LRB-_-LRB- cardinality_NN -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, i.e._FW ,_, the_DT number_NN of_IN non-zero_JJ entries_NNS ,_, is_VBZ commonly_RB used_VBN to_TO control_VB the_DT sparsity_NN structure_NN in_IN the_DT matrix_NN ;_: similarity_NN ,_, matrix_NN rank_NN =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ used_VBN to_TO encourage_VB the_DT low-rank_JJ structure_NN ._.
We_PRP propose_VBP a_DT multi-task_JJ learning_NN formulation_NN with_IN a_DT cardinality_NN regularization_NN and_CC a_DT rank_JJ constraint_NN given_VBN by_IN min_NN Z_NN ,_, P_NN ,_, Q_NNP ∈_NNP R_NNP d_FW ×_FW m_NN m_NN ∑_CD n_NN ∑_NN ℓ_NN -LRB-_-LRB- L_NN z_SYM T_NN ℓ_NN x_NN ℓ_FW i_FW ,_, y_FW ℓ_FW
constrained_VBN optimization_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 5_CD -RRB-_-RRB- ._.
Note_VB that_IN the_DT projected_VBN gradient_NN scheme_NN belongs_VBZ to_TO the_DT category_NN of_IN first-order_JJ methods_NNS and_CC has_VBZ demonstrated_VBN good_JJ scalability_NN in_IN many_JJ optimization_NN problems_NNS =_JJ -_: =[_NN 11_CD ,_, 25_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT objective_JJ function_NN in_IN Eq_NN ._.
-LRB-_-LRB- 5_CD -RRB-_-RRB- is_VBZ non-smooth_JJ and_CC the_DT feasible_JJ domain_NN is_VBZ non-trivial_JJ ._.
For_IN simplicity_NN ,_, we_PRP denote_VBP Eq_NN ._.
-LRB-_-LRB- 5_CD -RRB-_-RRB- as_IN min_NN T_NN f_FW -LRB-_-LRB- T_NN -RRB-_-RRB- +_CC g_NN -LRB-_-LRB- T_NN -RRB-_-RRB- subject_JJ to_TO T_NN ∈_CD M_NN ,_, -LRB-_-LRB- 6_CD -RRB-_-RRB- where_WRB the_DT functions_NNS f_FW -LRB-_-LRB- T_NN -RRB-_-RRB- and_CC g_NN -LRB-_-LRB- T_NN
oped_VBN for_IN learning_VBG clustered_VBN tasks_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ;_: a_DT shared_JJ low-rank_JJ structure_NN is_VBZ learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 3_CD ,_, 15_CD -RRB-_-RRB- ._.
Recently_RB ,_, trace_NN norm_NN regularization_NN has_VBZ been_VBN introduced_VBN into_IN the_DT multi-task_JJ learning_NN domain_NN =_JJ -_: =[_NN 1_CD ,_, 4_CD ,_, 20_CD ,_, 27_CD ,_, 28_CD -RRB-_-RRB- -_: =_SYM -_: to_TO capture_VB the_DT task_NN relationship_NN via_IN a_DT shared_JJ low-rank_JJ structure_NN of_IN the_DT model_NN parameters_NNS ,_, resulting_VBG in_IN a_DT tractable_JJ convex_NN optimization_NN problem_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
In_IN many_JJ real-world_JJ applications_NNS ,_, the_DT underlying_JJ pr_NN
TP_NN ‖_NN 1_CD ._.
It_PRP is_VBZ obvious_JJ that_IN each_DT entry_NN of_IN the_DT optimal_JJ matrix_NN TP_NN can_MD be_VB obtained_VBN by_IN solving_VBG min_NN ˆt_FW ∈_FW R_NN β_FW ‖_FW ˆt_FW −_FW ˆs_FW ‖_NN 2_CD +_CC γ_FW |_FW ˆt_FW |_FW ,_, -LRB-_-LRB- 19_CD -RRB-_-RRB- where_WRB ˆs_NN denotes_VBZ the_DT entry_NN in_FW ˆ_FW SP_NN corresponding_VBG to_TO ˆt_NN in_IN TP_NN ._.
It_PRP is_VBZ known_VBN =_JJ -_: =[_NN 33_CD -RRB-_-RRB- -_: =_SYM -_: that_IN the_DT optimal_JJ ˆt_NN to_TO Eq_NN ._.
-LRB-_-LRB- 19_CD -RRB-_-RRB- admits_VBZ an_DT analytical_JJ solution_NN ;_: for_IN completeness_NN ,_, we_PRP present_VBP its_PRP$ proof_NN in_IN Lemma_NNP 4.1_CD ._.
LEMMA_NN 4.1_CD ._.
The_DT minimizer_NN of_IN Eq_NN ._.
-LRB-_-LRB- 19_CD -RRB-_-RRB- can_MD be_VB expressed_VBN as_IN ˆt_FW ∗_FW ⎧_FW ⎨_FW ˆs_FW −_NN =_JJ ⎩_FW γ_FW ˆs_FW -RRB-_-RRB- 2_CD
st_RB ,_, we_PRP use_VBP the_DT entire_JJ data_NN sets_NNS ;_: for_IN MediaMill_NNP ,_, we_PRP generate_VBP several_JJ subsets_NNS by_IN randomly_RB sampling_NN 8000_CD data_NN points_NNS with_IN different_JJ numbers_NNS of_IN labels_NNS ._.
References_NNS and_CC Science_NNP are_VBP Yahoo_NNP webpages_NNS data_NN sets_VBZ =_JJ -_: =[_NN 34_CD -RRB-_-RRB- -_: =_JJ -_: :_: we_PRP preprocess_VBP the_DT data_NNS sets_NNS following_VBG the_DT same_JJ procedures_NNS in_IN -LRB-_-LRB- 15_CD -RRB-_-RRB- ._.
All_DT of_IN the_DT benchmark_JJ data_NN sets_NNS are_VBP normalized_VBN and_CC their_PRP$ statistics_NNS are_VBP summarized_VBN in_IN Table_NNP 1_CD ._.
Note_VB that_IN in_IN our_PRP$ multi-task_JJ learning_NN
-RRB-_-RRB- ∈_NN R_NN d_FW ×_FW m_NN ;_: Z_NN is_VBZ the_DT summation_NN of_IN a_DT sparse_JJ matrix_NN P_NN =_JJ -LRB-_-LRB- p1_NN ,_, ·_FW ·_FW ·_NN ,_, pm_NN -RRB-_-RRB- ∈_NN R_NN d_FW ×_FW m_NN and_CC a_DT low-rank_JJ matrix_NN Q_NNP =_JJ -LRB-_-LRB- q1_NN ,_, ·_FW ·_FW ·_NN ,_, qm_NN -RRB-_-RRB- ∈_NN R_NN d_FW ×_FW m_NN given_VBN by_IN Z_NN =_JJ P_NN +_CC Q_NNP ,_, -LRB-_-LRB- 1_LS -RRB-_-RRB- as_IN illustrated_VBN in_IN Figure_NNP 1_CD ._.
The_DT ℓ_NN 0_CD -_: norm_NN -LRB-_-LRB- cardinality_NN -RRB-_-RRB- =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, i.e._FW ,_, the_DT number_NN of_IN non-zero_JJ entries_NNS ,_, is_VBZ commonly_RB used_VBN to_TO control_VB the_DT sparsity_NN structure_NN in_IN the_DT matrix_NN ;_: similarity_NN ,_, matrix_NN rank_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- is_VBZ used_VBN to_TO encourage_VB the_DT low-rank_JJ structure_NN ._.
We_PRP propose_VBP a_DT multi-t_NN
amount_NN of_IN training_NN data_NNS is_VBZ available_JJ for_IN learning_VBG each_DT task_NN ._.
MTL_NNP has_VBZ been_VBN investigated_VBN by_IN many_JJ researchers_NNS from_IN different_JJ perspectives_NNS ._.
Hidden_JJ units_NNS of_IN neural_JJ networks_NNS are_VBP shared_VBN among_IN similar_JJ tasks_NNS =_JJ -_: =[_NN 6_CD ,_, 13_CD -RRB-_-RRB- -_: =_JJ -_: ;_: task_NN relatedness_NN are_VBP modeled_VBN using_VBG the_DT common_JJ prior_JJ distribution_NN in_IN hierarchical_JJ Bayesian_JJ models_NNS -LRB-_-LRB- 5_CD ,_, 29_CD ,_, 40_CD ,_, 41_CD -RRB-_-RRB- ;_: the_DT parameters_NNS of_IN Gaussian_NN Process_VBP covariance_NN are_VBP learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 21_CD -RRB-_-RRB- ;_:
6_CD ,_, 13_CD -RRB-_-RRB- ;_: task_NN relatedness_NN are_VBP modeled_VBN using_VBG the_DT common_JJ prior_JJ distribution_NN in_IN hierarchical_JJ Bayesian_JJ models_NNS -LRB-_-LRB- 5_CD ,_, 29_CD ,_, 40_CD ,_, 41_CD -RRB-_-RRB- ;_: the_DT parameters_NNS of_IN Gaussian_NN Process_VBP covariance_NN are_VBP learned_VBN from_IN multiple_JJ tasks_NNS =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_JJ -_: ;_: kernel_NN methods_NNS and_CC regularization_NN networks_NNS are_VBP extended_VBN to_TO multi-task_JJ learning_NN setting_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: a_DT convex_NN formulation_NN is_VBZ developed_VBN for_IN learning_VBG clustered_VBN tasks_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ;_: a_DT shared_JJ low-rank_JJ structure_NN is_VBZ learn_VB
e_LS the_DT SVD_NN of_IN T_NN ∗_NN Q_NNP and_CC r_NN =_JJ rank_NN -LRB-_-LRB- T_NN ∗_NN Q_NNP -RRB-_-RRB- ,_, where_WRB UT_NNP ∈_NNP R_NNP d_FW ×_FW r_NN and_CC UT_FW ∈_FW R_NN m_NN ×_CD r_NN are_VBP columnwise_JJ orthonormal_NN ,_, and_CC ΣT_FW ∈_FW R_NN r_NN ×_CD r_NN is_VBZ diagonal_JJ consisting_VBG of_IN non-zero_JJ singular_JJ values_NNS on_IN the_DT main_JJ diagonal_NN ._.
It_PRP is_VBZ known_VBN =_JJ -_: =[_NN 36_CD -RRB-_-RRB- -_: =_SYM -_: that_IN the_DT subdifferentials_NNS of_IN ‖_NNP TQ_NNP ‖_FW ∗_FW at_IN T_NN ∗_CD Q_NNP can_MD be_VB expressed_VBN as_IN ∂_FW ‖_FW T_NN ∗_NN Q_NNP ‖_NNP ∗_NN =_JJ -LCB-_-LRB- UT_JJ V_NN T_NN T_NN +_CC D_NN :_: D_NNP ∈_NNP R_NNP d_FW ×_FW m_NN ,_, U_NN T_NN T_NN D_NN =_JJ 0_CD ,_, DVT_NN =_JJ 0_CD ,_, ‖_NN D_NN ‖_NN 2_CD ≤_NN 1_CD -RCB-_-RRB- ._.
-LRB-_-LRB- 23_CD -RRB-_-RRB- On_IN the_DT other_JJ hand_NN ,_, we_PRP can_MD verify_VB that_IN T_NN ∗_NN Q_NNP is_VBZ optimal_JJ to_TO
ive_JJ classifiers_NNS may_MD lie_VB in_IN a_DT hypothesis_NN space_NN of_IN some_DT low-rank_JJ structure_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, in_IN which_WDT the_DT multiple_JJ learning_NN tasks_NNS can_MD be_VB coupled_VBN using_VBG a_DT set_NN of_IN shared_JJ factors_NNS ,_, i.e._FW ,_, the_DT basis_NN of_IN a_DT low-rank_JJ subspace_NN =_JJ -_: =[_NN 30_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN example_NN ,_, in_IN natural_JJ scene_NN categorization_NN problems_NNS ,_, images_NNS of_IN different_JJ labels_NNS may_MD share_VB similar_JJ background_NN of_IN a_DT low-rank_JJ structure_NN ;_: in_IN collaborative_JJ filtering_VBG or_CC recommender_NN system_NN ,_, only_RB a_DT few_JJ f_SYM
ℓ_NN -RRB-_-RRB- i_FW +_CC γ_FW ‖_FW P_NN ‖_NN 1_CD subject_JJ to_TO Z_NN =_JJ P_NN +_CC Q_NNP ,_, ‖_NNP Q_NNP ‖_FW ∗_FW ≤_FW τ_FW ._.
-LRB-_-LRB- 5_LS -RRB-_-RRB- The_DT optimization_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 5_CD -RRB-_-RRB- is_VBZ the_DT tightest_JJS convex_NN relaxation_NN of_IN Eq_NN ._.
-LRB-_-LRB- 2_LS -RRB-_-RRB- ._.
Such_JJ a_DT problem_NN can_MD be_VB reformulated_VBN as_IN a_DT semidefinite_NN program_NN -LRB-_-LRB- SDP_NN -RRB-_-RRB- =_JJ -_: =[_NN 35_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC then_RB solved_VBD using_VBG many_JJ off-theshelf_JJ optimization_NN solvers_NNS such_JJ as_IN SeDuMi_NN -LRB-_-LRB- 32_CD -RRB-_-RRB- ;_: however_RB ,_, SDP_NN is_VBZ computationally_RB expensive_JJ and_CC can_MD only_RB handle_VB several_JJ hundreds_NNS of_IN optimization_NN variables_NNS ._.
For_IN simpli_NNS
s_NN from_IN different_JJ perspectives_NNS ._.
Hidden_JJ units_NNS of_IN neural_JJ networks_NNS are_VBP shared_VBN among_IN similar_JJ tasks_NNS -LRB-_-LRB- 6_CD ,_, 13_CD -RRB-_-RRB- ;_: task_NN relatedness_NN are_VBP modeled_VBN using_VBG the_DT common_JJ prior_JJ distribution_NN in_IN hierarchical_JJ Bayesian_JJ models_NNS =_JJ -_: =[_NN 5_CD ,_, 29_CD ,_, 40_CD ,_, 41_CD -RRB-_-RRB- -_: =_JJ -_: ;_: the_DT parameters_NNS of_IN Gaussian_NN Process_VBP covariance_NN are_VBP learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 21_CD -RRB-_-RRB- ;_: kernel_NN methods_NNS and_CC regularization_NN networks_NNS are_VBP extended_VBN to_TO multi-task_JJ learning_NN setting_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: a_DT convex_NN formulation_NN
sian_JJ models_NNS -LRB-_-LRB- 5_CD ,_, 29_CD ,_, 40_CD ,_, 41_CD -RRB-_-RRB- ;_: the_DT parameters_NNS of_IN Gaussian_NN Process_VBP covariance_NN are_VBP learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 21_CD -RRB-_-RRB- ;_: kernel_NN methods_NNS and_CC regularization_NN networks_NNS are_VBP extended_VBN to_TO multi-task_JJ learning_NN setting_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: ;_: a_DT convex_NN formulation_NN is_VBZ developed_VBN for_IN learning_VBG clustered_VBN tasks_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ;_: a_DT shared_JJ low-rank_JJ structure_NN is_VBZ learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 3_CD ,_, 15_CD -RRB-_-RRB- ._.
Recently_RB ,_, trace_NN norm_NN regularization_NN has_VBZ been_VBN introduced_VBN into_IN
gularization_NN networks_NNS are_VBP extended_VBN to_TO multi-task_JJ learning_NN setting_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: a_DT convex_NN formulation_NN is_VBZ developed_VBN for_IN learning_VBG clustered_VBN tasks_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ;_: a_DT shared_JJ low-rank_JJ structure_NN is_VBZ learned_VBN from_IN multiple_JJ tasks_NNS =_JJ -_: =[_NN 3_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Recently_RB ,_, trace_NN norm_NN regularization_NN has_VBZ been_VBN introduced_VBN into_IN the_DT multi-task_JJ learning_NN domain_NN -LRB-_-LRB- 1_CD ,_, 4_CD ,_, 20_CD ,_, 27_CD ,_, 28_CD -RRB-_-RRB- to_TO capture_VB the_DT task_NN relationship_NN via_IN a_DT shared_JJ low-rank_JJ structure_NN of_IN the_DT model_NN parameter_NN
s_NN from_IN different_JJ perspectives_NNS ._.
Hidden_JJ units_NNS of_IN neural_JJ networks_NNS are_VBP shared_VBN among_IN similar_JJ tasks_NNS -LRB-_-LRB- 6_CD ,_, 13_CD -RRB-_-RRB- ;_: task_NN relatedness_NN are_VBP modeled_VBN using_VBG the_DT common_JJ prior_JJ distribution_NN in_IN hierarchical_JJ Bayesian_JJ models_NNS =_JJ -_: =[_NN 5_CD ,_, 29_CD ,_, 40_CD ,_, 41_CD -RRB-_-RRB- -_: =_JJ -_: ;_: the_DT parameters_NNS of_IN Gaussian_NN Process_VBP covariance_NN are_VBP learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 21_CD -RRB-_-RRB- ;_: kernel_NN methods_NNS and_CC regularization_NN networks_NNS are_VBP extended_VBN to_TO multi-task_JJ learning_NN setting_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: a_DT convex_NN formulation_NN
,_, y_FW ℓ_FW -RRB-_-RRB- i_LS ,_, g_NN -LRB-_-LRB- T_NN -RRB-_-RRB- =_JJ γ_FW ‖_FW P_NN ‖_NN 1_CD ,_, ℓ_NN =_JJ 1_CD i_LS =_JJ 1_CD and_CC the_DT set_VBN M_NN is_VBZ defined_VBN as_IN -LCB-_-LRB- -LRB-_-LRB- M_NN =_JJ T_NN P_NN ∣_CD T_NN =_JJ Q_NNP -RRB-_-RRB- ,_, P_NN ∈_NN R_NN d_FW ×_FW m_NN ,_, ‖_NNP Q_NNP ‖_NNP ∗_NNP ≤_NNP τ_NNP ,_, Q_NNP ∈_NNP R_NNP d_FW ×_FW m_NN -RCB-_-RRB- ._.
Note_VB that_IN f_LS -LRB-_-LRB- T_NN -RRB-_-RRB- is_VBZ a_DT smooth_JJ convex_NN function_NN with_IN Lipschitz_NNP continuous_JJ gradient_NN Lf_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: as_IN :_: ‖_FW ∇_FW f_FW -LRB-_-LRB- Tx_NN -RRB-_-RRB- −_FW ∇_FW f_FW -LRB-_-LRB- Ty_NN -RRB-_-RRB- ‖_NN F_NN ≤_FW Lf_FW ‖_FW Tx_NN −_NN Ty_NN ‖_NN F_NN ,_, ∀_NN Tx_NN ,_, Ty_NNP ∈_NNP M_NNP ,_, -LRB-_-LRB- 7_CD -RRB-_-RRB- g_NN -LRB-_-LRB- T_NN -RRB-_-RRB- is_VBZ a_DT non-smooth_JJ convex_NN function_NN ,_, and_CC M_NN is_VBZ a_DT compact_JJ and_CC convex_JJ set_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ;_: moreover_RB ,_, for_IN any_DT L_NNP ≥_NNP Lf_NNP ,_, the_DT following_JJ inequality_NN holds_VBZ -LRB-_-LRB- 26_CD -RRB-_-RRB- :_: f_LS -LRB-_-LRB-
erms_VBZ Algorithms_NNP Keywords_NNP Multi-task_JJ learning_NN ,_, sparse_JJ and_CC low-rank_JJ patterns_NNS ,_, trace_NN norm_NN 1_CD ._.
INTRODUCTION_NN In_IN the_DT past_JJ decade_NN there_EX has_VBZ been_VBN a_DT growing_VBG interest_NN in_IN the_DT problem_NN of_IN multi-task_JJ learning_NN -LRB-_-LRB- MTL_NN -RRB-_-RRB- =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
It_PRP has_VBZ been_VBN applied_VBN successfully_RB in_IN many_JJ areas_NNS of_IN data_NNS mining_NN and_CC machine_NN learning_NN -LRB-_-LRB- 2_CD ,_, Permission_NN to_TO make_VB digital_JJ or_CC hard_JJ copies_NNS of_IN all_DT or_CC part_NN of_IN this_DT work_NN for_IN personal_JJ or_CC classroom_NN use_NN is_VBZ granted_VBN
s_NN from_IN different_JJ perspectives_NNS ._.
Hidden_JJ units_NNS of_IN neural_JJ networks_NNS are_VBP shared_VBN among_IN similar_JJ tasks_NNS -LRB-_-LRB- 6_CD ,_, 13_CD -RRB-_-RRB- ;_: task_NN relatedness_NN are_VBP modeled_VBN using_VBG the_DT common_JJ prior_JJ distribution_NN in_IN hierarchical_JJ Bayesian_JJ models_NNS =_JJ -_: =[_NN 5_CD ,_, 29_CD ,_, 40_CD ,_, 41_CD -RRB-_-RRB- -_: =_JJ -_: ;_: the_DT parameters_NNS of_IN Gaussian_NN Process_VBP covariance_NN are_VBP learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 21_CD -RRB-_-RRB- ;_: kernel_NN methods_NNS and_CC regularization_NN networks_NNS are_VBP extended_VBN to_TO multi-task_JJ learning_NN setting_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: a_DT convex_NN formulation_NN
been_VBN known_VBN as_IN the_DT convex_JJ envelope_NN of_IN the_DT ℓ_NN 0_CD -_: norm_NN as_IN -LRB-_-LRB- 11_CD -RRB-_-RRB- :_: ‖_CD P_NN ‖_NN 1_CD ≤_FW ‖_FW P_NN ‖_NN 0_CD ,_, ∀_CD P_NN ∈_NN C_NN =_JJ -LCB-_-LRB- P_NN |_FW ‖_FW P_NN ‖_FW ∞_FW ≤_NN 1_CD -RCB-_-RRB- ._.
-LRB-_-LRB- 3_LS -RRB-_-RRB- Similarly_RB ,_, trace_NN norm_NN -LRB-_-LRB- nuclear_JJ norm_NN -RRB-_-RRB- has_VBZ been_VBN shown_VBN as_IN the_DT convex_NN envelop_NN of_IN the_DT rank_NN function_NN as_IN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: :_: ‖_CD Q_NNP ‖_FW ∗_FW ≤_FW rank_NN -LRB-_-LRB- Q_NNP -RRB-_-RRB- ,_, ∀_CD Q_NNP ∈_NN C_NN =_JJ -LCB-_-LRB- Q_NNP |_NNP ‖_NNP Q_NNP ‖_NNP 2_CD ≤_NN 1_CD -RCB-_-RRB- ._.
-LRB-_-LRB- 4_LS -RRB-_-RRB- Note_VB that_IN both_CC the_DT ℓ_NN 1_CD -_: norm_NN and_CC the_DT trace-norm_JJ functions_NNS are_VBP convex_JJ but_CC non-smooth_JJ ,_, and_CC they_PRP have_VBP been_VBN shown_VBN to_TO be_VB effective_JJ surrogates_NNS of_IN the_DT ℓ_NN 0_CD -_: norm_NN
-LRB-_-LRB- 5_CD -RRB-_-RRB- is_VBZ the_DT tightest_JJS convex_NN relaxation_NN of_IN Eq_NN ._.
-LRB-_-LRB- 2_LS -RRB-_-RRB- ._.
Such_JJ a_DT problem_NN can_MD be_VB reformulated_VBN as_IN a_DT semidefinite_NN program_NN -LRB-_-LRB- SDP_NN -RRB-_-RRB- -LRB-_-LRB- 35_CD -RRB-_-RRB- ,_, and_CC then_RB solved_VBD using_VBG many_JJ off-theshelf_JJ optimization_NN solvers_NNS such_JJ as_IN SeDuMi_NN =_JJ -_: =[_NN 32_CD -RRB-_-RRB- -_: =_JJ -_: ;_: however_RB ,_, SDP_NN is_VBZ computationally_RB expensive_JJ and_CC can_MD only_RB handle_VB several_JJ hundreds_NNS of_IN optimization_NN variables_NNS ._.
For_IN simplicity_NN ,_, in_IN this_DT paper_NN we_PRP assume_VBP that_IN all_DT of_IN the_DT m_NN tasks_NNS share_VBP the_DT same_JJ set_NN of_IN traini_NNS
s_NN from_IN different_JJ perspectives_NNS ._.
Hidden_JJ units_NNS of_IN neural_JJ networks_NNS are_VBP shared_VBN among_IN similar_JJ tasks_NNS -LRB-_-LRB- 6_CD ,_, 13_CD -RRB-_-RRB- ;_: task_NN relatedness_NN are_VBP modeled_VBN using_VBG the_DT common_JJ prior_JJ distribution_NN in_IN hierarchical_JJ Bayesian_JJ models_NNS =_JJ -_: =[_NN 5_CD ,_, 29_CD ,_, 40_CD ,_, 41_CD -RRB-_-RRB- -_: =_JJ -_: ;_: the_DT parameters_NNS of_IN Gaussian_NN Process_VBP covariance_NN are_VBP learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 21_CD -RRB-_-RRB- ;_: kernel_NN methods_NNS and_CC regularization_NN networks_NNS are_VBP extended_VBN to_TO multi-task_JJ learning_NN setting_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: a_DT convex_NN formulation_NN
oped_VBN for_IN learning_VBG clustered_VBN tasks_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ;_: a_DT shared_JJ low-rank_JJ structure_NN is_VBZ learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 3_CD ,_, 15_CD -RRB-_-RRB- ._.
Recently_RB ,_, trace_NN norm_NN regularization_NN has_VBZ been_VBN introduced_VBN into_IN the_DT multi-task_JJ learning_NN domain_NN =_JJ -_: =[_NN 1_CD ,_, 4_CD ,_, 20_CD ,_, 27_CD ,_, 28_CD -RRB-_-RRB- -_: =_SYM -_: to_TO capture_VB the_DT task_NN relationship_NN via_IN a_DT shared_JJ low-rank_JJ structure_NN of_IN the_DT model_NN parameters_NNS ,_, resulting_VBG in_IN a_DT tractable_JJ convex_NN optimization_NN problem_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
In_IN many_JJ real-world_JJ applications_NNS ,_, the_DT underlying_JJ pr_NN
of_IN high_JJ rank_NN ._.
6.2_CD Performance_NNP Evaluation_NN We_PRP compare_VBP the_DT proposed_JJ multi-task_JJ learning_NN formulation_NN with_IN other_JJ representative_JJ ones_NNS in_IN terms_NNS of_IN average_JJ Area_NNP Under_IN the_DT Curve_NN -LRB-_-LRB- AUC_NN -RRB-_-RRB- ,_, Macro_NN F1_NN ,_, and_CC Micro_NNP F1_NN =_JJ -_: =[_NN 39_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT reported_VBN experimental_JJ results_NNS are_VBP averaged_VBN over_IN five_CD random_JJ repetitions_NNS of_IN the_DT data_NNS sets_NNS into_IN training_NN and_CC test_NN sets_NNS of_IN the_DT ratio_NN 1:9_CD ._.
In_IN this_DT experiment_NN ,_, we_PRP stop_VBP the_DT iterative_JJ procedure_NN of_IN the_DT
ate_VBD ._.
The_DT proposed_VBN algorithm_NN for_IN solving_VBG the_DT formulation_NN in_IN Eq_NN ._.
-LRB-_-LRB- 5_CD -RRB-_-RRB- finds_VBZ a_DT globally_RB optimal_JJ solution_NN and_CC achieves_VBZ the_DT optimal_JJ convergence_NN rate_NN among_IN all_DT first-order_JJ methods_NNS ._.
Note_VB that_IN recent_JJ works_NNS in_IN =_JJ -_: =[_NN 12_CD ,_, 14_CD ,_, 37_CD -RRB-_-RRB- -_: =_SYM -_: consider_VB the_DT problem_NN of_IN decomposing_VBG a_DT given_VBN matrix_NN into_IN its_PRP$ underlying_JJ sparse_JJ component_NN and_CC low-rank_JJ component_NN in_IN a_DT different_JJ setting_NN :_: they_PRP study_VBP the_DT theoretical_JJ condition_NN under_IN which_WDT such_JJ two_CD compon_NN
iance_NN are_VBP learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 21_CD -RRB-_-RRB- ;_: kernel_NN methods_NNS and_CC regularization_NN networks_NNS are_VBP extended_VBN to_TO multi-task_JJ learning_NN setting_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: a_DT convex_NN formulation_NN is_VBZ developed_VBN for_IN learning_VBG clustered_VBN tasks_NNS =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_JJ -_: ;_: a_DT shared_JJ low-rank_JJ structure_NN is_VBZ learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 3_CD ,_, 15_CD -RRB-_-RRB- ._.
Recently_RB ,_, trace_NN norm_NN regularization_NN has_VBZ been_VBN introduced_VBN into_IN the_DT multi-task_JJ learning_NN domain_NN -LRB-_-LRB- 1_CD ,_, 4_CD ,_, 20_CD ,_, 27_CD ,_, 28_CD -RRB-_-RRB- to_TO capture_VB the_DT task_NN
oped_VBN for_IN learning_VBG clustered_VBN tasks_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ;_: a_DT shared_JJ low-rank_JJ structure_NN is_VBZ learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 3_CD ,_, 15_CD -RRB-_-RRB- ._.
Recently_RB ,_, trace_NN norm_NN regularization_NN has_VBZ been_VBN introduced_VBN into_IN the_DT multi-task_JJ learning_NN domain_NN =_JJ -_: =[_NN 1_CD ,_, 4_CD ,_, 20_CD ,_, 27_CD ,_, 28_CD -RRB-_-RRB- -_: =_SYM -_: to_TO capture_VB the_DT task_NN relationship_NN via_IN a_DT shared_JJ low-rank_JJ structure_NN of_IN the_DT model_NN parameters_NNS ,_, resulting_VBG in_IN a_DT tractable_JJ convex_NN optimization_NN problem_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
In_IN many_JJ real-world_JJ applications_NNS ,_, the_DT underlying_JJ pr_NN
oped_VBN for_IN learning_VBG clustered_VBN tasks_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ;_: a_DT shared_JJ low-rank_JJ structure_NN is_VBZ learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 3_CD ,_, 15_CD -RRB-_-RRB- ._.
Recently_RB ,_, trace_NN norm_NN regularization_NN has_VBZ been_VBN introduced_VBN into_IN the_DT multi-task_JJ learning_NN domain_NN =_JJ -_: =[_NN 1_CD ,_, 4_CD ,_, 20_CD ,_, 27_CD ,_, 28_CD -RRB-_-RRB- -_: =_SYM -_: to_TO capture_VB the_DT task_NN relationship_NN via_IN a_DT shared_JJ low-rank_JJ structure_NN of_IN the_DT model_NN parameters_NNS ,_, resulting_VBG in_IN a_DT tractable_JJ convex_NN optimization_NN problem_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
In_IN many_JJ real-world_JJ applications_NNS ,_, the_DT underlying_JJ pr_NN
+_CC U_NN ⊥_NN T_NN -LRB-_-LRB- λ_FW ∗_FW Σd_NN -RRB-_-RRB- V_NN ⊥_NN T_NN corresponds_VBZ to_TO the_DT SVD_NNP of_IN ˆ_NNP SQ._NNP ._.
This_DT completes_VBZ the_DT proof_NN of_IN this_DT theorem_NN ._.
Note_VB that_IN the_DT problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 22_CD -RRB-_-RRB- is_VBZ convex_NN ,_, and_CC can_MD be_VB solved_VBN via_IN an_DT algorithm_NN similar_JJ to_TO the_DT one_NN in_IN =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBN for_IN solving_VBG the_DT Euclidean_JJ projection_NN onto_IN the_DT ℓ1_NN ball_NN ._.
5_CD ._.
ALGORITHMS_NNS AND_CC CONVERGENCE_NN We_PRP present_VBP two_CD algorithms_NNS based_VBN on_IN the_DT projected_VBN gradient_NN scheme_NN presented_VBN in_IN Section_NN 3_CD for_IN solving_VBG the_DT c_NN
gularization_NN networks_NNS are_VBP extended_VBN to_TO multi-task_JJ learning_NN setting_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ;_: a_DT convex_NN formulation_NN is_VBZ developed_VBN for_IN learning_VBG clustered_VBN tasks_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ;_: a_DT shared_JJ low-rank_JJ structure_NN is_VBZ learned_VBN from_IN multiple_JJ tasks_NNS =_JJ -_: =[_NN 3_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Recently_RB ,_, trace_NN norm_NN regularization_NN has_VBZ been_VBN introduced_VBN into_IN the_DT multi-task_JJ learning_NN domain_NN -LRB-_-LRB- 1_CD ,_, 4_CD ,_, 20_CD ,_, 27_CD ,_, 28_CD -RRB-_-RRB- to_TO capture_VB the_DT task_NN relationship_NN via_IN a_DT shared_JJ low-rank_JJ structure_NN of_IN the_DT model_NN parameter_NN
ate_VBD ._.
The_DT proposed_VBN algorithm_NN for_IN solving_VBG the_DT formulation_NN in_IN Eq_NN ._.
-LRB-_-LRB- 5_CD -RRB-_-RRB- finds_VBZ a_DT globally_RB optimal_JJ solution_NN and_CC achieves_VBZ the_DT optimal_JJ convergence_NN rate_NN among_IN all_DT first-order_JJ methods_NNS ._.
Note_VB that_IN recent_JJ works_NNS in_IN =_JJ -_: =[_NN 12_CD ,_, 14_CD ,_, 37_CD -RRB-_-RRB- -_: =_SYM -_: consider_VB the_DT problem_NN of_IN decomposing_VBG a_DT given_VBN matrix_NN into_IN its_PRP$ underlying_JJ sparse_JJ component_NN and_CC low-rank_JJ component_NN in_IN a_DT different_JJ setting_NN :_: they_PRP study_VBP the_DT theoretical_JJ condition_NN under_IN which_WDT such_JJ two_CD compon_NN
oped_VBN for_IN learning_VBG clustered_VBN tasks_NNS -LRB-_-LRB- 19_CD -RRB-_-RRB- ;_: a_DT shared_JJ low-rank_JJ structure_NN is_VBZ learned_VBN from_IN multiple_JJ tasks_NNS -LRB-_-LRB- 3_CD ,_, 15_CD -RRB-_-RRB- ._.
Recently_RB ,_, trace_NN norm_NN regularization_NN has_VBZ been_VBN introduced_VBN into_IN the_DT multi-task_JJ learning_NN domain_NN =_JJ -_: =[_NN 1_CD ,_, 4_CD ,_, 20_CD ,_, 27_CD ,_, 28_CD -RRB-_-RRB- -_: =_SYM -_: to_TO capture_VB the_DT task_NN relationship_NN via_IN a_DT shared_JJ low-rank_JJ structure_NN of_IN the_DT model_NN parameters_NNS ,_, resulting_VBG in_IN a_DT tractable_JJ convex_NN optimization_NN problem_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
In_IN many_JJ real-world_JJ applications_NNS ,_, the_DT underlying_JJ pr_NN
n_NN appropriate_JJ L_NN -LRB-_-LRB- hence_RB the_DT appropriate_JJ step_NN size_NN 1\/L_NN -RRB-_-RRB- by_IN ensuring_VBG the_DT inequality_NN in_IN Eq_NN ._.
-LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
By_IN applying_VBG an_DT appropriate_JJ step_NN size_NN and_CC the_DT associated_VBN projected_JJ gradient_NN in_IN Eq_NN ._.
-LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, we_PRP can_MD verify_VB that_IN =_JJ -_: =[_NN 7_CD ,_, 25_CD -RRB-_-RRB- -_: =_JJ -_: F_NN -LRB-_-LRB- T_NN -RRB-_-RRB- −_NN F_NN -LRB-_-LRB- TL_NN ,_, S_NN -RRB-_-RRB- ≥_FW 〈_FW T_NN −_NN S_NN ,_, PL_NN -LRB-_-LRB- S_NN -RRB-_-RRB- 〉_CD +_CC 1_CD 2L_NN ‖_NN PL_NN -LRB-_-LRB- S_NN -RRB-_-RRB- ‖_NN 2_CD F_NN ._.
-LRB-_-LRB- 15_CD -RRB-_-RRB- Moreover_RB ,_, by_IN replacing_VBG S_NN with_IN T_NN in_IN Eq_NN ._.
-LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, we_PRP have_VBP F_NN -LRB-_-LRB- T_NN -RRB-_-RRB- −_NN F_NN -LRB-_-LRB- TL_NN ,_, T_NN -RRB-_-RRB- ≥_NN 1_CD 2L_NN ‖_NN PL_NN -LRB-_-LRB- T_NN -RRB-_-RRB- ‖_CD 2F_NN ._.
-LRB-_-LRB- 16_CD -RRB-_-RRB- Note_VBP that_IN the_DT inequality_NN in_IN Eq_NN ._.
-LRB-_-LRB- 15_CD -RRB-_-RRB- charac_NN
