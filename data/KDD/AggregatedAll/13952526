High-precision_JJ phrase-based_JJ document_NN classification_NN on_IN a_DT modern_JJ scale_NN
We_PRP present_VBP a_DT document_NN classification_NN system_NN that_WDT employs_VBZ lazy_JJ learning_NN from_IN labeled_JJ phrases_NNS ,_, and_CC argue_VBP that_IN the_DT system_NN can_MD be_VB highly_RB effective_JJ whenever_WRB the_DT following_JJ property_NN holds_VBZ :_: most_JJS of_IN information_NN on_IN document_NN labels_NNS is_VBZ captured_VBN in_IN phrases_NNS ._.
We_PRP call_VBP this_DT property_NN near_IN sufficiency_NN ._.
Our_PRP$ research_NN contribution_NN is_VBZ twofold_JJ :_: -LRB-_-LRB- a_LS -RRB-_-RRB- we_PRP quantify_VBP the_DT near_JJ sufficiency_NN property_NN using_VBG the_DT Information_NNP Bottleneck_NNP principle_NN and_CC show_VBP that_IN it_PRP is_VBZ easy_JJ to_TO check_VB on_IN a_DT given_VBN dataset_NN ;_: -LRB-_-LRB- b_LS -RRB-_-RRB- we_PRP reveal_VBP that_IN in_IN all_DT practical_JJ cases_NNS --_: from_IN small-scale_JJ to_TO very_RB large-scale_JJ --_: manual_JJ labeling_NN of_IN phrases_NNS is_VBZ feasible_JJ :_: the_DT natural_JJ language_NN constrains_VBZ the_DT number_NN of_IN common_JJ phrases_NNS composed_VBN of_IN a_DT vocabulary_NN to_TO grow_VB linearly_RB with_IN the_DT size_NN of_IN the_DT vocabulary_NN ._.
Both_DT these_DT contributions_NNS provide_VBP firm_JJ foundation_NN to_TO applicability_NN of_IN the_DT phrase-based_JJ classification_NN -LRB-_-LRB- PBC_NN -RRB-_-RRB- framework_NN to_TO a_DT variety_NN of_IN large-scale_JJ tasks_NNS ._.
We_PRP deployed_VBD the_DT PBC_NNP system_NN on_IN the_DT task_NN of_IN job_NN title_NN classification_NN ,_, as_IN a_DT part_NN of_IN LinkedIn_NNP 's_POS data_NNS standardization_NN effort_NN ._.
The_DT system_NN significantly_RB outperforms_VBZ its_PRP$ predecessor_NN both_CC in_IN terms_NNS of_IN precision_NN and_CC coverage_NN ._.
It_PRP is_VBZ currently_RB being_VBG used_VBN in_IN LinkedIn_NNP 's_POS ad_NN targeting_VBG product_NN ,_, and_CC more_JJR applications_NNS are_VBP being_VBG developed_VBN ._.
We_PRP argue_VBP that_IN PBC_NN excels_NNS in_IN high_JJ explainability_NN of_IN the_DT classification_NN results_NNS ,_, as_RB well_RB as_IN in_IN low_JJ development_NN and_CC low_JJ maintenance_NN costs_NNS ._.
We_PRP benchmark_JJ PBC_NN against_IN existing_VBG high-precision_JJ document_NN classification_NN algorithms_NNS and_CC conclude_VBP that_IN it_PRP is_VBZ most_RBS useful_JJ in_IN multilabel_JJ classification_NN ._.
the_DT task_NN accurately_RB and_CC fairly_RB quickly_RB --_: mostly_RB based_VBN on_IN their_PRP$ common_JJ knowledge_NN ._.
In_IN contrast_NN ,_, to_TO create_VB a_DT common_JJ knowledge_NN base_NN in_IN an_DT automatic_JJ classification_NN system_NN is_VBZ notoriously_RB difficult_JJ -LRB-_-LRB- see_VB ,_, e.g._FW =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Second_JJ ,_, humans_NNS are_VBP much_RB better_JJR than_IN machines_NNS at_IN multilabel_JJ classification_NN ._.
Questions_NNS such_JJ as_IN ``_`` How_WRB many_JJ classes_NNS does_VBZ a_DT data_NN instance_NN belong_VB to_TO ?_. ''_''
or_CC ``_`` Should_MD this_DT data_NN instance_NN be_VB assigned_VBN to_TO any_DT clas_NNS
ply_RB can_MD not_RB stay_VB in_IN focus_NN for_IN long_JJ ,_, and_CC therefore_RB their_PRP$ output_NN is_VBZ often_RB noisy_JJ and_CC inconsistent_JJ ._.
This_DT causes_VBZ some_DT frustration_NN among_IN researchers_NNS who_WP apply_VBP crowdsourcing_VBG to_TO text_NN classification_NN -LRB-_-LRB- see_VB ,_, e.g._FW =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Nevertheless_RB ,_, manual_JJ labeling_NN of_IN hundreds_NNS of_IN thousands_NNS data_NNS instances_NNS -LRB-_-LRB- which_WDT was_VBD impractical_JJ just_RB a_DT few_JJ year_NN ago_RB -RRB-_-RRB- is_VBZ now_RB perfectly_RB practical_JJ ._.
We_PRP introduce_VBP Phrase-Based_NNP Multilabel_NNP Classification_NN as_IN
hrase_NN -LRB-_-LRB- i.e._FW an_DT ngram_NN of_IN words_NNS -RRB-_-RRB- ._.
This_DT setup_NN is_VBZ in_IN the_DT focus_NN of_IN our_PRP$ paper_NN --_: we_PRP term_VBP it_PRP Phrase-Based_JJ Classification_NN -LRB-_-LRB- PBC_NN -RRB-_-RRB- ._.
Variations_NNS of_IN PBC_NNP have_VBP been_VBN studied_VBN for_IN years_NNS in_IN Information_NNP Retrieval_NNP -LRB-_-LRB- see_VB ,_, e.g._FW =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
PBC_NNP is_VBZ very_RB natural_JJ for_IN multilabel_JJ text_NN classification_NN -LRB-_-LRB- when_WRB each_DT document_NN may_MD belong_VB to_TO a_DT number_NN of_IN classes_NNS -RRB-_-RRB- ,_, as_IN a_DT number_NN of_IN decision_NN stumps_NNS may_MD be_VB triggered_VBN for_IN a_DT document_NN ._.
Another_DT advantage_NN of_IN P_NN
mic_JJ data_NNS mining_NN applications_NNS ._.
Text_NN classification_NN has_VBZ been_VBN explored_VBN for_IN two_CD decades_NNS at_IN least_JJS -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, and_CC machine_NN learning_NN techniques_NNS have_VBP proved_VBN successful_JJ for_IN a_DT variety_NN of_IN text_NN classification_NN scenarios_NNS =_JJ -_: =[_NN 10_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
There_EX are_VBP four_CD possible_JJ text_NN classification_NN setups_NNS :_: •_FW Eager_FW learning_NN from_IN labeled_JJ documents_NNS --_: the_DT most_RBS common_JJ setup_NN ,_, in_IN which_WDT a_DT generic_JJ machine_NN learning_NN classifier_NN is_VBZ trained_VBN on_IN a_DT set_NN of_IN labeled_JJ docum_NN
._.
We_PRP use_VBP the_DT SVM_NNP classifier_NN for_IN this_DT comparison_NN ._.
Note_VB that_DT SVM_NN is_VBZ not_RB a_DT strawman_NN --_: it_PRP is_VBZ one_CD of_IN the_DT best_JJS classification_NN models_NNS available_JJ ,_, particularly_RB well_RB suited_VBN for_IN text_NN -LRB-_-LRB- see_VB ,_, e.g._FW -LRB-_-LRB- 14_CD -RRB-_-RRB- -RRB-_-RRB- ._.
We_PRP followed_VBD =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: to_TO set_VB up_RP a_DT multilabel_JJ SVM_NN framework_NN ._.
We_PRP used_VBD SVMlight_NN 15_CD with_IN parameters_NNS c_NN =_JJ 0.1_CD and_CC j_NN =_JJ 2_CD which_WDT were_VBD chosen_VBN based_VBN on_IN our_PRP$ prior_JJ knowledge_NN ._.
We_PRP train_VBP an_DT SVM_NN on_IN a_DT portion_NN of_IN the_DT job_NN title_NN data_NNS ,_, then_RB te_IN
DUCTION_NNP Automatic_NNP classification_NN of_IN text_NN documents_NNS plays_VBZ a_DT preeminent_JJ role_NN in_IN numerous_JJ industrial_JJ and_CC academic_JJ data_NNS mining_NN applications_NNS ._.
Text_NN classification_NN has_VBZ been_VBN explored_VBN for_IN two_CD decades_NNS at_IN least_JJS =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC machine_NN learning_NN techniques_NNS have_VBP proved_VBN successful_JJ for_IN a_DT variety_NN of_IN text_NN classification_NN scenarios_NNS -LRB-_-LRB- 10_CD ,_, 14_CD -RRB-_-RRB- ._.
There_EX are_VBP four_CD possible_JJ text_NN classification_NN setups_NNS :_: •_FW Eager_FW learning_NN from_IN labeled_JJ docu_NN
hort_NN documents_NNS have_VBP little_JJ content_NN beyond_IN a_DT few_JJ phrases_NNS ._.
Still_RB ,_, longer_JJR texts_NNS can_MD hold_VB this_DT property_NN as_RB well_RB ._.
1_CD In_IN Section_NN 2_CD we_PRP formalize_VBP this_DT property_NN in_IN terms_NNS of_IN the_DT Information_NNP Bottleneck_NNP principle_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
1_CD A_NN -LRB-_-LRB- rather_RB extreme_JJ -RRB-_-RRB- example_NN would_MD be_VB to_TO categorize_VB pieces_NNS of_IN code_NN according_VBG to_TO the_DT programming_NN language_NN this_DT code_NN was_VBD written_VBN in_IN ._.
Regardless_RB of_IN the_DT code_NN 's_POS length_NN ,_, just_RB a_DT few_JJ keywords_NNS may_MD be_VB enough_RB
ssification_NN ._.
Our_PRP$ offline_JJ evaluation_NN shows_VBZ that_IN we_PRP achieve_VBP about_IN 95_CD %_NN classification_NN precision_NN ._.
In_IN Section_NN 6_CD ,_, we_PRP compare_VBP our_PRP$ phrase-based_JJ classification_NN with_IN Support_NN Vector_NNP Machine_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- classification_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC show_VBP that_IN our_PRP$ method_NN significantly_RB outperforms_VBZ four_CD SVM_NNP versions_NNS on_IN the_DT multilabel_JJ job_NN title_NN classification_NN task_NN ._.
Our_PRP$ system_NN has_VBZ been_VBN successfully_RB deployed_VBN in_IN LinkedIn_NNP 's_POS ad_NN targeting_VBG product_NN ._.
3_CD
off_RB between_IN sufficiency_NN for_IN Y_NN -LRB-_-LRB- attributed_VBN by_IN a_DT high_JJ value_NN of_IN the_DT mutual_JJ information_NN I_NN -LRB-_-LRB- T_NN ;_: Y_NN -RRB-_-RRB- -RRB-_-RRB- and_CC simplicity_NN of_IN representation_NN -LRB-_-LRB- attributed_VBN by_IN a_DT low_JJ value_NN of_IN I_NN -LRB-_-LRB- X_NN ;_: T_NN -RRB-_-RRB- -RRB-_-RRB- ._.
The_DT Data-Processing_NN Inequality_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: states_VBZ that_IN I_NN -LRB-_-LRB- X_NN ;_: Y_NN -RRB-_-RRB- ≥_NN I_CD -LRB-_-LRB- T_NN ;_: Y_NN -RRB-_-RRB- so_IN that_IN the_DT compressed_VBN variable_JJ T_NN can_MD not_RB hold_VB more_JJR information_NN on_IN Y_NN than_IN the_DT original_JJ variable_JJ X._NNP As_IN shown_VBN in_IN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, whenever_WRB the_DT I_NN -LRB-_-LRB- X_NN ;_: Y_NN -RRB-_-RRB- =_JJ I_NN -LRB-_-LRB- T_NN ;_: Y_NN -RRB-_-RRB- equality_NN holds_VBZ ,_,
on_RB ,_, Paris_NNP are_VBP not_RB the_DT same_JJ ._.
However_RB ,_, those_DT cases_NNS are_VBP rare_JJ from_IN the_DT statistical_JJ point_NN of_IN view_NN ,_, and_CC therefore_RB for_IN all_DT practical_JJ purposes_NNS words_NNS '_POS ordering_VBG can_MD be_VB ignored_VBN in_IN text_NN classification_NN -LRB-_-LRB- see_VB ,_, e.g._FW =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Figure_NN 4_CD :_: Phrase_NN Classification_NN ._.
A_DT single_JJ human_JJ icon_NN means_VBZ involvement_NN of_IN the_DT content_NN manager_NN ;_: multiple_JJ human_JJ icons_NNS mean_VBP crowdsourcing_VBG ._.
techniques_NNS -LRB-_-LRB- such_JJ as_IN shallow_JJ parsing_NN -RRB-_-RRB- ,_, to_TO identify_VB phrases_NNS tha_NN
mic_JJ data_NNS mining_NN applications_NNS ._.
Text_NN classification_NN has_VBZ been_VBN explored_VBN for_IN two_CD decades_NNS at_IN least_JJS -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, and_CC machine_NN learning_NN techniques_NNS have_VBP proved_VBN successful_JJ for_IN a_DT variety_NN of_IN text_NN classification_NN scenarios_NNS =_JJ -_: =[_NN 10_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
There_EX are_VBP four_CD possible_JJ text_NN classification_NN setups_NNS :_: •_FW Eager_FW learning_NN from_IN labeled_JJ documents_NNS --_: the_DT most_RBS common_JJ setup_NN ,_, in_IN which_WDT a_DT generic_JJ machine_NN learning_NN classifier_NN is_VBZ trained_VBN on_IN a_DT set_NN of_IN labeled_JJ docum_NN
his_PRP$ setup_NN is_VBZ fairly_RB rare_JJ in_IN the_DT text_NN domain_NN as_IN it_PRP is_VBZ highly_RB inefficient_JJ in_IN many_JJ practical_JJ cases_NNS ._.
•_FW Eager_FW learning_NN from_IN labeled_JJ features_NNS --_: usually_RB coupled_VBN with_IN learning_VBG from_IN labeled_VBN documents_NNS -LRB-_-LRB- see_VB ,_, e.g._FW =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
•_CD Lazy_JJ learning_NN from_IN labeled_JJ features_NNS --_: an_DT ensemble_NN of_IN decision-stump-like_JJ classifiers_NNS is_VBZ constructed_VBN ,_, each_DT of_IN which_WDT is_VBZ triggered_VBN if_IN a_DT document_NN contains_VBZ a_DT certain_JJ feature_NN ,_, such_JJ as_IN a_DT word_NN or_CC a_DT phrase_NN
._.
e._VB the_DT extend_VBP to_TO which_WDT we_PRP can_MD replace_VB any_DT document_NN by_IN a_DT set_NN of_IN extracted_VBN phrases_NNS without_IN significantly_RB increasing_VBG misclassification_NN rate_NN -RRB-_-RRB- ,_, we_PRP appeal_VBP to_TO the_DT Information_NNP Bottleneck_NNP principle_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
See_VB =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: for_IN a_DT recent_JJ survey_NN of_IN this_DT approach_NN ._.
Information_NNP Bottleneck_NNP considers_VBZ an_DT input_NN variable_NN X_NN and_CC an_DT output_NN variable_JJ Y_NN ._.
-LRB-_-LRB- In_IN our_PRP$ case_NN ,_, the_DT document_NN variable_NN is_VBZ input_NN and_CC the_DT class_NN variable_NN is_VBZ output_NN -RRB-_-RRB- ._.
T_NN
