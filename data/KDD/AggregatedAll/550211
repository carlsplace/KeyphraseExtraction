Experimental_JJ comparisons_NNS of_IN online_NN and_CC batch_NN versions_NNS of_IN bagging_VBG and_CC boosting_VBG
Bagging_VBG and_CC boosting_VBG are_VBP well-known_JJ ensemble_NN learning_NN methods_NNS ._.
They_PRP combine_VBP multiple_JJ learned_VBN base_NN models_NNS with_IN the_DT aim_NN of_IN improving_VBG generalization_NN performance_NN ._.
To_TO date_NN ,_, they_PRP have_VBP been_VBN used_VBN primarily_RB in_IN batch_NN mode_NN ,_, i.e._FW ,_, they_PRP require_VBP multiple_JJ passes_NNS through_IN the_DT training_NN data_NNS ._.
In_IN previous_JJ work_NN ,_, we_PRP presented_VBD online_NN bagging_VBG and_CC boosting_VBG algorithms_NNS that_WDT only_RB require_VBP one_CD pass_NN through_IN the_DT training_NN data_NNS and_CC presented_VBN experimental_JJ results_NNS on_IN some_DT relatively_RB small_JJ datasets_NNS ._.
Through_IN additional_JJ experiments_NNS on_IN a_DT variety_NN of_IN larger_JJR synthetic_JJ and_CC real_JJ datasets_NNS ,_, this_DT paper_NN demonstrates_VBZ that_IN our_PRP$ online_JJ versions_NNS perform_VBP comparably_RB to_TO their_PRP$ batch_NN counterparts_NNS in_IN terms_NNS of_IN classification_NN accuracy_NN ._.
We_PRP also_RB demonstrate_VBP the_DT substantial_JJ reduction_NN in_IN running_VBG time_NN we_PRP obtain_VBP with_IN our_PRP$ online_JJ algorithms_NNS because_IN they_PRP require_VBP fewer_JJR passes_NNS through_IN the_DT training_NN data_NNS ._.
the_DT weight_NN distribution_NN since_IN the_DT difficulty_NN of_IN a_DT sample_NN is_VBZ not_RB known_VBN a_DT priori_FW ._.
Thus_RB ,_, the_DT basic_JJ idea_NN is_VBZ to_TO estimate_VB the_DT importance_NN λ_NN of_IN a_DT sample_NN by_IN propagating_VBG it_PRP through_IN the_DT set_NN of_IN weak_JJ classifiers_NNS =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN fact_NN ,_, λ_NN is_VBZ increased_VBN proportional_JJ to_TO the_DT error_NN e_SYM of_IN the_DT weak_JJ classifier_NN if_IN the_DT sample_NN is_VBZ misclassified_VBN and_CC decreased_VBN otherwise_RB ._.
Thus_RB ,_, the_DT work-flow_NN for_IN on-line_JJ boosting_VBG for_IN feature_NN selections_NNS sel_VBP
e_LS learning_NN algorithms_NNS process_VBP each_DT training_NN instance_NN once_RB ``_`` on_IN arrival_NN ''_'' without_IN the_DT need_NN for_IN storage_NN and_CC reprocessing_NN ,_, and_CC maintain_VB a_DT current_JJ hypothesis_NN that_WDT reflects_VBZ all_PDT the_DT training_NN instances_NNS so_RB far_RB =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT way_NN ,_, the_DT learning_NN algorithms_NNS take_VBP as_IN input_NN a_DT single_JJ labeled_JJ training_NN instance_NN as_RB well_RB as_IN a_DT hypothesis_NN and_CC output_NN an_DT updated_VBN hypothesis_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
Recently_RB ,_, ensembles_NNS of_IN classifiers_NNS have_VBP been_VBN succ_JJ
d_NN from_IN US_NNP Forest_NNP Service_NNP -LRB-_-LRB- USFS_NNP -RRB-_-RRB- Region_NN 2_CD Resource_NNP Information_NNP System_NNP -LRB-_-LRB- RIS_NNP -RRB-_-RRB- data_NNS ._.
It_PRP contains_VBZ 581_CD ,_, 012_CD instances_NNS and_CC 54_CD attributes_NNS ,_, and_CC it_PRP has_VBZ been_VBN used_VBN in_IN several_JJ papers_NNS on_IN data_NNS stream_NN classification_NN =_JJ -_: =[_NN 10_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Poker-Hand_JJ Consists_NNS of_IN 1_CD ,_, 000_CD ,_, 000_CD instances_NNS and_CC 11_CD attributes_NNS ._.
Each_DT record_NN of_IN the_DT Poker-Hand_NNP dataset_NN is_VBZ an_DT example_NN of_IN a_DT hand_NN consisting_VBG of_IN five_CD playing_NN cards_NNS drawn_VBN from_IN a_DT standard_JJ deck_NN of_IN 52_CD ._.
Each_DT
d_NN from_IN US_NNP Forest_NNP Service_NNP -LRB-_-LRB- USFS_NNP -RRB-_-RRB- Region_NN 2_CD Resource_NNP Information_NNP System_NNP -LRB-_-LRB- RIS_NNP -RRB-_-RRB- data_NNS ._.
It_PRP contains_VBZ 581_CD ,_, 012_CD instances_NNS and_CC 54_CD attributes_NNS ,_, and_CC it_PRP has_VBZ been_VBN used_VBN in_IN several_JJ papers_NNS on_IN data_NNS stream_NN classification_NN =_JJ -_: =[_NN 13_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Poker-Hand_JJ Consists_NNS of_IN 1_CD ,_, 000_CD ,_, 000_CD instances_NNS and_CC 11_CD attributes_NNS ._.
Each_DT record_NN of_IN the_DT Poker-Hand_NNP dataset_NN is_VBZ an_DT example_NN of_IN a_DT hand_NN consisting_VBG of_IN five_CD playing_NN cards_NNS drawn_VBN from_IN a_DT standard_JJ deck_NN of_IN 52_CD ._.
Each_DT
es_NNS where_WRB P_NN -LRB-_-LRB- K_NN =_JJ k_NN -RRB-_-RRB- follows_VBZ a_DT binomial_JJ distribution_NN ._.
This_DT binomial_JJ distribution_NN for_IN large_JJ values_NNS of_IN N_NN tends_VBZ to_TO a_DT Poisson_NNP -LRB-_-LRB- 1_LS -RRB-_-RRB- distribution_NN ,_, where_WRB Poisson_NNP -LRB-_-LRB- 1_LS -RRB-_-RRB- =_JJ exp_NN -LRB-_-LRB- −_NN 1_CD -RRB-_-RRB- \/_: k_NN !_. ._.
Using_VBG this_DT fact_NN ,_, Oza_NNP and_CC Russell_NNP =_SYM -_: =[_NN 25_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBN Online_NNP Bagging_NNP ,_, an_DT online_NN method_NN that_WDT instead_RB of_IN sampling_NN with_IN replacement_NN ,_, gives_VBZ each_DT example_NN a_DT weight_NN according_VBG to_TO Poisson_NNP -LRB-_-LRB- 1_LS -RRB-_-RRB- ._.
Boosting_VBG algorithms_NNS combine_VBP multiple_JJ base_NN models_NNS to_TO obtain_VB a_DT
d_NN from_IN US_NNP Forest_NNP Service_NNP -LRB-_-LRB- USFS_NNP -RRB-_-RRB- Region_NN 2_CD Resource_NNP Information_NNP System_NNP -LRB-_-LRB- RIS_NNP -RRB-_-RRB- data_NNS ._.
It_PRP contains_VBZ 581_CD ,_, 012_CD instances_NNS and_CC 54_CD attributes_NNS ,_, and_CC it_PRP has_VBZ been_VBN used_VBN in_IN several_JJ papers_NNS on_IN data_NNS stream_NN classification_NN =_JJ -_: =[_NN 11_CD ,_, 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Poker-Hand_JJ Consists_NNS of_IN 1_CD ,_, 000_CD ,_, 000_CD instances_NNS and_CC 11_CD attributes_NNS ._.
Each_DT record_NN of_IN the_DT Poker-Hand_NNP dataset_NN is_VBZ an_DT example_NN of_IN a_DT hand_NN consisting_VBG of_IN five_CD playing_NN cards_NNS drawn_VBN from_IN a_DT standard_JJ deck_NN of_IN 52_CD ._.
Each_DT
ing_NN can_MD be_VB found_VBN in_IN the_DT literature_NN ._.
In_IN this_DT work_NN ,_, we_PRP adopt_VBP the_DT definition_NN that_IN online_JJ learning_NN algorithms_NNS process_VBP each_DT training_NN example_NN once_RB ``_`` on_IN arrival_NN ''_'' ,_, without_IN the_DT need_NN for_IN storage_NN or_CC reprocessing_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT way_NN ,_, they_PRP take_VBP as_IN input_NN a_DT single_JJ training_NN example_NN as_RB well_RB as_IN a_DT hypothesis_NN and_CC output_NN an_DT updated_VBN hypothesis_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
We_PRP consider_VBP online_JJ learning_NN as_IN a_DT particular_JJ case_NN of_IN incremental_JJ learning_NN ._.
The_DT
of_IN 200_CD lesions_NNS with_IN even_JJ class_NN distribution_NN is_VBZ used_VBN for_IN the_DT primed_JJ training_NN ._.
Primed_VBN off-line_JJ training_NN is_VBZ a_DT simple_JJ but_CC effective_JJ technique_NN to_TO improve_VB the_DT predictive_JJ performance_NN of_IN the_DT final_JJ model_NN -LRB-_-LRB- see_VB =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: for_IN an_DT example_NN -RRB-_-RRB- ._.
We_PRP exploit_VBP the_DT memory_NN management_NN scheme_NN proposed_VBN before_RB for_IN online_JJ Hoeffding_NNP trees_NNS that_WDT dynamically_RB activates_VBZ most_RBS promising_JJ nodes_NNS ,_, for_IN tracking_VBG feature_NN distributions_NNS and_CC a_DT split_JJ att_NN
ts_NNS receive_VBP equal_JJ weight_NN and_CC a_DT simple_JJ majority_NN vote_NN is_VBZ performed_VBN to_TO make_VB a_DT prediction_NN -RRB-_-RRB- ,_, the_DT algorithm_NN has_VBZ limited_VBN potential_JJ to_TO boost_VB the_DT performance_NN of_IN the_DT underlying_VBG weak_JJ classifiers_NNS ._.
Oza_NNP and_CC Russell_NNP =_SYM -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: propose_VBP incremental_JJ versions_NNS of_IN bagging_VBG and_CC boosting_VBG that_WDT differ_VBP from_IN our_PRP$ work_NN because_IN they_PRP require_VBP the_DT underlying_VBG weak_JJ learner_NN to_TO be_VB incremental_JJ ._.
The_DT method_NN is_VBZ of_IN limited_JJ use_NN for_IN large_JJ datasets_NNS if_IN t_NN
ier_RB updating_VBG is_VBZ still_RB treated_VBN in_IN an_DT offline_JJ learning_NN mode_NN ._.
Online_JJ applications_NNS deal_VBP with_IN sequential_JJ data_NNS ,_, thus_RB require_VBP the_DT capability_NN of_IN weak_JJ classifiers_NNS updated_VBN in_IN an_DT online_JJ fashion_NN ._.
Oza_NNP and_CC Russell_NNP =_SYM -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: make_VB the_DT primary_JJ efforts_NNS on_IN studying_VBG the_DT sequential_JJ learning_NN of_IN boosted_VBN classifiers_NNS ,_, called_VBN online_NN boosting_VBG ._.
They_PRP prove_VBP that_IN with_IN the_DT same_JJ training_NN set_NN ,_, online_NN boosting_VBG converges_VBZ statistically_RB to_TO off_RP
nally_RB has_VBZ required_VBN access_NN to_TO the_DT entire_JJ dataset_NN at_IN once_RB ,_, i.e._FW ,_, it_PRP performs_VBZ batch_NN learning_NN ._.
However_RB ,_, this_DT is_VBZ clearly_RB impractical_JJ for_IN very_RB large_JJ datasets_NNS that_WDT can_MD not_RB be_VB loaded_VBN into_IN memory_NN all_DT at_IN once_RB ._.
-LRB-_-LRB- =_JJ -_: =_JJ Oza_NNP and_CC Russell_NNP ,_, 2001_CD -_: =_JJ -_: ;_: Oza_NNP ,_, 2001_CD -RRB-_-RRB- apply_VBP ensemble_NN learning_VBG to_TO such_JJ large_JJ datasets_NNS ._.
In_IN particular_JJ ,_, this_DT work_NN develops_VBZ online_NN bagging_VBG and_CC boosting_VBG ,_, Le._NNP ,_, they_PRP learn_VBP in_IN an_DT online_JJ manner_NN ._.
That_DT is_VBZ ,_, whereas_IN standard_JJ bagging_NN and_CC b_NN
feature_NN selection_NN as_IN described_VBN above_IN works_NNS offline_JJ ._.
Thus_RB ,_, all_DT training_NN samples_NNS must_MD be_VB given_VBN in_IN advance_NN ._.
In_IN our_PRP$ work_NN we_PRP use_VBP on-line_JJ feature_NN selection_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- based_VBN on_IN an_DT online_JJ version_NN of_IN AdaBoost_NN -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Therefore_RB ,_, each_DT boosting_VBG step_NN of_IN the_DT off-line_JJ algorithm_NN has_VBZ to_TO be_VB done_VBN on-line_JJ ._.
The_DT mechanism_NN performs_VBZ an_DT update_VB of_IN weak_JJ classifiers_NNS whenever_WRB a_DT new_JJ training_NN sample_NN arrives_VBZ ,_, which_WDT allows_VBZ to_TO adaptivel_VB
uristics_NNS and_CC approximations_NNS exist_VBP that_WDT alleviate_VBP this_DT problem_NN ._.
Examples_NNS of_IN typically_RB batch_NN algorithms_NNS with_IN online_JJ versions_NNS are_VBP decision_NN trees_NNS -LRB-_-LRB- 11_CD ,_, 20_CD ,_, 39_CD -RRB-_-RRB- ,_, SVMs_NNS -LRB-_-LRB- 10_CD ,_, 14_CD ,_, 30_CD -RRB-_-RRB- ,_, and_CC bagging_VBG and_CC boosting_VBG =_JJ -_: =[_NN 31_CD ,_, 32_CD -RRB-_-RRB- -_: =_SYM -_: ._.
An_DT alternative_JJ approach_NN is_VBZ what_WP we_PRP refer_VBP to_TO as_IN the_DT wrapper_NN approach_NN ._.
In_IN the_DT wrapper_NN approach_NN ,_, the_DT batch_NN machine_NN learning_NN algorithms_NNS are_VBP not_RB modified_VBN but_CC are_VBP used_VBN within_IN a_DT meta-learning_JJ layer_NN that_IN ap_NN
unks_NNS that_WDT maintains_VBZ a_DT fixed-size_JJ committee_NN ._.
In_IN each_DT iteration_NN it_PRP attempts_VBZ to_TO identify_VB a_DT committee_NN member_NN that_WDT should_MD be_VB replaced_VBN by_IN the_DT model_NN built_VBN from_IN the_DT most_RBS recent_JJ chunk_NN of_IN data_NNS ._.
Oza_NNP and_CC Russell_NNP =_SYM -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: propose_VBP incremental_JJ versions_NNS of_IN bagging_VBG and_CC boosting_VBG that_WDT require_VBP the_DT underlying_VBG weak_JJ learner_NN to_TO be_VB incremental_JJ ._.
The_DT method_NN is_VBZ of_IN limited_JJ use_NN for_IN large_JJ datasets_NNS if_IN the_DT underlying_VBG incremental_JJ learning_NN
ithms_NNS that_WDT have_VBP been_VBN shown_VBN to_TO improve_VB generalization_NN performance_NN compared_VBN to_TO the_DT individual_JJ base_NN models_NNS ._.
Theoretical_JJ analysis_NN of_IN boosting_VBG 's_POS performance_NN supports_VBZ these_DT results_NNS -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
In_IN previous_JJ work_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- =_SYM -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP developed_VBD online_JJ versions_NNS of_IN bagging_VBG and_CC boosting_VBG ._.
Online_JJ learning_NN algorithms_NNS process_VBP each_DT training_NN example_NN once_RB ``_`` on_IN arrival_NN ''_'' without_IN the_DT need_NN for_IN storage_NN and_CC reprocessing_NN ,_, and_CC maintain_VB a_DT current_NN
f_LS the_DT original_JJ AdaBoost_NNP algorithm_NN have_VBP become_VBN popular_JJ in_IN recent_JJ literature_NN ._.
This_DT even_RB includes_VBZ the_DT Boosting_VBG Feature_NN Selection_NN -LRB-_-LRB- BFS_NN -RRB-_-RRB- algorithm_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- ,_, AdaBoost_NNP ._.
R_NN -LRB-_-LRB- 42_CD -RRB-_-RRB- ,_, AveBoost_NN -LRB-_-LRB- 43_CD -RRB-_-RRB- ,_, and_CC Online_NNP Boosting_NNP =_SYM -_: =[_NN 44_CD -RRB-_-RRB- -_: =_SYM -_: ._.
3.2.1_FW AdaBoost_FW ._.
Ml_NNP Explained_NNP AdaBoost_NNP ._.
M1_NN was_VBD developed_VBN to_TO boost_VB the_DT performance_NN of_IN weak_JJ learner_NN classifiers_NNS by_IN generating_VBG various_JJ weak_JJ classification_NN hypotheses_NNS ,_, and_CC combining_VBG them_PRP through_IN weighted_JJ m_NN
emental_JJ support_NN vector_NN machine_NN algorithm_NN for_IN continuous_JJ learning_NN ._.
There_EX has_VBZ been_VBN work_NN related_VBN to_TO boosting_VBG ensembles_NNS on_IN data_NNS streams_NNS ._.
Fern_NNP et_FW al._FW -LRB-_-LRB- 6_CD -RRB-_-RRB- proposed_VBD online_JJ boosting_VBG ensembles_NNS ,_, and_CC Oza_NNP et_FW al._FW =_SYM -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: studied_VBN both_CC online_JJ bagging_NN and_CC online_NN boosting_VBG ._.
Frank_NNP et_FW al._FW -LRB-_-LRB- 7_CD -RRB-_-RRB- used_VBD a_DT boosting_VBG scheme_NN similar_JJ to_TO our_PRP$ boosting_VBG scheme_NN ._.
But_CC none_NN of_IN these_DT work_NN took_VBD concept_NN drift_NN into_IN consideration_NN ._.
Previous_JJ ensembl_NN
own_JJ to_TO be_VB very_RB effective_JJ in_IN improving_VBG generalization_NN performance_NN compared_VBN to_TO the_DT individual_JJ base_NN models_NNS ._.
Theoretical_JJ analysis_NN of_IN boosting_VBG 's_POS performance_NN supports_VBZ these_DT results_NNS -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
In_IN previous_JJ work_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- =_SYM -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: ,_, we_PRP developed_VBD online_JJ versions_NNS of_IN bagging_VBG and_CC boosting_VBG ._.
Online_JJ learning_NN algorithms_NNS process_VBP each_DT training_NN example_NN once_RB ``_`` on_IN arrival_NN ''_'' without_IN the_DT need_NN for_IN storage_NN and_CC reprocessing_NN ,_, and_CC maintain_VB a_DT curre_NN
nally_RB has_VBZ required_VBN access_NN to_TO the_DT entire_JJ dataset_NN at_IN once_RB ,_, i.e._FW ,_, it_PRP performs_VBZ batchslearning_NN ._.
However_RB ,_, this_DT is_VBZ clearly_RB impractical_JJ for_IN very_RB large_JJ datasets_NNS that_WDT can_MD not_RB be_VB loaded_VBN into_IN memory_NN all_DT at_IN once_RB ._.
-LRB-_-LRB- =_JJ -_: =_JJ Oza_NNP and_CC Russell_NNP ,_, 2001_CD -_: =_JJ -_: ;_: Oza_NNP ,_, 2001_CD -RRB-_-RRB- apply_VBP ensemble_NN learning_VBG to_TO such_JJ large_JJ datasets_NNS ._.
In_IN particular_JJ ,_, this_DT work_NN develops_VBZ online_NN bagging_VBG and_CC boosting_VBG ,_, i.e._FW ,_, they_PRP learn_VBP in_IN an_DT online_JJ manner_NN ._.
That_DT is_VBZ ,_, whereas_IN standard_JJ bagging_NN and_CC
mply_RB generates_VBZ a_DT set_NN of_IN Bootstrap_NN samples_NNS as_IN in_IN Bagging_NNP ,_, generates_VBZ a_DT boosted_VBN classifier_NN for_IN each_DT sample_NN ,_, and_CC combines_VBZ the_DT results_NNS uniformly_RB ._.
An_DT online_JJ version_NN of_IN a_DT Boosting_VBG algorithm_NN was_VBD presented_VBN in_IN =_JJ -_: =[_NN 141_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT was_VBD shown_VBN to_TO be_VB comparable_JJ in_IN accuracy_NN to_TO Boosting_NNP ,_, while_IN much_RB faster_RBR in_IN terms_NNS of_IN running_VBG time_NN ._.
Many_JJ more_JJR extensions_NNS are_VBP listed_VBN at_IN the_DT beginning_NN of_IN the_DT present_JJ section_NN ._.
8_CD Evaluation_NN and_CC Applic_JJ
,_, because_IN we_PRP do_VBP not_RB know_VB a_DT priori_FW the_DT difficulty_NN of_IN a_DT sample_NN -LRB-_-LRB- i.e._FW ,_, we_PRP do_VBP not_RB know_VB if_IN we_PRP have_VBP seen_VBN the_DT sample_NN before_IN -RRB-_-RRB- ._.
We_PRP use_VBP ideas_NNS proposed_VBN by_IN Oza_NNP et_FW al._FW -LRB-_-LRB- 21_CD -RRB-_-RRB- and_CC the_DT experimental_JJ comparisons_NNS he_PRP did_VBD =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- also_RB some_DT other_JJ approaches_NNS exist_VBP ,_, e.g._FW for_IN the_DT Arc-x4_NN algorithm_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- -RRB-_-RRB- ._.
The_DT basic_JJ idea_NN is_VBZ that_IN the_DT importance_NN -LRB-_-LRB- difficulty_NN -RRB-_-RRB- of_IN a_DT sample_NN can_MD be_VB estimated_VBN by_IN propagating_VBG it_PRP through_IN the_DT set_NN of_IN weak_JJ classi_NNS
he_PRP errors_NNS of_IN the_DT weak_JJ classifiers_NNS are_VBP known_VBN ._.
The_DT crucial_JJ step_NN is_VBZ the_DT computation_NN of_IN the_DT weight_NN distribution_NN since_IN the_DT difficulty_NN of_IN a_DT sample_NN is_VBZ not_RB known_VBN a_DT priori_FW ._.
To_TO overcome_VB this_DT problem_NN Oza_NNP et_FW al._FW =_SYM -_: =[_NN 12_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBN to_TO compute_VB the_DT importance_NN λ_NN of_IN a_DT sample_NN by_IN propagating_VBG it_PRP through_IN the_DT set_NN of_IN weak_JJ classifiers_NNS ._.
In_IN fact_NN ,_, λ_NN is_VBZ increased_VBN proportional_JJ to_TO the_DT error_NN e_SYM of_IN the_DT weak_JJ classifier_NN if_IN the_DT sample_NN is_VBZ mis_FW
ine_NN learning_NN algorithms_NNS process_VBP each_DT training_NN example_NN once_RB ``_`` on_IN arrival_NN ''_'' ,_, without_IN the_DT need_NN for_IN storage_NN or_CC reprocessing_NN ,_, and_CC maintain_VB a_DT current_JJ hypothesis_NN that_WDT reflects_VBZ all_PDT the_DT training_NN examples_NNS so_RB far_RB =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT way_NN ,_, the_DT learning_NN algorithms_NNS take_VBP as_IN input_NN a_DT single_JJ training_NN example_NN as_RB well_RB as_IN a_DT hypothesis_NN and_CC output_NN an_DT updated_VBN hypothesis_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
We_PRP consider_VBP on-line_JJ learning_NN as_IN a_DT particular_JJ case_NN of_IN increm_NN
d_NN access_NN to_TO the_DT entire_JJ dataset_NN at_IN once_RB ,_, i.e._FW ,_, it_PRP performs_VBZ batch_NN learning_NN ._.
However_RB ,_, this_DT is_VBZ clearly_RB impractical_JJ for_IN very_RB large_JJ datasets_NNS that_WDT can_MD not_RB be_VB loaded_VBN into_IN memory_NN all_DT at_IN once_RB ._.
Some_DT recent_JJ work_NN -LRB-_-LRB- =_JJ -_: =_JJ Oza_NNP and_CC Russell_NNP ,_, 2001_CD -_: =_JJ -_: ;_: Oza_NNP ,_, 2001_CD -RRB-_-RRB- applies_VBZ ensemble_NN learning_VBG to_TO such_JJ large_JJ datasets_NNS ._.
In_IN particular_JJ ,_, this_DT work_NN develops_VBZ online_JJ versions_NNS of_IN bagging_VBG and_CC boosting_VBG ._.
That_DT is_VBZ ,_, whereas_IN standard_JJ bagging_NN and_CC boosting_VBG require_VBP at_IN leas_NNS
zes_NNS of_IN the_DT training_NN and_CC test_NN sets_NNS in_IN oursve-fold_JJ crossvalidation_NN runs_NNS ._.
Data_NNP Set_NNP Training_NNP Test_NNP Inputs_NNP Classes_NNS Set_FW Set_FW Promoters_NNS 84_CD 22_CD 57_CD 2_CD Balance_NN 500_CD 125_CD 4_CD 3_CD Soybean-Large_NN 307_CD 376_CD 35_CD 19_CD Breast_NN Cancer_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: 559_CD 140_CD 9_CD 2_CD German_NNP Credit_NNP 800_CD 200_CD 20_CD 2_CD Car_NN Evaluation_NN 1382_CD 346_CD 6_CD 4_CD Chess_NNP 2556_CD 640_CD 36_CD 2_CD Mushroom_NN 6499_CD 1625_CD 22_CD 2_CD Nursery_NN 10368_CD 2592_CD 8_CD 5_CD Connect4_NN 54045_CD 13512_CD 42_CD 3_CD Synthetic-1_NN 80000_CD 20000_CD 20_CD 2_CD Synthetic_JJ -_:
twork_VB and_CC use_VB it_PRP to_TO classify_VB examples_NNS ._.
1_CD Ensemble_NN learning_NN algorithms_NNS combine_VBP the_DT predictions_NNS of_IN multiple_JJ base_NN models_NNS ,_, each_DT of_IN which_WDT is_VBZ learned_VBN using_VBG a_DT traditional_JJ algorithm_NN ._.
Bagging_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- and_CC Boosting_VBG =_JJ -_: =[_NN 4_CD -_: =-]_CD are_VBP well-known_JJ ensemble_NN learning_NN algorithms_NNS that_WDT have_VBP been_VBN shown_VBN to_TO be_VB very_RB eective_JJ in_IN improving_VBG generalization_NN performance_NN compared_VBN to_TO the_DT individual_JJ base_NN models_NNS ._.
Theoretical_JJ analysis_NN of_IN boosting_VBG 's_POS
tree_NN or_CC neural_JJ network_NN and_CC use_VB it_PRP to_TO classify_VB examples_NNS ._.
1_CD Ensemble_NN learning_NN algorithms_NNS combine_VBP the_DT predictions_NNS of_IN multiple_JJ base_NN models_NNS ,_, each_DT of_IN which_WDT is_VBZ learned_VBN using_VBG a_DT traditional_JJ algorithm_NN ._.
Bagging_VBG =_JJ -_: =[_NN 3_CD -_: =-]_NN and_CC Boosting_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- are_VBP well-known_JJ ensemble_NN learning_NN algorithms_NNS that_WDT have_VBP been_VBN shown_VBN to_TO be_VB very_RB eective_JJ in_IN improving_VBG generalization_NN performance_NN compared_VBN to_TO the_DT individual_JJ base_NN models_NNS ._.
Theoretical_JJ analy_NN
s_NNS having_VBG 600MHz_NNP Pentium_NNP III_NNP processors_NNS and_CC 2GB_NN of_IN memory_NN ._.
4.1_CD The_DT Data_NN We_PRP tested_VBD our_PRP$ algorithms_NNS on_IN several_JJ UCI_NN datasets_NNS -LRB-_-LRB- 2_CD -RRB-_-RRB- ,_, two_CD datasets_NNS -LRB-_-LRB- Census_NNP Income_NNP and_CC Forest_NNP Covertype_NNP -RRB-_-RRB- from_IN the_DT UCI_NNP KDD_NNP archive_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC three_CD synthetic_JJ datasets_NNS ._.
We_PRP give_VBP their_PRP$ sizes_NNS and_CC numbers_NNS of_IN attributes_NNS and_CC classes_NNS in_IN Table_NNP 1_CD ._.
All_DT three_CD of_IN our_PRP$ synthetic_JJ datasets_NNS have_VBP two_CD classes_NNS ._.
The_DT prior_JJ probability_NN of_IN each_DT class_NN is_VBZ 0.5_CD ,_,
rithms_NNS are_VBP essentially_RB identical_JJ ._.
We_PRP ran_VBD these_DT experiments_NNS on_IN Dell_NNP 6350_CD computers_NNS having_VBG 600MHz_NNP Pentium_NNP III_NNP processors_NNS and_CC 2GB_NN of_IN memory_NN ._.
4.1_CD The_DT Data_NN We_PRP tested_VBD our_PRP$ algorithms_NNS on_IN several_JJ UCI_NN datasets_NNS =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: ,_, two_CD datasets_NNS -LRB-_-LRB- Census_NNP Income_NNP and_CC Forest_NNP Covertype_NNP -RRB-_-RRB- from_IN the_DT UCI_NNP KDD_NNP archive_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ,_, and_CC three_CD synthetic_JJ datasets_NNS ._.
We_PRP give_VBP their_PRP$ sizes_NNS and_CC numbers_NNS of_IN attributes_NNS and_CC classes_NNS in_IN Table_NNP 1_CD ._.
All_DT three_CD of_IN our_PRP$ s_NN
we_PRP discuss_VBP some_DT experiments_NNS that_WDT demonstrate_VBP the_DT performance_NN of_IN our_PRP$ online_JJ algorithms_NNS relative_JJ to_TO their_PRP$ batch_NN counterparts_NNS ._.
For_IN decision_NN trees_NNS ,_, we_PRP have_VBP reimplemented_VBN the_DT lossless_JJ ITI_NNP online_NN algorithm_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: in_IN C_NN +_CC +_CC ;_: batch_NN and_CC online_NN Naive_JJ Bayes_NNP algorithms_NNS are_VBP essentially_RB identical_JJ ._.
We_PRP ran_VBD these_DT experiments_NNS on_IN Dell_NNP 6350_CD computers_NNS having_VBG 600MHz_NNP Pentium_NNP III_NNP processors_NNS and_CC 2GB_NN of_IN memory_NN ._.
4.1_CD The_DT Data_NN We_PRP test_VBP
n_NN shown_VBN to_TO be_VB very_RB eective_JJ in_IN improving_VBG generalization_NN performance_NN compared_VBN to_TO the_DT individual_JJ base_NN models_NNS ._.
Theoretical_JJ analysis_NN of_IN boosting_VBG 's_POS performance_NN supports_VBZ these_DT results_NNS -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
In_IN previous_JJ work_NN -LRB-_-LRB- =_JJ -_: =_JJ 7_CD -RRB-_-RRB- ,_, we_PRP -_: =_SYM -_: developed_VBD online_JJ versions_NNS of_IN these_DT algorithms_NNS ._.
Online_JJ learning_NN algorithms_NNS process_VBP each_DT training_NN instance_NN once_IN \_NN on_IN arrival_NN ''_'' without_IN the_DT need_NN for_IN storage_NN and_CC reprocessing_NN ,_, and_CC maintain_VB a_DT current_JJ hy_NN
