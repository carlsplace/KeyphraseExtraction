IMMC_NNP :_: incremental_JJ maximum_JJ margin_NN criterion_NN
Subspace_NN learning_NN approaches_NNS have_VBP attracted_VBN much_JJ attention_NN in_IN academia_NN recently_RB ._.
However_RB ,_, the_DT classical_JJ batch_NN algorithms_NNS no_RB longer_RB satisfy_VB the_DT applications_NNS on_IN streaming_NN data_NNS or_CC large-scale_JJ data_NNS ._.
To_TO meet_VB this_DT desirability_NN ,_, Incremental_JJ Principal_NN Component_NN Analysis_NN -LRB-_-LRB- IPCA_NN -RRB-_-RRB- algorithm_NN has_VBZ been_VBN well_RB established_VBN ,_, but_CC it_PRP is_VBZ an_DT unsupervised_JJ subspace_NN learning_NN approach_NN and_CC is_VBZ not_RB optimal_JJ for_IN general_JJ classification_NN tasks_NNS ,_, such_JJ as_IN face_NN recognition_NN and_CC Web_NN document_NN categorization_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP an_DT incremental_JJ supervised_JJ subspace_NN learning_NN algorithm_NN ,_, called_VBN Incremental_NNP Maximum_NNP Margin_NN Criterion_NN -LRB-_-LRB- IMMC_NN -RRB-_-RRB- ,_, to_TO infer_VB an_DT adaptive_JJ subspace_NN by_IN optimizing_VBG the_DT Maximum_NNP Margin_NN Criterion_NN ._.
We_PRP also_RB present_VBP the_DT proof_NN for_IN convergence_NN of_IN the_DT proposed_VBN algorithm_NN ._.
Experimental_JJ results_NNS on_IN both_CC synthetic_JJ dataset_NN and_CC real_JJ world_NN datasets_NNS show_VBP that_IN IMMC_NN converges_VBZ to_TO the_DT similar_JJ subspace_NN as_IN that_DT of_IN batch_NN approach_NN ._.
ncludes_NNS only_RB a_DT single_JJ new_JJ data_NNS point_NN at_IN each_DT time_NN step_NN and_CC that_IN it_PRP requires_VBZ setting_VBG a_DT learning_NN rate_NN ._.
To_TO circumvent_VB the_DT difficulty_NN of_IN incrementally_RB updating_VBG the_DT product_NN of_IN scatter_NN matrices_NNS ,_, Yan_NNP et_FW al._FW =_SYM -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: used_VBD a_DT modified_VBN criterion_NN by_IN computing_VBG the_DT difference_NN of_IN the_DT between-class_JJ and_CC within-class_JJ scatter_NN matrices_NNS ._.
However_RB ,_, this_DT may_MD lead_VB to_TO regularization_NN problems_NNS of_IN the_DT two_CD scatter_NN matrices_NNS ._.
Lin_NNP et_FW al._FW ._.
algorithms_NNS reduce_VBP the_DT dimension_NN of_IN data_NNS by_IN linear_JJ algebra_NN transformations_NNS -LRB-_-LRB- such_JJ as_IN Principal_NN Component_NN Analysis_NN -LRB-_-LRB- PCA_NN -RRB-_-RRB- -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, Linear_JJ Discriminant_JJ Analysis_NN -LRB-_-LRB- LDA_NN -RRB-_-RRB- -LRB-_-LRB- 17_CD -RRB-_-RRB- and_CC Maximum_NNP Margin_NN Criterion_NN -LRB-_-LRB- MMC_NN -RRB-_-RRB- =_JJ -_: =[_NN 6_CD ,_, 24_CD -RRB-_-RRB- -_: =_JJ -_: ,_, etc._NN -RRB-_-RRB- or_CC nonlinear_JJ transformations_NNS -LRB-_-LRB- Locally_RB Linear_JJ Embedding_NN -LRB-_-LRB- LLE_NN -RRB-_-RRB- -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, ISOMAP_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, etc._NN -RRB-_-RRB- ._.
On_IN the_DT other_JJ hand_NN ,_, FS_NN algorithms_NNS reduce_VBP the_DT dimension_NN of_IN data_NNS by_IN select_JJ features_NNS from_IN the_DT original_JJ vectors_NNS
ncludes_NNS only_RB a_DT single_JJ new_JJ data_NNS point_NN at_IN each_DT time_NN step_NN and_CC that_IN it_PRP requires_VBZ setting_VBG a_DT learning_NN rate_NN ._.
To_TO circumvent_VB the_DT difficulty_NN of_IN incrementally_RB updating_VBG the_DT product_NN of_IN scatter_NN matrices_NNS ,_, Yan_NNP et_FW al._FW =_SYM -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: used_VBD a_DT modified_VBN criterion_NN by_IN computing_VBG the_DT difference_NN of_IN the_DT between-class_JJ and_CC within-class_JJ scatter_NN matrices_NNS ._.
However_RB ,_, this_DT may_MD lead_VB to_TO regularization_NN problems_NNS of_IN the_DT two_CD scatter_NN matrices_NNS ._.
Lin_NNP et_FW al._FW ._.
