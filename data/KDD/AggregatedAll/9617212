Fast_JJ collapsed_JJ gibbs_NNS sampling_NN for_IN latent_JJ dirichlet_NN allocation_NN
In_IN this_DT paper_NN we_PRP introduce_VBP a_DT novel_JJ collapsed_JJ Gibbs_NNP sampling_NN method_NN for_IN the_DT widely_RB used_VBN latent_JJ Dirichlet_NNP allocation_NN -LRB-_-LRB- LDA_NN -RRB-_-RRB- model_NN ._.
Our_PRP$ new_JJ method_NN results_VBZ in_IN significant_JJ speedups_NNS on_IN real_JJ world_NN text_NN corpora_NN ._.
Conventional_JJ Gibbs_NNP sampling_NN schemes_NNS for_IN LDA_NNP require_VB O_NN -LRB-_-LRB- K_NN -RRB-_-RRB- operations_NNS per_IN sample_NN where_WRB K_NN is_VBZ the_DT number_NN of_IN topics_NNS in_IN the_DT model_NN ._.
Our_PRP$ proposed_VBN method_NN draws_VBZ equivalent_JJ samples_NNS but_CC requires_VBZ on_IN average_NN significantly_RB less_RBR then_RB K_NN operations_NNS per_IN sample_NN ._.
On_IN real-word_JJ corpora_NN FastLDA_NN can_MD be_VB as_RB much_JJ as_IN 8_CD times_NNS faster_RBR than_IN the_DT standard_JJ collapsed_JJ Gibbs_NNP sampler_NN for_IN LDA_NNP ._.
No_DT approximations_NNS are_VBP necessary_JJ ,_, and_CC we_PRP show_VBP that_IN our_PRP$ fast_JJ sampling_NN scheme_NN produces_VBZ exactly_RB the_DT same_JJ results_NNS as_IN the_DT standard_JJ -LRB-_-LRB- but_CC slower_JJR -RRB-_-RRB- sampling_NN scheme_NN ._.
Experiments_NNS on_IN four_CD real_JJ world_NN data_NNS sets_NNS demonstrate_VBP speedups_NNS for_IN a_DT wide_JJ range_NN of_IN collection_NN sizes_NNS ._.
For_IN the_DT PubMed_NNP collection_NN of_IN over_IN 8_CD million_CD documents_NNS with_IN a_DT required_JJ computation_NN time_NN of_IN 6_CD CPU_NNP months_NNS for_IN LDA_NNP ,_, our_PRP$ speedup_NN of_IN 5.7_CD can_MD save_VB 5_CD CPU_NNP months_NNS of_IN computation_NN ._.
a_DT in_IN applications_NNS such_JJ as_IN Web_NN search_NN ,_, speeding_VBG up_RP the_DT inference_NN procedure_NN in_IN topic_NN models_NNS is_VBZ in_IN great_JJ need_NN ._.
One_CD kind_NN of_IN the_DT accelerated_JJ procedures_NNS slices_VBZ up_RP the_DT sampling_NN probability_NN in_IN different_JJ ways_NNS =_JJ -_: =[_NN 48_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN any_DT particular_JJ word_NN and_CC document_NN ,_, the_DT distributions_NNS we_PRP want_VBP to_TO draw_VB sample_NN from_IN are_VBP often_RB skewed_VBN such_JJ that_IN most_JJS of_IN the_DT probability_NN 99mass_NN is_VBZ concentrated_VBN on_IN a_DT few_JJ topics_NNS ,_, which_WDT leads_VBZ to_TO a_DT possi_NN
A_DT model_NN learning_NN times_NNS by_IN reducing_VBG the_DT total_JJ computational_JJ cost_NN :_: --_: Gomes_NNP ,_, Welling_NNP and_CC Perona_NNP -LRB-_-LRB- 10_CD -RRB-_-RRB- presented_VBD an_DT enhancement_NN of_IN the_DT VEM_NN algorithm_NN using_VBG a_DT bounded_JJ amount_NN of_IN memory_NN ._.
--_: Porteous_NNP and_CC et_FW al._FW =_SYM -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT method_NN to_TO accelerate_VB the_DT computation_NN of_IN -LRB-_-LRB- Eq_NN .1_NN -RRB-_-RRB- ._.
The_DT acceleration_NN is_VBZ achieved_VBN by_IN no_DT approximations_NNS but_CC using_VBG the_DT property_NN that_IN the_DT probability_NN vectors_NNS ,_, θd_NN ,_, are_VBP sparse_JJ in_IN most_JJS cases_NNS ._.
3_CD PLDA_NN
riables_NNS in_IN the_DT model_NN ,_, in_IN effect_NN reducing_VBG the_DT state_NN space_NN of_IN the_DT Markov_NNP chain_NN ._.
Collapsed_VBN sampling_NN has_VBZ been_VBN previously_RB demonstrated_VBN to_TO be_VB effective_JJ for_IN LDA_NNP and_CC its_PRP$ variants_NNS -LRB-_-LRB- Griffiths_NNP &_CC Steyvers_NNP ,_, 2004_CD ;_: =_JJ -_: =_JJ Porteous_NNP ,_, Newman_NNP ,_, Ihler_NNP ,_, Asuncion_NNP ,_, Smyth_NNP ,_, &_CC Welling_NNP ,_, 2008_CD -_: =_JJ -_: ;_: Titov_NNP &_CC McDonald_NNP ,_, 2008_CD -RRB-_-RRB- ._.
It_PRP is_VBZ typically_RB preferred_VBN over_IN the_DT explicit_JJ Gibbs_NNP sampling_NN of_IN all_PDT the_DT hidden_JJ variables_NNS ,_, because_IN of_IN the_DT smaller_JJR search_NN space_NN and_CC generally_RB shorter_JJR mixing_VBG time_NN ._.
Our_PRP$ sampler_NN an_DT
ables_NNS in_IN the_DT model_NN ,_, in_IN effect_NN reducing_VBG the_DT state_NN space_NN of_IN the_DT Markov_NNP chain_NN ._.
Collapsed_VBN sampling_NN has_VBZ been_VBN previously_RB demonstrated_VBN to_TO be_VB effective_JJ for_IN LDA_NNP and_CC its_PRP$ variants_NNS -LRB-_-LRB- Griffiths_NNP and_CC Steyvers_NNP ,_, 2004_CD ;_: =_JJ -_: =_JJ Porteous_NNP et_FW al._FW ,_, 2008_CD -_: =_JJ -_: ;_: Titov_NNP and_CC McDonald_NNP ,_, 2008_CD -RRB-_-RRB- ._.
Our_PRP$ sampler_NN integrates_VBZ over_IN all_DT but_CC three_CD sets_NNS 7_CD Multiple_JJ permutations_NNS can_MD contribute_VB to_TO the_DT probability_NN of_IN a_DT single_JJ document_NN 's_POS topic_NN assignments_NNS zd_VBP ,_, if_IN there_EX are_VBP topics_NNS t_NN
e_LS is_VBZ a_DT wealth_NN of_IN literature_NN on_IN approximate_JJ inference_NN algorithms_NNS for_IN topic_NN models_NNS ,_, such_JJ Gibbs_NNP sampling_NN and_CC variational_JJ inference_NN -LRB-_-LRB- Blei_NNP et_FW al._FW ,_, 2003_CD ;_: Griffiths_NNP &_CC Steyvers_NNP ,_, 2004_CD ;_: Mukherjee_NNP &_CC Blei_NNP ,_, 2009_CD ;_: =_JJ -_: =_JJ Porteous_NNP et_FW al._FW ,_, 2008_CD -_: =_JJ -_: ;_: Teh_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- ,_, little_JJ is_VBZ known_VBN about_IN the_DT complexity_NN of_IN exact_JJ inference_NN ._.
In_IN this_DT paper_NN ,_, we_PRP consider_VBP the_DT computational_JJ complexity_NN of_IN inference_NN in_IN topic_NN models_NNS ,_, beginning_VBG with_IN one_CD of_IN the_DT simplest_JJS a_DT
where_WRB N_NN ¬_FW nd_FW wk_NN ,_, N_NN ¬_FW nd_FW k_NN ,_, and_CC N_NN ¬_FW nd_FW kd_NN are_VBP expected_JJ counts_NNS derived_VBN from_IN q_NN -LRB-_-LRB- z_SYM -RRB-_-RRB- ._.
For_IN more_JJR details_NNS ,_, see_VBP Asuncion_NNP et_FW al._FW -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
While_IN other_JJ algorithms_NNS such_JJ as_IN fast_RB collapsed_VBN Gibbs_NNP sampling_NN can_MD also_RB be_VB used_VBN =_JJ -_: =[_NN 40_CD -RRB-_-RRB- -_: =_JJ -_: ,_, CVB0_NN is_VBZ very_RB fast_RB --_: later_RB in_IN the_DT paper_NN we_PRP will_MD show_VB timing_NN results_NNS when_WRB applying_VBG CVB0_NN to_TO software_NN artifacts_NNS ._.
Furthermore_RB ,_, topic_NN modeling_NN can_MD easily_RB scale_VB to_TO hundreds_NNS of_IN thousands_NNS of_IN artifacts_NNS ,_, espec_NN
ampling_VBG performance_NN with_IN a_DT small_JJ memory_NN footprint_NN ._.
SparseLDA_NN is_VBZ approximately_RB 20_CD times_NNS faster_RBR than_IN highly_RB optimized_VBN traditional_JJ LDA_NN and_CC twice_RB the_DT speedup_NN of_IN previously_RB published_VBN fast_JJ sampling_NN methods_NNS =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2_CD ._.
BACKGROUND_NN A_DT statistical_JJ topic_NN model_NN represents_VBZ the_DT words_NNS in_IN documents_NNS in_IN a_DT collection_NN W_NN as_IN mixtures_NNS of_IN T_NN ``_`` topics_NNS ,_, ''_'' whichare_JJ multinomials_NNS over_IN a_DT vocabulary_NN of_IN size_NN V_NN ._.
Each_DT document_NN d_NN is_VBZ associa_NN
r_NN the_DT LDA_NNP model_NN are_VBP the_DT number_NN of_IN topics_NNS K_NNP and_CC Dirichlet_NNP priors_NNS α_NN and_CC β_NN ._.
We_PRP experimented_VBD with_IN topics_NNS K_NN :_: -LRB-_-LRB- 600_CD −_CD 1400_CD -RRB-_-RRB- ,_, again_RB with_IN step_NN size_NN 200_CD ._.
We_PRP fixed_VBD β_NN to_TO 0.01_CD and_CC tested_VBN two_CD values_NNS for_IN α_NN :_: 2_CD 50_CD K_NN -LRB-_-LRB- =_JJ -_: =_JJ Porteous_NNP et_FW al._FW ,_, 2008_CD -_: =--RRB-_NN and_CC K_NN -LRB-_-LRB- Griffiths_NNP and_CC Steyvers_NNP ,_, 2004_CD -RRB-_-RRB- ._.
We_PRP used_VBD Gibbs_NNP sampling_NN on_IN the_DT ``_`` document_NN collection_NN ''_'' obtained_VBN from_IN the_DT input_NN matrix_NN and_CC estimated_VBD the_DT sense_NN distributions_NNS as_IN described_VBN in_IN Section_NN 4_CD ._.
We_PRP ran_VBD the_DT c_NN
ime_NN and_CC less_JJR time_NN is_VBZ spent_VBN in_IN synchronizing_VBG and_CC exchanging_VBG data_NNS between_IN processors_NNS ._.
All_DT tests_NNS were_VBD conducted_VBN on_IN a_DT 16-core_JJ AMD_NNP Opteron_NNP 2.7_CD GHz_FW server_FW computer_NN with_IN 64_CD gigabytes_NNS of_IN RAM_NNP ._.
Remark_NN :_: FastLDA_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ a_DT modified_VBN version_NN of_IN the_DT Collapsed_NNP Gibbs_NNP sampler_NN for_IN LDA_NNP ,_, which_WDT improves_VBZ running_VBG time_NN significantly_RB by_IN avoiding_VBG computing_VBG all_DT elements_NNS of_IN the_DT sampling_NN distribution_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
However_RB ,_, in_IN this_DT project_NN w_NN
a_DT wealth_NN of_IN literature_NN on_IN approximate_JJ inference_NN algorithms_NNS for_IN topic_NN models_NNS ,_, such_JJ Gibbs_NNP sampling_NN and_CC variational_JJ inference_NN -LRB-_-LRB- Blei_NNP et_FW al._FW ,_, 2003_CD ;_: Griffiths_NNP and_CC Steyvers_NNP ,_, 2004_CD ;_: Mukherjee_NNP and_CC Blei_NNP ,_, 2009_CD ;_: =_JJ -_: =_JJ Porteous_NNP et_FW al._FW ,_, 2008_CD -_: =_JJ -_: ;_: Teh_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- ,_, little_JJ is_VBZ known_VBN about_IN the_DT computational_JJ complexity_NN of_IN exact_JJ inference_NN ._.
Furthermore_RB ,_, the_DT existing_VBG inference_NN algorithms_NNS ,_, although_IN well-motivated_JJ ,_, do_VBP not_RB provide_VB guarantees_NNS of_IN optima_NN
optimised_VBN computing_NN kernels_NNS ._.
In_IN addition_NN to_TO targeting_VBG computing_NN platforms_NNS ,_, improvements_NNS may_MD be_VB gained_VBN from_IN heuristics_NNS like_IN the_DT statistically_RB motivated_JJ acceleration_NN of_IN multinomial_JJ samplers_NNS proposed_VBN in_IN =_JJ -_: =[_NN 31_CD -RRB-_-RRB- -_: =_JJ -_: ,_, especially_RB for_IN the_DT large_JJ sampling_NN spaces_NNS of_IN dependent_JJ latent_JJ variables_NNS ._.
2_CD http:\/\/www.cs.toronto.edu\/˜roweis\/data.html_NN ._.
3_CD http:\/\/www.daviddlewis.com\/resources\/testcollections\/reuters21578\/.15_NN Refere_NN
the_DT data_NNS structure_NN -RRB-_-RRB- ._.
Accelerated_VBN algorithms_NNS of_IN this_DT type_NN exist_VBP for_IN many_JJ common_JJ probabilistic_JJ models_NNS ._.
In_IN some_DT cases_NNS ,_, such_JJ as_IN k-means_NN ,_, it_PRP is_VBZ possible_JJ to_TO accelerate_VB the_DT computation_NN of_IN an_DT exact_JJ solution_NN =_JJ -_: =[_NN 1_CD ,_, 16_CD ,_, 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN other_JJ algorithms_NNS ,_, such_JJ as_IN expectation_NN --_: maximization_NN for_IN Gaussian_JJ mixtures_NNS ,_, the_DT evaluations_NNS are_VBP only_RB approximate_JJ but_CC can_MD be_VB controlled_VBN by_IN tuning_NN a_DT quality_NN parameter_NN -LRB-_-LRB- 13_CD ,_, 10_CD ,_, 9_CD -RRB-_-RRB- ._.
In_IN -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, a_DT similar_JJ
j_NN 1_CD +_CC Wβ_NN -RRB-_-RRB- ,_, ._. ._.
,_, 1_CD \/_: -LRB-_-LRB- N_NN ¬_FW ij_FW K_NN +_CC Wβ_NN -RRB-_-RRB- -RRB-_-RRB- Then_RB ,_, the_DT normalization_NN constant_NN is_VBZ given_VBN by_IN Z_NN =_JJ X_NN ak_FW bk_FW ck_FW k_NN To_TO construct_VB an_DT initial_JJ upper_JJ bound_VBN Z0_NN on_IN Z_NN ,_, we_PRP turn_VBP to_TO the_DT generalized_VBN version_NN of_IN Hölder_NNP 's_POS inequality_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT states_VBZ Z0_NN =_JJ ‖_FW a_FW ‖_FW p_NN ‖_CD b_NN ‖_FW q_FW ‖_FW c_NN ‖_CD r_NN ≥_NN Z_NN where_WRB 1\/p_NN +_CC 1\/q_NN +_CC 1\/r_NN =_JJ 1_CD Notice_NNP that_IN ,_, as_IN we_PRP examine_VBP topics_NNS in_IN order_NN ,_, we_PRP learn_VBP the_DT actual_JJ value_NN of_IN the_DT product_NN ak_FW bk_FW ck_FW ._.
We_PRP can_MD use_VB these_DT calculations_NNS to_TO i_FW
supervised_VBN learning_NN methods_NNS ._.
Blei_NNP et_NNP al_NNP -LRB-_-LRB- 3_CD -RRB-_-RRB- introduced_VBD the_DT LDA_NNP model_NN within_IN a_DT general_JJ Bayesian_JJ framework_NN and_CC developed_VBD a_DT variational_JJ algorithm_NN for_IN learning_VBG the_DT model_NN from_IN data_NNS ._.
Griffiths_NNP and_CC Steyvers_NNP =_SYM -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: subsequently_RB proposed_VBN a_DT learning_NN algorithm_NN based_VBN on_IN collapsed_JJ Gibbs_NNP sampling_NN ._.
Both_CC the_DT variational_JJ and_CC Gibbs_NNP sampling_NN approaches_NNS have_VBP their_PRP$ advantages_NNS :_: the_DT variational_JJ approach_NN is_VBZ arguably_RB faster_JJR com_NN
ine_NN learning_NN and_CC data_NN mining_NN ,_, particularly_RB in_IN text_NN analysis_NN and_CC computer_NN vision_NN ,_, with_IN the_DT Gibbs_NNP sampling_NN algorithm_NN in_IN common_JJ use_NN ._.
For_IN example_NN ,_, Wei_NNP and_CC Croft_NNP -LRB-_-LRB- 19_CD -RRB-_-RRB- and_CC Chemudugunta_NNP ,_, Smyth_NNP ,_, and_CC Steyvers_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP successfully_RB applied_VBN the_DT LDA_NNP model_NN to_TO information_NN retrieval_NN and_CC shown_VBN that_IN it_PRP can_MD significantly_RB outperform_VB --_: in_IN terms_NNS of_IN precision-recall_JJ --_: alternative_JJ methods_NNS such_JJ as_IN latent_JJ semantic_JJ analysis_NN ._.
outperform_JJ --_: in_IN terms_NNS of_IN precision-recall_JJ --_: alternative_JJ methods_NNS such_JJ as_IN latent_JJ semantic_JJ analysis_NN ._.
LDA_NN models_NNS have_VBP also_RB been_VBN increasingly_RB applied_VBN to_TO problems_NNS involving_VBG very_RB large_JJ text_NN corpora_NN :_: Buntine_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Mimno_NNP and_CC McCallum_NNP -LRB-_-LRB- 12_CD -RRB-_-RRB- and_CC Newman_NNP et_FW al_FW -LRB-_-LRB- 15_CD -RRB-_-RRB- have_VBP all_DT used_VBN the_DT LDA_NNP model_NN to_TO automatically_RB generate_VB topic_NN models_NNS for_IN millions_NNS of_IN documents_NNS and_CC used_VBD these_DT models_NNS as_IN the_DT basis_NN for_IN automated_JJ indexing_NN a_DT
ty_NN distribution_NN over_IN words_NNS ._.
The_DT mixing_VBG coefficients_NNS for_IN each_DT document_NN and_CC the_DT wordtopic_JJ distributions_NNS are_VBP unobserved_JJ -LRB-_-LRB- hidden_JJ -RRB-_-RRB- and_CC are_VBP learned_VBN from_IN data_NNS using_VBG unsupervised_JJ learning_NN methods_NNS ._.
Blei_NNP et_NNP al_NNP =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: introduced_VBD the_DT LDA_NNP model_NN within_IN a_DT general_JJ Bayesian_JJ framework_NN and_CC developed_VBD a_DT variational_JJ algorithm_NN for_IN learning_VBG the_DT model_NN from_IN data_NNS ._.
Griffiths_NNP and_CC Steyvers_NNP -LRB-_-LRB- 6_CD -RRB-_-RRB- subsequently_RB proposed_VBD a_DT learning_NN algori_NN
