Learning_VBG the_DT kernel_NN matrix_NN in_IN discriminant_JJ analysis_NN via_IN quadratically_RB constrained_VBN quadratic_JJ programming_NN
The_DT kernel_NN function_NN plays_VBZ a_DT central_JJ role_NN in_IN kernel_NN methods_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP consider_VBP the_DT automated_JJ learning_NN of_IN the_DT kernel_NN matrix_NN over_IN a_DT convex_JJ combination_NN of_IN pre-specified_JJ kernel_NN matrices_NNS in_IN Regularized_NNP Kernel_NNP Discriminant_NNP Analysis_NNP -LRB-_-LRB- RKDA_NNP -RRB-_-RRB- ,_, which_WDT performs_VBZ lineardiscriminant_JJ analysis_NN in_IN the_DT feature_NN space_NN via_IN the_DT kernel_NN trick_NN ._.
Previous_JJ studies_NNS have_VBP shown_VBN that_IN this_DT kernel_NN learning_NN problem_NN can_MD be_VB formulated_VBN as_IN a_DT semidefinite_NN program_NN -LRB-_-LRB- SDP_NN -RRB-_-RRB- ,_, which_WDT is_VBZ however_RB computationally_RB expensive_JJ ,_, even_RB with_IN the_DT recent_JJ advances_NNS in_IN interior_JJ point_NN methods_NNS ._.
Based_VBN on_IN the_DT equivalence_JJ relationship_NN between_IN RKDA_NNP and_CC least_JJS square_JJ problems_NNS in_IN the_DT binary-class_JJ case_NN ,_, we_PRP propose_VBP a_DT Quadratically_RB Constrained_VBN Quadratic_JJ Programming_NN -LRB-_-LRB- QCQP_NN -RRB-_-RRB- formulation_NN for_IN the_DT kernel_NN learning_NN problem_NN ,_, which_WDT can_MD be_VB solved_VBN more_RBR efficiently_RB than_IN SDP_NNP ._.
While_IN most_JJS existing_VBG work_NN on_IN kernel_NN learning_NN deal_NN with_IN binary-class_JJ problems_NNS only_RB ,_, we_PRP show_VBP that_IN our_PRP$ QCQP_NN formulation_NN can_MD be_VB extended_VBN naturally_RB to_TO the_DT multi-class_JJ case_NN ._.
Experimental_JJ results_NNS on_IN both_CC binary-class_JJ and_CC multi-class_JJ benchmarkdata_NN sets_NNS show_VBP the_DT efficacy_NN of_IN the_DT proposed_VBN QCQP_NN formulations_NNS ._.
nels_NNS equally_RB ._.
Varma_NNP and_CC Babu_NNP -LRB-_-LRB- 17_CD -RRB-_-RRB- proposed_VBD a_DT general_JJ multiple_JJ kernel_NN learning_VBG algorithm_NN that_IN not_RB only_RB considers_VBZ the_DT linear_JJ combination_NN of_IN base_NN kernels_NNS but_CC also_RB employs_VBZ more_RBR prior_JJ knowledge_NN ._.
Ye_FW et_FW al._FW =_SYM -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT QCQP_NN formulation_NN for_IN kernel_NN learning_VBG to_TO address_VB both_DT bi-class_JJ problems_NNS and_CC multi-class_JJ problems_NNS ._.
Multiple_JJ kernel_NN learning_NN techniques_NNS have_VBP been_VBN applied_VBN to_TO a_DT variety_NN of_IN domains_NNS ._.
Fu_FW et_FW al._FW -LRB-_-LRB- 6_CD
complexity_NN and_CC thus_RB it_PRP can_MD not_RB be_VB applied_VBN to_TO large-scale_JJ problems_NNS ._.
To_TO improve_VB the_DT efficiency_NN of_IN this_DT formulation_NN ,_, a_DT quadratically_RB constrained_VBN quadratic_JJ program_NN -LRB-_-LRB- QCQP_NN -RRB-_-RRB- -LRB-_-LRB- 7_CD -RRB-_-RRB- formulation_NN was_VBD proposed_VBN in_IN =_JJ -_: =[_NN 70_CD -RRB-_-RRB- -_: =_JJ -_: and_CC it_PRP is_VBZ more_RBR scalable_JJ than_IN the_DT SDP_NN formulations_NNS ._.
s16_NN 1.7_CD OTHER_NNP LDA_NNP EXTENSIONS_NNP Sparsity_NNP has_VBZ recently_RB received_VBN much_JJ attention_NN for_IN extending_VBG existing_VBG algorithms_NNS to_TO induce_VB sparse_JJ solutions_NNS -LRB-_-LRB- 15_CD ,_, 35_CD ,_, 80_CD -RRB-_-RRB- ._.
ing_IN a_DT single_JJ fixed_JJ kernel_NN ,_, recent_JJ developments_NNS in_IN the_DT SVM_NN and_CC other_JJ kernel_NN methods_NNS have_VBP shown_VBN encouraging_JJ results_NNS in_IN constructing_VBG the_DT kernel_NN from_IN a_DT number_NN of_IN homogeneous_JJ or_CC even_RB heterogeneous_JJ kernels_NNS =_JJ -_: =[_NN 1_CD ,_, 10_CD ,_, 13_CD ,_, 14_CD ,_, 18_CD ,_, 33_CD ,_, 23_CD ,_, 24_CD ,_, 29_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT provides_VBZ extra_JJ flexibility_NN and_CC also_RB allows_VBZ domain_NN knowledge_NN from_IN possibly_RB different_JJ information_NN sources_NNS to_TO be_VB incorporated_VBN to_TO the_DT base_NN kernels_NNS ._.
However_RB ,_, previous_JJ works_NNS in_IN this_DT so-called_JJ multipl_NN
or_CC a_DT fixed_JJ regularization_NN parameter_NN λ_NN ,_, the_DT optimal_JJ Gram_NN matrix_NN G_NN computed_VBN from_IN the_DT kernel_NN function_NN K_NN that_WDT maximizes_VBZ F_NN ∗_NN 1_CD -LRB-_-LRB- K_NN -RRB-_-RRB- given_VBN in_IN Eq_NN ._.
-LRB-_-LRB- 9_CD -RRB-_-RRB- can_MD be_VB p_NN θi_NN =_JJ 1_CD ,_, θi_FW ≥_FW 0_CD ∀_FW i_FW ._.
sobtained_VBN by_IN solving_VBG a_DT SDP_NN =_JJ -_: =[_NN 4_CD ,_, 25_CD -RRB-_-RRB- -_: =_SYM -_: ._.
General-purpose_JJ optimization_NN packages_NNS such_JJ as_IN SeDuMi_NN -LRB-_-LRB- 24_CD -RRB-_-RRB- use_VBP the_DT interiorpoint_JJ methods_NNS -LRB-_-LRB- 18_CD -RRB-_-RRB- to_TO solve_VB SDP_NNP ._.
However_RB ,_, it_PRP is_VBZ known_VBN that_IN SDP_NN is_VBZ expensive_JJ to_TO solve_VB practically_RB ,_, and_CC thus_RB for_IN problems_NNS of_IN m_NN
ed_VBN methods_NNS are_VBP similar_JJ to_TO those_DT proposed_VBN in_IN -LRB-_-LRB- 10_CD ,_, 12_CD -RRB-_-RRB- and_CC can_MD thus_RB be_VB used_VBN for_IN heterogeneous_JJ data_NNS integration_NN ._.
The_DT problem_NN of_IN kernel_NN learning_NN for_IN discriminant_JJ analysis_NN has_VBZ been_VBN addressed_VBN originally_RB in_IN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Kim_NNP et_FW al._FW -LRB-_-LRB- 10_CD -RRB-_-RRB- formulated_VBN this_DT problem_NN as_IN a_DT semidefinite_NN program_NN -LRB-_-LRB- SDP_NN -RRB-_-RRB- ,_, and_CC hence_RB the_DT globally_RB optimal_JJ solution_NN can_MD be_VB obtained_VBN ._.
The_DT algorithms_NNS in_IN -LRB-_-LRB- 6_CD ,_, 10_CD -RRB-_-RRB- deal_NN with_IN binary-class_JJ problems_NNS only_RB ._.
They_PRP
ntly_RB ._.
Lanckriet_NNP et_FW al._FW -LRB-_-LRB- 12_CD -RRB-_-RRB- pioneered_VBD the_DT work_NN of_IN learning_VBG a_DT linear_JJ combination_NN of_IN pre-specified_JJ kernels_NNS for_IN Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- -LRB-_-LRB- 5_CD ,_, 26_CD -RRB-_-RRB- using_VBG convex_NN programming_NN and_CC it_PRP has_VBZ been_VBN improved_VBN in_IN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: using_VBG the_DT Sequential_JJ Minimal_JJ Optimization_NN -LRB-_-LRB- SMO_NN -RRB-_-RRB- algorithm_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- ._.
Reformulation_NN of_IN this_DT problem_NN as_IN semi-infinite_JJ linear_JJ program_NN has_VBZ been_VBN proposed_VBN in_IN -LRB-_-LRB- 23_CD -RRB-_-RRB- along_IN with_IN extensions_NNS to_TO regression_NN and_CC one-cla_NN
rces_NNS and_CC it_PRP has_VBZ been_VBN applied_VBN for_IN integrating_VBG various_JJ types_NNS of_IN biological_JJ data_NNS ,_, e.g._FW ,_, amino_NN acid_NN sequences_NNS ,_, hydropathy_NN profiles_NNS ,_, and_CC gene_NN expression_NN data_NNS for_IN enhanced_VBN biological_JJ inference_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
Jebara_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: considered_VBN the_DT kernel_NN selection_NN problem_NN in_IN the_DT context_NN of_IN multi-task_JJ learning_NN ._.
Zien_NNP and_CC Ong_NNP -LRB-_-LRB- 30_CD -RRB-_-RRB- recently_RB studied_VBN multi-class_JJ multiple_JJ kernel_NN learning_NN ._.
This_DT paper_NN addresses_VBZ the_DT issue_NN of_IN kernel_NN learni_NNS
sions_NNS to_TO regression_NN and_CC one-class_JJ classification_NN ._.
While_IN most_JJS approaches_NNS produce_VBP stationary_JJ combinations_NNS ,_, Lewis_NNP et_FW al._FW -LRB-_-LRB- 13_CD -RRB-_-RRB- proposed_VBN to_TO use_VB different_JJ combinations_NNS for_IN different_JJ inputs_NNS ._.
Argyriou_FW et_FW al._FW =_SYM -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBN to_TO combine_VB potentially_RB an_DT infinite_JJ number_NN of_IN kernels_NNS ,_, which_WDT was_VBD formulated_VBN as_IN a_DT difference_NN of_IN convex_NN -LRB-_-LRB- DC_NN -RRB-_-RRB- program_NN ._.
In_IN general_JJ ,_, approaches_NNS based_VBN on_IN learning_VBG a_DT convex_NN combination_NN of_IN kernels_NNS fa_IN
By_IN simply_RB changing_VBG the_DT last_JJ term_NN in_IN the_DT above_JJ equation_NN to_TO t_NN and_CC moving_VBG it_PRP to_TO the_DT constraint_NN ,_, we_PRP prove_VBP this_DT theorem_NN ._.
Note_VB that_DT general-purpose_JJ optimization_NN software_NN packages_NNS like_IN SeDuMi_NN -LRB-_-LRB- 24_CD -RRB-_-RRB- and_CC MOSEK_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: also_RB solve_VB the_DT dual_JJ problem_NN ._.
Thus_RB the_DT coefficients_NNS θ1_NN ,_, ·_FW ·_FW ·_NN ,_, θp_NN can_MD be_VB obtained_VBN from_IN the_DT dual_JJ variables_NNS by_IN dividing_VBG the_DT corresponding_JJ traces_NNS of_IN centered_JJ kernel_NN matrices_NNS ._.
The_DT formulation_NN in_IN Eq_NN ._.
-LRB-_-LRB- 28_CD -RRB-_-RRB- is_VBZ
validation_NN ._.
The_DT methods_NNS proposed_VBN in_IN -LRB-_-LRB- 10_CD ,_, 12_CD -RRB-_-RRB- are_VBP restricted_JJ to_TO binary-class_JJ problems_NNS only_RB ._.
We_PRP used_VBD a_DT total_NN of_IN five_CD data_NNS sets_NNS for_IN this_DT experiment_NN ._.
The_DT USPS_NNP handwritten_JJ digits_NNS database_NN was_VBD described_VBN in_IN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP choose_VBP the_DT first_JJ 3_CD ,_, 6_CD ,_, and_CC 8_CD classes_NNS with_IN 100_CD samples_NNS in_IN each_DT class_NN for_IN the_DT experiments_NNS ._.
The_DT wine_NN and_CC waveform_NN data_NNS were_VBD obtained_VBN from_IN UCI_NNP Machine_NNP Learning_NNP Repository_NNP and_CC the_DT satimage_NN and_CC segment_NN
ated_VBN kernel_NN learning_NN has_VBZ been_VBN an_DT active_JJ area_NN of_IN research_NN recently_RB ._.
Lanckriet_NNP et_FW al._FW -LRB-_-LRB- 12_CD -RRB-_-RRB- pioneered_VBD the_DT work_NN of_IN learning_VBG a_DT linear_JJ combination_NN of_IN pre-specified_JJ kernels_NNS for_IN Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVM_NNP -RRB-_-RRB- =_JJ -_: =[_NN 5_CD ,_, 26_CD -RRB-_-RRB- -_: =_SYM -_: using_VBG convex_NN programming_NN and_CC it_PRP has_VBZ been_VBN improved_VBN in_IN -LRB-_-LRB- 3_CD -RRB-_-RRB- using_VBG the_DT Sequential_JJ Minimal_JJ Optimization_NN -LRB-_-LRB- SMO_NN -RRB-_-RRB- algorithm_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- ._.
Reformulation_NN of_IN this_DT problem_NN as_IN semi-infinite_JJ linear_JJ program_NN has_VBZ been_VBN propose_VBP
