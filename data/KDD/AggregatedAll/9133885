Relational_JJ learning_NN via_IN collective_JJ matrix_NN factorization_NN
Relational_JJ learning_NN is_VBZ concerned_VBN with_IN predicting_VBG unknown_JJ values_NNS of_IN a_DT relation_NN ,_, given_VBN a_DT database_NN of_IN entities_NNS and_CC observed_VBN relations_NNS among_IN entities_NNS ._.
An_DT example_NN of_IN relational_JJ learning_NN is_VBZ movie_NN rating_NN prediction_NN ,_, where_WRB entities_NNS could_MD include_VB users_NNS ,_, movies_NNS ,_, genres_NNS ,_, and_CC actors_NNS ._.
Relations_NNS encode_VBP users_NNS '_POS ratings_NNS of_IN movies_NNS ,_, movies_NNS '_POS genres_NNS ,_, and_CC actors_NNS '_POS roles_NNS in_IN movies_NNS ._.
A_DT common_JJ prediction_NN technique_NN given_VBN one_CD pairwise_JJ relation_NN ,_, for_IN example_NN a_DT #users_CD x_CC #movies_CD ratings_NNS matrix_NN ,_, is_VBZ low-rank_JJ matrix_NN factorization_NN ._.
In_IN domains_NNS with_IN multiple_JJ relations_NNS ,_, represented_VBN as_IN multiple_JJ matrices_NNS ,_, we_PRP may_MD improve_VB predictive_JJ accuracy_NN by_IN exploiting_VBG information_NN from_IN one_CD relation_NN while_IN predicting_VBG another_DT ._.
To_TO this_DT end_NN ,_, we_PRP propose_VBP a_DT collective_JJ matrix_NN factorization_NN model_NN :_: we_PRP simultaneously_RB factor_VBP several_JJ matrices_NNS ,_, sharing_VBG parameters_NNS among_IN factors_NNS when_WRB an_DT entity_NN participates_VBZ in_IN multiple_JJ relations_NNS ._.
Each_DT relation_NN can_MD have_VB a_DT different_JJ value_NN type_NN and_CC error_NN distribution_NN ;_: so_RB ,_, we_PRP allow_VBP nonlinear_JJ relationships_NNS between_IN the_DT parameters_NNS and_CC outputs_NNS ,_, using_VBG Bregman_NNP divergences_NNS to_TO measure_VB error_NN ._.
We_PRP extend_VBP standard_JJ alternating_VBG projection_NN algorithms_NNS to_TO our_PRP$ model_NN ,_, and_CC derive_VB an_DT efficient_JJ Newton_NNP update_VB for_IN the_DT projection_NN ._.
Furthermore_RB ,_, we_PRP propose_VBP stochastic_JJ optimization_NN methods_NNS to_TO deal_VB with_IN large_JJ ,_, sparse_JJ matrices_NNS ._.
Our_PRP$ model_NN generalizes_VBZ several_JJ existing_VBG matrix_NN factorization_NN methods_NNS ,_, and_CC therefore_RB yields_VBZ new_JJ large-scale_JJ optimization_NN algorithms_NNS for_IN these_DT problems_NNS ._.
Our_PRP$ model_NN can_MD handle_VB any_DT pairwise_JJ relational_JJ schema_NN and_CC a_DT wide_JJ variety_NN of_IN error_NN models_NNS ._.
We_PRP demonstrate_VBP its_PRP$ efficiency_NN ,_, as_RB well_RB as_IN the_DT benefit_NN of_IN sharing_VBG parameters_NNS among_IN relations_NNS ._.
ive_JJ Filtering_NN We_PRP propose_VBP a_DT probabilistic_JJ learning_NN framework_NN based_VBN on_IN collaborative_JJ filtering_VBG to_TO jointly_RB learn_VB the_DT user_NN activities_NNS and_CC profiles_NNS ._.
In_IN particular_JJ ,_, we_PRP apply_VBP collective_JJ matrix_NN factorization_NN =_JJ -_: =[_NN 7_CD ,_, 11_CD -RRB-_-RRB- -_: =_SYM -_: to_TO enforce_VB the_DT knowledge_NN share_NN among_IN the_DT user_NN activities_NNS and_CC the_DT user_NN profiles_NNS ._.
We_PRP factorize_VBP both_CC the_DT -LRB-_-LRB- postprocessed_VBN -RRB-_-RRB- ``_`` Users-Activities_NNP ''_'' matrix_NN and_CC ``_`` Users-Profiles_NNP ''_'' matrix_NN at_IN the_DT same_JJ time_NN ,_, and_CC require_VB t_NN
._.
Maximizing_VBG Equation_NN 1_CD is_VBZ a_DT weighted_JJ version_NN of_IN Exponential_JJ Family_NN PCA_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
Maximizing_VBG the_DT product_NN of_IN Equations_NNS 1_CD and_CC 2_CD with_IN respect_NN to_TO the_DT factors_NNS F_NN is_VBZ an_DT example_NN of_IN collective_JJ matrix_NN factorization_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Given_VBN that_IN the_DT number_NN of_IN parameters_NNS grow_VBP with_IN the_DT data_NNS ,_, we_PRP place_VBP a_DT multivariate_JJ Gaussian_NNP prior_RB on_IN each_DT row_NN of_IN U_NN :_: p_NN -LRB-_-LRB- U_NNP |_NNP ΘU_NNP -RRB-_-RRB- =_JJ m_NN ∏_NN N_NN -LRB-_-LRB- Ui_FW ·_FW |_FW µU_NN ,_, ΣU_NN -RRB-_-RRB- ,_, -LRB-_-LRB- 3_LS -RRB-_-RRB- i_LS =_JJ 1_CD where_WRB N_NN -LRB-_-LRB- ·_FW |_FW µU_NN ,_, ΣU_NN -RRB-_-RRB- is_VBZ a_DT Gaussian_NN with_IN mean_JJ vector_NN µ_NN
ed_VBN to_TO handle_VB two_CD relation_NN types_NNS ._.
Recently_RB some_DT unsupervised_JJ approaches_NNS have_VBP been_VBN proposed_VBN to_TO deal_VB with_IN graph_NN clustering_NN problems_NNS on_IN multi-relational_JJ domains_NNS -LRB-_-LRB- Long_NNP et_FW al._FW ,_, 2006b_CD ;_: Long_NNP et_FW al._FW ,_, 2006a_CD -RRB-_-RRB- ._.
-LRB-_-LRB- =_JJ -_: =_JJ Singh_NNP &_CC Gordon_NNP ,_, 2008_CD -_: =--RRB-_NN have_VBP proposed_VBN a_DT collective_JJ matrix_NN factorization_NN based_VBN on_IN minimizing_VBG Bregman_NNP divergences_NNS between_IN the_DT model_NN and_CC the_DT involved_VBN relation_NN matrices_NNS ._.
3_LS ._.
Multi-Relational_NNP Matrix_NNP Factorization_NNP Established_NNP mat_NN
ferent_JJ strategies_NNS have_VBP been_VBN developed_VBN to_TO enable_VB parameter_NN sharing_NN when_WRB jointly_RB factorize_VBP a_DT collection_NN of_IN related_JJ matrices_NNS so_IN that_DT knowledge_NN can_MD be_VB transferred_VBN across_IN different_JJ tasks_NNS ._.
For_IN example_NN ,_, in_IN -LRB-_-LRB- =_JJ -_: =_JJ Singh_NNP &_CC Gordon_NNP ,_, 2008_CD -_: =_JJ -_: ;_: Zhu_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- ,_, an_DT entity_NN is_VBZ required_VBN to_TO be_VB represented_VBN by_IN the_DT same_JJ latent_JJ features_NNS in_IN different_JJ matrices_NNS ._.
Xu_FW et_FW al._FW in_FW -LRB-_-LRB- Xu_NNP et_FW al._FW ,_, 2009_CD -RRB-_-RRB- extended_VBD such_JJ collective_JJ matrix_NN factorization_NN models_NNS to_TO a_DT
heterogeneous_JJ graph_NN structures_NNS with_IN interdependent_JJ predictions_NNS ._.
General_JJ statistical_JJ relational_JJ approaches_NNS can_MD model_VB heterogeneous_JJ data_NNS with_IN multiple_JJ link_NN and_CC node_NN types_NNS ._.
For_IN example_NN Singh_NNP and_CC Gordon_NNP =_SYM -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP considered_VBN multiple_JJ relations_NNS to_TO predict_VB links_NNS where_WRB each_DT relation_NN type_NN is_VBZ between_IN different_JJ node_NN types_NNS ._.
This_DT is_VBZ different_JJ from_IN our_PRP$ F_NN eature_NN F_NN usion_NN approach_NN because_IN all_DT relation_NN types_NNS belong_VBP to_TO
common_JJ ,_, whereas_IN in_IN LMF_NNP ,_, each_DT source_NN of_IN information_NN is_VBZ a_DT network_NN on_IN exactly_RB the_DT same_JJ set_NN of_IN users_NNS ._.
Singh_NNP and_CC Gordon_NNP have_VBP proposed_VBN a_DT model_NN for_IN relational_JJ learning_NN called_VBN Collective_NNP Matrix_NNP Factorization_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
They_PRP suggest_VBP a_DT generalized_JJ framework_NN for_IN inferring_VBG relations_NNS ,_, given_VBN a_DT set_NN of_IN entities_NNS and_CC observed_VBN relations_NNS among_IN them_PRP ._.
This_DT model_NN factors_VBZ multiple_JJ source_NN matrices_NNS simultaneously_RB ,_, and_CC uses_VBZ common_JJ f_SYM
utputs_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP focus_VBP on_IN the_DT `_`` different_JJ features_NNS '_POS version_NN ._.
lated_VBN ,_, which_WDT can_MD be_VB used_VBN to_TO provide_VB a_DT better_JJR representation_NN for_IN images_NNS ._.
We_PRP apply_VBP collective_JJ matrix_NN factorization_NN -LRB-_-LRB- CMF_NN -RRB-_-RRB- techniques_NNS -LRB-_-LRB- =_JJ -_: =_JJ Singh_NNP and_CC Gordon_NNP 2008_CD -_: =--RRB-_NN on_IN the_DT auxiliary_JJ image_NN and_CC text_NN data_NNS to_TO discover_VB the_DT semantic_JJ space_NN underlying_VBG the_DT image_NN and_CC text_NN domains_NNS ._.
The_DT traditional_JJ version_NN of_IN CMF_NN assumes_VBZ that_IN correspondence_NN exists_VBZ between_IN images_NNS and_CC text_NN d_NN
ommon_NN ,_, whereas_IN in_IN LMF_NNP ,_, each_DT source_NN of_IN information_NN is_VBZ a_DT network_NN on_IN exactly_RB the_DT same_JJ set_NN of_IN users_NNS ._.
Singh_NNP and_CC Gordon_NNP have_VBP proposed_VBN a_DT model_NN for_IN relational_JJ learning_NN called_VBN Collective_NNP Matrix_NNP Factorization_NNP -LRB-_-LRB- =_JJ -_: =_JJ Singh_NNP and_CC Gordon_NNP 2008_CD -_: =-]_CD ._.
They_PRP suggest_VBP a_DT generalized_JJ framework_NN for_IN inferring_VBG relations_NNS ,_, given_VBN a_DT set_NN of_IN entities_NNS and_CC observed_VBN relations_NNS among_IN them_PRP ._.
This_DT model_NN factors_VBZ multiple_JJ source_NN matrices_NNS simultaneously_RB ,_, and_CC uses_VBZ common_JJ
vector_NN vi_LS is_VBZ associated_VBN with_IN f._NN Then_RB ,_, Σ_NN scales_NNS by_IN the_DT prominence_NN of_IN each_DT singular_JJ vector_NN ._.
Finally_RB ,_, U_NN expresses_VBZ how_WRB much_JJ each_DT concept_NN is_VBZ associated_VBN with_IN each_DT singular_JJ vector_NN ._.
A_DT Bregman_NNP divergence_NN -LRB-_-LRB- =_JJ -_: =_JJ Singh_NNP and_CC Gordon_NNP 2008_CD -_: =--RRB-_NN measures_VBZ the_DT error_NN ,_, under_IN a_DT given_VBN function_NN F_NN ,_, of_IN using_VBG Â_NN instead_RB of_IN A_NN :_: ∑_NN DF_NN -LRB-_-LRB- Â_NN |_NN A_NN -RRB-_-RRB- =_JJ F_NN -LRB-_-LRB- aij_NN -RRB-_-RRB- −_NN F_NN ′_NN -LRB-_-LRB- aij_NN -RRB-_-RRB- âij_NN +_CC F_NN ∗_NN -LRB-_-LRB- F_NN ′_NN -LRB-_-LRB- aij_NN -RRB-_-RRB- -RRB-_-RRB- ,_, ij_NN where_WRB F_NN ∗_NN -LRB-_-LRB- m_NN -RRB-_-RRB- is_VBZ the_DT convex_NN dual_JJ -LRB-_-LRB- Legendre_NNP transform_VB -RRB-_-RRB- of_IN F_NN and_CC give_VB
ix_RB ,_, location-feature_JJ matrix_NN and_CC activity-activity_NN correlation_NN matrix_NN ,_, we_PRP can_MD train_VB a_DT recommender_NN system_NN ._.
We_PRP propose_VBP a_DT collaborative_JJ filtering_VBG model_NN under_IN the_DT collective_JJ matrix_NN factorization_NN framework_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC manage_VBP to_TO fill_VB the_DT missing_VBG entries_NNS in_IN the_DT location-activity_JJ matrix_NN ._.
Based_VBN on_IN the_DT filled_VBN location-activity_NN matrix_NN ,_, we_PRP will_MD rank_VB and_CC retrieve_VB the_DT top_JJ k_NN locations\/activities_NNS for_IN recommendations_NNS to_TO
ethods_NNS ._.
The_DT model-based_JJ methods_NNS benefit_VBP from_IN the_DT statistical_JJ and_CC machine_NN learning_NN techniques_NNS ,_, and_CC view_NN CF_NN as_IN a_DT missing-value_JJ prediction_NN problem_NN through_IN matrix_NN factorization_NN -LRB-_-LRB- Srebro_NNP and_CC Jaakkola_NNP 2003_CD ;_: =_JJ -_: =_JJ Singh_NNP and_CC Gordon_NNP 2008_CD -_: =--RRB-_NN ._.
However_RB ,_, most_JJS of_IN these_DT methods_NNS only_RB modeled_VBD two-party_JJ relations_NNS in_IN matrix_JJ forms_NNS ._.
In_IN contrast_NN ,_, we_PRP model_VBP the_DT three-party_JJ relations_NNS in_IN a_DT tensor_NN ;_: and_CC beyond_IN standard_JJ tensor_NN decomposition_NN -LRB-_-LRB- Lathauwer_NNP ,_, Mo_NNP
the_DT topologies_NNS of_IN both_CC the_DT unipartite_JJ friendship_NN network_NN and_CC the_DT bipartite_JJ user-service_JJ interaction_NN network_NN ._.
The_DT FIP_NNP model_NN has_VBZ a_DT close_JJ connection_NN with_IN recent_JJ works_NNS on_IN collective_JJ matrix_NN factorization_NN =_JJ -_: =[_NN 15_CD ,_, 29_CD ,_, 27_CD ,_, 32_CD -RRB-_-RRB- -_: =_JJ -_: ,_, where_WRB the_DT tasks_NNS of_IN learning_VBG relational_JJ data_NNS were_VBD also_RB formulated_VBN in_IN terms_NNS of_IN factorizing_VBG multiple_JJ matrices_NNS ._.
The_DT current_JJ work_NN continues_VBZ our_PRP$ prior_JJ investigations_NNS on_IN this_DT topic_NN and_CC further_RB examines_VBZ int_NN
se_FW edges_FW ,_, we_PRP treat_VBP the_DT element-wise_JJ loss_NN equally_RB -LRB-_-LRB- referred_VBN to_TO as_IN `_`` 0\/1_CD Weight_NNP Matix_NNP '_POS -RRB-_-RRB- ._.
This_DT type_NN of_IN weight_NN matrix_NN in_IN widely_RB used_VBN in_IN the_DT literature_NN ,_, especially_RB in_IN the_DT context_NN of_IN collaborative_JJ filtering_VBG =_JJ -_: =[_NN 5_CD ,_, 35_CD -RRB-_-RRB- -_: =_SYM -_: ._.
With_IN such_JJ 0\/1_CD weight_NN matrix_NN ,_, e.q._FW -LRB-_-LRB- 3.1_CD -RRB-_-RRB- can_MD be_VB simplified_VBN as_IN :_: ∑_CD argminF_NN ,_, G_NN -LRB-_-LRB- A_NN -LRB-_-LRB- i_FW ,_, j_NN -RRB-_-RRB- −_NN F_NN -LRB-_-LRB- i_FW ,_, :_: -RRB-_-RRB- G_NN -LRB-_-LRB- :_: ,_, j_NN -RRB-_-RRB- -RRB-_-RRB- 2_CD -LRB-_-LRB- 3.2_CD -RRB-_-RRB- i_LS ,_, j_NN ,_, A_NN -LRB-_-LRB- i_FW ,_, j_NN -RRB-_-RRB- -RRB-_-RRB- 0_CD s.t._NN for_IN allA_NN -LRB-_-LRB- i_FW ,_, j_NN -RRB-_-RRB- -RRB-_-RRB- 0_CD :_: F_NN -LRB-_-LRB- i_FW ,_, :_: -RRB-_-RRB- G_NN -LRB-_-LRB- :_: ,_, j_NN -RRB-_-RRB- ≤_NN A_NN -LRB-_-LRB- i_FW ,_, j_NN -RRB-_-RRB- In_IN the_DT rest_NN of_IN this_DT paper_NN ,_, we_PRP wil_VBP
._.
Maximizing_VBG Equation_NN 1_CD is_VBZ a_DT weighted_JJ version_NN of_IN Exponential_JJ Family_NN PCA_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
Maximizing_VBG the_DT product_NN of_IN Equations_NNS 1_CD and_CC 2_CD with_IN respect_NN to_TO the_DT factors_NNS F_NN is_VBZ an_DT example_NN of_IN collective_JJ matrix_NN factorization_NN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Given_VBN that_IN the_DT number_NN of_IN parameters_NNS grows_VBZ with_IN the_DT data_NNS ,_, we_PRP place_VBP a_DT multivariate_JJ Gaussian_NNP prior_RB on_IN each_DT row_NN of_IN U_NN :_: m_NN ∏_CD p_NN -LRB-_-LRB- U_NNP |_NNP ΘU_NNP -RRB-_-RRB- =_JJ N_NN -LRB-_-LRB- Ui_FW ·_FW |_FW µU_NN ,_, ΣU_NN -RRB-_-RRB- ,_, -LRB-_-LRB- 3_LS -RRB-_-RRB- i_LS =_JJ 1_CD where_WRB N_NN -LRB-_-LRB- ·_FW |_FW µU_NN ,_, ΣU_NN -RRB-_-RRB- is_VBZ a_DT Gaussian_NN with_IN mean_NN vector_NN
e_LS the_DT target_NN location_NN --_: activity_NN matrix_NN A_NN ,_, the_DT additional_JJ location_NN --_: feature_NN matrix_NN C_NN and_CC the_DT activity_NN --_: activity_NN correlation_NN matrix_NN together_RB ._.
Formally_RB ,_, we_PRP propose_VBP to_TO employ_VB collective_JJ matrix_NN factorization_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: for_IN developing_VBG a_DT collaborative_JJ location_NN and_CC activity_NN filtering_VBG model_NN ,_, and_CC the_DT objective_JJ function_NN is_VBZ :_: L_NN -LRB-_-LRB- X_NN ,_, Y_NN ,_, Z_NN -RRB-_-RRB- =_JJ ∑_NN -LRB-_-LRB- i_FW ,_, j_NN -RRB-_-RRB- ∈_CD DA_NN +_CC β2_NN -LRB-_-LRB- xi_FW ·_FW y_FW j_FW −_FW Aij_NN -RRB-_-RRB- 2_CD +_CC β1_FW ∑_FW -LRB-_-LRB- j_NN ,_, k_NN -RRB-_-RRB- ∈_CD DD_NN ∑_NN -LRB-_-LRB- xi_FW ·_FW z_SYM k_NN −_NN C_NN ik_NN -RRB-_-RRB- 2_CD -LRB-_-LRB- i_LS ,_, k_NN -RRB-_-RRB- ∈_CD DC_NN
data_NNS is_VBZ usually_RB presented_VBN as_IN two_CD or_CC more_JJR matrices_NNS -LRB-_-LRB- the_DT social_JJ and_CC the_DT target_NN relation_NN -RRB-_-RRB- ,_, in_IN order_NN to_TO apply_VB factorization_NN approaches_NNS to_TO social_JJ data_NNS ,_, one_CD has_VBZ to_TO resort_VB to_TO their_PRP$ multi-relational_JJ variants_NNS =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, special_JJ care_NN is_VBZ necessary_JJ if_IN ,_, as_IN in_IN the_DT cold-start_JJ scenario_NN analyzed_VBN here_RB ,_, no_DT information_NN about_IN the_DT test_NN users_NNS is_VBZ given_VBN on_IN the_DT target_NN relation_NN ._.
Approaching_VBG this_DT problem_NN with_IN multi-relationa_NNS
ta_NN for_IN improving_VBG the_DT prediction_NN accuracy_NN ,_, using_VBG multi-relational_JJ matrix_NN factorization_NN -LRB-_-LRB- MRMF_NN -RRB-_-RRB- and_CC a_DT weighted_JJ MRMF_NN ._.
This_DT approach_NN has_VBZ shown_VBN to_TO be_VB successful_JJ in_IN recommender_NN systems_NNS -LRB-_-LRB- Lippert_NNP et_FW al._FW 2008_CD ;_: =_JJ -_: =_JJ Singh_NNP and_CC Gordon_NNP 2008_CD -_: =-]_CD ,_, however_RB ,_, using_VBG it_PRP in_IN educational_JJ data_NNS mining_NN ,_, especially_RB in_IN predicting_VBG student_NN performance_NN is_VBZ still_RB a_DT new_JJ topic_NN ._.
Our_PRP$ main_JJ contributions_NNS are_VBP summarized_VBN as_IN the_DT following_NN :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- We_PRP propose_VBP a_DT new_JJ approac_NN
le_DT exponential_JJ smoothing_NN or_CC Holt-Winter_NN methods_NNS -LRB-_-LRB- Chatfield_NNP and_CC Yar_NNP ,_, 1988_CD -RRB-_-RRB- ._.
Another_DT open_JJ issue_NN is_VBZ that_IN each_DT solving-step_NN relates_VBZ to_TO one_CD or_CC many_JJ skills_NNS ,_, thus_RB ,_, use_VBP multi-relational_JJ matrix_NN factorization_NN -LRB-_-LRB- =_JJ -_: =_JJ Singh_NNP and_CC Gordon_NNP ,_, 2008_CD -_: =_JJ -_: ;_: Lippert_NNP et_FW al._FW ,_, 2008_CD -RRB-_-RRB- could_MD be_VB developed_VBN to_TO deal_VB with_IN this_DT problem_NN ._.
CONCLUSIONS_NNS In_IN this_DT chapter_NN ,_, we_PRP have_VBP discussed_VBN on_IN the_DT problem_NN of_IN predicting_VBG student_NN performance_NN as_RB well_RB as_IN personalized_JJ recommendi_NNS
know_VB ,_, there_EX has_VBZ not_RB been_VBN existing_VBG research_NN work_NN on_IN this_DT problem_NN ._.
Several_JJ existing_VBG works_NNS are_VBP relevant_JJ to_TO ours_PRP ._.
Transfer_NN learning_NN approaches_NNS are_VBP proposed_VBN to_TO transfer_VB knowledge_NN in_IN latent_JJ feature_NN space_NN -LRB-_-LRB- =_JJ -_: =_JJ Singh_NNP and_CC Gordon_NNP 2008_CD -_: =_JJ -_: ;_: Yoo_NNP and_CC Choi_NNP 2009_CD ;_: Pan_NNP et_FW al._FW 2010_CD ;_: Cao_NNP ,_, Liu_NNP ,_, and_CC Yang_NNP 2010_CD ;_: Pan_NNP et_FW al._FW 2011b_CD ;_: Vasuki_NNP et_FW al._FW 2011_CD -RRB-_-RRB- ,_, exploiting_VBG feature_NN covariance_NN -LRB-_-LRB- Adams_NNP ,_, Dahl_NNP ,_, and_CC Murray_NNP 2010_CD -RRB-_-RRB- or_CC compressed_VBN rating_NN patterns_NNS -LRB-_-LRB- Li_NNP ,_, Ya_NNP
a_DT sources_NNS complementing_VBG user_NN ratings_NNS such_JJ as_IN tagging_VBG information_NN have_VBP been_VBN exploited_VBN ;_: e.g._FW ,_, users_NNS tag_VBP movies_NNS -LRB-_-LRB- 33_CD -RRB-_-RRB- as_RB well_RB as_IN featuresof_JJ movies_NNS such_JJ as_IN movie_NN types_NNS or_CC movie_NN players_NNS ._.
Singh_NNP and_CC Gordon_NNP =_SYM -_: =[_NN 28_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBN Collective_NNP Matrix_NNP Factorization_NNP -LRB-_-LRB- CMF_NNP -RRB-_-RRB- to_TO take_VB advantage_NN of_IN correlations_NNS between_IN different_JJ data_NNS sets_NNS and_CC simultaneously_RB factorize_VB coupled_JJ matrices_NNS ._.
Given_VBN two_CD matrices_NNS X_NN and_CC Y_NN of_IN size_NN I_CD ×_CD M_NN an_DT
different_JJ ,_, but_CC closely_RB related_JJ origin_NN :_: formal_JJ concept_NN analysis_NN ._.
Joint_NNP -LRB-_-LRB- or_CC shared_VBN -RRB-_-RRB- subspace_NN learning_NN is_VBZ also_RB an_DT emerging_VBG topic_NN in_IN data_NNS mining_NN and_CC machine_NN learning_NN ._.
The_DT uses_NNS include_VBP relational_JJ learning_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, theme_NN discovery_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, tagging_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, multi-label_JJ classification_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, and_CC social_JJ media_NNS retrieval_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
Here_RB we_PRP have_VBP understood_VBN shared_JJ subspace_NN learning_NN in_IN a_DT broad_JJ sense_NN ;_: of_IN the_DT above_JJ ,_, only_RB -LRB-_-LRB- 4_CD ,_, 8_CD ,_, 14_CD -RRB-_-RRB- do_VBP
eneous_JJ domains_NNS ,_, thus_RB the_DT entities_NNS of_IN one_CD mode_NN can_MD be_VB instances_NNS of_IN multiple_JJ classes_NNS like_IN persons_NNS ,_, items_NNS ,_, places_NNS ,_, etc._FW et_FW al._FW ,_, 2006_CD -RRB-_-RRB- are_VBP nonparametric_JJ Bayesian_JJ approaches_NNS to_TO relational_JJ learning_NN ,_, while_IN -LRB-_-LRB- =_JJ -_: =_JJ Singh_NNP &_CC Gordon_NNP ,_, 2008_CD -_: =--RRB-_NN employ_VBP collective_JJ matrix_NN factorizations_NNS for_IN relational_JJ learning_NN ._.
-LRB-_-LRB- Getoor_NNP &_CC Taskar_NNP ,_, 2007_CD -RRB-_-RRB- presents_VBZ further_JJ approaches_NNS and_CC gives_VBZ a_DT detailed_JJ introduction_NN into_IN the_DT area_NN of_IN relational_JJ learning_NN ._.
Tensor_NNP de_IN
and_CC Yang_NNP 2010_CD -RRB-_-RRB- ._.
These_DT latent_JJ factors_NNS are_VBP discovered_VBN by_IN optimizing_VBG certain_JJ predefined_JJ objective_NN functions_NNS ,_, such_JJ as_IN empirical_JJ likelihood_NN and_CC geometric_JJ structure_NN ._.
Collective_JJ Matrix_NNP Factorization_NN -LRB-_-LRB- CMF_NN -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =_JJ Singh_NNP and_CC Gordon_NNP 2008_CD -_: =--RRB-_NN and_CC its_PRP$ tri-factorization_JJ variants_NNS have_VBP been_VBN extensively_RB studied_VBN for_IN transfer_NN learning_NN recently_RB -LRB-_-LRB- Gupta_NNP et_FW al._FW 2010_CD ;_: Long_NNP et_FW al._FW 2010_CD ;_: Wang_NNP et_FW al._FW 2011_CD ;_: Zhu_NNP et_FW al._FW 2011_CD ;_: Zhuang_NNP et_FW al._FW 2011_CD ;_: Long_NNP et_NNP a_DT
ar_IN value_NN decomposition_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ,_, into_IN the_DT same_JJ framework_NN as_IN matrix_JJ co-clustering_JJ algorithms_NNS like_IN probabilistic_JJ latent_JJ semantic_JJ indexing_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
Moreover_RB ,_, recentlystudied_JJ problems_NNS ,_, such_JJ as_IN relational_JJ learning_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: and_CC supervised\/semi-supervised_JJ matrix_NN factorization_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ,_, can_MD be_VB viewed_VBN as_IN the_DT simultaneous_JJ factorization_NN of_IN several_JJ matrices_NNS ,_, where_WRB the_DT low-rank_JJ representations_NNS share_VBP parameters_NNS ._.
The_DT modeling_NN choices_NNS
-LRB-_-LRB- 3_LS -RRB-_-RRB- where_WRB multi-view_JJ clustering_NN is_VBZ performed_VBN by_IN canonical_JJ correlation_NN analysis_NN -LRB-_-LRB- CCA_NN -RRB-_-RRB- which_WDT extracts_VBZ the_DT most_RBS correlated_JJ direction_NN shared_VBN by_IN each_DT view_NN ._.
Collective_JJ Matrix_NNP Factorization_NNP Singh_NNP and_CC Gordon_NNP =_SYM -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT Collective_NNP Matrix_NNP Factorization_NNP method_NN for_IN relational_JJ learning_NN ,_, where_WRB the_DT relational_JJ data_NNS are_VBP approximated_VBN by_IN a_DT set_NN of_IN matrix_NN factorizations_NNS which_WDT share_VBP common_JJ factor_NN matrix_NN for_IN the_DT same_JJ s_NN
h_NN in_IN covariance_NN matrix_NN and_CC as_IN random_JJ variables_NNS ._.
Figure_NN 2_CD illustrates_VBZ the_DT differences_NNS between_IN these_DT models_NNS ._.
The_DT linear_JJ combination_NN of_IN random_JJ variables_NNS assumed_VBN in_IN MRGPs_NNS is_VBZ similar_JJ to_TO Singh_NNP and_CC Gordon_NNP 's_POS =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: generalization_NN of_IN linear_JJ models_NNS to_TO multi-relational_JJ domains_NNS ._.
6_CD Conclusion_NN We_PRP developed_VBD a_DT nonparametric_JJ Bayesian_JJ framework_NN for_IN multi-relational_JJ data_NNS ,_, i.e._FW ,_, colored_JJ graphs_NNS based_VBN on_IN Gaussian_JJ processes_NNS ._.
uality_NN constraint_NN on_IN each_DT update_VBP of_IN U_NN -LRB-_-LRB- r_NN -RRB-_-RRB- i_FW ·_FW ._.
This_DT additional_JJ equality_NN constraint_NN can_MD be_VB folded_VBN into_IN the_DT Newton_NNP step_NN using_VBG a_DT Lagrange_NNP multiplier_NN ,_, yielding_VBG an_DT unconstrained_JJ optimization_NN -LRB-_-LRB- c.f._NN ,_, ch_NN ._.
10_CD =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Comparing_VBG the_DT extension_NN of_IN collective_JJ matrix_NN factorization_NN to_TO the_DT alternatives_NNS above_IN is_VBZ a_DT topic_NN for_IN future_JJ work_NN ._.
It_PRP should_MD be_VB noted_VBN that_IN our_PRP$ choice_NN of_IN X_NN =_JJ UV_NN T_NN is_VBZ not_RB the_DT only_JJ one_NN for_IN matrix_NN factor_NN
column_NN in_IN V_NN -LRB-_-LRB- and_CC vice-versa_NN -RRB-_-RRB- ._.
If_IN the_DT prediction_NN link_NN and_CC loss_NN correspond_VBP to_TO a_DT Bernoulli_NNP distribution_NN ,_, then_RB margin_NN losses_NNS are_VBP special_JJ cases_NNS of_IN biases_NNS ;_: -LRB-_-LRB- iv_LS -RRB-_-RRB- methods_NNS based_VBN on_IN plate_NN models_NNS ,_, such_JJ as_IN pLSI_NN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_JJ -_: ,_, can_MD be_VB placed_VBN in_IN our_PRP$ framework_NN just_RB as_RB well_RB as_IN methods_NNS that_IN factor_NN data_NN matrices_NNS ._.
While_IN these_DT features_NNS can_MD be_VB added_VBN to_TO collective_JJ matrix_NN factorization_NN ,_, we_PRP focus_VBP primarily_RB on_IN relational_JJ issues_NNS herein_RB
minimizing_VBG squared_VBN error_NN with_IN an_DT identity_NN link_NN yields_VBZ the_DT singular_JJ value_NN decomposition_NN -LRB-_-LRB- corresponding_VBG to_TO a_DT Gaussian_JJ error_NN model_NN -RRB-_-RRB- ,_, while_IN other_JJ choices_NNS extend_VBP generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 26_CD -RRB-_-RRB- to_TO matrices_NNS =_JJ -_: =[_NN 14_CD ,_, 17_CD -RRB-_-RRB- -_: =_JJ -_: and_CC lead_VB to_TO error_NN models_NNS such_JJ as_IN Poisson_NNP ,_, Gamma_NNP ,_, or_CC Bernoulli_NNP distributions_NNS ._.
In_IN domains_NNS with_IN more_JJR than_IN one_CD relation_NN matrix_NN ,_, one_PRP could_MD fit_VB each_DT relation_NN separately_RB ;_: however_RB ,_, this_DT approach_NN would_MD not_RB ta_VB
ifferent_JJ models_NNS :_: minimizing_VBG squared_VBD error_NN with_IN an_DT identity_NN link_NN yields_VBZ the_DT singular_JJ value_NN decomposition_NN -LRB-_-LRB- corresponding_VBG to_TO a_DT Gaussian_JJ error_NN model_NN -RRB-_-RRB- ,_, while_IN other_JJ choices_NNS extend_VBP generalized_JJ linear_JJ models_NNS =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_SYM -_: to_TO matrices_NNS -LRB-_-LRB- 14_CD ,_, 17_CD -RRB-_-RRB- and_CC lead_VB to_TO error_NN models_NNS such_JJ as_IN Poisson_NNP ,_, Gamma_NNP ,_, or_CC Bernoulli_NNP distributions_NNS ._.
In_IN domains_NNS with_IN more_JJR than_IN one_CD relation_NN matrix_NN ,_, one_PRP could_MD fit_VB each_DT relation_NN separately_RB ;_: however_RB ,_, this_DT
ots_NNS of_IN the_DT equations_NNS -LCB-_-LRB- q_NN -LRB-_-LRB- Ui_NN ·_NN -RRB-_-RRB- -RCB-_-RRB- m_NN i_FW =_JJ 1_CD ,_, -LCB-_-LRB- q_NN -LRB-_-LRB- Vi_NN ·_NN -RRB-_-RRB- -RCB-_-RRB- n_NN i_LS =_JJ 1_CD ,_, and_CC -LCB-_-LRB- q_NN -LRB-_-LRB- Zi_NN ·_NN -RRB-_-RRB- -RCB-_-RRB- r_NN i_LS =_JJ 1_CD ._.
We_PRP derive_VBP the_DT Newton_NNP step_NN for_IN Ui_NNP ·_NNP ,_, U_NNP new_JJ i_FW ·_FW =_JJ Ui_FW ·_FW −_FW η_FW ·_FW q_FW -LRB-_-LRB- Ui_FW ·_FW -RRB-_-RRB- -LRB-_-LRB- q_FW ′_FW -LRB-_-LRB- Ui_FW ·_FW -RRB-_-RRB- -RRB-_-RRB- −_NN 1_CD ,_, -LRB-_-LRB- 7_CD -RRB-_-RRB- where_WRB we_PRP suggest_VBP using_VBG the_DT Armijo_NNP criterion_NN =_JJ -_: =[_NN 28_CD -RRB-_-RRB- -_: =_SYM -_: to_TO set_VB η_NN ._.
To_TO concisely_RB describe_VB the_DT Hessian_JJ we_PRP introduce_VBP terms_NNS for_IN the_DT contribution_NN of_IN the_DT regularizer_NN ,_, Gi_NN ≡_NN diag_NN -LRB-_-LRB- ∇_NN 2_CD G_NN ∗_NN -LRB-_-LRB- Ui_NN ·_NN -RRB-_-RRB- -RRB-_-RRB- ,_, Hi_FW ≡_FW diag_NN -LRB-_-LRB- ∇_NN 2_CD H_NN ∗_NN -LRB-_-LRB- Vi_NN ·_NN -RRB-_-RRB- -RRB-_-RRB- ,_, Ii_NN ≡_NN diag_NN -LRB-_-LRB- ∇_NN 2_CD I_NN ∗_NN -LRB-_-LRB- Zi_NN ·_NN -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC terms_NNS for_IN the_DT
ase_VB its_PRP$ membership_NN in_IN others_NNS clusters_NNS ._.
Examples_NNS of_IN matrix_NN and_CC relational_JJ co-clustering_NN include_VBP pLSI_NN ,_, pLSI-pHITS_NN ,_, the_DT symmetric_JJ block_NN models_NNS of_IN Long_NNP et_NNP ._.
al._FW -LRB-_-LRB- 23_CD ,_, 24_CD ,_, 25_CD -RRB-_-RRB- ,_, and_CC Bregman_NNP tensor_NN clustering_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- which_WDT can_MD handle_VB higher_JJR arity_NN relations_NNS -RRB-_-RRB- ._.
Matrix_NNP analogs_NNS of_IN factor_NN analysis_NN place_NN no_DT stochastic_JJ constraint_NN on_IN the_DT parameters_NNS ._.
Collective_JJ matrix_NN factorization_NN has_VBZ been_VBN presented_VBN using_VBG matrix_NN factor_NN
its_PRP$ membership_NN in_IN one_CD cluster_NN ,_, it_PRP must_MD decrease_VB its_PRP$ membership_NN in_IN others_NNS clusters_NNS ._.
Examples_NNS of_IN matrix_NN and_CC relational_JJ co-clustering_NN include_VBP pLSI_NN ,_, pLSI-pHITS_NN ,_, the_DT symmetric_JJ block_NN models_NNS of_IN Long_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 23_CD ,_, 24_CD ,_, 25_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC Bregman_NNP tensor_NN clustering_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- -LRB-_-LRB- which_WDT can_MD handle_VB higher_JJR arity_NN relations_NNS -RRB-_-RRB- ._.
Matrix_NNP analogs_NNS of_IN factor_NN analysis_NN place_NN no_DT stochastic_JJ constraint_NN on_IN the_DT parameters_NNS ._.
Collective_JJ matrix_NN factorization_NN has_VBZ
ave_NN that_IN Xij_NN is_VBZ drawn_VBN from_IN the_DT distribution_NN in_IN ψF_NN with_IN natural_JJ parameter_NN -LRB-_-LRB- UV_NN T_NN -RRB-_-RRB- ij_NN ._.
Decomposable_JJ losses_NNS ,_, which_WDT can_MD be_VB expressed_VBN as_IN the_DT sum_NN of_IN losses_NNS over_IN elements_NNS ,_, follows_VBZ from_IN matrix_NN exchangeability_NN =_JJ -_: =[_NN 2_CD ,_, 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
A_DT matrix_NN X_NN is_VBZ row-and-column_NN exchangeable_JJ if_IN permuting_VBG the_DT rows_NNS and_CC columns_NNS of_IN X_NN does_VBZ not_RB change_VB the_DT distribution_NN of_IN X_NN ._.
For_IN example_NN ,_, if_IN X_NN is_VBZ a_DT document-word_JJ matrix_NN of_IN counts_NNS ,_, the_DT relative_JJ position_NN
aints_NNS allow_VBP us_PRP to_TO place_VB methods_NNS like_IN non-negative_JJ matrix_NN factorization_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- or_CC matrix_NN co-clustering_NN into_IN our_PRP$ framework_NN ._.
-LRB-_-LRB- ii_LS -RRB-_-RRB- non-Bregman_JJ matrix_NN factorizations_NNS ,_, such_JJ as_IN max-margin_JJ matrix_NN factorization_NN =_JJ -_: =[_NN 30_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT can_MD immediately_RB take_VB advantage_NN of_IN the_DT large_JJ scale_NN optimization_NN techniques_NNS in_IN Sections_NNS 4-5_CD ;_: -LRB-_-LRB- iii_LS -RRB-_-RRB- row_NN and_CC column_NN biases_NNS ,_, where_WRB a_DT column_NN of_IN U_NN is_VBZ paired_VBN with_IN a_DT fixed_JJ ,_, constant_JJ column_NN in_IN V_NN -LRB-_-LRB- and_CC vi_LS
ss_RB ,_, with_IN an_DT orthogonality_NN constraint_NN on_IN the_DT shared_JJ factors_NNS ._.
Principal_NN components_NNS analysis_NN ,_, which_WDT factors_VBZ a_DT doubly_RB centered_JJ matrix_NN under_IN squared_VBN loss_NN ,_, has_VBZ also_RB been_VBN extended_VBN to_TO the_DT three-factor_JJ schema_NN =_JJ -_: =[_NN 36_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Another_DT interesting_JJ type_NN of_IN schema_NN contains_VBZ multiple_JJ parallel_JJ relations_NNS between_IN two_CD entity_NN types_NNS ._.
An_DT example_NN of_IN this_DT sort_NN of_IN schema_NN is_VBZ max-margin_JJ matrix_NN factorization_NN -LRB-_-LRB- MMMF_NN -RRB-_-RRB- -LRB-_-LRB- 30_CD -RRB-_-RRB- ._.
In_IN MMMF_NNP ,_, the_DT goal_NN
e_LS or_CC more_RBR related_JJ concepts_NNS -LRB-_-LRB- one_CD concept_NN per_IN row_NN -RRB-_-RRB- ,_, while_IN X_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- lists_VBZ the_DT features_NNS of_IN each_DT entity_NN ._.
An_DT example_NN of_IN a_DT supervised_JJ matrix_NN factorization_NN algorithm_NN is_VBZ the_DT support_NN vector_NN decomposition_NN machine_NN =_JJ -_: =[_NN 29_CD -RRB-_-RRB- -_: =_JJ -_: :_: in_IN SVDMs_NNS ,_, the_DT features_NNS X_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- are_VBP factored_JJ under_IN squared_VBN loss_NN ,_, while_IN the_DT labels_NNS X_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- are_VBP factored_JJ under_IN Hinge_NN loss_NN ._.
A_DT similar_JJ model_NN was_VBD proposed_VBN by_IN Zhu_NNP et_FW al._FW -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, using_VBG a_DT once-differentiable_JJ var_NN
loss_NN ,_, while_IN the_DT labels_NNS X_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- are_VBP factored_JJ under_IN Hinge_NN loss_NN ._.
A_DT similar_JJ model_NN was_VBD proposed_VBN by_IN Zhu_NNP et_FW al._FW -LRB-_-LRB- 37_CD -RRB-_-RRB- ,_, using_VBG a_DT once-differentiable_JJ variant_NN of_IN the_DT Hinge_NNP loss_NN ._.
Another_DT example_NN is_VBZ supervised_VBN LSI_NNP =_SYM -_: =[_NN 35_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT factors_NNS both_CC the_DT data_NNS and_CC label_NN matrices_NNS under_IN squared_VBN loss_NN ,_, with_IN an_DT orthogonality_NN constraint_NN on_IN the_DT shared_JJ factors_NNS ._.
Principal_NN components_NNS analysis_NN ,_, which_WDT factors_VBZ a_DT doubly_RB centered_JJ matrix_NN under_IN
ort_NN vector_NN decomposition_NN machine_NN -LRB-_-LRB- 29_CD -RRB-_-RRB- :_: in_IN SVDMs_NNS ,_, the_DT features_NNS X_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- are_VBP factored_JJ under_IN squared_VBN loss_NN ,_, while_IN the_DT labels_NNS X_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- are_VBP factored_JJ under_IN Hinge_NN loss_NN ._.
A_DT similar_JJ model_NN was_VBD proposed_VBN by_IN Zhu_NNP et_FW al._FW =_SYM -_: =[_NN 37_CD -RRB-_-RRB- -_: =_JJ -_: ,_, using_VBG a_DT once-differentiable_JJ variant_NN of_IN the_DT Hinge_NNP loss_NN ._.
Another_DT example_NN is_VBZ supervised_VBN LSI_NNP -LRB-_-LRB- 35_CD -RRB-_-RRB- ,_, which_WDT factors_NNS both_CC the_DT data_NNS and_CC label_NN matrices_NNS under_IN squared_VBN loss_NN ,_, with_IN an_DT orthogonality_NN constraint_NN on_IN
its_PRP$ membership_NN in_IN one_CD cluster_NN ,_, it_PRP must_MD decrease_VB its_PRP$ membership_NN in_IN others_NNS clusters_NNS ._.
Examples_NNS of_IN matrix_NN and_CC relational_JJ co-clustering_NN include_VBP pLSI_NN ,_, pLSI-pHITS_NN ,_, the_DT symmetric_JJ block_NN models_NNS of_IN Long_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 23_CD ,_, 24_CD ,_, 25_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC Bregman_NNP tensor_NN clustering_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- -LRB-_-LRB- which_WDT can_MD handle_VB higher_JJR arity_NN relations_NNS -RRB-_-RRB- ._.
Matrix_NNP analogs_NNS of_IN factor_NN analysis_NN place_NN no_DT stochastic_JJ constraint_NN on_IN the_DT parameters_NNS ._.
Collective_JJ matrix_NN factorization_NN has_VBZ
ution_NN in_IN ψF_NN is_VBZ uniquely_RB identified_VBN by_IN its_PRP$ natural_JJ parameters_NNS ._.
For_IN regular_JJ exponential_JJ families_NNS log_VBP pF_NN -LRB-_-LRB- x_NN |_CD θ_NN -RRB-_-RRB- =_JJ log_NN p0_NN -LRB-_-LRB- x_NN -RRB-_-RRB- +_CC F_NN ∗_NN -LRB-_-LRB- x_NN -RRB-_-RRB- −_FW DF_FW ∗_NN -LRB-_-LRB- x_NN |_FW |_FW f_FW -LRB-_-LRB- θ_NN -RRB-_-RRB- -RRB-_-RRB- where_WRB the_DT matching_JJ prediction_NN link_NN is_VBZ f_LS -LRB-_-LRB- θ_NN -RRB-_-RRB- =_JJ ∇_NN F_NN -LRB-_-LRB- θ_NN -RRB-_-RRB- =_JJ -_: =[_NN 15_CD ,_, 4_CD ,_, 14_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Minimizing_VBG a_DT Bregman_NNP divergence_NN under_IN a_DT matching_JJ link_NN is_VBZ equivalent_JJ to_TO maximum_NN likelihood_NN for_IN the_DT corresponding_JJ exponential_JJ family_NN distribution_NN ._.
The_DT relationship_NN between_IN matrix_NN factorization_NN and_CC exp_NN
where_WRB A_DT ◦_NN B_NN is_VBZ the_DT matrix_NN dot_NN product_NN tr_NN -LRB-_-LRB- A_NN T_NN B_NN -RRB-_-RRB- =_JJ P_NN ij_FW AijBij_FW and_CC F_NN ∗_NN is_VBZ the_DT convex_JJ dual_JJ F_NN ∗_NN -LRB-_-LRB- µ_NN -RRB-_-RRB- =_JJ sup_NN θ_NN ∈_CD dom_NN F_NN -LRB-_-LRB- 〈_FW θ_FW ,_, µ_FW 〉_FW −_FW F_NN -LRB-_-LRB- θ_NN -RRB-_-RRB- -RRB-_-RRB- ._.
If_IN F_NN ∗_NN is_VBZ differentiable_JJ ,_, this_DT is_VBZ equivalent_JJ to_TO the_DT standard_JJ definition_NN =_JJ -_: =[_NN 10_CD ,_, 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, except_IN that_IN the_DT standard_JJ definition_NN uses_VBZ arguments_NNS Z_NN and_CC ∇_NN F_NN ∗_NN -LRB-_-LRB- Y_NN -RRB-_-RRB- instead_RB of_IN Z_NN and_CC Y_NN ._.
If_IN F_NN decomposes_VBZ into_IN a_DT sum_NN over_IN components_NNS of_IN Z_NN ,_, we_PRP can_MD define_VB a_DT weighted_JJ ~_NN ajit\/cmf_NN ._.
A_DT longer_JJR version_NN of_IN the_DT p_NN
orization_NN ._.
Other_JJ regularizers_NNS have_VBP been_VBN proposed_VBN specifically_RB for_IN factorization_NN ;_: for_IN example_NN ,_, the_DT trace_NN norm_NN of_IN UV_NN T_NN ,_, the_DT sum_NN of_IN its_PRP$ singular_JJ values_NNS ,_, has_VBZ been_VBN proposed_VBN as_IN a_DT continuous_JJ proxy_NN for_IN rank_NN =_JJ -_: =[_NN 33_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN clarity_NN ,_, we_PRP treat_VBP hard_JJ constraints_NNS C_NN separately_RB from_IN regularizers_NNS ._.
Examples_NNS of_IN hard_JJ constraints_NNS include_VBP orthogonality_NN ;_: stochasticity_NN of_IN rows_NNS ,_, columns_NNS ,_, or_CC blocks_NNS -LRB-_-LRB- for_IN example_NN ,_, in_IN matrix_NN co-cluste_NN
would_MD be_VB L2_NN -LRB-_-LRB- V_NN ,_, Z_NN |_FW ˜_FW W_NN -RRB-_-RRB- =_JJ DF2_NN -LRB-_-LRB- V_NN Z_NN T_NN |_FW |_FW Y_NN ,_, ˜_NN W_NN -RRB-_-RRB- +_CC DH_NN -LRB-_-LRB- 0_CD |_FW |_FW V_NN -RRB-_-RRB- +_CC DI_NN -LRB-_-LRB- 0_CD |_FW |_FW Z_NN -RRB-_-RRB- ._.
Since_IN V_NN is_VBZ a_DT shared_JJ factor_NN we_PRP average_VBP the_DT losses_NNS :_: L_NN -LRB-_-LRB- U_NN ,_, V_NN ,_, Z_NNP |_NNP W_NNP ,_, ˜_NNP W_NNP -RRB-_-RRB- =_JJ αL1_NN -LRB-_-LRB- U_NN ,_, V_NN |_NN W_NN -RRB-_-RRB- +_CC -LRB-_-LRB- 1_CD −_FW α_FW -RRB-_-RRB- L2_NN -LRB-_-LRB- V_NN ,_, Z_NN |_FW ˜_FW W_NN -RRB-_-RRB- ,_, -LRB-_-LRB- 3_CD -RRB-_-RRB- where_WRB α_FW ∈_FW =_SYM -_: =[_NN 0_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: weights_NNS the_DT relative_JJ importance_NN of_IN relations_NNS ._.
Each_DT term_NN in_IN the_DT loss_NN ,_, L1_NN and_CC L2_NN ,_, is_VBZ decomposable_JJ and_CC twice-differentiable_JJ ,_, which_WDT is_VBZ all_DT that_WDT is_VBZ required_VBN for_IN the_DT alternating_VBG projections_NNS technique_NN descr_NN
its_PRP$ membership_NN in_IN one_CD cluster_NN ,_, it_PRP must_MD decrease_VB its_PRP$ membership_NN in_IN others_NNS clusters_NNS ._.
Examples_NNS of_IN matrix_NN and_CC relational_JJ co-clustering_NN include_VBP pLSI_NN ,_, pLSI-pHITS_NN ,_, the_DT symmetric_JJ block_NN models_NNS of_IN Long_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 23_CD ,_, 24_CD ,_, 25_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC Bregman_NNP tensor_NN clustering_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- -LRB-_-LRB- which_WDT can_MD handle_VB higher_JJR arity_NN relations_NNS -RRB-_-RRB- ._.
Matrix_NNP analogs_NNS of_IN factor_NN analysis_NN place_NN no_DT stochastic_JJ constraint_NN on_IN the_DT parameters_NNS ._.
Collective_JJ matrix_NN factorization_NN has_VBZ
minimizing_VBG squared_VBN error_NN with_IN an_DT identity_NN link_NN yields_VBZ the_DT singular_JJ value_NN decomposition_NN -LRB-_-LRB- corresponding_VBG to_TO a_DT Gaussian_JJ error_NN model_NN -RRB-_-RRB- ,_, while_IN other_JJ choices_NNS extend_VBP generalized_JJ linear_JJ models_NNS -LRB-_-LRB- 26_CD -RRB-_-RRB- to_TO matrices_NNS =_JJ -_: =[_NN 14_CD ,_, 17_CD -RRB-_-RRB- -_: =_JJ -_: and_CC lead_VB to_TO error_NN models_NNS such_JJ as_IN Poisson_NNP ,_, Gamma_NNP ,_, or_CC Bernoulli_NNP distributions_NNS ._.
In_IN domains_NNS with_IN more_JJR than_IN one_CD relation_NN matrix_NN ,_, one_PRP could_MD fit_VB each_DT relation_NN separately_RB ;_: however_RB ,_, this_DT approach_NN would_MD not_RB ta_VB
is_VBZ possible_JJ to_TO traverse_VB links_NNS from_IN any_DT entity_NN type_NN to_TO any_DT other_JJ ;_: if_IN not_RB ,_, we_PRP can_MD fit_VB each_DT connected_JJ component_NN in_IN the_DT schema_NN separately_RB ._.
This_DT corresponds_VBZ to_TO a_DT fully_RB connected_VBN entity-relationship_NN model_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP fit_VBP each_DT relation_NN matrix_NN as_IN the_DT product_NN of_IN latent_JJ factors_NNS ,_, X_NN -LRB-_-LRB- ij_NN -RRB-_-RRB- ≈_FW f_FW -LRB-_-LRB- ij_NN -RRB-_-RRB- -LRB-_-LRB- U_NN -LRB-_-LRB- i_LS -RRB-_-RRB- -LRB-_-LRB- U_NN -LRB-_-LRB- j_NN -RRB-_-RRB- -RRB-_-RRB- T_NN -RRB-_-RRB- ,_, where_WRB U_NN -LRB-_-LRB- i_LS -RRB-_-RRB- ∈_NN R_NN ni_FW ×_FW kij_NN and_CC U_NN -LRB-_-LRB- j_NN -RRB-_-RRB- ∈_NN R_NN nj_FW ×_FW kij_FW for_IN kij_FW ∈_FW -LCB-_-LRB- 1_CD ,_, 2_CD ,_, ..._: -RCB-_-RRB- ._.
Unless_IN otherwise_RB noted_VBN ,_, the_DT p_NN
ution_NN in_IN ψF_NN is_VBZ uniquely_RB identified_VBN by_IN its_PRP$ natural_JJ parameters_NNS ._.
For_IN regular_JJ exponential_JJ families_NNS log_VBP pF_NN -LRB-_-LRB- x_NN |_CD θ_NN -RRB-_-RRB- =_JJ log_NN p0_NN -LRB-_-LRB- x_NN -RRB-_-RRB- +_CC F_NN ∗_NN -LRB-_-LRB- x_NN -RRB-_-RRB- −_FW DF_FW ∗_NN -LRB-_-LRB- x_NN |_FW |_FW f_FW -LRB-_-LRB- θ_NN -RRB-_-RRB- -RRB-_-RRB- where_WRB the_DT matching_JJ prediction_NN link_NN is_VBZ f_LS -LRB-_-LRB- θ_NN -RRB-_-RRB- =_JJ ∇_NN F_NN -LRB-_-LRB- θ_NN -RRB-_-RRB- =_JJ -_: =[_NN 15_CD ,_, 4_CD ,_, 14_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Minimizing_VBG a_DT Bregman_NNP divergence_NN under_IN a_DT matching_JJ link_NN is_VBZ equivalent_JJ to_TO maximum_NN likelihood_NN for_IN the_DT corresponding_JJ exponential_JJ family_NN distribution_NN ._.
The_DT relationship_NN between_IN matrix_NN factorization_NN and_CC exp_NN
ediction_NN errors_NNS ._.
One_CD approach_NN to_TO reducing_VBG this_DT cost_NN is_VBZ to_TO compute_VB errors_NNS only_RB on_IN a_DT subset_NN of_IN observed_VBN relations_NNS ,_, picked_VBD randomly_RB at_IN each_DT iteration_NN ._.
This_DT technique_NN is_VBZ known_VBN as_IN stochastic_JJ approximation_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT best-known_JJ stochastic_JJ approximation_NN algorithm_NN is_VBZ stochastic_JJ gradient_NN descent_NN ;_: but_CC ,_, since_IN inverting_VBG the_DT Hessian_JJ is_VBZ not_RB a_DT significant_JJ part_NN of_IN our_PRP$ computational_JJ cost_NN ,_, we_PRP will_MD recommend_VB a_DT stochastic_JJ
lgorithm_NN ,_, which_WDT need_MD not_RB store_VB all_PDT the_DT data_NNS in_IN memory_NN ._.
As_IN mentioned_VBN above_RB ,_, we_PRP can_MD often_RB improve_VB the_DT rate_NN of_IN convergence_NN by_IN moving_VBG from_IN stochastic_JJ gradient_NN descent_NN to_TO stochastic_JJ Newton-Raphson_NNP updates_NNS =_JJ -_: =[_NN 7_CD ,_, 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN the_DT threefactor_JJ model_NN the_DT stochastic_JJ Hessians_NNPS are_VBP grows_VBZ linearly_RB in_IN the_DT number_NN of_IN entities_NNS related_VBN to_TO x_NN -LRB-_-LRB- i_LS -RRB-_-RRB- r_NN where_WRB ˆq_FW ′_FW τ_NN -LRB-_-LRB- Ui_NN ·_NN -RRB-_-RRB- =_JJ αV_NN T_NN s_NN ·_FW ˆ_FW D1_NN ,_, iVs_NN ·_NN +_CC Gi_NN ,_, ˆq_FW ′_FW τ_FW -LRB-_-LRB- Zi_FW ·_FW -RRB-_-RRB- =_JJ -LRB-_-LRB- 1_CD −_FW α_FW -RRB-_-RRB- V_NN T_NN s_NN ·_FW ˆ_FW D4_NN ,_, iVs_FW ·_FW
-LRB-_-LRB- 1_LS -RRB-_-RRB- The_DT loss_NN D_NN -LRB-_-LRB- ·_NN ,_, ·_NN -RRB-_-RRB- quantifies_VBZ ≈_NN in_IN the_DT model_NN ._.
It_PRP is_VBZ typically_RB convex_VBN in_IN its_PRP$ second_JJ argument_NN ,_, and_CC often_RB decomposes_VBZ into_IN a_DT weighted_JJ sum_NN over_IN the_DT elements_NNS of_IN X._NNP For_IN example_NN ,_, the_DT loss_NN for_IN weighted_JJ SVD_NN =_JJ -_: =[_NN 32_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ DW_NN -LRB-_-LRB- X_NN ,_, UV_NN T_NN -RRB-_-RRB- =_JJ |_FW |_FW W_NN ⊙_NN -LRB-_-LRB- X_NN −_NN UV_NN T_NN -RRB-_-RRB- |_FW |_FW 2_CD F_NN ro_NN ,_, where_WRB ⊙_NN denotes_VBZ the_DT element-wise_JJ product_NN of_IN matrices_NNS ._.
Prediction_NN links_VBZ f_FW allow_VB nonlinear_JJ relationships_NNS between_IN UV_NN T_NN and_CC the_DT data_NNS X._NNP The_NNP choices_NNS of_IN f_FW and_CC
ons_JJ approach_NN is_VBZ conceptually_RB simple_JJ ,_, and_CC allows_VBZ one_CD to_TO take_VB advantage_NN of_IN decomposability_NN ,_, there_EX is_VBZ a_DT panoply_NN of_IN alternatives_NNS for_IN factoring_VBG a_DT single_JJ matrix_NN ._.
The_DT more_RBR popular_JJ ones_NNS includes_VBZ majorization_NN =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT iteratively_RB minimize_VBP a_DT sequence_NN of_IN convex_NN upper_JJ bounding_VBG functions_NNS tangent_JJ to_TO the_DT objective_NN ,_, including_VBG the_DT multiplicative_JJ update_VB for_IN NMF_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- and_CC the_DT EM_NNP algorithm_NN ,_, which_WDT is_VBZ used_VBN both_DT for_IN pLSI_NN
ave_NN that_IN Xij_NN is_VBZ drawn_VBN from_IN the_DT distribution_NN in_IN ψF_NN with_IN natural_JJ parameter_NN -LRB-_-LRB- UV_NN T_NN -RRB-_-RRB- ij_NN ._.
Decomposable_JJ losses_NNS ,_, which_WDT can_MD be_VB expressed_VBN as_IN the_DT sum_NN of_IN losses_NNS over_IN elements_NNS ,_, follows_VBZ from_IN matrix_NN exchangeability_NN =_JJ -_: =[_NN 2_CD ,_, 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
A_DT matrix_NN X_NN is_VBZ row-and-column_NN exchangeable_JJ if_IN permuting_VBG the_DT rows_NNS and_CC columns_NNS of_IN X_NN does_VBZ not_RB change_VB the_DT distribution_NN of_IN X_NN ._.
For_IN example_NN ,_, if_IN X_NN is_VBZ a_DT document-word_JJ matrix_NN of_IN counts_NNS ,_, the_DT relative_JJ position_NN
k_NN prediction_NN method_NN ,_, we_PRP choose_VBP a_DT measure_NN that_IN favors_NNS neither_CC inherently_RB :_: ranking_NN ._.
We_PRP induce_VBP a_DT ranking_NN of_IN movies_NNS for_IN each_DT user_NN ,_, measuring_VBG the_DT quality_NN of_IN the_DT ranking_NN using_VBG mean_JJ average_JJ precision_NN -LRB-_-LRB- MAP_NN -RRB-_-RRB- =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_JJ -_: :_: queries_NNS correspond_VBP to_TO user_NN 's_POS requests_NNS for_IN ratings_NNS ,_, ``_`` relevant_JJ ''_'' items_NNS are_VBP the_DT movies_NNS of_IN the_DT held-out_JJ links_NNS ,_, we_PRP use_VBP only_RB the_DT top_JJ 200_CD movies_NNS in_IN each_DT ranking_NN 2_CD ,_, and_CC the_DT averaging_NN is_VBZ over_IN users_NNS ._.
Most_JJS mov_NN
tasks_NNS :_: -LRB-_-LRB- i_LS -RRB-_-RRB- predicting_VBG whether_IN a_DT user_NN rated_VBD a_DT particular_JJ movie_NN :_: israted_VBN ;_: and_CC -LRB-_-LRB- ii_LS -RRB-_-RRB- predicting_VBG the_DT value_NN of_IN a_DT rating_NN for_IN a_DT particular_JJ movie_NN :_: rating_NN ._.
User_NN ratings_NNS are_VBP sampled_VBN from_IN the_DT Netflix_NNP Prize_NNP data_NN =_JJ -_: =[_NN 27_CD -RRB-_-RRB- -_: =_JJ -_: :_: a_DT rating_NN can_MD be_VB viewed_VBN as_IN a_DT relation_NN taking_VBG on_RP five_CD ordinal_JJ values_NNS -LRB-_-LRB- 1-5_CD stars_NNS -RRB-_-RRB- ,_, i.e._FW ,_, Rating_NNP -LRB-_-LRB- user_NN ,_, movie_NN -RRB-_-RRB- ._.
We_PRP augment_VBP these_DT ratings_NNS with_IN two_CD additional_JJ sources_NNS of_IN movie_NN information_NN ,_, from_IN the_DT Interne_NNP
cific_JJ to_TO squared_VBN loss_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, while_IN later_JJ generalizations_NNS to_TO regular_JJ exponential_JJ families_NNS -LRB-_-LRB- 25_CD -RRB-_-RRB- use_NN EM_NN ._.
An_DT equivalent_JJ formulation_NN in_IN terms_NNS of_IN regular_JJ Bregman_NNP divergences_NNS -LRB-_-LRB- 24_CD -RRB-_-RRB- uses_VBZ iterative_JJ majorization_NN =_JJ -_: =[_NN 22_CD ,_, 34_CD -RRB-_-RRB- -_: =_SYM -_: as_IN the_DT inner_JJ loop_NN of_IN alternating_VBG projection_NN ._.
An_DT improvement_NN on_IN Bregman_NNP co-clustering_JJ accounts_NNS for_IN systematic_JJ biases_NNS ,_, block_NN effects_NNS ,_, in_IN the_DT matrix_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
The_DT three-factor_JJ schema_NN E1_NN ∼_NN E2_NN ∼_NN E3_NN also_RB includ_VBD
-LRB-_-LRB- Zij_NN -RRB-_-RRB- +_CC F_NN ∗_NN -LRB-_-LRB- Yij_NN -RRB-_-RRB- −_FW YijZij_FW -RRB-_-RRB- ._.
Examples_NNS include_VBP weighted_JJ versions_NNS of_IN squared_VBN loss_NN ,_, F_NN -LRB-_-LRB- x_NN -RRB-_-RRB- =_JJ x_NN 2_CD ,_, and_CC I-divergence_NN ,_, F_NN -LRB-_-LRB- x_NN -RRB-_-RRB- =_JJ x_NN log_NN x_NN −_FW x._FW Our_PRP$ primary_JJ focus_NN is_VBZ on_IN decomposable_JJ regular_JJ Bregman_NNP divergences_NNS =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT correspond_VBP to_TO maximum_JJ likelihood_NN in_IN exponential_JJ families_NNS :_: Definition_NN 2_CD ._.
A_DT parametric_JJ family_NN of_IN distributions_NNS ψF_NN =_JJ -LCB-_-LRB- pF_NN -LRB-_-LRB- x_NN |_CD θ_NN -RRB-_-RRB- :_: θ_NN -RCB-_-RRB- is_VBZ a_DT regular_JJ exponential_JJ family_NN if_IN each_DT density_NN has_VBZ the_DT form_NN lo_FW
where_WRB A_DT ◦_NN B_NN is_VBZ the_DT matrix_NN dot_NN product_NN tr_NN -LRB-_-LRB- A_NN T_NN B_NN -RRB-_-RRB- =_JJ P_NN ij_FW AijBij_FW and_CC F_NN ∗_NN is_VBZ the_DT convex_JJ dual_JJ F_NN ∗_NN -LRB-_-LRB- µ_NN -RRB-_-RRB- =_JJ sup_NN θ_NN ∈_CD dom_NN F_NN -LRB-_-LRB- 〈_FW θ_FW ,_, µ_FW 〉_FW −_FW F_NN -LRB-_-LRB- θ_NN -RRB-_-RRB- -RRB-_-RRB- ._.
If_IN F_NN ∗_NN is_VBZ differentiable_JJ ,_, this_DT is_VBZ equivalent_JJ to_TO the_DT standard_JJ definition_NN =_JJ -_: =[_NN 10_CD ,_, 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, except_IN that_IN the_DT standard_JJ definition_NN uses_VBZ arguments_NNS Z_NN and_CC ∇_NN F_NN ∗_NN -LRB-_-LRB- Y_NN -RRB-_-RRB- instead_RB of_IN Z_NN and_CC Y_NN ._.
If_IN F_NN decomposes_VBZ into_IN a_DT sum_NN over_IN components_NNS of_IN Z_NN ,_, we_PRP can_MD define_VB a_DT weighted_JJ ~_NN ajit\/cmf_NN ._.
A_DT longer_JJR version_NN of_IN the_DT p_NN
._.
2.1_CD Bregman_NNP Divergences_NNP A_NNP large_JJ class_NN of_IN matrix_NN factorization_NN algorithms_NNS restrict_VBP D_NN to_TO generalized_JJ Bregman_NNP divergences_NNS :_: e.g._FW ,_, singular_JJ value_NN decomposition_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- and_CC non-negative_JJ matrix_NN factorization_NN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Definition_NN 1_CD -LRB-_-LRB- -LRB-_-LRB- 17_CD -RRB-_-RRB- -RRB-_-RRB- ._.
For_IN a_DT closed_JJ ,_, proper_JJ ,_, convex_JJ function_NN F_NN :_: R_NN m_NN ×_CD n_NN →_NN R_NN ,_, the_DT generalized_JJ Bregman_NNP divergence_NN between_IN matrices_NNS Z_NN and_CC Y_NN is_VBZ DF_NN -LRB-_-LRB- Z_NN |_FW |_FW Y_NN -RRB-_-RRB- =_JJ F_NN -LRB-_-LRB- Z_NN -RRB-_-RRB- +_CC F_NN ∗_NN -LRB-_-LRB- Y_NN -RRB-_-RRB- −_CD Y_NN ◦_NN Z_NN where_WRB A_DT ◦_NN B_NN is_VBZ the_DT matr_NN
r_NN methods_NNS ,_, such_JJ as_IN the_DT fast_JJ variant_NN of_IN maxmargin_NN matrix_NN factorization_NN -LRB-_-LRB- 30_CD -RRB-_-RRB- ._.
The_DT next_JJ level_NN of_IN generality_NN is_VBZ a_DT three-entity-type_JJ model_NN E1_NN ∼_NN E2_NN ∼_NN E3_NN ._.
A_DT well-known_JJ example_NN of_IN such_PDT a_DT schema_NN is_VBZ pLSI-pHITS_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT models_NNS document-word_JJ counts_NNS and_CC document-document_JJ citations_NNS :_: E1_NN =_JJ words_NNS and_CC E2_NN =_JJ E3_NN =_JJ documents_NNS ,_, but_CC it_PRP is_VBZ trivial_JJ to_TO allow_VB E2_NN ̸_NN =_JJ E3_NN ._.
Given_VBN relations_NNS E1_NN ∼_NN E2_NN and_CC E2_NN ∼_NN E3_NN ,_, with_IN corresponding_VBG in_IN
