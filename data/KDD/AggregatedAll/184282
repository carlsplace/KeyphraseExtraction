Random_JJ projection_NN in_IN dimensionality_NN reduction_NN :_: applications_NNS to_TO image_NN and_CC text_NN data_NNS
Random_JJ projections_NNS have_VBP recently_RB emerged_VBN as_IN a_DT powerful_JJ method_NN for_IN dimensionality_NN reduction_NN ._.
Theoretical_JJ results_NNS indicate_VBP that_IN the_DT method_NN preserves_VBZ distances_NNS quite_RB nicely_RB ;_: however_RB ,_, empirical_JJ results_NNS are_VBP sparse_JJ ._.
We_PRP present_VBP experimental_JJ results_NNS on_IN using_VBG random_JJ projection_NN as_IN a_DT dimensionality_NN reduction_NN tool_NN in_IN a_DT number_NN of_IN cases_NNS ,_, where_WRB the_DT high_JJ dimensionality_NN of_IN the_DT data_NNS would_MD otherwise_RB lead_VB to_TO burden-some_JJ computations_NNS ._.
Our_PRP$ application_NN areas_NNS are_VBP the_DT processing_NN of_IN both_CC noisy_JJ and_CC noiseless_JJ images_NNS ,_, and_CC information_NN retrieval_NN in_IN text_NN documents_NNS ._.
We_PRP show_VBP that_IN projecting_VBG the_DT data_NNS onto_IN a_DT random_JJ lower-dimensional_JJ subspace_NN yields_NNS results_VBZ comparable_JJ to_TO conventional_JJ dimensionality_NN reduction_NN methods_NNS such_JJ as_IN principal_JJ component_NN analysis_NN :_: the_DT similarity_NN of_IN data_NN vectors_NNS is_VBZ preserved_VBN well_RB under_IN random_JJ projection_NN ._.
However_RB ,_, using_VBG random_JJ projections_NNS is_VBZ computationally_RB significantly_RB less_RBR expensive_JJ than_IN using_VBG ,_, e.g._FW ,_, principal_JJ component_NN analysis_NN ._.
We_PRP also_RB show_VBP experimentally_RB that_IN using_VBG a_DT sparse_JJ random_JJ matrix_NN gives_VBZ additional_JJ computational_JJ savings_NNS in_IN random_JJ projection_NN ._.
a_DT exchanged_VBN between_IN parties_NNS in_IN order_NN to_TO achieve_VB secure_JJ clustering_NN ._.
Dimensionality_NN reduction_NN techniques_NNS have_VBP been_VBN studied_VBN in_IN the_DT context_NN of_IN pattern_NN recognition_NN -LRB-_-LRB- Fukunaga_NNP ,_, 1990_CD -RRB-_-RRB- ,_, information_NN retrieval_NN -LRB-_-LRB- =_JJ -_: =_JJ Bingham_NNP &_CC Mannila_NNP ,_, 2001_CD -_: =_JJ -_: ;_: Faloutsos_NNP &_CC Lin_NNP ,_, 1995_CD ;_: Jagadish_NNP ,_, 1991_CD -RRB-_-RRB- ,_, and_CC data_NN mining_NN -LRB-_-LRB- Fern_NNP &_CC Brodley_NNP ,_, 2003_CD ;_: Faloutsos_NNP &_CC Lin_NNP ,_, 1995_CD -RRB-_-RRB- ._.
To_TO the_DT best_JJS of_IN our_PRP$ knowledge_NN ,_, dimensionality_NN reduction_NN has_VBZ not_RB been_VBN used_VBN in_IN the_DT context_NN of_IN data_NNS
ture_NN modality_NN are_VBP concatenated_VBN ,_, resulting_VBG in_IN an_DT even_RB larger_JJR vector_NN ._.
Since_IN the_DT spill_NN tree_NN requires_VBZ a_DT low_JJ dimensional_JJ representation_NN ,_, the_DT shot_NN vectors_NNS are_VBP reduced_VBN to_TO 100_CD dimensions_NNS by_IN random_JJ projection_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Random_JJ projection_NN is_VBZ simple_JJ ,_, fast_RB ,_, and_CC is_VBZ known_VBN to_TO preserve_VB neighborhood_NN structure_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- making_VBG it_PRP a_DT good_JJ choice_NN for_IN preprocessing_VBG nearest_JJS neighbor_NN data_NNS ._.
The_DT complete_JJ procedure_NN for_IN preparing_VBG the_DT neare_NN
iews_NNS of_IN the_DT object_NN densely_RB and_CC learning_VBG the_DT manifold_NN formed_VBN by_IN the_DT samples_NNS ._.
Learning_NNP employs_VBZ the_DT EM_NN algorithm_NN -LRB-_-LRB- 27_CD -RRB-_-RRB- and_CC takes_VBZ place_NN in_IN a_DT ``_`` universal_JJ ,_, ''_'' lower-dimensional_JJ ,_, space_NN computed_VBD through_IN RP_NN -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 29_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT purpose_NN of_IN the_DT first_JJ stage_NN is_VBZ to_TO generate_VB rough_JJ hypothetical_JJ matches_NNS between_IN the_DT models_NNS and_CC the_DT scene_NN fast_RB ,_, while_IN keeping_VBG the_DT space_NN requirements_NNS of_IN indexing_NN low_JJ ._.
To_TO account_VB for_IN indexing_VBG a_DT sparse_NN
input_NN space_NN is_VBZ used_VBN to_TO compute_VB distance_NN between_IN instances_NNS ._.
RP_NN :_: The_DT data_NN is_VBZ first_RB projected_VBN into_IN a_DT lower_JJR dimensional_JJ space_NN of_IN dimension_NN d_NN ′_NN =_JJ log_NN n_NN ɛ_NN 2_CD log_NN 1_CD ɛ_NN using_VBG the_DT Random_JJ Projection_NN -LRB-_-LRB- RP_NN -RRB-_-RRB- method_NN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP set_VBD A_NN =_JJ R_NN ⊤_NN R_NN ,_, where_WRB R_NN is_VBZ the_DT projection_NN matrix_NN used_VBN by_IN RP_NN ._.
ɛ_NN was_VBD set_VBN to_TO 0.25_CD for_IN the_DT experiments_NNS in_IN Section_NN 6.1_CD ._.
Datasets_NNP Original_NNP RP_NNP PCA_NNP ITML_NNP LMNN_NNP IDML-LM_NNP IDML-IT_NNP Amazon_NNP 0.4046_CD 0.3964_CD 0.1554_CD 0_CD ._.
pets_NNS ._.
However_RB ,_, it_PRP may_MD prove_VB less_RBR efficient_JJ in_IN identifying_VBG topics_NNS represented_VBN by_IN relatively_RB small_JJ groups_NNS of_IN documents_NNS ._.
There_EX also_RB exists_VBZ a_DT class_NN of_IN decomposition_NN techniques_NNS based_VBN on_IN random_JJ projections_NNS =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Even_RB though_IN these_DT decompositions_NNS fairly_RB well_RB preserve_VB distances_NNS and_CC similarities_NNS between_IN vectors_NNS ,_, they_PRP are_VBP of_IN little_JJ use_NN in_IN our_PRP$ approach_NN ._.
The_DT reasons172_NNP S._NNP Osinski_NNP is_VBZ that_IN they_PRP rely_VBP on_IN randomly_RB gene_NN
nough_JJ ._.
1.3_CD Random_NNP Projections_NNPS for_IN Estimating_VBG l2_NN Distances_NNS The_DT method_NN of_IN -LRB-_-LRB- normal_JJ -RRB-_-RRB- random_JJ projections_NNS -LRB-_-LRB- 26_CD -RRB-_-RRB- has_VBZ become_VBN a_DT standard_JJ technique_NN for_IN efficiently_RB computing_VBG the_DT l2_NN distances_NNS in_IN machine_NN learning_NN =_JJ -_: =[_NN 3_CD ,_, 8_CD ,_, 21_CD ,_, 6_CD ,_, 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT idea_NN is_VBZ to_TO multiply_VB the_DT original_JJ data_NNS matrix_NN A_NNP ∈_NNP Rn_NNP ×_NNP D_NNP by_IN a_DT random_JJ matrix_NN R_NNP ∈_NNP RD_NNP ×_NNP k_NN ,_, resulting_VBG in_IN a_DT much_RB smaller_JJR matrix_NN B_NN =_JJ A_NN ×_NN R_NN ∈_FW Rn_FW ×_FW k_NN ._.
Entries_NNS of_IN R_NN are_VBP sampled_VBN i.i.d._NN from_IN the_DT standard_JJ normal_JJ
bedding_NN exhibited_VBD in_IN -LRB-_-LRB- 8_CD -RRB-_-RRB- consists_VBZ in_IN random_JJ projections_NNS from_IN Rd_NNP into_IN Rd_NNP ′_NNP ,_, represented_VBN by_IN matrices_NNS d_FW ′_FW ×_FW d_FW with_IN random_JJ orthonormal_JJ vectors_NNS ._.
Similar_JJ results_NNS may_MD be_VB obtained_VBN by_IN using_VBG simpler_JJR embeddings_NNS =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, represented_VBN through_IN random_JJ d_FW ′_FW ×_FW d_FW matrices_NNS P_NN =_JJ 1_CD \/_: √_FW d_FW ′_NN -LRB-_-LRB- rij_NN -RRB-_-RRB- ,_, where_WRB rij_NN are_VBP random_JJ variables_NNS such_JJ that_IN :_: E_NN -LRB-_-LRB- rij_NN -RRB-_-RRB- =_JJ 0_CD ,_, V_NN ar_NN -LRB-_-LRB- rij_NN -RRB-_-RRB- =_JJ 1_CD For_IN sake_NN of_IN simplicity_NN ,_, we_PRP call_VBP random_JJ projections_NNS even_RB this_DT kind_NN
t_NN P_NN =_JJ I_NN ,_, where_WRB I_PRP is_VBZ the_DT d_FW ×_FW d_FW identity_NN matrix_NN ._.
Hence_RB ,_, the_DT data_NN is_VBZ not_RB transformed_VBN in_IN this_DT case_NN ._.
RP_NN :_: The_DT data_NN is_VBZ first_RB projected_VBN into_IN a_DT lower_JJR dimensional_JJ space_NN using_VBG the_DT Random_JJ Projection_NN -LRB-_-LRB- RP_NN -RRB-_-RRB- method_NN -LRB-_-LRB- =_JJ -_: =_JJ Bingham_NNP and_CC Mannila_NNP ,_, 2001_CD -_: =--RRB-_NN ._.
Dimensionality_NN of_IN the_DT target_NN space_NN was_VBD set_VBN at_IN d_FW ′_FW =_JJ log_NN n_NN ɛ2log_NN 1_CD ɛ_NN ,_, as_IN prescribed_VBN in_IN -LRB-_-LRB- Bingham_NNP and_CC Mannila_NNP ,_, 2001_CD -RRB-_-RRB- ._.
We_PRP use_VBP the_DT projection_NN matrix_NN constructed_VBN by_IN RP_NN as_IN P_NN ._.
ɛ_NN was_VBD set_VBN to_TO 0.25_CD for_IN the_DT e_SYM
us_PRP not_RB only_RB with_IN interesting_JJ theoretical_JJ results_NNS but_CC also_RB with_IN useful_JJ practical_JJ tools_NNS ._.
Experimental_JJ papers_NNS which_WDT use_VBP random_JJ projections_NNS deal_VBP with_IN :_: information_NN retrieval_NN for_IN text_NN documents_NNS and_CC images_NNS =_JJ -_: =[_NN 51_CD -RRB-_-RRB- -_: =_JJ -_: ,_, learning_VBG Gaussian_JJ mixture_NN models_NNS -LRB-_-LRB- 52_CD -RRB-_-RRB- ,_, data_NNS mining_NN and_CC PCA_NN -LRB-_-LRB- 53_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 27_CD -RRB-_-RRB- to_TO name_VB just_RB a_DT few_JJ ._.
Although_IN many_JJ consider_VBP the_DT usage_NN of_IN random_JJ projections_NNS ,_, they_PRP do_VBP not_RB consider_VB the_DT differences_NNS between_IN diffe_NN
gonal_JJ vectors_NNS ._.
For_IN this_DT reason_NN creating_VBG extremely_RB sparse_JJ vectors_NNS with_IN only_RB 1_CD :_: s_NN and_CC −_NN 1_CD :_: s_NN would_MD produce_VB approximately_RB sparse_JJ vectors_NNS ,_, given_VBN that_IN the_DT dimensionality_NN of_IN the_DT vectors_NNS is_VBZ sufficiently_RB large_JJ ._.
=_SYM -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: Xt_FW ×_FW dRd_NN ×_CD k_NN =_JJ ˜_FW Xt_FW ×_FW k_NN -LRB-_-LRB- 5.1_CD -RRB-_-RRB- Bingham_NNP and_CC Mannila_NNP tested_VBD the_DT performance_NN of_IN random_JJ projection_NN in_IN comparison_NN to_TO other_JJ dimension_NN reduction_NN techniques_NNS ,_, such_JJ as_IN SVD_NNP ,_, in_IN different_JJ applications_NNS ,_, image_NN processing_NN
nces_NNS of_IN X_NN in_IN expectations_NNS -LRB-_-LRB- 34_CD -RRB-_-RRB- ._.
In_IN recent_JJ years_NNS ,_, the_DT random_JJ projection_NN technique_NN has_VBZ successfully_RB been_VBN applied_VBN to_TO solve_VB many_JJ computational_JJ data_NNS analysis_NN problems_NNS ,_, such_JJ as_IN principal_JJ component_NN analysis_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, singular_JJ value_NN decomposition_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, least_JJS squares_NNS -LRB-_-LRB- 8_CD -RRB-_-RRB- and_CC manifold_NN learning_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP to_TO incorporate_VB the_DT random_JJ projection_NN technique_NN into_IN the_DT nonnegative_JJ matrix_NN factorization_NN pro_NN
e_LS proceed_VB to_TO use_VB simpler_JJR methods_NNS to_TO select_JJ features_NNS ._.
It_PRP has_VBZ been_VBN proved_VBN repeatedly_RB that_IN the_DT Occam_NNP 's_POS razor_NN does_VBZ work_VB well_RB in_IN more_JJR cases_NNS than_IN not_RB ._.
A_DT classical_JJ example_NN in_IN this_DT case_NN is_VBZ random_JJ projections_NNS =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP project_VBP the_DT data_NNS on_IN various_JJ random_JJ directions_NNS and_CC get_VB the_DT clustering_NN results_VBZ from_IN these_DT directions_NNS and_CC combine_VB the_DT results_NNS using_VBG the_DT following_JJ algorithm_NN ._.
Table_NNP 1_CD ._.
Aggregating_VBG Clusterings_NNS :_: The_DT Alg_NN
or_CC comparison_NN purposes_NNS ,_, we_PRP also_RB include_VBP the_DT results_NNS of_IN i.i.d_JJ Gaussian_JJ random_JJ matrix_NN and_CC FJLT_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- ._.
Experiment_NN 2_CD :_: This_DT experiment_NN was_VBD motivated_VBN by_IN the_DT recent_JJ work_NN in_IN -LRB-_-LRB- 6_CD -RRB-_-RRB- and_CC the_DT experiment_NN suggested_VBN by_IN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN each_DT image_NN in_IN the_DT database_NN ,_, we_PRP random_JJ select_JJ 20_CD subimages_NNS with_IN size_NN of_IN 32_CD ×_NN 32_CD ,_, yielding_VBG Q_NNP =_JJ 20_CD ∗_CD 1000_CD =_JJ 2_CD ×_CD 10_CD 4_CD vectors_NNS in_IN total_NN ._.
For_IN each_DT M_NN ,_, we_PRP did_VBD a_DT similar_JJ experiment_NN in_IN Experiment_NN 1_CD calculati_NN
ast_NN 1_CD −_NN 1_CD N_NN and_CC thus_RB ,_, which_WDT implies_VBZ that_IN -LRB-_-LRB- 4_LS -RRB-_-RRB- holds_VBZ with_IN 18234_CD ._.
SIMULATION_NN RESULTS_NNS To_TO demonstrate_VB the_DT effectiveness_NN of_IN the_DT SRM_NN in_IN dimensionality_NN reduction_NN ,_, we_PRP conduct_VBP the_DT experiments_NNS as_IN described_VBN in_IN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Specifically_RB ,_, the_DT dataset_NN consists_VBZ of_IN N_NN =_JJ 1000_CD image_NN windows_NNS of_IN size_NN 50_CD ×_NN 50_CD -LRB-_-LRB- i.e._FW ,_, the_DT original_JJ dimensionality_NN d_NN =_JJ 2500_CD -RRB-_-RRB- ._.
These_DT image_NN windows_NNS are_VBP chosen_VBN randomly_RB from_IN thirteen_CD 8-bit_JJ grayscale_NN natur_NN
input_NN space_NN is_VBZ used_VBN to_TO compute_VB distance_NN between_IN instances_NNS ._.
RP_NN :_: The_DT data_NN is_VBZ first_RB projected_VBN into_IN a_DT lower_JJR dimensional_JJ space_NN of_IN dimension_NN d_NN ′_NN =_JJ log_NN n_NN ɛ_NN 2_CD log_NN 1_CD ɛ_NN using_VBG the_DT Random_JJ Projection_NN -LRB-_-LRB- RP_NN -RRB-_-RRB- method_NN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP set_VBD A_NN =_JJ R_NN ⊤_NN R_NN ,_, where_WRB R_NN is_VBZ the_DT projection_NN matrix_NN used_VBN by_IN RP_NN ._.
ɛ_NN was_VBD set_VBN to_TO 0.25_CD for_IN the_DT experiments_NNS in_IN Section_NN 6.1_CD ._.
Datasets_NNP Original_NNP RP_NNP PCA_NNP ITML_NNP LMNN_NNP IDML-LM_NNP IDML-IT_NNP Amazon_NNP 0.4046_CD 0.3964_CD 0.1554_CD 0_CD ._.
jection_NN has_VBZ been_VBN applied_VBN on_IN various_JJ types_NNS of_IN problems_NNS like_IN machine_NN learning_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
Its_PRP$ power_NN comes_VBZ from_IN the_DT strong_JJ theoretical_JJ results_NNS that_WDT guarantee_VBP a_DT very_RB high_JJ chance_NN of_IN success_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
Bingham_NNP ._.
et_FW al_FW =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: present_JJ experimental_JJ results_NNS on_IN using_VBG RP_NN as_IN a_DT dimensionality_NN reduction_NN tool_NN ,_, their_PRP$ application_NN areas_NNS were_VBD the_DT processing_NN of_IN both_CC noisy_JJ and_CC noiseless_JJ images_NNS ,_, and_CC information_NN retrieval_NN in_IN text_NN document_NN
projection_NN as_IN our_PRP$ hash_JJ function_NN ._.
This_DT is_VBZ because_IN it_PRP is_VBZ simple_JJ ,_, database-friendly_JJ -LRB-_-LRB- 1_LS -RRB-_-RRB- and_CC yields_VBZ comparable_JJ results_NNS to_TO conventional_JJ dimensionality_NN reduction_NN ,_, such_JJ as_IN PCA_NNP ,_, for_IN both_CC image_NN and_CC text_NN data_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Furthermore_RB ,_, using_VBG random_JJ projection_NN ,_, we_PRP can_MD extend_VB the_DT search_NN algorithm_NN to_TO support_VB exact_JJ NN_NNP search_NN in_IN L1_NN norm_NN ,_, which_WDT is_VBZ found_VBN to_TO be_VB effective_JJ for_IN multimedia_NNS data_NNS -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
Experiment_NN results_VBZ on_IN image_NN dat_NN
dicate_VB a_DT better_JJR neighborhood_NN topology_NN preservation_NN ._.
It_PRP can_MD be_VB seen_VBN that_IN in_IN each_DT configuration_NN the_DT Gaussian_JJ approach_NN produces_VBZ better_JJR mappings_NNS ._.
be_VB very_RB expensive_JJ to_TO compute_VB ._.
By_IN using_VBG random_JJ projections_NNS =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: one_PRP can_MD overcome_VB that_IN ,_, but_CC that_DT would_MD probably_RB come_VB with_IN a_DT loss_NN of_IN mapping_NN quality_NN ._.
A_DT SOM_NN computed_VBN directly_RB with_IN the_DT Gaussians_NNP ,_, on_IN the_DT other_JJ hand_NN ,_, requires_VBZ only_RB a_DT fraction_NN of_IN the_DT computational_JJ effort_NN
20_CD -RRB-_-RRB- ._.
In_IN parallel_NN ,_, random_JJ projections_NNS -LRB-_-LRB- RP_NN -RRB-_-RRB- or_CC the_DT so-called_JJ Johnson-Lindenstrauss_NNP type_NN embeddings_NNS -LRB-_-LRB- 12_CD -RRB-_-RRB- became_VBD popular_JJ and_CC found_VBN applications_NNS in_IN both_CC theoretical_JJ computer_NN science_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- and_CC data_NN analytics_NNS =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT paper_NN focuses_VBZ on_IN the_DT application_NN of_IN the_DT random_JJ projection_NN method_NN -LRB-_-LRB- see_VB Section_NNP 2.3_CD -RRB-_-RRB- to_TO the_DT k-means_NN clustering_NN problem_NN -LRB-_-LRB- see_VB Definition_NNP 1_CD -RRB-_-RRB- ._.
Formally_RB ,_, assuming_VBG as_IN input_NN a_DT set_NN of_IN n_NN points_NNS in_IN d_FW dimen_FW
._.
In_IN the_DT second_JJ stage_NN ,_, we_PRP learn_VBP the_DT manifold_NN formed_VBN by_IN a_DT dense_JJ number_NN of_IN sampled_VBN views_NNS using_VBG the_DT EM_NN algorithm_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- ._.
Learning_NNP takes_VBZ place_NN in_IN a_DT ``_`` universal_JJ ''_'' ,_, lower-dimensional_JJ ,_, space_NN computed_VBN through_IN =_JJ -_: =_JJ RP_NN -LRB-_-LRB- 9_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT stage_NN reduces_VBZ storage_NN requirements_NNS considerably_RB -LRB-_-LRB- i.e._FW ,_, only_RB a_DT few_JJ parameters_NNS need_VBP to_TO be_VB stored_VBN for_IN each_DT manifold_NN -RRB-_-RRB- and_CC it_PRP allows_VBZ us_PRP to_TO rank_VB the_DT hypotheses_NNS generated_VBN by_IN the_DT first_JJ stage_NN ._.
Ranking_JJ s_NN
iews_NNS of_IN the_DT object_NN densely_RB and_CC learning_VBG the_DT manifold_NN formed_VBN by_IN the_DT samples_NNS ._.
Learning_NNP employs_VBZ the_DT EM_NN algorithm_NN -LRB-_-LRB- 28_CD -RRB-_-RRB- and_CC takes_VBZ place_NN in_IN a_DT ``_`` universal_JJ ''_'' ,_, lower-dimensional_JJ ,_, space_NN computed_VBD through_IN RP_NN -LRB-_-LRB- 2_CD =_JJ -_: =_JJ 9_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 30_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT purpose_NN of_IN the_DT first_JJ stage_NN is_VBZ to_TO generate_VB rough_JJ hypothetical_JJ matches_NNS between_IN the_DT models_NNS and_CC the_DT scene_NN fast_RB ,_, while_IN keeping_VBG the_DT space_NN requirements_NNS of_IN indexing_NN low_JJ ._.
To_TO account_VB for_IN indexing_VBG a_DT sparse_NN
ti_IN and_CC Mehrotra_NNP recently_RB proposed_VBD an_DT integration_NN of_IN dimensionality_NN reduction_NN with_IN clustering_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
Random_JJ projections_NNS have_VBP been_VBN used_VBN recently_RB for_IN dimensionality_NN reduction_NN in_IN image_NN and_CC text_NN data_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- ._.
Theoretical_JJ results_NNS and_CC experiments_NNS on_IN noisy_JJ image_NN data_NNS demonstrate_VBP the_DT ability_NN of_IN this_DT method_NN in_IN preserving_VBG the_DT distances_NNS ._.
And_CC ,_, finally_RB ,_, a_DT nonlinear_JJ dimensionality_NN reduction_NN was_VBD proposed_VBN in_IN -LRB-_-LRB-
tual_JJ data_NNS ,_, prior_RB to_TO applying_VBG Latent_JJ Semantic_JJ Indexing_NN -LRB-_-LRB- LSI_NNP -RRB-_-RRB- ._.
In_IN their_PRP$ work_NN the_DT columns_NNS of_IN the_DT random_JJ projection_NN matrix_NN are_VBP required_VBN to_TO be_VB orthogonal_JJ to_TO each_DT other_JJ ,_, which_WDT is_VBZ proven_VBN not_RB necessary_JJ later_RB =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, it_PRP is_VBZ noticed_VBN that_IN the_DT random_JJ vector_NN may_MD be_VB organized_VBN carefully_RB for_IN a_DT specific_JJ purpose_NN ._.
In_IN the_DT paper_NN by_IN Kaski_NNP -LRB-_-LRB- 61_CD -RRB-_-RRB- random_JJ projections_NNS were_VBD used_VBN on_IN textual_JJ data_NNS in_IN WEBSOM_NNP ,_, a_DT program_NN that_IN org_NN
e_LS -RRB-_-RRB- of_IN the_DT data_NNS exchanged_VBN between_IN parties_NNS in_IN order_NN to_TO achieve_VB secure_JJ clustering_NN ._.
Dimensionality_NN reduction_NN techniques_NNS have_VBP been_VBN studied_VBN in_IN the_DT context_NN of_IN pattern_NN recognition_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, information_NN retrieval_NN =_JJ -_: =[_NN 5_CD ,_, 9_CD ,_, 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC data_NN mining_NN -LRB-_-LRB- 10_CD ,_, 9_CD -RRB-_-RRB- ._.
To_TO our_PRP$ best_JJS knowledge_NN ,_, dimensionality_NN reduction_NN has_VBZ not_RB been_VBN used_VBN in_IN the_DT context_NN of_IN data_NNS privacy_NN in_IN any_DT detail_NN ._.
The_DT notable_JJ exception_NN is_VBZ our_PRP$ preliminary_JJ work_NN presented_VBN in_IN -LRB-_-LRB- 2_CD
Evaluations_NNS We_PRP compare_VBP CRS_NNP with_IN random_JJ projections_NNS -LRB-_-LRB- RP_NN -RRB-_-RRB- using_VBG real_JJ data_NNS ,_, including_VBG n_NN =_JJ 100_CD randomly_RB sampled_VBN documents_NNS from_IN the_DT NSF_NN data_NNS -LRB-_-LRB- 7_CD -RRB-_-RRB- -LRB-_-LRB- sparsity_FW ≈_FW 1_CD %_NN -RRB-_-RRB- ,_, n_NN =_JJ 100_CD documents_NNS from_IN the_DT NEWSGROUP_NN data_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- sparsity_FW ≈_FW 1_CD %_NN -RRB-_-RRB- ,_, and_CC one_CD class_NN of_IN the_DT COREL_NNP image_NN data_NNS -LRB-_-LRB- n_NN =_JJ 80_CD ,_, sparsity_FW ≈_FW 5_CD %_NN -RRB-_-RRB- ._.
We_PRP estimate_VBP all_DT pairwise_JJ inner_JJ products_NNS ,_, l1_NN and_CC l2_NN distances_NNS ,_, using_VBG both_DT CRS_NNP and_CC RP_NNP ._.
For_IN each_DT pair_NN ,_, we_PRP obtain_VBP 50_CD runs_VBZ a_DT
the_DT Pareto_NNP front_NN ,_, in_IN effect_NN a_DT voting_NN scheme_NN among_IN the_DT various_JJ criteria_NNS ._.
They_PRP approach_VBP the_DT search_NN problem_NN using_VBG an_DT Evolutionary_JJ Local_JJ Selection_NN Algorithm_NN -LRB-_-LRB- ELSA_NN -RRB-_-RRB- ._.
Others_NNS have_VBP taken_VBN to_TO random_JJ searching_VBG =_JJ -_: =[_NN 1_CD ,_, 9_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Fern_NNP and_CC Brodley_NNP -LRB-_-LRB- 20_CD -RRB-_-RRB- point_NN 2_CD out_RP that_IN while_IN random_JJ projection_NN has_VBZ promising_JJ theoretical_JJ properties_NNS it_PRP results_VBZ in_IN highly_RB unstable_JJ clustering_NN performance_NN ._.
They_PRP attempt_VBP to_TO resolve_VB this_DT using_VBG a_DT cluster_NN
utational_JJ complexity_NN of_IN SVD_NN approximation_NN algorithms_NNS is_VBZ dependent_JJ on_IN the_DT number_NN of_IN factors_NNS required_VBN -LRB-_-LRB- k_NN -RRB-_-RRB- ,_, the_DT sparseness_NN of_IN the_DT original_JJ term-document_JJ matrix_NN ,_, and_CC the_DT number_NN of_IN documents_NNS in_IN the_DT corpus_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Many_JJ approximation_NN algorithms_NNS for_IN computing_VBG only_RB the_DT k_NN largest_JJS singular_JJ values_NNS and_CC their_PRP$ associated_VBN vectors_NNS exist_VBP -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- ._.
Furthermore_RB ,_, the_DT data_NNS in_IN Figure_NNP 5_CD of_IN -LRB-_-LRB- 29_CD -RRB-_-RRB- indicates_VBZ that_IN the_DT complexity_NN of_IN t_NN
rch_NN methods_NNS in_IN attempts_NNS to_TO scale_VB to_TO large_JJ ,_, high_JJ dimensional_JJ datasets_NNS ._.
With_IN such_JJ datasets_NNS ,_, random_JJ searching_VBG becomes_VBZ a_DT viable_JJ heuristic_NN method_NN and_CC has_VBZ been_VBN used_VBN with_IN many_JJ of_IN the_DT aforementioned_JJ criteria_NNS =_JJ -_: =[_NN 1_CD ;_: 12_CD ;_: 29_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Sampling_NN is_VBZ another_DT method_NN used_VBN to_TO improve_VB the_DT scalability_NN of_IN algorithms_NNS ._.
Mitra_NNP et_FW al._FW partition_NN the_DT original_JJ feature_NN set_VBN into_IN clusters_NNS based_VBN on_IN a_DT similarity_NN function_NN and_CC select_JJ representatives_NNS from_IN
mply_RB choose_VB a_DT random_JJ set_NN of_IN genes_NNS and_CC use_VB them_PRP as_IN landmarks_NNS ._.
With_IN random_JJ landmarks_NNS ,_, the_DT correlation_NN signature_NN model_NN behaves_VBZ similar_JJ to_TO the_DT random_JJ projection_NN ,_, a_DT popular_JJ dimensionality_NN reduction_NN method_NN =_JJ -_: =[_NN 3,4,5,6,7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, except_IN that_IN the_DT random_JJ projection_NN Input_NN :_: Microarray_NN table_NN M_NN -LRB-_-LRB- n_NN ×_NN m_NN ,_, n_NN genes_NNS and_CC m_NN conditions_NNS -RRB-_-RRB- ,_, set_VBN of_IN k_NN landmark_NN genes_NNS L_NN =_JJ -LCB-_-LRB- l1_NN ,_, ..._: ,_, lk_NN -RCB-_-RRB- Output_NN :_: Set_NN of_IN gene_NN signature_NN vectors_NNS S_NNP ={_NNP −_NNP →_NNP sig_NN -LRB-_-LRB- g1_NN -RRB-_-RRB- ,_, ..._: ,_, −_NN
m_NN down_IN noise_NN or_CC unimportant_JJ details_NNS in_IN the_DT data_NNS and_CC to_TO allow_VB the_DT underlying_JJ semantic_JJ structure_NN to_TO become_VB evident_JJ ._.
Awkwardness_NN of_IN the_DT reduction_NN of_IN dimensions_NNS is_VBZ a_DT well-reported_JJ problem_NN in_IN applying_VBG LSA_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 4_LS -RRB-_-RRB- -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
Hence_RB ,_, we_PRP use_VBP also_RB the_DT term_NN dimension_NN reduction_NN for_IN noise_NN reduction_NN ._.
A_DT potential_JJ technique_NN for_IN automatic_JJ noise_NN reduction_NN is_VBZ a_DT model_NN validation_NN method_NN ,_, commonly_RB applied_VBN to_TO information_NN retrie_NN
one_CD ,_, for_IN example_NN ,_, in_IN -LRB-_-LRB- 53_CD -RRB-_-RRB- ._.
For_IN larger_JJR collections_NNS it_PRP is_VBZ possible_JJ to_TO reduce_VB the_DT dimensionality_NN either_CC by_IN analyzing_VBG principal_JJ components_NNS or_CC -LRB-_-LRB- in_IN the_DT case_NN of_IN very_RB large_JJ collections_NNS -RRB-_-RRB- by_IN random_JJ projections_NNS =_JJ -_: =[_NN 25_CD ;_: 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT model_NN vector_NN matrix_NN Mt_NN is_VBZ of_IN size_NN m_NN ×_FW d_FW ,_, where_WRB m_NN is_VBZ the_DT number_NN of_IN maps2_NN .3_NN The_DT Self-Organizing_JJ Map_NN 9_CD units_NNS ._.
The_DT values_NNS of_IN Mt_NN are_VBP updated_VBN in_IN each_DT iteration_NN t._NNP The_NNP matrix_NN U_NN of_IN size_NN m_NN ×_CD m_NN defines_VBZ th_DT
ed_VBN in_IN O_NN -LRB-_-LRB- ndq_NN -RRB-_-RRB- operations_NNS ._.
This_DT is_VBZ much_RB less_JJR than_IN the_DT standard_JJ way_NN of_IN computing_NN PCA_NN -LRB-_-LRB- which_WDT costs_VBZ O_NN -LRB-_-LRB- nd2_NN -RRB-_-RRB- operations_NNS for_IN the_DT computation_NN of_IN the_DT covariance_NN matrix_NN ,_, and_CC O_NN -LRB-_-LRB- d3_NN -RRB-_-RRB- operations_NNS for_IN diagonalization_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Hence_RB ,_, our_PRP$ method_NN seems_VBZ suitable_JJ for_IN processing_VBG very_RB large_JJ data_NNS sets_NNS -LRB-_-LRB- alternative_JJ efficient_JJ procedures_NNS are_VBP based_VBN ,_, e.g._FW on_IN the_DT EM_NN algorithm_NN -LRB-_-LRB- 33_CD -RRB-_-RRB- ,_, but_CC they_PRP have_VBP not_RB been_VBN extended_VBN to_TO fuzzy_JJ data_NNS -RRB-_-RRB- ._.
As_IN a_DT
lower_JJR bound_VBD of_IN Alon_NNP -LRB-_-LRB- 3_CD -RRB-_-RRB- dashes_NNS any_DT hope_NN of_IN reducing_VBG the_DT number_NN of_IN rows_NNS of_IN Φ_NN by_IN more_JJR than_IN a_DT factor_NN of_IN O_NN -LRB-_-LRB- log_NN -LRB-_-LRB- 1_CD \/_: ε_NN -RRB-_-RRB- -RRB-_-RRB- ,_, so_IN the_DT obvious_JJ question_NN is_VBZ whether_IN the_DT matrix_NN can_MD be_VB made_VBN sparse_JJ ._.
Bingham_NN and_CC Mannila_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: considered_VBN sparse_JJ projection_NN heuristics_NNS for_IN dimension-reduction_JJ based_JJ algorithms_NNS ,_, and_CC noticed_VBD that_IN in_IN practice_NN they_PRP seem_VBP to_TO give_VB a_DT considerable_JJ speedup_NN with_IN little_JJ compromise_NN in_IN quality_NN ._.
Achlioptas_NNP 's_POS
d._VB The_DT minimal_JJ ``_`` safe_JJ ''_'' dimension_NN can_MD be_VB obtained_VBN from_IN Johnson-Lindenstrauss_FW lemma_FW ,_, however_RB the_DT currently_RB known_VBN bound_VBN is_VBZ still_RB quite_RB high_JJ and_CC experiments_NNS showed_VBD that_IN even_RB smaller_JJR dimensions_NNS can_MD be_VB used_VBN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Interestingly_RB ,_, the_DT resulting_VBG dimension_NN does_VBZ not_RB depend_VB on_IN original_JJ one_NN ,_, only_RB on_IN number_NN of_IN original_JJ points_NNS ._.
With_IN current_JJ best_RB known_VBN bound_VBN ,_, the_DT lemma_NN looks_VBZ as_IN follows_VBZ :_: Theorem_NN 2_CD -LRB-_-LRB- Johnson-Lindenstrauss_NN
mply_RB choose_VB a_DT random_JJ set_NN of_IN genes_NNS and_CC use_VB them_PRP as_IN landmarks_NNS ._.
With_IN random_JJ landmarks_NNS ,_, the_DT correlation_NN signature_NN model_NN behaves_VBZ similar_JJ to_TO the_DT random_JJ projection_NN ,_, a_DT popular_JJ dimensionality_NN reduction_NN method_NN =_JJ -_: =[_NN 1_CD ,_, 3_CD ,_, 22_CD ,_, 31_CD ,_, 32_CD ,_, 45_CD -RRB-_-RRB- -_: =_JJ -_: ,_, except_IN that_IN the_DT random_JJ projection_NN projects_NNS the_DT original_JJ high-dimensional_JJ space_NN onto_IN a_DT random_JJ subspace_NN while_IN the_DT correlation_NN signatures_NNS project_VBP the_DT original_JJ space_NN onto_IN a_DT subspace_NN whose_WP$ coordinates_VBZ co_NN
ength_NN ._.
These_DT row_NN vectors_NNS of_IN R_NN can_MD be_VB viewed_VBN as_IN d_NN random_JJ faces_NNS in_IN R_NN m_NN ._.
Random_JJ projection_NN has_VBZ been_VBN previously_RB studied_VBN as_IN a_DT general_JJ dimensionality_NN reduction_NN method_NN for_IN numerous_JJ clustering_NN problems_NNS -LRB-_-LRB- 33_CD -RRB-_-RRB- --_: =_JJ -_: =[_NN 35_CD -RRB-_-RRB- -_: =_JJ -_: ,_, as_RB well_RB as_IN for_IN learning_VBG nonlinear_JJ manifolds_NNS -LRB-_-LRB- 36_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 37_CD -RRB-_-RRB- ._.
Regardless_RB of_IN whether_IN the_DT estimated_VBN signal_NN is_VBZ sparse_JJ or_CC not_RB ,_, random_JJ projection_NN has_VBZ the_DT following_JJ advantages_NNS over_IN classical_JJ methods_NNS such_JJ as_IN pri_NN
de_IN c_NN je_FW průměrný_FW počet_FW různých_FW termů_FW v_LS dokumentu_NN ._.
Proto_FW byly_FW hledány_NN rychlejší_NN metody_NN ,_, které_VBN by_IN dosahovaly_JJ podobných_NN výsledků_NN za_NN cenu_NN horší_NN chyby_NN aproximace_NN ._.
Jednou_NNP z_SYM těchto_FW metod_FW je_FW náhodná_FW projekce_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, která_FW při_FW dostatečně_FW vysoké_FW redukované_FW dimenzi_FW d_NN ,_, d_FW <<_FW n_NN dobře_NN zachovává_NN vzdálenosti_VBD a_DT úhly_JJ mezi_NN vektory_NN ._.
Náhodná_FW projekce_FW je_FW založena_FW na_TO násobení_FW matice_FW termů_FW v_LS dokumentech_FW maticí_FW náhodné_NN projekce_NN
presence_NN and_CC absence_NN with_IN the_DT same_JJ weighting_NN ,_, is_VBZ not_RB considered_VBN as_IN a_DT good_JJ distance_NN measure_NN for_IN text_NN documents_NNS ,_, as_IN opposed_VBN to_TO its_PRP$ widespread_JJ usage_NN in_IN other_JJ forms_NNS of_IN data_NNS as_IN image_NN and_CC bio-medical_JJ data_NN =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
sA_NN good_JJ feature_NN should_MD be_VB able_JJ to_TO distinguish_VB between_IN different_JJ classes_NNS of_IN documents_NNS ._.
In_IN other_JJ words_NNS ,_, when_WRB a_DT set_NN of_IN documents_NNS is_VBZ projected_VBN on_IN a_DT feature_NN dimension_NN ,_, the_DT projection_NN of_IN documents_NNS belongin_NN
SI_NN for_IN dimension_NN reduction_NN with_IN bounded_VBN distance_NN distortion_NN error_NN ,_, the_DT method_NN of_IN Random_JJ Projection_NN -LRB-_-LRB- RP_NN -RRB-_-RRB- has_VBZ recently_RB received_VBN attention_NN from_IN the_DT machine_NN learning_NN and_CC information_NN retrieval_NN communities_NNS =_JJ -_: =[_NN 1_CD ,_, 4_CD ,_, 9_CD ,_, 12_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Unlike_IN LSI_NNP ,_, the_DT new_JJ dimensions_NNS in_IN RP_NN are_VBP generated_VBN randomly_RB -LRB-_-LRB- random_JJ linear_JJ combinations_NNS of_IN original_JJ terms_NNS -RRB-_-RRB- with_IN no_DT ordering_VBG of_IN ''_'' importance_NN ''_'' ._.
The_DT new_JJ dimensions_NNS are_VBP only_RB approximately_RB orthogonal_JJ ._.
How_WRB
der_NN of_IN hundreds_NNS or_CC thousands_NNS ._.
It_PRP may_MD even_RB be_VB a_DT continuous_JJ stream_NN of_IN data_NNS in_IN some_DT domains_NNS ._.
It_PRP is_VBZ thus_RB not_RB feasible_JJ to_TO use_VB each_DT and_CC every_DT instance_NN on_IN the_DT curve_NN for_IN mining_NN ._.
Thus_RB dimensionality_NN reduction_NN =_JJ -_: =[_NN 5_CD ,_, 10_CD -RRB-_-RRB- -_: =_SYM -_: with_IN minimal_JJ information_NN loss_NN is_VBZ needed_VBN ._.
The_DT application_NN of_IN AutoDomainMine_NN is_VBZ primarily_RB in_IN the_DT Heat_NNP Treating_NNP domain_NN ._.
Its_PRP$ goal_NN is_VBZ to_TO enhance_VB a_DT system_NN called_VBN QuenchMiner_NNP -LRB-_-LRB- 4_CD -RRB-_-RRB- that_WDT does_VBZ predictive_JJ analys_NNS
ificantly_RB less_RBR expensive_JJ than_IN the_DT other_JJ techniques_NNS and_CC preserves_VBZ distances_NNS quite_RB nicely_RB ._.
It_PRP has_VBZ been_VBN shown_VBN that_IN RP_NN yields_VBZ results_NNS comparable_JJ to_TO conventional_JJ dimensionnality_NN reduction_NN techniques_NNS -LRB-_-LRB- 42_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 48_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 58_CD -RRB-_-RRB- ._.
RPs_NNS ,_, however_RB ,_, are_VBP highly_RB unstable_JJ ,_, which_WDT becomes_VBZ problematic_JJ when_WRB used_VBN for_IN data_NN clustering_NN purposes_NNS ._.
We_PRP made_VBD an_DT experimental_JJ observation_NN showing_VBG that_IN data_NNS points_NNS belonging_VBG to_TO a_DT ``_`` natural_JJ ''_'' clust_NN
recently_RB emerged_VBD as_IN a_DT powerful_JJ method_NN for_IN dimensionality_NN reduction_NN ._.
The_DT accuracy_NN obtained_VBN after_IN the_DT dimensionality_NN has_VBZ been_VBN reduced_VBN using_VBG random_JJ projection_NN is_VBZ almost_RB as_RB good_JJ as_IN the_DT original_JJ accuracy_NN =_JJ -_: =[_NN 85_CD ,_, 1_CD ,_, 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT key_JJ idea_NN of_IN random_JJ projection_NN arises_VBZ from_IN the_DT Johnson-Lindenstrauss_JJ lemma_NN -LRB-_-LRB- 75_CD -RRB-_-RRB- :_: ``_`` if_IN points_NNS in_IN a_DT vector_NN space_NN are_VBP projected_VBN onto_IN a_DT randomly_RB selected_VBN subspace_NN of_IN suitably_RB high_JJ dimension_NN ,_, then_RB the_DT
the_DT most_RBS principal_JJ features_NNS associated_VBN with_IN a_DT multimedia_NNS item_NN while_IN preserving_VBG the_DT final_JJ semantic_JJ retrieval_NN performance_NN ._.
There_EX are_VBP several_JJ methods_NNS used_VBN in_IN CBMIR_NNP for_IN reducing_VBG dimensionality_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Principal_NN Component_NN Analysis_NN -LRB-_-LRB- PCA_NN -RRB-_-RRB- is_VBZ one_CD of_IN the_DT most_RBS commonly_RB used_VBN methods_NNS transforming_VBG a_DT number_NN of_IN possibly_RB correlated_VBN variables_NNS into_IN a_DT smaller_JJR number_NN of_IN uncorrelated_JJ variables_NNS referred_VBN to_TO as_IN prin_NN
edure_NN that_WDT improves_VBZ on_IN the_DT images_NNS obtained_VBN by_IN fast_JJ algorithms_NNS such_JJ as_IN the_DT adaptive_JJ median_NN filter_NN ;_: or_CC as_IN a_DT pre-processing_JJ procedure_NN that_WDT cleans_VBZ up_RP images_NNS before_IN dimensionality_NN reduction_NN in_IN data_NNS mining_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Our_PRP$ computational_JJ cost_NN can_MD be_VB reduced_VBN further_RB by_IN better_JJR implementations_NNS of_IN minimization_NN routines_NNS for_IN solving_VBG -LRB-_-LRB- 3_CD -RRB-_-RRB- ,_, see_VBP for_IN examples_NNS ,_, the_DT continuation_NN method_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- and_CC the_DT primal-dual_JJ formulation_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- fo_NN
e_LS used_VBN for_IN JL_NNP ._.
He_PRP also_RB showed_VBD how_WRB to_TO obtain_VB a_DT constant_JJ speedup_NN by_IN making_VBG roughly_RB a_DT 2\/3_CD of_IN the_DT matrix_NN null_NN ._.
His_PRP$ motivation_NN was_VBD to_TO make_VB random_JJ projections_NNS easier_JJR to_TO use_VB in_IN practice_NN ._.
Bingham_NN and_CC Mannila_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: also_RB considered_VBN sparse_JJ projections_NNS heuristics_NNS for_IN dimension-reduction_JJ based_JJ algorithms_NNS ,_, and_CC noticed_VBD that_IN in_IN practice_NN they_PRP seem_VBP to_TO give_VB a_DT considerable_JJ speedup_NN with_IN little_JJ compromise_NN in_IN quality_NN ._.
The_DT FJ_NN
ength_NN ._.
These_DT row_NN vectors_NNS of_IN R_NN can_MD be_VB viewed_VBN as_IN d_NN random_JJ faces_NNS in_IN R_NN m_NN ._.
Random_JJ projection_NN has_VBZ been_VBN previously_RB studied_VBN as_IN a_DT general_JJ dimensionality_NN reduction_NN method_NN for_IN numerous_JJ clustering_NN problems_NNS -LRB-_-LRB- 33_CD -RRB-_-RRB- --_: =_JJ -_: =[_NN 35_CD -RRB-_-RRB- -_: =_JJ -_: ,_, as_RB well_RB as_IN for_IN learning_VBG nonlinear_JJ manifolds_NNS -LRB-_-LRB- 36_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 37_CD -RRB-_-RRB- ._.
Regardless_RB of_IN whether_IN the_DT estimated_VBN signal_NN is_VBZ sparse_JJ or_CC not_RB ,_, random_JJ projection_NN has_VBZ the_DT following_JJ advantages_NNS over_IN classical_JJ methods_NNS such_JJ as_IN pri_NN
Random_JJ projections_NNS have_VBP been_VBN found_VBN to_TO be_VB a_DT computationally_RB efficient_JJ ,_, yet_CC sufficiently_RB accurate_JJ method_NN for_IN dimensionality_NN reduction_NN ._.
Promising_JJ experimental_JJ results_NNS are_VBP reported_VBN in_IN Bingham_NNP and_CC Mannila_NNP =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT key_JJ idea_NN of_IN random_JJ mapping_NN arises_VBZ from_IN the_DT Johnson-Lindenstrauss_JJ lemma_NN -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, which_WDT states_VBZ that_IN any_DT point_NN set_VBN in_IN a_DT Euclidean_JJ space_NN can_MD be_VB embedded_VBN in_IN a_DT Euclidean_JJ space_NN of_IN dimension_NN without_IN distor_NN
ch_NN might_MD be_VB a_DT natural_JJ extension_NN to_TO the_DT sketch_NN we_PRP propose_VBP here_RB ._.
2_CD Background_NN ._.
Random_JJ projections_NNS are_VBP a_DT powerful_JJ method_NN of_IN dimensionality_NN reduction_NN that_WDT have_VBP been_VBN applied_VBN in_IN numerous_JJ practical_JJ problems_NNS =_JJ -_: =[_NN 5_CD ,_, 17_CD ,_, 11_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC have_VBP also_RB served_VBN as_IN a_DT useful_JJ tool_NN in_IN algorithmic_JJ design_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ._.
The_DT basis_NN of_IN projections_NNS is_VBZ the_DT remarkable_JJ Johnson-Lindenstrauss_JJ lemma_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, which_WDT say_VBP that_IN for_IN any_DT data_NNS set_NN of_IN n_NN points_NNS in_IN d_NN dimens_NNS
the_DT method_NN from_IN being_VBG used_VBN in_IN many_JJ cases_NNS with_IN high-dimensional_JJ data_NNS ._.
In_IN this_DT work_NN ,_, we_PRP consider_VBP two_CD methods_NNS for_IN dimensionality_NN reduction_NN ,_, principal_JJ component_NN analysis_NN -LRB-_-LRB- PCA_NN -RRB-_-RRB- and_CC random_JJ projection_NN -LRB-_-LRB- RP_NN -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =_JJ Bingham_NNP and_CC Mannila_NNP ,_, 2001_CD -_: =_JJ -_: ;_: Fradkin_NNP and_CC Madigan_NNP ,_, 2003_CD ;_: Fern_NNP and_CC Brodley_NNP ,_, 2003_CD ;_: Kaski_NNP ,_, 1998_CD -RRB-_-RRB- ._.
We_PRP investigate_VBP which_WDT of_IN these_DT is_VBZ most_RBS suited_VBN for_IN being_VBG used_VBN in_IN conjunction_NN with_IN nearest_JJS neighbor_NN classification_NN when_WRB dealing_VBG with_IN two_CD
a_DT subspace_NN of_IN R_NN n_NN ,_, either_CC linear_JJ or_CC non-linear_JJ ._.
n_NN is_VBZ typically_RB very_RB large_JJ while_IN the_DT intrinsic_JJ dimensionality_NN of_IN the_DT document_NN space_NN might_MD be_VB much_RB lower_JJR ._.
Many_JJ dimensionality_NN reduction_NN techniques_NNS -LRB-_-LRB- 1_LS -RRB-_-RRB- -LRB-_-LRB- 2_LS -RRB-_-RRB- =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =-[_NN 7_CD -RRB-_-RRB- -LRB-_-LRB- 14_CD -RRB-_-RRB- -LRB-_-LRB- 15_CD -RRB-_-RRB- have_VBP been_VBN applied_VBN to_TO document_VB representation_NN and_CC indexing_NN ._.
Among_IN these_DT techniques_NNS ,_, Latent_JJ Semantic_JJ Indexing_NN -LRB-_-LRB- LSI_NNP -RRB-_-RRB- -LRB-_-LRB- 7_CD -RRB-_-RRB- by_IN Singular_JJ Value_NN Decomposition_NN -LRB-_-LRB- SVD_NN -RRB-_-RRB- is_VBZ a_DT well-known_JJ successful_JJ appr_NN
Random_JJ projections_NNS have_VBP been_VBN found_VBN to_TO be_VB a_DT computationally_RB efficient_JJ ,_, yet_CC sufficiently_RB accurate_JJ method_NN for_IN dimensionality_NN reduction_NN ._.
Promising_JJ experimental_JJ results_NNS are_VBP reported_VBN in_IN Bingham_NNP and_CC Mannila_NNP =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT key_JJ idea_NN of_IN random_JJ map8sping_NN arises_VBZ from_IN the_DT Johnson-Lindenstrauss_JJ lemma_NN -LRB-_-LRB- 24_CD -RRB-_-RRB- ,_, which_WDT states_VBZ that_IN any_DT n_NN point_NN set_VBN in_IN a_DT Euclidean_JJ space_NN can_MD be_VB embedded_VBN in_IN a_DT Euclidean_JJ space_NN of_IN dimension_NN O_NN -LRB-_-LRB- log_NN n_NN \/_: ``_`` 2_CD
the_DT semantic_JJ space_NN ._.
Experiments_NNS show_VBP that_IN eLSI_NN retains_VBZ the_DT retrieval_NN quality_NN of_IN LSI_NNP but_CC is_VBZ several_JJ orders_NNS of_IN magnitude_NN more_RBR efficient_JJ ._.
It_PRP outperforms_VBZ four_CD major_JJ fast_JJ dimensionality_NN reduction_NN methods_NNS =_JJ -_: =[_NN 6_CD ,_, 9_CD ,_, 15_CD ,_, 23_CD -RRB-_-RRB- -_: =_SYM -_: in_IN retrieval_NN quality_NN ._. ._.
We_PRP conducted_VBD extensive_JJ experiments_NNS with_IN LSI_NNP using_VBG a_DT large_JJ corpus_NN and_CC found_VBD that_IN proper_JJ normalization_NN of_IN semantic_JJ vectors_NNS for_IN terms_NNS and_CC documents_NNS improve_VBP recall_NN by_IN 76_CD %_NN compared_VBN
rnesses_VBZ the_DT capabilities_NNS of_IN random_JJ projection_NN to_TO deliver_VB a_DT computationally_RB feasible_JJ solution_NN for_IN online_NN operation_NN ._.
6.3_CD Identifying_VBG spam_NN :_: A_DT random_JJ projection_NN approach_NN Its_PRP$ been_VBN a_DT well_RB established_VBN fact_NN -LRB-_-LRB- =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 8_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 9_CD -RRB-_-RRB- -RRB-_-RRB- that_IN if_IN dataset_NN is_VBZ clustered_VBN in_IN high_JJ dimension_NN then_RB with_IN high_JJ probability_NN it_PRP is_VBZ clustered_VBN along_IN any_DT random_JJ subspace_NN ._.
Additionally_RB it_PRP is_VBZ significantly_RB less_RBR expensive_JJ to_TO compute_VB than_IN othe_NN
,_, 1998_CD -RRB-_-RRB- ._.
Performing_VBG a_DT random_JJ projection_NN of_IN the_DT original_JJ -LRB-_-LRB- sparse_JJ -RRB-_-RRB- data_NNS --_: i.e._FW forming_VBG a_DT w_FW ×_FW k_NN random_JJ matrix_NN and_CC projecting_VBG the_DT original_JJ w_NN ×_CD d_NN matrix_NN through_IN it_PRP --_: is_VBZ O_NN -LRB-_-LRB- zkw_NN -RRB-_-RRB- -LRB-_-LRB- Papadimitriou_NNP et_FW al._FW ,_, 1998_CD ;_: =_JJ -_: =_JJ Bingham_NNP and_CC Mannila_NNP ,_, 2001_CD -_: =--RRB-_NN ,_, where_WRB k_NN is_VBZ the_DT dimensionality_NN of_IN the_DT vectors_NNS ._.
By_IN contrast_NN ,_, producing_VBG context_NN vectors_NNS with_IN Random_JJ Indexing_NN is_VBZ only_RB O_NN -LRB-_-LRB- wk_NN -RRB-_-RRB- ,_, since_IN it_PRP is_VBZ not_RB reliant_JJ on_IN the_DT initial_JJ construction_NN of_IN the_DT huge_JJ cooccurrence_NN
CA_NNP or_CC ICA_NNP is_VBZ computationally_RB expensive_JJ ._.
Theoretical_JJ insights_NNS suggest_VBP that_IN Random_JJ Mapping_NN -LRB-_-LRB- RM_NN -RRB-_-RRB- is_VBZ a_DT promising_JJ alternative_NN since_IN it_PRP requires_VBZ much_RB less_JJR computation_NN ._.
Empirical_JJ results_NNS for_IN text_NN clustering_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: indicate_VBP that_IN PCA_NNP and_CC ICA_NNP outperform_JJ RM_NN ._.
However_RB ,_, the_DT significantly_RB lower_JJR computational_JJ costs_NNS make_VBP RM_NN an_DT attractive_JJ possibility_NN and_CC worth_JJ exploring_VBG in_IN other_JJ scenarios_NNS ._.
No_DT direct_JJ comparison_NN between_IN RM_NN
ue_RB ,_, to_TO exploit_VB known_JJ classes_NNS of_IN documents_NNS ._.
For_IN this_DT one_CD supervised_JJ technique_NN ,_, they_PRP demonstrate_VBP a_DT small_JJ improvement_NN in_IN classification_NN accuracy_NN when_WRB using_VBG the_DT reduced_VBN feature_NN space_NN ._.
Bingham_NN and_CC Mannila_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: compare_VB a_DT PCA-like_JJ DR_NN technique_NN against_IN a_DT computationally_RB efficient_JJ approach_NN based_VBN on_IN random_JJ projection_NN ,_, and_CC find_VBP that_IN the_DT simpler_JJR approach_NN is_VBZ able_JJ to_TO achieve_VB comparable_JJ performance_NN ._.
However_RB ,_, for_IN tex_NN
√_NN 3_CD ,_, 0_CD ,_, √_NN 3_CD -RCB-_-RRB- ,_, such_JJ that_IN P_NN rob_NN -LRB-_-LRB- rij_NN =_JJ 0_CD -RRB-_-RRB- =_JJ 2\/3_CD ,_, P_NN rob_NN -LRB-_-LRB- rij_NN =_JJ √_NN 3_CD -RRB-_-RRB- =_JJ P_NN rob_NN -LRB-_-LRB- rij_NN =_JJ −_FW √_FW 3_LS -RRB-_-RRB- =_JJ 1\/6_CD ._.
In_IN this_DT case_NN also_RB we_PRP have_VBP E_NN -LRB-_-LRB- rij_NN -RRB-_-RRB- =_JJ 0_CD and_CC V_NN ar_NN -LRB-_-LRB- rij_NN -RRB-_-RRB- =_JJ 1_CD and_CC the_DT JL_NN lemma_NN holds_VBZ ._.
-LRB-_-LRB- 3_LS -RRB-_-RRB- Normal_JJ random_JJ projections_NNS =_JJ -_: =[_NN 41_CD -RRB-_-RRB- -_: =_JJ -_: :_: this_DT JL_NN lemma_NN compliant_JJ randomized_VBN map_NN is_VBZ represented_VBN by_IN a_DT d_FW ′_FW ×_FW d_FW matrix_NN R_NN =_JJ 1_CD \/_: √_FW d_FW ′_NN -LRB-_-LRB- rij_NN -RRB-_-RRB- ,_, where_WRB rij_NN are_VBP distributed_VBN according_VBG to_TO a_DT gaussian_NN with_IN 0_CD mean_NN and_CC unit_NN variance_NN ._.
-LRB-_-LRB- 4_LS -RRB-_-RRB- Random_JJ Subspace_NN -LRB-_-LRB- RS_NN
this_DT curse_NN of_IN dimensionality_NN and_CC make_VB a_DT variety_NN of_IN automatic_JJ text_NN processing_NN tasks_NNS more_RBR tractable_JJ ._.
Dimensionality_NN reduction_NN techniques_NNS have_VBP been_VBN studied_VBN in_IN great_JJ depth_NN in_IN document_NN retrieval_NN systems_NNS -LRB-_-LRB- =_JJ -_: =_JJ Bingham_NNP &_CC Mannila_NNP ,_, 2001_CD -_: =_JJ -_: ;_: Deerwester_NNP et_FW al._FW ,_, 1990_CD ;_: Efron_NNP ,_, 2002_CD ;_: Isbell_NNP &_CC Viola_NNP ,_, 1999_CD -RRB-_-RRB- ,_, but_CC there_EX is_VBZ a_DT dearth_NN of_IN information_NN on_IN how_WRB dimensionality_NN reduction_NN relates_VBZ to_TO document_VB clustering_NN ._.
This_DT study_NN will_MD begin_VB to_TO address_VB thi_NN
response_NN of_IN unit_NN i_FW in_IN the_DT network_NN is_VBZ given_VBN by_IN :_: yi_NN -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ -LRB-_-LRB- content_JJ responsei_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- -LRB-_-LRB- context_NN responsei_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- -LRB-_-LRB- 1_LS -RRB-_-RRB- content_NN responsei_NN -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ TF_NN -LRB-_-LRB- D_NN -LRB-_-LRB- x_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, wi_NN x_NN -RRB-_-RRB- ,_, α_NN -RRB-_-RRB- -LRB-_-LRB- 2_LS -RRB-_-RRB- context_NN responsei_NN -LRB-_-LRB- t_NN -RRB-_-RRB- =_JJ TF_NN -LRB-_-LRB- D_NN -LRB-_-LRB- y_NN -LRB-_-LRB- t-1_NN -RRB-_-RRB- ,_, wi_FW y_FW -RRB-_-RRB- ,_, β_NN -RRB-_-RRB- =_JJ -_: =-LRB-_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: where_WRB x_NN -LRB-_-LRB- t_NN -RRB-_-RRB- is_VBZ the_DT external_JJ -LRB-_-LRB- content_JJ -RRB-_-RRB- input_NN coming_VBG into_IN the_DT system_NN at_IN time_NN t_NN ,_, y_NN -LRB-_-LRB- t-1_NN -RRB-_-RRB- is_VBZ the_DT recurrent_JJ -LRB-_-LRB- context_NN -RRB-_-RRB- input_NN ,_, i.e._FW ,_, the_DT activation_NN levels_NNS of_IN the_DT units_NNS ,_, at_IN time_NN t-1_NN ,_, D_NN is_VBZ some_DT similarity_NN measu_NN
each_DT word_NN type_NN to_TO one_CD of_IN two_CD new_JJ subclusters_NNS ._.
Random_JJ projections_NNS have_VBP been_VBN shown_VBN to_TO be_VB a_DT good_JJ and_CC computationally_RB inexpensive_JJ dimensionality_NN reduction_NN technique_NN ,_, especially_RB for_IN high_JJ dimensional_JJ data_NNS -LRB-_-LRB- =_JJ -_: =_JJ Bingham_NNP and_CC Mannila_NNP ,_, 2001_CD -_: =--RRB-_NN ._.
Although_IN our_PRP$ best_JJS performance_NN does_VBZ not_RB come_VB from_IN random_JJ projections_NNS ,_, we_PRP still_RB obtain_VB substantial_JJ speed-ups_NNS over_IN a_DT single_JJ pass_NN fine_NN decoder_NN when_WRB using_VBG random_JJ projections_NNS in_IN coarse_JJ passes_NNS ._.
4.2_CD Freque_NN
mproved_VBD the_DT precision_NN results_VBZ 14.7_CD %_NN ..._: 58.3_CD %_NN on_IN all_PDT the_DT test_NN sets_NNS ._.
The_DT difficulties_NNS encountered_VBN during_IN the_DT selection_NN of_IN the_DT optimal_JJ dimension_NN for_IN LSA_NNP is_VBZ a_DT well-reported_JJ problem_NN -LRB-_-LRB- Landauer_NNP et_FW al._FW ,_, 1998_CD ;_: =_JJ -_: =_JJ Bingham_NNP and_CC Mannila_NNP ,_, 2001_CD -_: =_JJ -_: ;_: Globerson_NNP and_CC Tishby_NNP ,_, 2003_CD -RRB-_-RRB- ,_, and_CC the_DT selection_NN is_VBZ typically_RB performed_VBN by_IN ad_FW hoc_FW heuristics_NNS ._.
Blei_NNP et_FW al._FW -LRB-_-LRB- 2003_CD -RRB-_-RRB- have_VBP shown_VBN that_IN LDA_NN outperforms_VBZ PLSA_NN in_IN document_NN classification_NN and_CC collaborative_JJ filte_NN
._.
Many_JJ other_JJ weighting_NN schemes_NNS are_VBP possible_JJ ,_, see_VB e.g._FW -LRB-_-LRB- 15_CD -RRB-_-RRB- ._.
The_DT resulting_VBG very_RB high_JJ dimensional_JJ space_NN was_VBD then_RB mapped_VBN into_IN vector_NN space_NN of_IN dimension_NN 50_CD ,_, using_VBG the_DT random_JJ projection_NN method_NN proposed_VBN in_IN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT process_NN 15_CD γsdistance_NN evaluations_NNS 10000_CD 1000_CD HC_NN UHC_NN PHC_NN UPHC_NN UPHC_NN -LRB-_-LRB- apprx_NN -RRB-_-RRB- 100_CD 10_CD 20_CD 30_CD 40_CD 50_CD r_NN distance_NN evaluations_NNS 10000_CD 1000_CD HC_NN AHC_NN 10_CD AHC_NN 100_CD AHC_NN 1000_CD UHC_NN PHC_NN 100_CD UPHC_NN -LRB-_-LRB- apprx_NN -RRB-_-RRB- 10_CD 20_CD 30_CD 40_CD 50_CD
elet_NN domain_NN ,_, all_DT but_CC 1_CD the_DT largest_JJS magnitude_NN coefficients_NNS are_VBP set_VBN to_TO ,_, and_CC the_DT remaining_VBG coefficients_NNS are_VBP 6_CD quantized_VBN to_TO ._.
The_DT feature_NN vector_NN as_IN described_VBN is_VBZ quite_RB 293_CD ;_: :_: large_JJ ,_, ,_, so_RB random_JJ projection_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: using_VBG random_JJ 1728341_CD unit-length_JJ vectors_NNS is_VBZ used_VBN to_TO reduce_VB the_DT dimensionality_NN of_IN the_DT feature_NN vector_NN to_TO dimensions_NNS ._.
The_DT average_NN of_IN each_DT color_NN channel_NN -LRB-_-LRB- the_DT ranges_NNS were_VBD =_JJ -LRB-_-LRB- -RRB-_-RRB- ,_, -LRB-_-LRB- ?_.
-LRB-_-LRB- 5_CD ,_, and_CC #_# @_SYM 7AB1_CD #_# @_SYM 7AB1_CD -LRB-_-LRB-
cessfully_RB used_VBN for_IN protein_NN mapping_NN -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, reconstruction_NN ofRandom_NN Projection_NN Ensemble_NN Classifiers_NNS 3_CD frequency-sparse_JJ signals_NNS -LRB-_-LRB- 7_CD ,_, 9_CD -RRB-_-RRB- ,_, face_NN recognition_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC textual_JJ and_CC visual_JJ information_NN retrieval_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Random_JJ projections_NNS were_VBD also_RB utilized_VBN as_IN part_NN of_IN an_DT ensemble_NN algorithm_NN for_IN clustering_NN in_IN -LRB-_-LRB- 10_CD -RRB-_-RRB- and_CC for_IN gene_NN expression_NN data_NN analysis_NN in_IN -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
Essentially_RB ,_, the_DT random_JJ projection_NN algorithm_NN is_VBZ used_VBN to_TO r_NN
-RRB-_-RRB- ._.
Random_JJ projection_NN -LRB-_-LRB- or_CC sketches_NNS -RRB-_-RRB- has_VBZ been_VBN used_VBN as_IN an_DT effective_JJ dimension_NN reduction_NN technique_NN in_IN data_NN mining_NN ._.
For_IN example_NN ,_, in_IN -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, Fern_NNP et_NNP ._.
al._FW apply_VB random_JJ projection_NN for_IN point_NN data_NNS clustering_NN ._.
In_IN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT authors_NNS conducted_VBD empirical_JJ study_NN to_TO show_VB that_IN random_JJ projection_NN out_RB performs_VBZ principle_NN component_NN analysis_NN and_CC discrete_JJ Cosine_NNP transform_VB in_IN image_NN and_CC text_NN compression_NN ,_, in_IN terms_NNS of_IN introducing_VBG le_FW
ime_NN 4complexity_NN of_IN O_NN -LRB-_-LRB- nd_NN 2_CD -RRB-_-RRB- +_CC O_NN -LRB-_-LRB- d_NN 3_CD -RRB-_-RRB- ,_, which_WDT is_VBZ not_RB suitable_JJ for_IN high_JJ dimensional_JJ data_NNS ._.
So_RB a_DT computationally_RB simpler_JJR but_CC less_RBR accurate_JJ dimension_NN reduction_NN method_NN ,_, the_DT random_JJ projection_NN ,_, was_VBD proposed_VBN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT projection_NN can_MD be_VB represented_VBN as_IN Y_NN =_JJ XM_NNP ,_, where_WRB X_NN is_VBZ the_DT n_NN ∗_FW d_FW original_JJ dataset_NN ,_, M_NN is_VBZ a_DT d_FW ∗_FW e_LS randomized_VBN matrix_NN ,_, e_SYM is_VBZ the_DT dimension_NN after_IN reduction_NN ._.
The_DT elements_NNS of_IN M_NN are_VBP i.i.d._JJ samples_NNS of_IN a_DT cer_NN
ontext_NN of_IN sparse_JJ text_NN document_NN data_NNS ._.
For_IN a_DT sparse_JJ data_NN matrix_NN XdN_NN with_IN about_IN c_NN nonzero_NN entries_NNS per_IN column_NN ,_, the_DT computational_JJ complexity_NN of_IN SVD_NNP is_VBZ of_IN order_NN O_NN -LRB-_-LRB- dcN_NN -RRB-_-RRB- -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
Latent_JJ semantic_JJ indexing_NN -LRB-_-LRB- LSI_NNP -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =_JJ 9_CD ,_, 22_CD -RRB-_-RRB- is_VBZ a_DT -_: =_JJ -_: dimensionality_NN reduction_NN method_NN for_IN text_NN document_NN data_NNS ._.
Using_VBG LSI_NNP ,_, the_DT document_NN data_NNS is_VBZ presented_VBN in_IN a_DT lower-dimensional_JJ \_NN topic_NN ''_'' space_NN :_: the_DT documents_NNS are_VBP characterized_VBN by_IN some_DT underlying_JJ -LRB-_-LRB- latent_JJ
an_DT space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- has_VBZ used_VBN random_JJ projections_NNS in_IN learning_VBG high-dimensional_JJ Gaussian_JJ mixture_NN models_NNS ._.
Other_JJ applications_NNS of_IN random_JJ projection_NN include_VBP e.g._FW =_JJ -_: =[_NN 4_CD ,_, 28_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT problems_NNS of_IN dimensionality_NN reduction_NN and_CC similarity_NN search_NN have_VBP often_RB been_VBN addressed_VBN in_IN the_DT information_NN retrieval_NN literature_NN ,_, and_CC other_JJ approaches_NNS than_IN random_JJ projection_NN have_VBP been_VBN presented_VBN ._.
Ost_NN
g_NN the_DT time_NN series_NN into_IN sections_NNS and_CC indexing_NN only_RB the_DT section_NN means_VBZ ._.
Aggarwal_NNP et_FW al._FW -LRB-_-LRB- 2_LS -RRB-_-RRB- index_NN market_NN basket_NN data_NNS by_IN a_DT specic_JJ signature_NN table_NN ,_, which_WDT easens_VBZ the_DT similarity_NN search_NN ._.
Wavelet_NNP transforms_VBZ -LRB-_-LRB- -LRB-_-LRB- =_JJ -_: =_JJ 12_CD ,_, 27_CD -RRB-_-RRB- -_: =_JJ -_: etc._NN -RRB-_-RRB- are_VBP a_DT common_JJ method_NN of_IN signal_NN compression_NN ._.
2_CD ._.
METHODSFORDIMENSIONALITYREDUCTION_NN 2.1_CD Random_NNP projection_NN In_IN random_JJ projection_NN ,_, the_DT original_JJ d-dimensional_JJ data_NNS is_VBZ projected_VBN to_TO a_DT k-dimensional_NN -LRB-_-LRB- ksd_NN -RRB-_-RRB-
\/_: websom_NN \/_: cause_VB signicant_JJ distortions_NNS in_IN the_DT data_NNS set_VBN if_IN R_NN is_VBZ not_RB orthogonal_JJ ._.
Orthogonalizing_NNP R_NNP is_VBZ unfortunately_RB computationally_RB expensive_JJ ._.
Instead_RB ,_, we_PRP can_MD rely_VB on_IN a_DT result_NN presented_VBN by_IN Hecht-Nielsen_JJ -LRB-_-LRB- =_JJ -_: =_JJ 13_CD -RRB-_-RRB- -_: =_JJ -_: :_: in_IN a_DT high-dimensional_JJ space_NN ,_, there_EX exists_VBZ a_DT much_RB larger_JJR number_NN of_IN almost_RB orthogonal_JJ than_IN orthogonal_JJ directions_NNS ._.
Thus_RB vectors_NNS having_VBG random_JJ directions_NNS might_MD be_VB suciently_RB close_JJ to_TO orthogonal_JJ ,_, and_CC equi_NNS
An_DT image_NN is_VBZ presented_VBN as_IN a_DT matrix_NN of_IN pixel_NN brightness_NN values_NNS ,_, the_DT distribution_NN of_IN which_WDT is_VBZ generally_RB approximately_RB Gaussian_JJ :_: symmetric_JJ and_CC bell-shaped_JJ ._.
Text_NN document_NN data_NNS is_VBZ presented_VBN in_IN vector_NN space_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_JJ -_: ,_, in_IN which_WDT each_DT document_NN forms_VBZ one_CD d-dimensional_NN vector_NN where_WRB d_NN is_VBZ the_DT vocabulary_NN size_NN ._.
The_DT i-th_JJ element_NN of_IN the_DT vector_NN indicates_VBZ -LRB-_-LRB- some_DT function_NN of_IN -RRB-_-RRB- the_DT frequency_NN of_IN the_DT i-th_JJ vocabulary_NN term_NN in_IN the_DT doc_NN
ough_VB the_DT origin_NN is_VBZ p_NN k_NN =d_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ._.
The_DT choice_NN of_IN the_DT random_JJ matrix_NN R_NN is_VBZ one_CD of_IN the_DT key_JJ points_NNS of_IN interest_NN ._.
The_DT elements_NNS r_NN ij_NN of_IN R_NN are_VBP often_RB Gaussian_JJ distributed_VBN ,_, but_CC this_DT need_MD not_RB be_VB the_DT case_NN ._.
Achlioptas_NN =_JJ -_: =[_NN 1_CD -_: =-]_NN has_VBZ recently_RB shown_VBN that_IN the_DT Gaussian_JJ distribution_NN can_MD be_VB replaced_VBN by_IN a_DT much_RB simpler_JJR distribution_NN such_JJ as_IN r_NN ij_NN =_JJ p_NN 3_CD 8_CD -RRB-_-RRB- -RRB-_-RRB- :_: +1_NN with_IN probability_NN 1_CD 6_CD 0_CD with_IN probability_NN 2_CD 3_CD 1_CD with_IN probability_NN 1_CD 6_CD :_: -LRB-_-LRB-
of_IN N_NN d-dimensional_NN observations_NNS ,_, X_NN RP_NN kN_NN =_JJ RkdXdN_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- is_VBZ the_DT projection_NN of_IN the_DT data_NNS onto_IN a_DT lower_JJR k-dimensional_NN subspace_NN ._.
The_DT key_JJ idea_NN of_IN random_JJ mapping_NN arises_VBZ from_IN the_DT Johnson-Lindenstrauss_JJ lemma_NN -LRB-_-LRB- 15_CD =_JJ -_: =]_NN -_: =_JJ -_: :_: if_IN points_NNS in_IN a_DT vector_NN space_NN are_VBP projected_VBN onto_IN a_DT randomly_RB selected_VBN subspace_NN of_IN suitably_RB high_JJ dimension_NN ,_, then_RB the_DT distances_NNS between_IN the_DT points_NNS are_VBP approximately_RB preserved_VBN ._.
For_IN a_DT simple_JJ proof_NN of_IN this_DT
o_NN using_VBG LSI_NNP and_CC SOM_NNP ._.
Kleinberg_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- and_CC Indyk_NNP and_CC Motwani_NNP -LRB-_-LRB- 14_CD -RRB-_-RRB- use_VBP random_JJ projections_NNS in_IN nearest-neighbor_JJ search_NN in_IN a_DT high_JJ dimensional_JJ Euclidean_JJ space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN =_JJ -_: =[_NN 6_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: has_VBZ used_VBN random_JJ projections_NNS in_IN learning_VBG high-dimensional_JJ Gaussian_JJ mixture_NN models_NNS ._.
Other_JJ applications_NNS of_IN random_JJ projection_NN include_VBP e.g._FW -LRB-_-LRB- 4_CD ,_, 28_CD -RRB-_-RRB- ._.
The_DT problems_NNS of_IN dimensionality_NN reduction_NN and_CC similarity_NN
ted_VBN set_NN of_IN documents_NNS ._.
In_IN their_PRP$ approach_NN ,_, the_DT columns_NNS of_IN the_DT random_JJ projection_NN matrix_NN are_VBP assumed_VBN strictly_RB orthogonal_JJ ,_, but_CC actually_RB this_DT need_MD not_RB be_VB the_DT case_NN ,_, as_IN we_PRP shall_MD see_VB in_IN our_PRP$ experiments_NNS ._.
Kaski_NN =_JJ -_: =[_NN 17_CD ,_, 16_CD -RRB-_-RRB- -_: =_SYM -_: has_VBZ presented_VBN experimental_JJ results_NNS in_IN using_VBG the_DT random_JJ mapping_NN in_IN the_DT context_NN of_IN the_DT WEBSOM_NN 1_CD system_NN ._.
Kurimo_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- applies_VBZ random_JJ projection_NN to_TO the_DT indexing_NN of_IN audio_JJ documents_NNS ,_, prior_RB to_TO using_VBG LSI_NNP and_CC
in_IN a_DT vector_NN space_NN are_VBP projected_VBN onto_IN a_DT randomly_RB selected_VBN subspace_NN of_IN suitably_RB high_JJ dimension_NN ,_, then_RB the_DT distances_NNS between_IN the_DT points_NNS are_VBP approximately_RB preserved_VBN ._.
For_IN a_DT simple_JJ proof_NN of_IN this_DT result_NN ,_, see_VBP =_JJ -_: =[_NN 10_CD ,_, 8_CD -_: =-]_CD ._.
Random_JJ projection_NN is_VBZ computationally_RB very_RB simple_JJ :_: forming_VBG the_DT random_JJ matrix_NN R_NN and_CC projecting_VBG the_DT d_NN N_NN data_NNS matrix_NN X_NN into_IN k_NN dimensions_NNS is_VBZ of_IN order_NN O_NN -LRB-_-LRB- dkN_NN -RRB-_-RRB- ,_, and_CC if_IN the_DT data_NNS matrix_NN X_NN is_VBZ sparse_JJ with_IN ab_NN
dimensionality_NN reduction_NN and_CC similarity_NN search_NN have_VBP often_RB been_VBN addressed_VBN in_IN the_DT information_NN retrieval_NN literature_NN ,_, and_CC other_JJ approaches_NNS than_IN random_JJ projection_NN have_VBP been_VBN presented_VBN ._.
Ostrovsky_NNP and_CC Rabani_NNP =_SYM -_: =[_NN 21_CD -RRB-_-RRB- -_: =_JJ -_: give_VB a_DT dimension_NN reduction_NN operation_NN that_WDT is_VBZ suitable_JJ for_IN clustering_NN ._.
Agrawal_NNP et_FW al._FW -LRB-_-LRB- 3_LS -RRB-_-RRB- map_NN time_NN series_NN into_IN frequency_NN domain_NN by_IN the_DT discrete_JJ Fourier_NN transform_VB and_CC only_RB retain_VB thesrst_JJ few_JJ frequencie_NNS
se_FW size_NN is_VBZ dd_NN for_IN d-dimensional_JJ data_NNS -RRB-_-RRB- is_VBZ very_RB expensive_JJ to_TO compute_VB ._.
The_DT computational_JJ complexity_NN of_IN estimating_VBG the_DT PCA_NNP is_VBZ O_NN -LRB-_-LRB- d_NN 2_CD N_NN -RRB-_-RRB- +_CC O_NN -LRB-_-LRB- d_NN 3_CD -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
There_EX exists_VBZ computationally_RB less_RBR expensive_JJ methods_NNS -LRB-_-LRB- =_JJ -_: =_JJ 26_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: forsnding_VBG only_RB a_DT few_JJ eigenvectors_NNS and_CC eigenvalues_NNS of_IN a_DT large_JJ matrix_NN ;_: in_IN our_PRP$ experiments_NNS ,_, we_PRP use_VBP appropriate_JJ Matlab_NNP routines_NNS to_TO realize_VB these_DT ._.
A_DT closely_RB related_JJ method_NN is_VBZ singular_JJ value_NN decomposition_NN
n_NN methods_NNS ._.
Section_NN 3_CD gives_VBZ the_DT experimental_JJ results_NNS of_IN dimensionality_NN reduction_NN on_IN image_NN data_NNS ,_, and_CC Section_NNP 4_CD on_IN text_NN data_NNS ._.
Finally_RB ,_, Section_NN 5_CD gives_VBZ a_DT conclusion_NN ._.
1.1_CD Related_JJ work_NN Papadimitriou_NNP et_FW al._FW =_SYM -_: =[_NN 22_CD -_: =-]_CD use_NN random_JJ projection_NN in_IN the_DT preprocessing_NN of_IN textual_JJ data_NNS ,_, prior_RB to_TO applying_VBG LSI_NNP ._.
They_PRP present_VBP experimental_JJ results_NNS on_IN an_DT articially_RB generated_VBN set_NN of_IN documents_NNS ._.
In_IN their_PRP$ approach_NN ,_, the_DT columns_NNS of_IN th_DT
se_FW size_NN is_VBZ dd_NN for_IN d-dimensional_JJ data_NNS -RRB-_-RRB- is_VBZ very_RB expensive_JJ to_TO compute_VB ._.
The_DT computational_JJ complexity_NN of_IN estimating_VBG the_DT PCA_NNP is_VBZ O_NN -LRB-_-LRB- d_NN 2_CD N_NN -RRB-_-RRB- +_CC O_NN -LRB-_-LRB- d_NN 3_CD -RRB-_-RRB- -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
There_EX exists_VBZ computationally_RB less_RBR expensive_JJ methods_NNS -LRB-_-LRB- =_JJ -_: =_JJ 26_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: forsnding_VBG only_RB a_DT few_JJ eigenvectors_NNS and_CC eigenvalues_NNS of_IN a_DT large_JJ matrix_NN ;_: in_IN our_PRP$ experiments_NNS ,_, we_PRP use_VBP appropriate_JJ Matlab_NNP routines_NNS to_TO realize_VB these_DT ._.
A_DT closely_RB related_JJ method_NN is_VBZ singular_JJ value_NN decomposition_NN
ng_NN the_DT random_JJ mapping_NN in_IN the_DT context_NN of_IN the_DT WEBSOM_NN 1_CD system_NN ._.
Kurimo_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- applies_VBZ random_JJ projection_NN to_TO the_DT indexing_NN of_IN audio_JJ documents_NNS ,_, prior_RB to_TO using_VBG LSI_NNP and_CC SOM_NNP ._.
Kleinberg_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- and_CC Indyk_NNP and_CC Motwani_NNP =_SYM -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: use_VB random_JJ projections_NNS in_IN nearest-neighbor_JJ search_NN in_IN a_DT high_JJ dimensional_JJ Euclidean_JJ space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- has_VBZ used_VBN random_JJ projections_NNS in_IN learning_VBG high-dimensiona_NNS
X_NN SV_NN D_NN =_JJ U_NN T_NN k_NN X_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- where_WRB Uk_NN is_VBZ of_IN size_NN dk_NN and_CC contains_VBZ these_DT k_NN singular_JJ vectors_NNS ._.
Like_IN PCA_NNP ,_, SVD_NNP is_VBZ also_RB expensive_JJ to_TO compute_VB ._.
There_EX exists_VBZ numerical_JJ routines_NNS such_JJ as_IN the_DT power_NN or_CC the_DT Lanczos_NN method_NN -LRB-_-LRB- =_JJ -_: =_JJ 5_CD -_: =-]_NN that_WDT are_VBP more_RBR ecient_JJ than_IN PCA_NNP for_IN sparse_JJ data_NNS matrices_NNS X_NN ,_, and_CC that_DT is_VBZ why_WRB we_PRP shall_MD use_VB SVD_NNP instead_RB of_IN PCA_NNP in_IN the_DT context_NN of_IN sparse_JJ text_NN document_NN data_NNS ._.
For_IN a_DT sparse_JJ data_NN matrix_NN XdN_NN with_IN about_IN c_NN nonze_NN
o_NN using_VBG LSI_NNP and_CC SOM_NNP ._.
Kleinberg_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- and_CC Indyk_NNP and_CC Motwani_NNP -LRB-_-LRB- 14_CD -RRB-_-RRB- use_VBP random_JJ projections_NNS in_IN nearest-neighbor_JJ search_NN in_IN a_DT high_JJ dimensional_JJ Euclidean_JJ space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN =_JJ -_: =[_NN 6_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: has_VBZ used_VBN random_JJ projections_NNS in_IN learning_VBG high-dimensional_JJ Gaussian_JJ mixture_NN models_NNS ._.
Other_JJ applications_NNS of_IN random_JJ projection_NN include_VBP e.g._FW -LRB-_-LRB- 4_CD ,_, 28_CD -RRB-_-RRB- ._.
The_DT problems_NNS of_IN dimensionality_NN reduction_NN and_CC similarity_NN
but_CC actually_RB this_DT need_MD not_RB be_VB the_DT case_NN ,_, as_IN we_PRP shall_MD see_VB in_IN our_PRP$ experiments_NNS ._.
Kaski_NN -LRB-_-LRB- 17_CD ,_, 16_CD -RRB-_-RRB- has_VBZ presented_VBN experimental_JJ results_NNS in_IN using_VBG the_DT random_JJ mapping_NN in_IN the_DT context_NN of_IN the_DT WEBSOM_NN 1_CD system_NN ._.
Kurimo_NN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: applies_VBZ random_JJ projection_NN to_TO the_DT indexing_NN of_IN audio_JJ documents_NNS ,_, prior_RB to_TO using_VBG LSI_NNP and_CC SOM_NNP ._.
Kleinberg_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- and_CC Indyk_NNP and_CC Motwani_NNP -LRB-_-LRB- 14_CD -RRB-_-RRB- use_VBP random_JJ projections_NNS in_IN nearest-neighbor_JJ search_NN in_IN a_DT high_JJ dimensi_NN
CT_NN is_VBZ also_RB optimal_JJ for_IN human_JJ eye_NN :_: the_DT distortions_NNS introduced_VBN occur_VBP at_IN the_DT highest_JJS frequencies_NNS only_RB ,_, and_CC the_DT human_JJ eye_NN tends_VBZ to_TO neglect_NN these_DT as_IN noise_NN ._.
DCT_NNP can_MD be_VB performed_VBN by_IN simple_JJ matrix_NN operations_NNS =_JJ -_: =[_NN 23_CD ,_, 27_CD -RRB-_-RRB- -_: =_JJ -_: :_: an_DT image_NN is_VBZ transformed_VBN to_TO the_DT DCT_NNP space_NN and_CC dimensionality_NN reduction_NN is_VBZ done_VBN in_IN the_DT inverse_NN transform_VB by_IN discarding_VBG the_DT transform_VB coecients_NNS corresponding_VBG to_TO the_DT highest_JJS frequencies_NNS ._.
Computing_NNP the_DT D_NN
uction_NN operation_NN that_WDT is_VBZ suitable_JJ for_IN clustering_NN ._.
Agrawal_NNP et_FW al._FW -LRB-_-LRB- 3_LS -RRB-_-RRB- map_NN time_NN series_NN into_IN frequency_NN domain_NN by_IN the_DT discrete_JJ Fourier_NN transform_VB and_CC only_RB retain_VB thesrst_JJ few_JJ frequencies_NNS ._.
Keogh_NNP and_CC Pazzani_NNP =_SYM -_: =[_NN 18_CD -_: =-]_CD reduce_VB the_DT dimension_NN of_IN time_NN series_NN data_NNS by_IN segmenting_VBG the_DT time_NN series_NN into_IN sections_NNS and_CC indexing_NN only_RB the_DT section_NN means_VBZ ._.
Aggarwal_NNP et_FW al._FW -LRB-_-LRB- 2_LS -RRB-_-RRB- index_NN market_NN basket_NN data_NNS by_IN a_DT specic_JJ signature_NN table_NN ,_, wh_NN
g_NN the_DT time_NN series_NN into_IN sections_NNS and_CC indexing_NN only_RB the_DT section_NN means_VBZ ._.
Aggarwal_NNP et_FW al._FW -LRB-_-LRB- 2_LS -RRB-_-RRB- index_NN market_NN basket_NN data_NNS by_IN a_DT specic_JJ signature_NN table_NN ,_, which_WDT easens_VBZ the_DT similarity_NN search_NN ._.
Wavelet_NNP transforms_VBZ -LRB-_-LRB- -LRB-_-LRB- =_JJ -_: =_JJ 12_CD ,_, 27_CD -RRB-_-RRB- -_: =_JJ -_: etc._NN -RRB-_-RRB- are_VBP a_DT common_JJ method_NN of_IN signal_NN compression_NN ._.
2_CD ._.
METHODSFORDIMENSIONALITYREDUCTION_NN 2.1_CD Random_NNP projection_NN In_IN random_JJ projection_NN ,_, the_DT original_JJ d-dimensional_JJ data_NNS is_VBZ projected_VBN to_TO a_DT k-dimensional_NN -LRB-_-LRB- ksd_NN -RRB-_-RRB-
n_NN retrieval_NN literature_NN ,_, and_CC other_JJ approaches_NNS than_IN random_JJ projection_NN have_VBP been_VBN presented_VBN ._.
Ostrovsky_NNP and_CC Rabani_NNP -LRB-_-LRB- 21_CD -RRB-_-RRB- give_VBP a_DT dimension_NN reduction_NN operation_NN that_WDT is_VBZ suitable_JJ for_IN clustering_NN ._.
Agrawal_NNP et_FW al._FW =_SYM -_: =[_NN 3_CD -RRB-_-RRB- -_: =_JJ -_: map_NN time_NN series_NN into_IN frequency_NN domain_NN by_IN the_DT discrete_JJ Fourier_NN transform_VB and_CC only_RB retain_VB thesrst_JJ few_JJ frequencies_NNS ._.
Keogh_NNP and_CC Pazzani_NNP -LRB-_-LRB- 18_CD -RRB-_-RRB- reduce_VB the_DT dimension_NN of_IN time_NN series_NN data_NNS by_IN segmenting_VBG the_DT time_NN
in_IN a_DT vector_NN space_NN are_VBP projected_VBN onto_IN a_DT randomly_RB selected_VBN subspace_NN of_IN suitably_RB high_JJ dimension_NN ,_, then_RB the_DT distances_NNS between_IN the_DT points_NNS are_VBP approximately_RB preserved_VBN ._.
For_IN a_DT simple_JJ proof_NN of_IN this_DT result_NN ,_, see_VBP =_JJ -_: =[_NN 10_CD ,_, 8_CD -_: =-]_CD ._.
Random_JJ projection_NN is_VBZ computationally_RB very_RB simple_JJ :_: forming_VBG the_DT random_JJ matrix_NN R_NN and_CC projecting_VBG the_DT d_NN N_NN data_NNS matrix_NN X_NN into_IN k_NN dimensions_NNS is_VBZ of_IN order_NN O_NN -LRB-_-LRB- dkN_NN -RRB-_-RRB- ,_, and_CC if_IN the_DT data_NNS matrix_NN X_NN is_VBZ sparse_JJ with_IN ab_NN
an_DT space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- has_VBZ used_VBN random_JJ projections_NNS in_IN learning_VBG high-dimensional_JJ Gaussian_JJ mixture_NN models_NNS ._.
Other_JJ applications_NNS of_IN random_JJ projection_NN include_VBP e.g._FW =_JJ -_: =[_NN 4_CD ,_, 28_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT problems_NNS of_IN dimensionality_NN reduction_NN and_CC similarity_NN search_NN have_VBP often_RB been_VBN addressed_VBN in_IN the_DT information_NN retrieval_NN literature_NN ,_, and_CC other_JJ approaches_NNS than_IN random_JJ projection_NN have_VBP been_VBN presented_VBN ._.
Ost_NN
d_NN only_RB retain_VBP thesrst_JJ few_JJ frequencies_NNS ._.
Keogh_NNP and_CC Pazzani_NNP -LRB-_-LRB- 18_CD -RRB-_-RRB- reduce_VB the_DT dimension_NN of_IN time_NN series_NN data_NNS by_IN segmenting_VBG the_DT time_NN series_NN into_IN sections_NNS and_CC indexing_NN only_RB the_DT section_NN means_VBZ ._.
Aggarwal_NNP et_FW al._FW =_SYM -_: =[_NN 2_CD -_: =-]_CD index_NN market_NN basket_NN data_NNS by_IN a_DT specic_JJ signature_NN table_NN ,_, which_WDT easens_VBZ the_DT similarity_NN search_NN ._.
Wavelet_NNP transforms_VBZ -LRB-_-LRB- -LRB-_-LRB- 12_CD ,_, 27_CD -RRB-_-RRB- etc._NN -RRB-_-RRB- are_VBP a_DT common_JJ method_NN of_IN signal_NN compression_NN ._.
2_CD ._.
METHODSFORDIMENSIONALITYRED_NN
experimental_JJ results_NNS in_IN using_VBG the_DT random_JJ mapping_NN in_IN the_DT context_NN of_IN the_DT WEBSOM_NN 1_CD system_NN ._.
Kurimo_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- applies_VBZ random_JJ projection_NN to_TO the_DT indexing_NN of_IN audio_JJ documents_NNS ,_, prior_RB to_TO using_VBG LSI_NNP and_CC SOM_NNP ._.
Kleinberg_NN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Indyk_NNP and_CC Motwani_NNP -LRB-_-LRB- 14_CD -RRB-_-RRB- use_VBP random_JJ projections_NNS in_IN nearest-neighbor_JJ search_NN in_IN a_DT high_JJ dimensional_JJ Euclidean_JJ space_NN ,_, and_CC also_RB present_JJ theoretical_JJ insights_NNS ._.
Dasgupta_NN -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- has_VBZ used_VBN random_JJ projections_NNS
