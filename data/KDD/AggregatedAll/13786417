Trading_NN representability_NN for_IN scalability_NN :_: adaptive_JJ multi-hyperplane_JJ machine_NN for_IN nonlinear_JJ classification_NN
Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- are_VBP among_IN the_DT most_RBS popular_JJ and_CC successful_JJ classification_NN algorithms_NNS ._.
Kernel_NNP SVMs_NNS often_RB reach_VBP state-of-the-art_JJ accuracies_NNS ,_, but_CC suffer_VBP from_IN the_DT curse_NN of_IN kernelization_NN due_JJ to_TO linear_JJ model_NN growth_NN with_IN data_NNS size_NN on_IN noisy_JJ data_NNS ._.
Linear_JJ SVMs_NNS have_VBP the_DT ability_NN to_TO efficiently_RB learn_VB from_IN truly_RB large_JJ data_NNS ,_, but_CC they_PRP are_VBP applicable_JJ to_TO a_DT limited_JJ number_NN of_IN domains_NNS due_JJ to_TO low_JJ representational_JJ power_NN ._.
To_TO fill_VB the_DT representability_NN and_CC scalability_NN gap_NN between_IN linear_JJ and_CC nonlinear_JJ SVMs_NNS ,_, we_PRP propose_VBP the_DT Adaptive_JJ Multi-hyperplane_JJ Machine_NN -LRB-_-LRB- AMM_NN -RRB-_-RRB- algorithm_NN that_WDT accomplishes_VBZ fast_JJ training_NN and_CC prediction_NN and_CC has_VBZ capability_NN to_TO solve_VB nonlinear_JJ classification_NN problems_NNS ._.
AMM_NN model_NN consists_VBZ of_IN a_DT set_NN of_IN hyperplanes_NNS -LRB-_-LRB- weights_NNS -RRB-_-RRB- ,_, each_DT assigned_VBN to_TO one_CD of_IN the_DT multiple_JJ classes_NNS ,_, and_CC predicts_VBZ based_VBN on_IN the_DT associated_VBN class_NN of_IN the_DT weight_NN that_WDT provides_VBZ the_DT largest_JJS prediction_NN ._.
The_DT number_NN of_IN weights_NNS is_VBZ automatically_RB determined_VBN through_IN an_DT iterative_JJ algorithm_NN based_VBN on_IN the_DT stochastic_JJ gradient_NN descent_NN algorithm_NN which_WDT is_VBZ guaranteed_VBN to_TO converge_VB to_TO a_DT local_JJ optimum_NN ._.
Since_IN the_DT generalization_NN bound_VBD decreases_NNS with_IN the_DT number_NN of_IN weights_NNS ,_, a_DT weight_NN pruning_NN mechanism_NN is_VBZ proposed_VBN and_CC analyzed_VBN ._.
The_DT experiments_NNS on_IN several_JJ large_JJ data_NNS sets_NNS show_VBP that_IN AMM_NN is_VBZ nearly_RB as_RB fast_JJ during_IN training_NN and_CC prediction_NN as_IN the_DT state-of-the-art_JJ linear_JJ SVM_NN solver_NN and_CC that_IN it_PRP can_MD be_VB orders_NNS of_IN magnitude_NN faster_RBR than_IN kernel_NN SVM_NN ._.
In_IN accuracy_NN ,_, AMM_NN is_VBZ somewhere_RB between_IN linear_NN and_CC kernel_NN SVMs_NNS ._.
For_IN example_NN ,_, on_IN an_DT OCR_NN task_NN with_IN 8_CD million_CD highly_RB dimensional_JJ training_NN examples_NNS ,_, AMM_NNP trained_VBD in_IN 300_CD seconds_NNS on_IN a_DT single-core_JJ processor_NN had_VBD 0.54_CD %_NN error_NN rate_NN ,_, which_WDT was_VBD significantly_RB lower_JJR than_IN 2.03_CD %_NN error_NN rate_NN of_IN a_DT linear_JJ SVM_NN trained_VBN in_IN the_DT same_JJ time_NN and_CC comparable_JJ to_TO 0.43_CD %_NN error_NN rate_NN of_IN a_DT kernel_NN SVM_NN trained_VBN in_IN 2_CD days_NNS on_IN 512_CD processors_NNS ._.
The_DT results_NNS indicate_VBP that_IN AMM_NN could_MD be_VB an_DT attractive_JJ option_NN when_WRB solving_VBG large-scale_JJ classification_NN problems_NNS ._.
The_DT software_NN is_VBZ available_JJ at_IN www.dabi.temple.edu\/~vucetic\/AMM.html_NN ._.
