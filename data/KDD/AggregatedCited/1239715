Sequential_JJ cost-sensitive_JJ decision_NN making_VBG with_IN reinforcement_NN learning_VBG
Recently_RB ,_, there_EX has_VBZ been_VBN increasing_VBG interest_NN in_IN the_DT issues_NNS of_IN cost-sensitive_JJ learning_NN and_CC decision_NN making_NN in_IN a_DT variety_NN of_IN applications_NNS of_IN data_NN mining_NN ._.
A_DT number_NN of_IN approaches_NNS have_VBP been_VBN developed_VBN that_WDT are_VBP effective_JJ at_IN optimizing_VBG cost-sensitive_JJ decisions_NNS when_WRB each_DT decision_NN is_VBZ considered_VBN in_IN isolation_NN ._.
However_RB ,_, the_DT issue_NN of_IN sequential_JJ decision_NN making_NN ,_, with_IN the_DT goal_NN of_IN maximizing_VBG total_JJ benefits_NNS accrued_VBN over_IN a_DT period_NN of_IN time_NN instead_RB of_IN immediate_JJ benefits_NNS ,_, has_VBZ rarely_RB been_VBN addressed_VBN ._.
In_IN the_DT present_JJ paper_NN ,_, we_PRP propose_VBP a_DT novel_JJ approach_NN to_TO sequential_JJ decision_NN making_NN based_VBN on_IN the_DT reinforcement_NN learning_NN framework_NN ._.
Our_PRP$ approach_NN attempts_VBZ to_TO learn_VB decision_NN rules_NNS that_WDT optimize_VBP a_DT sequence_NN of_IN cost-sensitive_JJ decisions_NNS so_RB as_IN to_TO maximize_VB the_DT total_JJ benefits_NNS accrued_VBN over_IN time_NN ._.
We_PRP use_VBP the_DT domain_NN of_IN targeted_VBN '_POS marketing_NN as_IN a_DT testbed_NN for_IN empirical_JJ evaluation_NN of_IN the_DT proposed_VBN method_NN ._.
We_PRP conducted_VBD experiments_NNS using_VBG approximately_RB two_CD years_NNS of_IN monthly_JJ promotion_NN data_NNS derived_VBN from_IN the_DT well-known_JJ KDD_NNP Cup_NNP 1998_CD donation_NN data_NNS set_VBN ._.
The_DT experimental_JJ results_NNS show_VBP that_IN the_DT proposed_VBN method_NN for_IN optimizing_VBG total_JJ accrued_JJ benefits_NNS out_RP performs_VBZ the_DT usual_JJ targeted-marketing_JJ methodology_NN of_IN optimizing_VBG each_DT promotion_NN in_IN isolation_NN ._.
We_PRP also_RB analyze_VBP the_DT behavior_NN of_IN the_DT targeting_VBG rules_NNS that_WDT were_VBD obtained_VBN and_CC discuss_VBP their_PRP$ appropriateness_NN to_TO the_DT application_NN domain_NN ._.
ial_JJ nature_NN of_IN some_DT CRM_NNP problems_NNS ,_, Pednault_NNP et_FW al._FW applied_VBD the_DT framework_NN of_IN reinforcement_NN learning_VBG to_TO address_VB the_DT issue_NN of_IN sequential_JJ decision_NN making_VBG when_WRB interactions_NNS can_MD occur_VB among_IN decision_NN outcomes_NNS =_JJ -_: =[_NN 34_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Reinforcement_NN learning_NN refers_VBZ to_TO a_DT class_NN of_IN problems_NNS and_CC associated_VBN techniques_NNS in_IN which_WDT the_DT learner_NN is_VBZ to_TO learn_VB how_WRB to_TO make_VB sequential_JJ decisions_NNS based_VBN on_IN delayed_VBN reinforcement_NN so_RB as_IN to_TO maximize_VB cumu_NN
ave_RB been_VBN developed_VBN -LRB-_-LRB- 4_CD ,_, 17_CD ,_, 6_CD -RRB-_-RRB- that_IN out-perform_JJ classification_NN based_VBN methods_NNS ._.
The_DT problem_NN of_IN optimizing_VBG a_DT sequence_NN of_IN cost-sensitive_JJ decisions_NNS ,_, however_RB ,_, has_VBZ rarely_RB been_VBN addressed_VBN ._.
In_IN a_DT companion_NN paper_NN =_JJ -_: =[_NN 10_CD -_: =-]_CD ,_, we_PRP proposed_VBD to_TO apply_VB the_DT framework_NN of_IN reinforcement_NN learning_VBG to_TO address_VB the_DT issue_NN This_DT author_NN 's_POS present_JJ address_NN :_: Dept._NNP of_IN Comp_NNP ._.
Sci_NNP ._.
and_CC Eng_NNP ._.
U.C.S.D._NNP ,_, La_NNP Jolla_NNP ,_, CA_NNP 92093_CD ._.
The_DT work_NN was_VBD performed_VBN w_NN
an_DT be_VB quickly_RB and_CC effectively_RB approximated_VBN by_IN examining_VBG simple_JJ attributes_NNS of_IN the_DT queue_NN of_IN waiting_VBG instances_NNS ._.
There_EX are_VBP no_DT known_JJ existing_VBG methods_NNS for_IN classifying_VBG sequences_NNS of_IN time_NN sensitive_JJ instances_NNS ._.
=_SYM -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: study_VB a_DT sequential_JJ CSC_NNP problem_NN where_WRB the_DT cost_NN of_IN each_DT instance_NN is_VBZ dependent_JJ on_IN the_DT labels_NNS assigned_VBN to_TO prior_JJ instances_NNS ._.
Reinforcement_NN learning_NN is_VBZ used_VBN to_TO minimize_VB costs_NNS over_IN a_DT sequence_NN of_IN interacting_VBG
of_IN reinforcement_NN learning_NN methods_NNS -LRB-_-LRB- 57_CD ,_, 19_CD -RRB-_-RRB- ._.
Its_PRP$ importance_NN notwithstanding_IN ,_, in_IN this_DT paper_NN we_PRP will_MD not_RB consider_VB further_RB the_DT on-line_JJ exploration\/exploitation_NN trade-off_NN ;_: the_DT interested_JJ reader_NN should_MD see_VB =_JJ -_: =[_NN 42_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Instead_RB ,_, we_PRP will_MD look_VB more_RBR deeply_RB at_IN the_DT problem_NN of_IN acquiring_VBG data_NNS for_IN improving_VBG the_DT performance_NN of_IN a_DT model_NN in_IN settings_NNS where_WRB there_EX will_MD be_VB explicit_JJ training_NN and_CC testing_NN phases_NNS ._.
The_DT general_JJ ideas_NNS ap_IN
between_IN activities_NNS initiated_VBN to_TO support_VB learning_NN and_CC those_DT that_WDT exploit_VBP what_WP is_VBZ already_RB known_VBN has_VBZ been_VBN explored_VBN in_IN the_DT robotics_NNS literature_NN ;_: a_DT similar_JJ framework_NN may_MD prove_VB useful_JJ for_IN business_NN as_RB well_RB -LRB-_-LRB- =_JJ -_: =_JJ Pednault_NNP et_FW al._FW ,_, 2002_CD -_: =--RRB-_NN ._.
6_CD ._.
Conclusion_NN and_CC Implications_NNS Because_IN the_DT information_NN required_VBN for_IN effective_JJ predictive_JJ modeling_NN often_RB is_VBZ costly_JJ to_TO obtain_VB ,_, it_PRP is_VBZ beneficial_JJ to_TO devise_VB mechanisms_NNS to_TO direct_VB the_DT acquisition_NN of_IN data_NNS
ue_NN MDP_NN ._.
Furthermore_RB ,_, it_PRP could_MD be_VB difficult_JJ to_TO quantify_VB how_WRB suboptimal_JJ it_PRP is_VBZ ._.
We_PRP can_MD also_RB use_VB indirect_JJ methods_NNS and_CC learn_VB a_DT Q-value_JJ function_NN using_VBG the_DT data_NNS ._.
This_DT is_VBZ the_DT approach_NN taken_VBN by_IN Pednault_NNP et_FW al._FW =_SYM -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, called_VBN batch_NN reinforcement_NN learning_NN ._.
However_RB ,_, as_IN we_PRP saw_VBD in_IN Section_NN 2.1_CD indirect_JJ methods_NNS with_IN function_NN approximation_NN are_VBP not_RB guaranteed_VBN to_TO yield_VB a_DT good_JJ policy_NN ._.
Another_DT problem_NN is_VBZ that_IN we_PRP still_RB need_VBP t_NN
a_DT different_JJ action_NN plan_NN for_IN each_DT different_JJ customer_NN ._.
This_DT makes_VBZ the_DT group-marketing_JJ problem_NN different_JJ from_IN the_DT direct-marketing_NN problem_NN that_IN some_DT authors_NNS have_VBP considered_VBN in_IN the_DT data_NNS mining_NN literature_NN =_JJ -_: =[_NN 8_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN group_NN or_CC batch_NN marketing_NN ,_, we_PRP are_VBP interested_JJ in_IN finding_VBG a_DT plan_NN containing_VBG no_DT conditional_JJ branches_NNS ._.
We_PRP must_MD build_VB an_DT N-step_JJ plan_NN ahead_RB of_IN time_NN ,_, and_CC evaluate_VB the_DT plan_NN according_VBG to_TO cross-validation_NN
irline_NN ._.
1_CD Introduction_NN In_IN the_DT last_JJ years_NNS there_EX has_VBZ been_VBN an_DT increasing_VBG interest_NN in_IN the_DT allocation_NN of_IN marketing_NN resources_NNS both_CC in_IN the_DT marketing_NN -LRB-_-LRB- e.g._FW -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 19_CD -RRB-_-RRB- -RRB-_-RRB- and_CC in_IN the_DT data_NNS mining_NN -LRB-_-LRB- e.g._FW -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 4_CD -RRB-_-RRB- -RRB-_-RRB- communities_NNS ._.
There_EX is_VBZ common_JJ agreement_NN that_IN marketing_NN initiatives_NNS should_MD be_VB evaluated_VBN by_IN measuring_VBG their_PRP$ impact_NN on_IN the_DT customer_NN lifetime_NN value_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, i.e._FW the_DT long-term_JJ value_NN generated_VBN by_IN a_DT relat_NN
of_IN reinforcement_NN learning_NN methods_NNS -LRB-_-LRB- 57_CD ,_, 19_CD -RRB-_-RRB- ._.
Its_PRP$ importance_NN notwithstanding_IN ,_, in_IN this_DT paper_NN we_PRP will_MD not_RB consider_VB further_RB the_DT on-line_JJ exploration\/exploitation_NN trade-off_NN ;_: the_DT interested_JJ reader_NN should_MD see_VB =_JJ -_: =[_NN 42_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Instead_RB ,_, we_PRP will_MD look_VB more_RBR deeply_RB at_IN the_DT problem_NN of_IN acquiring_VBG data_NNS for_IN improving_VBG the_DT performance_NN of_IN a_DT model_NN in_IN settings_NNS where_WRB there_EX will_MD be_VB explicit_JJ training_NN and_CC testing_NN phases_NNS ._.
The_DT general_JJ ideas_NNS ap_IN
es_NNS used_VBN ._.
2_CD Related_NNP Work_NNP Cost-sensitive_JJ regression_NN and_CC cost-sensitive_JJ classification_NN in_IN non-sequential_JJ decision_NN making_NN ,_, supervised_JJ learning_NN settings_NNS have_VBP been_VBN widely_RB studied_VBN ,_, for_IN example_NN in_IN -LRB-_-LRB- 1_CD -RRB-_-RRB- ._.
While_IN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: addresses_VBZ one_CD aspect_NN of_IN cost-sensitive_JJ sequential_JJ decision-making_NN ,_, it_PRP should_MD be_VB noted_VBN that_IN in_IN their_PRP$ special_JJ case_NN ,_, cost_NN is_VBZ only_RB associated_VBN with_IN actions_NNS --_: not_RB with_IN observing_VBG state_NN features_NNS --_: so_RB standar_JJ
