A_DT cross-collection_JJ mixture_NN model_NN for_IN comparative_JJ text_NN mining_NN
In_IN this_DT paper_NN ,_, we_PRP define_VBP and_CC study_VBP a_DT novel_JJ text_NN mining_NN problem_NN ,_, which_WDT we_PRP refer_VBP to_TO as_IN Comparative_JJ Text_NN Mining_NN -LRB-_-LRB- CTM_NN -RRB-_-RRB- ._.
Given_VBN a_DT set_NN of_IN comparable_JJ text_NN collections_NNS ,_, the_DT task_NN of_IN comparative_JJ text_NN mining_NN is_VBZ to_TO discover_VB any_DT latent_JJ common_JJ themes_NNS across_IN all_DT collections_NNS as_RB well_RB as_IN summarize_VB the_DT similarity_NN and_CC differences_NNS of_IN these_DT collections_NNS along_IN each_DT common_JJ theme_NN ._.
This_DT general_JJ problem_NN subsumes_VBZ many_JJ interesting_JJ applications_NNS ,_, including_VBG business_NN intelligence_NN and_CC opinion_NN summarization_NN ._.
We_PRP propose_VBP a_DT generative_JJ probabilistic_JJ mixture_NN model_NN for_IN comparative_JJ text_NN mining_NN ._.
The_DT model_NN simultaneously_RB performs_VBZ cross-collection_JJ clustering_NN and_CC within-collection_NN clustering_NN ,_, and_CC can_MD be_VB applied_VBN to_TO an_DT arbitrary_JJ set_NN of_IN comparable_JJ text_NN collections_NNS ._.
The_DT model_NN can_MD be_VB estimated_VBN efficiently_RB using_VBG the_DT Expectation-Maximization_NN -LRB-_-LRB- EM_NN -RRB-_-RRB- algorithm_NN ._.
We_PRP evaluate_VBP the_DT model_NN on_IN two_CD different_JJ text_NN data_NNS sets_NNS -LRB-_-LRB- i.e._FW ,_, a_DT news_NN article_NN data_NNS set_NN and_CC a_DT laptop_JJ review_NN data_NNS set_VBN -RRB-_-RRB- ,_, and_CC compare_VB it_PRP with_IN a_DT baseline_NN clustering_NN method_NN also_RB based_VBN on_IN a_DT mixture_NN model_NN ._.
Experiment_NN results_NNS show_VBP that_IN the_DT model_NN is_VBZ quite_RB effective_JJ in_IN discovering_VBG the_DT latent_JJ common_JJ themes_NNS across_IN collections_NNS and_CC performs_VBZ significantly_RB better_JJR than_IN our_PRP$ baseline_NN mixture_NN model_NN ._.
of_IN all_DT of_IN its_PRP$ documents_NNS so_IN that_IN actually_RB there_EX is_VBZ no_DT difference_NN to_TO the_DT common_JJ TFIDF_NN formula_NN ._.
Our_PRP$ approach_NN is_VBZ also_RB situated_VBN in_IN the_DT context_NN of_IN contrastive_JJ summarization_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- and_CC comparative_JJ text_NN mining_NN =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: which_WDT is_VBZ a_DT subtask_NN of_IN contextual_JJ text_NN mining_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- ._.
Contrastive_JJ summarization_NN has_VBZ a_DT rather_RB narrow_JJ application_NN field_NN ,_, it_PRP only_RB regards_VBZ the_DT binary_JJ case_NN -LRB-_-LRB- two_CD classes_NNS -RRB-_-RRB- and_CC is_VBZ focused_VBN on_IN opinion_NN mining_NN ._.
Hav_NN
evaluation_NN measures_NNS like_IN Basic_JJ Elements_NNS -LRB-_-LRB- BE_VB -RRB-_-RRB- ,_, as_RB well_RB as_IN a_DT comparison_NN of_IN our_PRP$ cluster_NN algorithm_NN with_IN baseline_NN ranking_NN strategies_NNS like_IN TF_NN ∗_NN IDF_NN ,_, 9_CD can_MD be_VB found_VBN in_IN -LRB-_-LRB- 7_CD ,_, 8_CD -RRB-_-RRB- ._.
6_CD Related_JJ Work_NN and_CC Discussion_NN In_IN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: the_DT authors_NNS define_VBP the_DT problem_NN of_IN ``_`` comparative_JJ text_NN mining_NN ''_'' -LRB-_-LRB- CTM_NNP -RRB-_-RRB- for_IN a_DT given_VBN text_NN collection_NN as_IN ``_`` -LRB-_-LRB- 1_LS -RRB-_-RRB- discovering_VBG the_DT different_JJ common_JJ themes_NNS across_IN all_PDT the_DT collections_NNS ;_: -LRB-_-LRB- 2_LS -RRB-_-RRB- for_IN each_DT discovered_VBN theme_NN
rd_NN depends_VBZ on_IN the_DT relevance_NN in_IN the_DT topic_NN and_CC on_IN the_DT perspective_NN of_IN the_DT speaker_NN or_CC author_NN ._.
ccLDA_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- builds_VBZ on_IN the_DT standard_JJ LDA_NN model_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- and_CC the_DT cross-collection_JJ mixture_NN model_NN -LRB-_-LRB- ccMix_NN -RRB-_-RRB- by_IN Zhai_NNP et_FW al._FW =_SYM -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
ccLDA_NN discovers_VBZ the_DT topics_NNS across_IN multiple_JJ text_NN collection_NN and_CC estimates_NNS for_IN each_DT topic_NN a_DT shared_JJ distribution_NN and_CC collection_NN specific_JJ distributions_NNS ._.
The_DT model_NN of_IN Lin_NNP et_FW al._FW -LRB-_-LRB- 12_CD -RRB-_-RRB- assigns_VBZ every_DT word_NN a_DT
provide_VB a_DT good_JJ framework_NN for_IN such_JJ studies_NNS ._.
Thus_RB ,_, we_PRP propose_VBP here_RB a_DT new_JJ model_NN ,_, ccLDA_NN ,_, which_WDT extends_VBZ over_IN the_DT Latent_JJ Dirichlet_NNP Allocation_NNP -LRB-_-LRB- LDA_NNP -RRB-_-RRB- -LRB-_-LRB- Blei_NNP et_FW al._FW ,_, 2003_CD -RRB-_-RRB- and_CC crosscollection_NN mixture_NN -LRB-_-LRB- ccMix_NN -RRB-_-RRB- -LRB-_-LRB- =_JJ -_: =_JJ Zhai_NNP et_FW al._FW ,_, 2004_CD -_: =--RRB-_CD models_NNS on_IN blogs_NNS and_CC forums_NNS ._.
We_PRP also_RB provide_VBP a_DT qualitative_JJ and_CC quantitative_JJ analysis_NN of_IN the_DT model_NN on_IN the_DT cross-cultural_JJ data_NNS ._.
1_CD Introduction_NN In_IN today_NN 's_POS society_NN ,_, people_NNS from_IN different_JJ cultural_JJ backgro_NN
s_NN in_IN blogs_NNS -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC to_TO track_VB news_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
We_PRP coded_VBD the_DT algorithm_NN following_VBG the_DT description_NN in_IN the_DT original_JJ paper_NN ._.
An_DT extension_NN of_IN probabilistic_JJ mixture_NN method_NN for_IN topic_NN discovery_NN presented_VBD described_VBN in_IN =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_JJ -_: was_VBD used_VBN in_IN -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
This_DT method_NN ,_, M2_NN ,_, outputs_VBZ set_NN of_IN bursty_JJ topics_NNS represented_VBN by_IN word_NN distributions_NNS ._.
Each_DT of_IN the_DT different_JJ distributions_NNS points_NNS to_TO a_DT subject_NN ._.
The_DT original_JJ papers_NNS reports_NNS on_IN a_DT number_NN of_IN
WO_NN and_CC SEM_NN ._.
Note_VB that_IN although_IN in_IN this_DT thesis_NN we_PRP only_RB experiment_NN with_IN these_DT simple_JJ similarity_NN measures_NNS ,_, our_PRP$ optimization_NN framework_NN would_MD allow_VB us_PRP to_TO potentially_RB use_VB more_RBR sophisticated_JJ measures_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 5_CD ,_, 21_CD ,_, 2_CD ,_, 15_CD ,_, 23_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
10Chapter_NN 5_CD Optimization_NNP Algorithms_NNP A_NNP brute-force_JJ solution_NN to_TO the_DT optimization_NN problem_NN defined_VBN in_IN Chapter_NN 3_CD would_MD be_VB to_TO enumerate_VB all_PDT the_DT possible_JJ candidate_NN summaries_NNS -LRB-_-LRB- i.e._FW ,_, k_NN sentence_NN pairs_NNS -RRB-_-RRB- and_CC
nds_NNS ._.
Due_JJ to_TO the_DT curse_NN of_IN dimensionality_NN ,_, the_DT sparsity_NN of_IN the_DT data_NNS could_MD affect_VB the_DT clustering_NN performance_NN ._.
3.1.2_CD Unstructured_JJ PLSA_NN Probabilistic_JJ latent_JJ semantic_JJ analysis_NN -LRB-_-LRB- PLSA_NN -RRB-_-RRB- -LRB-_-LRB- 5_CD -RRB-_-RRB- and_CC its_PRP$ extensions_NNS =_JJ -_: =[_NN 19_CD ,_, 12_CD ,_, 11_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP recently_RB been_VBN applied_VBN to_TO many_JJ text_NN mining_NN problems_NNS with_IN promising_JJ results_NNS ._.
If_IN we_PRP ignore_VBP the_DT structure_NN of_IN the_DT phrases_NNS ,_, we_PRP could_MD apply_VB PLSA_NNP on_IN the_DT head_NN terms_NNS to_TO extract_VB topics_NNS ,_, i.e._FW aspects_NNS ._.
As_IN in_IN
al._FW 2003_CD -RRB-_-RRB- have_VBP been_VBN proposed_VBN to_TO model_VB latent_JJ topics_NNS among_IN documents_NNS and_CC have_VBP been_VBN successfully_RB applied_VBN to_TO multiple_JJ text_NN mining_NN tasks_NNS -LRB-_-LRB- McCallum_NNP et_FW al._FW 2007_CD ;_: Mimno_NNP and_CC McCallum_NNP 2007_CD ;_: Steyversetal_JJ .2004_NN ;_: =_JJ -_: =_JJ Zhai_NNP et_FW al._FW 2004_CD -_: =--RRB-_NN ._.
Inspired_VBN by_IN the_DT recent_JJ success_NN of_IN topic_NN models_NNS for_IN text_NN mining_NN ,_, in_IN this_DT work_NN ,_, we_PRP are_VBP investigating_VBG a_DT paradigm_NN shift_NN to_TO enable_VB search\/analysis_NN at_IN topic_NN level_NN for_IN the_DT academic_JJ network_NN with_IN the_DT follow_VB
ifferent_JJ streams_NNS ,_, both_DT semantically_RB and_CC temporally_RB ,_, could_MD not_RB be_VB fully_RB explored_VBN ._.
In_IN -LRB-_-LRB- 2_CD ,_, 9_CD ,_, 13_CD -RRB-_-RRB- ,_, the_DT semantic_JJ correlation_NN between_IN different_JJ topics_NNS in_IN static_JJ text_NN collections_NNS was_VBD considered_VBN ._.
Similarly_RB ,_, =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: explored_VBN common_JJ topics_NNS in_IN multiple_JJ static_JJ text_NN collections_NNS ._.
A_DT very_RB recent_JJ work_NN by_IN Wang_NNP et_FW al._FW -LRB-_-LRB- 16_CD -RRB-_-RRB- firstly_RB proposed_VBD a_DT topic_NN mining_NN method_NN that_WDT aimed_VBD to_TO discover_VB common_JJ -LRB-_-LRB- bursty_JJ -RRB-_-RRB- topics_NNS over_IN multiple_JJ t_NN
ackground_NN model_NN and_CC the_DT region-based_JJ topic_NN models_NNS ._.
The_DT purpose_NN of_IN using_VBG a_DT background_NN model_NN is_VBZ to_TO make_VB the_DT topics_NNS concentrated_VBD more_RBR on_IN more_RBR discriminative_JJ words_NNS ,_, which_WDT leads_VBZ to_TO more_RBR informative_JJ models_NNS =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
p_NN -LRB-_-LRB- w_FW |_FW r_NN ,_, Ψ_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -RRB-_-RRB- =_JJ λBp_NN -LRB-_-LRB- w_NN |_NN B_NN -RRB-_-RRB- +_CC -LRB-_-LRB- 1_CD −_NN λB_NN -RRB-_-RRB- ∑_CD p_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -LRB-_-LRB- w_FW |_FW z_SYM -RRB-_-RRB- p_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -LRB-_-LRB- z_SYM |_CD r_NN -RRB-_-RRB- z_SYM ∈_NN Z_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- p_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -LRB-_-LRB- w_FW |_FW z_SYM -RRB-_-RRB- is_VBZ from_IN θ_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, and_CC p_NN -LRB-_-LRB- t_NN -RRB-_-RRB- -LRB-_-LRB- z_SYM |_CD r_NN -RRB-_-RRB- is_VBZ from_IN φ_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ._.
p_NN -LRB-_-LRB- w_NN |_NN B_NN -RRB-_-RRB- is_VBZ the_DT background_NN model_NN ,_, which_WDT we_PRP set_VBD as_IN follows_VBZ ._.
∑_CD p_NN -LRB-_-LRB- w_NN |_NN B_NN -RRB-_-RRB- =_JJ
requirement_NN -RRB-_-RRB- ,_, we_PRP introduce_VBP a_DT dummy_NN functional_JJ topic_NN zB_NN for_IN every_DT sentence_NN in_IN the_DT document_NN ._.
We_PRP use_VBP this_DT functional_JJ topic_NN to_TO capture_VB the_DT document-independent_JJ word_NN distribution_NN ,_, i.e._FW ,_, corpus_NN background_NN -LRB-_-LRB- =_JJ -_: =_JJ Zhai_NNP et_FW al._FW ,_, 2004_CD -_: =--RRB-_NN ._.
As_IN a_DT result_NN ,_, in_IN strTM_NNP ,_, every_DT sentence_NN is_VBZ treated_VBN as_IN a_DT mixture_NN of_IN content_NN and_CC functional_JJ topics_NNS ._.
Formally_RB ,_, we_PRP assume_VBP a_DT corpus_NN consists_VBZ of_IN D_NN documents_NNS with_IN a_DT vocabulary_NN of_IN size_NN V_NN ,_, and_CC there_EX are_VBP k_NN con_NN
ur_NN study_NN as_IN quite_RB effective_JJ in_IN tracking_VBG popular_JJ events_NNS in_IN social_JJ communities_NNS ._.
Topic_NNP Modeling_NNP ._.
Topic_JJ modeling_NN approaches_NNS -LRB-_-LRB- 12_CD -RRB-_-RRB- -LRB-_-LRB- 4_CD -RRB-_-RRB- have_VBP been_VBN developed_VBN to_TO mine_VB variations_NNS of_IN topics_NNS in_IN different_JJ contexts_NNS =_JJ -_: =[_NN 28_CD -RRB-_-RRB- -_: =_JJ -_: ,_, evolution_NN of_IN topics_NNS -LRB-_-LRB- 20_CD -RRB-_-RRB- ,_, and_CC correlated_VBD patterns_NNS in_IN multiple_JJ text_NN streams_NNS -LRB-_-LRB- 26_CD -RRB-_-RRB- ._.
These_DT methods_NNS generally_RB do_VBP not_RB consider_VB the_DT network_NN structures_NNS ._.
Recently_RB ,_, incorporating_VBG network_NN regularization_NN in_IN top_NN
WO_NN and_CC SEM_NN ._.
Note_VB that_IN although_IN in_IN this_DT paper_NN we_PRP only_RB experiment_NN with_IN these_DT simple_JJ similarity_NN measures_NNS ,_, our_PRP$ optimization_NN framework_NN would_MD allow_VB us_PRP to_TO potentially_RB use_VB more_RBR sophisticated_JJ measures_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 5_CD ,_, 21_CD ,_, 2_CD ,_, 15_CD ,_, 23_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
5_CD ._.
OPTIMIZATION_NNP ALGORITHMS_NNP A_NNP brute-force_JJ solution_NN to_TO the_DT optimization_NN problem_NN defined_VBN in_IN Section_NN 3_CD would_MD be_VB to_TO enumerate_VB all_PDT the_DT possible_JJ candidate_NN summaries_NNS -LRB-_-LRB- i.e._FW ,_, k_NN sentence_NN pairs_NNS -RRB-_-RRB- and_CC compute_VB th_DT
of_IN θ1_NN and_CC θ2_NN ._.
Step_NN 1_CD is_VBZ trivial_JJ ;_: below_IN we_PRP describe_VBP Steps_NNS 2_CD &_CC 3_CD in_IN detail_NN ._.
3.1_CD Theme_NN Extraction_NN We_PRP extract_VBP themes_NNS from_IN each_DT subcollection_NN Ci_NN using_VBG a_DT simple_JJ probabilistic_JJ mixture_NN model_NN as_IN described_VBN in_IN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT method_NN ,_, words_NNS are_VBP regarded_VBN as_IN data_NNS drawn_VBN from_IN a_DT mixture_NN model_NN with_IN component_NN models_NNS for_IN the_DT theme_NN word_NN distributions_NNS and_CC a_DT background_NN word_NN distribution_NN ._.
Words_NNS in_IN the_DT same_JJ document_NN share_NN the_DT
posed_VBN to_TO find_VB adjectives_NNS that_WDT are_VBP indicative_JJ of_IN positive_JJ or_CC negative_JJ opinions_NNS ._.
-LRB-_-LRB- 32_CD -RRB-_-RRB- proposes_VBZ a_DT similar_JJ method_NN for_IN nouns_NNS ._.
Other_JJ related_JJ works_NNS on_IN sentiment_NN classification_NN and_CC opinions_NNS discovery_NN include_VBP =_JJ -_: =[_NN 9_CD ,_, 15_CD ,_, 16_CD ,_, 23_CD ,_, 27_CD ,_, 33_CD ,_, 34_CD ,_, 35_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN -LRB-_-LRB- 11_CD ,_, 19_CD -RRB-_-RRB- ,_, several_JJ unsupervised_JJ and_CC supervised_JJ techniques_NNS are_VBP proposed_VBN to_TO analyze_VB opinions_NNS in_IN customer_NN reviews_NNS ._.
Specifically_RB ,_, they_PRP identify_VBP product_NN features_NNS that_WDT have_VBP been_VBN commented_VBN on_RP by_IN customer_NN
s_NN between_IN the_DT two_CD languages_NNS ._.
To_TO evaluate_VB the_DT results_NNS quantitatively_RB ,_, we_PRP select_VBP a_DT sample_NN of_IN representative_JJ topics_NNS from_IN English_NNP by_IN performing_VBG word_NN clustering_NN using_VBG the_DT simple_JJ mixture_NN model_NN presented_VBN in_IN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP generated_VBD 30_CD clusters_NNS in_IN this_DT way_NN ._.
¡_FW sWe_FW then_RB take_VB the_DT top_JJ 5_CD English_JJ documents_NNS from_IN the_DT three_CD randomly_RB chosen_VBN clusters_NNS ,_, to_TO generate_VB 15_CD seed_NN English_JJ documents_NNS ._.
For_IN each_DT English_NNP document_NN ,_, we_PRP use_VBP the_DT
ltiple_NN clusters_NNS in_IN the_DT pseudo-feedback_JJ documents_NNS ,_, to_TO ensure_VB sufficient_JJ representation_NN of_IN different_JJ aspects_NNS of_IN the_DT topic_NN ._.
We_PRP rely_VBP on_IN the_DT mixture_NN multinomial_JJ model_NN ,_, which_WDT is_VBZ used_VBN for_IN theme_NN discovery_NN in_IN =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Specifically_RB ,_, we_PRP assume_VBP the_DT N_NN documents_NNS contain_VBP K_NN clusters_NNS -LCB-_-LRB- Ci_FW |_FW i_FW =_JJ 1_CD ,_, 2_CD ,_, ·_NNP ·_NNP ·_NNP K_NNP -RCB-_-RRB- ,_, each_DT characterized_VBN by_IN a_DT multinomial_JJ word_NN distribution_NN -LRB-_-LRB- also_RB known_VBN as_IN unigram_JJ language_NN model_NN -RRB-_-RRB- θi_NN and_CC corresponding_VBG to_TO
le_FW ,_, a_DT user_NN may_MD like_VB the_DT price_NN and_CC fuel_NN efficiency_NN of_IN a_DT new_JJ Toyota_NNP Camry_NNP ,_, but_CC dislike_VB its_PRP$ power_NN and_CC safety_NN aspects_NNS ._.
Indeed_RB ,_, people_NNS tend_VBP to_TO have_VB different_JJ opinions_NNS about_IN different_JJ features_NNS of_IN a_DT product_NN =_JJ -_: =[_NN 28_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
As_IN another_DT example_NN ,_, a_DT voter_NN may_MD agree_VB with_IN some_DT points_NNS made_VBN by_IN a_DT presidential_JJ candidate_NN ,_, but_CC disagree_VBP with_IN some_DT others_NNS ._.
In_IN reality_NN ,_, a_DT general_JJ statement_NN of_IN good_JJ or_CC bad_JJ about_IN a_DT query_NN is_VBZ not_RB so_RB informa_VB
of_IN a_DT word_NN in_IN the_DT spatiotemporal_JJ theme_NN model_NN Previous_JJ work_NN has_VBZ shown_VBN that_IN mixture_NN models_NNS of_IN multinomial_JJ distributions_NNS -LRB-_-LRB- i.e._FW ,_, mixture_NN language_NN models_NNS -RRB-_-RRB- are_VBP quite_RB effective_JJ in_IN extracting_VBG themes_NNS from_IN text_NN =_JJ -_: =[_NN 12_CD ,_, 1_CD ,_, 9_CD ,_, 27_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT basic_JJ idea_NN of_IN such_JJ approaches_NNS is_VBZ to_TO assume_VB that_IN each_DT word_NN in_IN the_DT collection_NN is_VBZ a_DT sample_NN from_IN a_DT mixture_NN model_NN with_IN d_NN multiple_JJ multinomial_JJ distributions_NNS as_IN components_NNS ,_, each_DT representing_VBG a_DT theme_NN ._.
B_NN
Keywords_NNS :_: Statistical_JJ topic_NN models_NNS ,_, multinomial_JJ distribution_NN ,_, topic_NN model_NN labeling_NN 1_CD ._.
INTRODUCTION_NN Statistical_JJ topic_NN modeling_NN has_VBZ attracted_VBN much_JJ attention_NN recently_RB in_IN machine_NN learning_NN and_CC text_NN mining_NN =_JJ -_: =[_NN 11_CD ,_, 4_CD ,_, 28_CD ,_, 22_CD ,_, 9_CD ,_, 2_CD ,_, 16_CD ,_, 18_CD ,_, 14_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: due_JJ to_TO its_PRP$ broad_JJ applications_NNS ,_, including_VBG extracting_VBG scientific_JJ research_NN topics_NNS -LRB-_-LRB- 9_CD ,_, 2_CD -RRB-_-RRB- ,_, temporal_JJ text_NN mining_NN -LRB-_-LRB- 17_CD ,_, 24_CD -RRB-_-RRB- ,_, spatiotemporal_JJ text_NN mining_NN -LRB-_-LRB- 16_CD ,_, 18_CD -RRB-_-RRB- ,_, authortopic_JJ analysis_NN -LRB-_-LRB- 22_CD ,_, 18_CD -RRB-_-RRB- ,_, opinion_NN extra_JJ
le_FW ,_, a_DT user_NN may_MD like_VB the_DT price_NN and_CC fuel_NN efficiency_NN of_IN a_DT new_JJ Toyota_NNP Camry_NNP ,_, but_CC dislike_VB its_PRP$ power_NN and_CC safety_NN aspects_NNS ._.
Indeed_RB ,_, people_NNS tend_VBP to_TO have_VB different_JJ opinions_NNS about_IN different_JJ features_NNS of_IN a_DT product_NN =_JJ -_: =[_NN 28_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
As_IN another_DT example_NN ,_, a_DT voter_NN may_MD agree_VB with_IN some_DT points_NNS made_VBN by_IN a_DT presidential_JJ candidate_NN ,_, but_CC disagree_VBP with_IN some_DT others_NNS ._.
In_IN reality_NN ,_, a_DT general_JJ statement_NN of_IN good_JJ or_CC bad_JJ about_IN a_DT query_NN is_VBZ not_RB so_RB informa_VB
of_IN a_DT word_NN in_IN the_DT spatiotemporal_JJ theme_NN model_NN Previous_JJ work_NN has_VBZ shown_VBN that_IN mixture_NN models_NNS of_IN multinomial_JJ distributions_NNS -LRB-_-LRB- i.e._FW ,_, mixture_NN language_NN models_NNS -RRB-_-RRB- are_VBP quite_RB effective_JJ in_IN extracting_VBG themes_NNS from_IN text_NN =_JJ -_: =[_NN 12_CD ,_, 1_CD ,_, 9_CD ,_, 27_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT basic_JJ idea_NN of_IN such_JJ approaches_NNS is_VBZ to_TO assume_VB that_IN each_DT word_NN in_IN the_DT collection_NN is_VBZ a_DT sample_NN from_IN a_DT mixture_NN model_NN with_IN d_NN 535_CD multiple_JJ multinomial_JJ distributions_NNS as_IN components_NNS ,_, each_DT representing_VBG a_DT them_PRP
xt_IN documents_NNS have_VBP been_VBN considered_VBN in_IN some_DT recent_JJ work_NN on_IN temporal_JJ text_NN mining_NN -LRB-_-LRB- 9_CD ,_, 16_CD ,_, 14_CD ,_, 4_CD -RRB-_-RRB- ._.
Also_RB ,_, author-topic_JJ analysis_NN is_VBZ studied_VBN in_IN -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, and_CC crosscollection_NN comparative_JJ text_NN mining_NN is_VBZ studied_VBN in_IN =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
All_DT these_DT studies_NNS consider_VBP some_DT kinds_NNS of_IN context_NN information_NN ,_, i.e._FW ,_, time_NN ,_, authorship_NN ,_, and_CC subcollection_NN ._.
Time_NN ,_, authorship_NN ,_, and_CC subcollection_NN are_VBP by_IN no_DT means_VBZ the_DT only_JJ possible_JJ context_NN information_NN of_IN
tensive_JJ disease_NN ._.
In_IN comparison_NN with_IN individual_JJ words_NNS ,_, a_DT concept_NN is_VBZ more_RBR meaningful_JJ ;_: in_IN comparison_NN with_IN multi-word_JJ phrases_NNS ,_, a_DT concept_NN well_RB solves_VBZ polysemy_NN and_CC synonymy_NN problems_NNS ._.
Mixture_NN language_NN model_NN =_JJ -_: =[_NN 35_CD ,_, 36_CD ,_, 37_CD -RRB-_-RRB- -_: =_SYM -_: has_VBZ been_VBN well_RB studied_VBN and_CC applied_VBN in_IN information_NN retrieval_NN which_WDT is_VBZ proved_VBN to_TO be_VB a_DT solid_JJ method_NN for_IN giving_VBG consistently_RB higher_JJR precision_NN and_CC recall_NN ._.
It_PRP can_MD automatically_RB interpolate_VB between_IN the_DT model_NN
criminative_JJ ._.
PLSA_NNP has_VBZ also_RB been_VBN extended_VBN in_IN several_JJ studies_NNS mostly_RB to_TO accommodate_VB a_DT topic_NN hierarchy_NN -LRB-_-LRB- 36_CD -RRB-_-RRB- ,_, incorporate_VB context_NN variables_NNS such_JJ as_IN time_NN and_CC location_NN -LRB-_-LRB- 66_CD -RRB-_-RRB- ,_, and_CC analyze_VB sentiments_NNS -LRB-_-LRB- 65_CD -RRB-_-RRB- ._.
In_IN =_JJ -_: =[_NN 130_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT background_NN topic_NN is_VBZ introduced_VBN to_TO PLSA_NNP to_TO make_VB the_DT extracted_VBN topic_NN models_NNS more_RBR focusing_VBG on_IN the_DT content_NN words_NNS rather_RB than_IN the_DT common_JJ words_NNS in_IN the_DT collection_NN ._.
6.10_CD Summary_NN In_IN this_DT section_NN ,_, we_PRP review_VBP
s_NNS rather_RB than_IN the_DT individual_JJ words_NNS within_IN documents_NNS ._.
The_DT corpus_NN used_VBN ,_, 11,000_CD newswire_NN articles_NNS ,_, is_VBZ also_RB of_IN much_RB smaller_JJR scale_NN than_IN the_DT OCA_NNP corpus_NN ._.
Zhai_NNP et_FW al._FW present_VBP a_DT cross-collection_JJ mixture_NN model_NN =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT uses_VBZ an_DT EM_NN approach_NN to_TO find_VB themes_NNS shared_VBN between_IN collections_NNS ._.
The_DT themes_NNS are_VBP represented_VBN as_IN multinomials_NNS over_IN words_NNS ._.
The_DT model_NN is_VBZ tested_VBN on_IN a_DT corpus_NN consisting_VBG of_IN three_CD collections_NNS ,_, each_DT with_IN
d_NN with_IN one_CD or_CC more_JJR of_IN the_DT aliases_NNS for_IN the_DT gene_NN ._.
The_DT key_JJ terms_NNS in_IN the_DT gene_NN expression_NN stop_NN word_NN list_NN are_VBP removed_VBN from_IN the_DT abstracts_NNS ._.
For_IN more_JJR details_NNS about_IN the_DT data_NNS integration_NN server_NN ,_, please_VB refer_VBP to_TO =_JJ -_: =[_NN 38_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Gene_NN Expression_NN Stop_NNP Word_NNP List_NN ._.
According_VBG to_TO -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, in_IN the_DT enriched_JJ gene_NN expression_NN data_NNS sets_NNS ,_, each_DT gene_NN will_MD have_VB words_NNS and_CC expressions_NNS in_IN common_JJ including_VBG :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- standard_JJ English_JJ words_NNS ,_, such_JJ as_IN ``_`` the_DT ''_'' ,_,
we_PRP can_MD leverage_NN the_DT associated_VBN network_NN structure_NN to_TO discover_VB interesting_JJ topic_NN and\/or_CC network_NN patterns_NNS ._.
Statistical_JJ topic_NN models_NNS have_VBP recently_RB been_VBN successfully_RB applied_VBN to_TO multiple_JJ text_NN mining_NN tasks_NNS =_JJ -_: =[_NN 10_CD ,_, 4_CD ,_, 28_CD ,_, 26_CD ,_, 20_CD ,_, 15_CD ,_, 27_CD -RRB-_-RRB- -_: =_SYM -_: to_TO discover_VB a_DT number_NN of_IN topics_NNS from_IN text_NN ._.
Some_DT recent_JJ work_NN has_VBZ incorporated_VBN into_IN topic_NN modeling_NN context_NN information_NN -LRB-_-LRB- 20_CD -RRB-_-RRB- ,_, such_JJ as_IN time_NN -LRB-_-LRB- 27_CD -RRB-_-RRB- ,_, geographic_JJ location_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, and_CC authorship_NN -LRB-_-LRB- 26_CD ,_, 23_CD ,_, 19_CD -RRB-_-RRB- ,_, to_TO
s_NNS rather_RB than_IN the_DT individual_JJ words_NNS within_IN documents_NNS ._.
The_DT corpus_NN used_VBN ,_, 11,000_CD newswire_NN articles_NNS ,_, is_VBZ also_RB of_IN much_RB smaller_JJR scale_NN than_IN the_DT OCA_NNP corpus_NN ._.
Zhai_NNP et_FW al._FW present_VBP a_DT cross-collection_JJ mixture_NN model_NN =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT uses_VBZ an_DT EM_NN approach_NN to_TO find_VB themes_NNS shared_VBN between_IN collections_NNS ._.
The_DT themes_NNS are_VBP represented_VBN as_IN multinomials_NNS over_IN words_NNS ._.
The_DT model_NN is_VBZ tested_VBN on_IN a_DT corpus_NN consisting_VBG of_IN three_CD collections_NNS ,_, each_DT with_IN
task_NN is_VBZ difficult_JJ given_VBN that_IN we_PRP do_VBP not_RB have_VB training_NN data_NNS ._.
Later_RB we_PRP will_MD describe_VB how_WRB we_PRP can_MD learn_VB all_PDT these_DT distributions_NNS simultaneously_RB by_IN fitting_NN a_DT mixture_NN probabilistic_JJ model_NN to_TO all_PDT the_DT text_NN data_NN =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
3_LS ._.
COMPARATIVE_NNP TEXT_NNP MINING_NNP METHODS_NNS In_IN order_NN to_TO develop_VB a_DT complete_JJ comparative_JJ text_NN mining_NN system_NN ,_, we_PRP need_VBP to_TO solve_VB three_CD technical_JJ challenges_NNS :_: 1_CD ._.
Theme_NN mining_NN and_CC extraction_NN :_: Mine\/extract_VB the_DT common_JJ
c_NN Analysis_NN has_VBZ been_VBN used_VBN to_TO solve_VB problems_NNS in_IN a_DT variety_NN of_IN applications_NNS on_IN account_NN of_IN its_PRP$ flexibility_NN ._.
Such_JJ applications_NNS include_VBP information_NN retrieval_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, text_NN learning_NN and_CC mining_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- -LRB-_-LRB- 7_CD -RRB-_-RRB- -LRB-_-LRB- 14_CD -RRB-_-RRB- -LRB-_-LRB- 18_CD -RRB-_-RRB- =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_JJ -_: ,_, co-citation_NN analysis_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, social_JJ annotation_NN analysis_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, web_NN usage_NN mining_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- and_CC personalize_VB web_NN search_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- ._.
6_CD Conclusion_NN In_IN this_DT paper_NN ,_, we_PRP have_VBP proposed_VBN a_DT mixture_NN model_NN for_IN expert_NN findin_NN
rs_NNS associated_VBN with_IN this_DT step_NN :_: n_NN nearest_IN neighbors_NNS in_IN the_DT graph_NN G_NN computed_VBN with_IN Eqn_NN ._.
1_CD ,_, and_CC t_NN iterations_NNS of_IN adjusting_VBG ¯_FW θi_FW ._.
3.2_CD Facet_NNP modeling_NN 3.2.1_CD Statistical_JJ Topic_JJ Models_NNS Statistical_JJ topic_NN modeling_NN =_JJ -_: =[_NN 1_CD ,_, 8_CD ,_, 27_CD ,_, 18_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ quite_RB effective_JJ for_IN mining_NN topics_NNS in_IN a_DT text_NN collection_NN ._.
In_IN this_DT kind_NN of_IN approaches_NNS ,_, a_DT document_NN is_VBZ often_RB assumed_VBN to_TO be_VB generated_VBN from_IN a_DT mixture_NN of_IN k_NN topic_NN models_NNS ._.
Probabilistic_JJ Latent_JJ Semantic_NNP Anal_NNP
tract_NN and_CC integrate_VB opinions_NNS ._.
scribe_NN this_DT step_NN in_IN more_JJR detail_NN in_IN the_DT next_JJ section_NN ._.
4_LS ._.
SEMI-SUPERVISED_NNP PLSA_NNP FOR_IN OPINION_NNP INTEGRATION_NNP Probabilistic_NNP latent_JJ semantic_JJ analysis_NN -LRB-_-LRB- PLSA_NN -RRB-_-RRB- -LRB-_-LRB- 6_CD -RRB-_-RRB- and_CC its_PRP$ extensions_NNS =_JJ -_: =[_NN 21_CD ,_, 13_CD ,_, 11_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP recently_RB been_VBN applied_VBN to_TO many_JJ text_NN mining_NN problems_NNS with_IN promising_JJ results_NNS ._.
Our_PRP$ work_NN adds_VBZ to_TO this_DT line_NN yet_RB another_DT novel_JJ use_NN of_IN such_JJ models_NNS for_IN opinion_NN integration_NN ._.
As_IN in_IN most_JJS topic_NN models_NNS ,_, our_PRP$ g_NN
can_MD not_RB directly_RB applied_VBN to_TO heterogeneous_JJ network_NN clustering_NN with_IN four_CD types_NNS of_IN objects_NNS ,_, for_IN each_DT algorithm_NN ,_, we_PRP will_MD simplify_VB the_DT network_NN when_WRB necessary_JJ to_TO make_VB all_PDT the_DT algorithms_NNS comparable_JJ ._.
For_IN PLSA_NN =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_JJ -_: ,_, only_RB the_DT term_NN type_NN and_CC paper_NN type_NN in_IN the_DT network_NN are_VBP used_VBN ._.
No6_NN Actually_RB ,_, the_DT extremely_RB poor_JJ quality_NN when_WRB λP_NN is_VBZ very_RB small_JJ is_VBZ partially_RB caused_VBN by_IN the_DT improper_JJ accuracy_NN measure_NN at_IN those_DT occasions_NNS ._.
Whe_NN
as_IN highly_RB prolific_JJ ;_: authors_NNS with_IN ≥_NN 10_CD and_CC -LRB-_-LRB- 20_CD papers_NNS are_VBP labeled_VBN as_IN prolific_JJ and_CC authors_NNS with_IN -LRB-_-LRB- 10_CD papers_NNS are_VBP labeled_VBN as_IN low_JJ prolific_JJ ._.
For_IN attribute_NN ``_`` primary_JJ topic_NN ''_'' ,_, we_PRP use_VBP a_DT topic_NN modeling_NN approach_NN =_JJ -_: =[_NN 11_CD ,_, 27_CD -RRB-_-RRB- -_: =_SYM -_: to_TO extract_VB 100_CD research_NN topics_NNS from_IN a_DT document_NN collection_NN composed_VBN of_IN paper_NN titles_NNS from_IN the_DT selected_VBN authors_NNS ._.
Each_DT extracted_VBN topic_NN consists_VBZ of_IN a_DT probability_NN distribution_NN of_IN keywords_NNS which_WDT are_VBP most_JJS re_IN
