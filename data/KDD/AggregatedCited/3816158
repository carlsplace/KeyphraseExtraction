A_DT scalable_JJ modular_JJ convex_NN solver_NN for_IN regularized_VBN risk_NN minimization_NN
A_DT wide_JJ variety_NN of_IN machine_NN learning_NN problems_NNS can_MD be_VB described_VBN as_IN minimizing_VBG a_DT regularized_VBN risk_NN functional_JJ ,_, with_IN different_JJ algorithms_NNS using_VBG different_JJ notions_NNS of_IN risk_NN and_CC different_JJ regularizers_NNS ._.
Examples_NNS include_VBP linear_JJ Support_NN Vector_NNP Machines_NNP -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- ,_, Logistic_JJ Regression_NN ,_, Conditional_JJ Random_NNP Fields_NNP -LRB-_-LRB- CRFs_NNS -RRB-_-RRB- ,_, and_CC Lasso_NNP amongst_IN others_NNS ._.
This_DT paper_NN describes_VBZ the_DT theory_NN and_CC implementation_NN of_IN a_DT highly_RB scalable_JJ and_CC modular_JJ convex_NN solver_NN which_WDT solves_VBZ all_PDT these_DT estimation_NN problems_NNS ._.
It_PRP can_MD be_VB parallelized_VBN on_IN a_DT cluster_NN of_IN workstations_NNS ,_, allows_VBZ for_IN data-locality_NN ,_, and_CC can_MD deal_VB with_IN regularizers_NNS such_JJ as_IN l1_NN and_CC l2_NN penalties_NNS ._.
At_IN present_NN ,_, our_PRP$ solver_NN implements_VBZ 20_CD different_JJ estimation_NN problems_NNS ,_, can_MD be_VB easily_RB extended_VBN ,_, scales_NNS to_TO millions_NNS of_IN observations_NNS ,_, and_CC is_VBZ up_IN to_TO 10_CD times_NNS faster_RBR than_IN specialized_VBN solvers_NNS for_IN many_JJ applications_NNS ._.
The_DT open_JJ source_NN code_NN is_VBZ freely_RB available_JJ as_IN part_NN of_IN the_DT ELEFANT_NNP toolbox_NN ._.
that_IN ξ_NN =_JJ 1_CD ∑_CD n_NN n_NN i_FW =_JJ 1_CD ξi_NN ._.
While_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- has_VBZ a_DT huge_JJ number_NN of_IN constraints_NNS ,_, Algorithm_NN 1_CD is_VBZ a_DT cutting-plane_JJ procedure_NN that_WDT always_RB constructs_NNS a_DT solution_NN of_IN precision_NN ɛ_NN with_IN at_IN most_JJS O_NN -LRB-_-LRB- C_NN ɛ_NN -RRB-_-RRB- active_JJ constraints_NNS =_JJ -_: =[_NN 15_CD ,_, 18_CD ,_, 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT experiments_NNS from_IN Section_NN 6_CD ,_, the_DT number_NN of_IN active_JJ constraints_NNS was_VBD typically_RB around_IN 30_CD --_: independent_JJ of_IN the_DT size_NN of_IN the_DT training_NN set_NN ._.
Algorithm_NN 1_CD maintains_VBZ a_DT working_VBG set_NN of_IN m_NN constraints_NNS 〈_FW w_FW ,_, ¯_NN
ching_VBG data_NNS vectors_NNS and_CC scheduling_NN model_NN updates_NNS ,_, we_PRP opted_VBD to_TO develop_VB our_PRP$ own_JJ solver_NN implementing_VBG the_DT 1-slack_JJ cutting_VBG plane_NN algorithm_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
We_PRP briefly_RB outline_VBP our_PRP$ approach_NN here_RB using_VBG the_DT notation_NN from_IN =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Consider_VB the_DT following_JJ unconstrained_JJ formulation_NN that_IN isequivalent_NN to_TO the_DT constrained_VBN problem_NN from_IN -LRB-_-LRB- 6_CD -RRB-_-RRB- :_: w_FW ∗_FW =_JJ arg_NN min_NN L_NN -LRB-_-LRB- w_NN -RRB-_-RRB- w_NN where_WRB L_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ 1_CD 2_CD |_CD |_CD w_NN |_NNP |_NNP 2_CD +_CC CR_NN -LRB-_-LRB- w_NN -RRB-_-RRB- N_NN ∑_NN R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ max_NN -LRB-_-LRB- 0_CD ,_, l_NN -LRB-_-LRB- Yn_NN ,_, H_NN -RRB-_-RRB- −_FW w_FW H_NN T_NN ∆_NN Ψ_NN -LRB-_-LRB- Xn_NN ,_,
a_DT small_JJ set_NN of_IN critical_JJ constraints_NNS ,_, which_WDT will_MD be_VB the_DT only_JJ ones_NNS to_TO be_VB ultimately_RB enforced_VBN -LRB-_-LRB- 15_CD -RRB-_-RRB- ._.
4.5_CD ._.
The_DT BMRM_NNP Algorithm_NNP We_PRP use_VBP the_DT ``_`` Bundle_NN Methods_NNS for_IN Regularized_NNP Risk_NNP Minimization_NNP ''_'' -LRB-_-LRB- BMRM_NN -RRB-_-RRB- solver_NN of_IN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT merely_RB requires_VBZ that_IN for_IN each_DT candidate_NN θ_NN ,_, we_PRP compute_VBP the_DT difference_NN in_IN gradient_NN -LRB-_-LRB- w.r.t._FW θ_FW -RRB-_-RRB- of_IN the_DT score_NN function_NN of_IN the_DT true_JJ assignments_NNS -LRB-_-LRB- x_NN same_JJ n_NN -RRB-_-RRB- ,_, and_CC the_DT most_RBS violated_VBN constraint_NN -LRB-_-LRB- k_NN viol_NN
the_DT former_JJ stopping_VBG condition_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
Theorem_NN 1_CD by_IN Teo_NN et_FW al._FW -LRB-_-LRB- 2007_CD -RRB-_-RRB- guarantees_VBZ convergence_NN of_IN the_DT CPA_NNP algorithm_NN inO_NN -LRB-_-LRB- 1_CD ε_NN -RRB-_-RRB- time_NN for_IN a_DT broad_JJ class_NN of_IN risk_NN functions_NNS :_: 2161FRANC_NN AND_CC SONNENBURG_NN Theorem_NN 1_CD -LRB-_-LRB- =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =--RRB-_NN Assume_VB that_IN ‖_FW ∂_FW R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- ‖_FW ≤_FW G_NN for_IN all_DT w_FW ∈_FW W_NN ,_, whereW_NN is_VBZ some_DT domain_NN of_IN interest_NN containing_VBG all_DT wt_JJ ′_NN for_IN t_NN ′_FW ≤_FW t._FW In_IN this_DT case_NN ,_, for_IN any_DT ε_NN -RRB-_-RRB- 0_CD and_CC C_NN -RRB-_-RRB- 0_CD ,_, Algorithm_NN 1_CD satisfies_VBZ the_DT stopping_VBG condition_NN -LRB-_-LRB- 7_CD -RRB-_-RRB- after_IN
with_IN respect_NN to_TO w_NN -RRB-_-RRB- is_VBZ w_NN 1_CD X_NN n_NN ð_FW ^_FW ynÞ_NN :_: ð14Þ_NN N_NN n_NN Equations_NNS -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC -LRB-_-LRB- 14_CD -RRB-_-RRB- define_VBP the_DT new_JJ constraint_NN to_TO be_VB added_VBN to_TO the_DT optimization_NN problem_NN ._.
Pseudocode_NN for_IN this_DT algorithm_NN is_VBZ described_VBN in_IN Algorithm_NNP 1_CD ._.
See_VB =_JJ -_: =[_NN 38_CD -RRB-_-RRB- -_: =_SYM -_: for_IN more_JJR details_NNS ._.
Let_VB us_PRP investigate_VB the_DT complexity_NN of_IN solving_VBG -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
Using_VBG the_DT joint_JJ feature_NN map_NN as_IN in_IN -LRB-_-LRB- 6_CD -RRB-_-RRB- and_CC the_DT loss_NN as_IN in_IN -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, the_DT argument_NN in_IN -LRB-_-LRB- 11_CD -RRB-_-RRB- becomes_VBZ h_NN ðG_NN ;_: G_NN 0_CD ;_: yÞ_NN ;_: wiþ_FW ðy_FW ;_: y_NN n_NN Þ_NN X_NN X_NN yii0c_NN
._.
The_DT optimization_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 12_CD -RRB-_-RRB- is_VBZ in_IN the_DT shape_NN :_: w_FW ∗_FW =_JJ argminwf_NN -LRB-_-LRB- w_NN -RRB-_-RRB- =_JJ λ_NN 2_CD ‖_CD w_NN ‖_NN 2_CD +_CC R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- -LRB-_-LRB- 13_CD -RRB-_-RRB- where_WRB R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- is_VBZ an_DT upper_JJ bound_VBN of_IN the_DT empirical_JJ risk_NN that_IN we_PRP want_VBP to_TO minimize_VB ._.
The_DT approach_NN described_VBN in_IN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: belongs_VBZ to_TO the_DT familiy_NN of_IN bundle_NN methods_NNS and_CC solves_VBZ the_DT general_JJ optimization_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 13_CD -RRB-_-RRB- whatever_WDT R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- provided_VBD that_IN it_PRP is_VBZ convex_NN ._.
We_PRP briefly_RB describe_VBP the_DT method_NN in_IN the_DT case_NN where_WRB R_NN -LRB-_-LRB- w_NN -RRB-_-RRB- is_VBZ con_NN
GDs_NNS ._.
Parallel_JJ optimization_NN methods_NNS have_VBP recently_RB attracted_VBN attention_NN as_IN a_DT way_NN to_TO scale_VB up_RP machine_NN learning_NN algorithms_NNS ._.
Map-Reduce_NNP -LRB-_-LRB- Dean_NNP &_CC Ghemawat_NNP ,_, 2008_CD -RRB-_-RRB- style_NN optimization_NN methods_NNS -LRB-_-LRB- Chu_NNP et_FW al._FW ,_, 2007_CD ;_: =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =--RRB-_NN have_VBP been_VBN successful_JJ early_JJ approaches_NNS ._.
We_PRP also_RB note_VBP recent_JJ studies_NNS -LRB-_-LRB- Mann_NNP et_FW al._FW ,_, 2009_CD ;_: Zinkevich_NNP et_FW al._FW ,_, 2010_CD -RRB-_-RRB- that_WDT have_VBP parallelized_VBN SGDs_NNS without_IN using_VBG the_DT Map-Reduce_NNP framework_NN ._.
In_IN our_PRP$ experiments_NNS ,_,
ion_NN to_TO structured_JJ learning_NN -LRB-_-LRB- or_CC multi-class_JJ -RRB-_-RRB- is_VBZ discussed_VBN ._.
Recently_RB ,_, several_JJ new_JJ algorithms_NNS have_VBP been_VBN presented_VBN ,_, along_IN with_IN a_DT rate_NN of_IN convergence_NN analysis_NN -LRB-_-LRB- Joachims_NNP ,_, 2006_CD ;_: Shalev-Shwartz_NNP et_FW al._FW ,_, 2007_CD ;_: =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =_JJ -_: ;_: Tsochantaridis_NNP et_FW al._FW ,_, 2004_CD -RRB-_-RRB- ._.
All_DT of_IN these_DT algorithms_NNS are_VBP similar_JJ to_TO ours_PRP in_IN having_VBG a_DT relatively_RB low_JJ dependence_NN on_IN n_NN in_IN terms_NNS of_IN memory_NN and_CC computation_NN ._.
Among_IN these_DT ,_, Shalev-Shwartz_NNP et_FW al._FW -LRB-_-LRB- 2007_CD -RRB-_-RRB- and_CC
and_CC users_NNS with_IN an_DT acceptable_JJ memory_NN footprint_NN ._.
We_PRP achieve_VBP these_DT goals_NNS by_IN combining_VBG -LRB-_-LRB- a_DT -RRB-_-RRB- recent_JJ results_NNS in_IN optimization_NN ,_, in_IN particular_JJ the_DT application_NN of_IN bundle_NN methods_NNS to_TO convex_VB optimization_NN problems_NNS =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- b_NN -RRB-_-RRB- techniques_NNS for_IN representing_VBG functions_NNS on_IN matrices_NNS ,_, in_IN particular_JJ maximum_NN margin_NN matrix_NN factorizations_NNS -LRB-_-LRB- 10_CD ,_, 11_CD ,_, 12_CD -RRB-_-RRB- and_CC -LRB-_-LRB- c_LS -RRB-_-RRB- the_DT application_NN of_IN structured_JJ estimation_NN for_IN ranking_JJ problems_NNS ._.
We_PRP descr_VBP
exploiting_VBG this_DT technique_NN ,_, which_WDT are_VBP valid_JJ for_IN both_CC infinite_JJ and_CC finite_JJ programs_NNS ._.
One_CD based_VBN on_IN a_DT batch_NN scenario_NN ,_, inspired_VBN by_IN SVMStruct_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, and_CC one_CD based_VBN on_IN an_DT online_JJ setting_NN ,_, inspired_VBN by_IN BMRM\/Pegasos_NN =_JJ -_: =[_NN 15_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
3.1_CD A_NN Variant_JJ of_IN SVMStruct_NN The_DT work_NN of_IN -LRB-_-LRB- 16_CD ,_, 10_CD -RRB-_-RRB- on_IN SVMStruct-like_JJ optimization_NN methods_NNS can_MD be_VB used_VBN directly_RB to_TO solve_VB regularized_JJ risk_NN minimization_NN problems_NNS ._.
The_DT basic_JJ idea_NN is_VBZ to_TO compute_VB gradients_NNS of_IN
and_CC O_NN -LRB-_-LRB- log_NN -LRB-_-LRB- 1_CD \/_: ɛ_NN -RRB-_-RRB- -RRB-_-RRB- convergence_NN ,_, whenever_WRB the_DT loss_NN is_VBZ sufficiently_RB smooth_JJ ._.
An_DT important_JJ feature_NN of_IN our_PRP$ algorithm_NN is_VBZ that_IN it_PRP automatically_RB takes_VBZ advantage_NN of_IN smoothness_NN in_IN the_DT problem_NN ._.
Our_PRP$ work_NN builds_VBZ on_IN =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT describes_VBZ the_DT basic_JJ extension_NN of_IN SVMPerf_NN to_TO general_JJ convex_NN problems_NNS ._.
The_DT current_JJ paper_NN provides_VBZ a_DT -RRB-_-RRB- significantly_RB improved_VBD performance_NN bounds_NNS which_WDT match_VBP better_RBR what_WP can_MD be_VB observed_VBN in_IN practice_NN
ion_NN methods_NNS for_IN these_DT formulations_NNS ._.
Recently_RB ,_, some_DT good_JJ methods_NNS have_VBP been_VBN proposed_VBN for_IN SVMs_NNS with_IN structured_JJ outputs_NNS ,_, that_WDT also_RB specialize_VBP nicely_RB for_IN Crammer-Singer_NNP multi-class_JJ linear_JJ SVMs_NNS ._.
Teo_NNP et_FW al._FW =_SYM -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: suggested_VBD a_DT bundle_NN method_NN and_CC Joachims_NNPS et_FW al._FW -LRB-_-LRB- 13_CD -RRB-_-RRB- gave_VBD a_DT cutting_VBG plane_NN method_NN that_WDT is_VBZ very_RB close_JJ to_TO it_PRP ;_: these_DT methods_NNS can_MD be_VB viewed_VBN as_IN extensions_NNS of_IN the_DT method_NN given_VBN by_IN Joachims_NNP -LRB-_-LRB- 12_CD -RRB-_-RRB- for_IN binary_JJ line_NN
italise_NN on_IN recent_JJ advances_NNS in_IN large-margin_JJ structured_JJ estimation_NN -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, which_WDT consist_VBP of_IN obtaining_VBG convex_NN relaxations_NNS of_IN this_DT problem_NN ._.
Without_IN going_VBG into_IN the_DT details_NNS of_IN the_DT solution_NN -LRB-_-LRB- see_VB ,_, for_IN example_NN ,_, =_JJ -_: =[_NN 15_CD ,_, 16_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, it_PRP can_MD be_VB shown_VBN that_IN a_DT convex_NN relaxation_NN of_IN this_DT problem_NN can_MD be_VB obtained_VBN ,_, which_WDT is_VBZ given_VBN by_IN min_NN θ_NN 1_CD N_NN subject_JJ to_TO N_NN ∑_FW i_FW =_JJ 1_CD ξi_NN +_CC λ_NN 2_CD ‖_FW θ_FW ‖_FW 2_CD 2_CD -LRB-_-LRB- 6a_NN -RRB-_-RRB- 〈_CD h_NN -LRB-_-LRB- S_NN i_LS ,_, U_NNP i_LS ,_, y_FW i_FW -RRB-_-RRB- −_CD h_NN -LRB-_-LRB- S_NN i_LS ,_, U_NNP i_LS ,_, y_NN -RRB-_-RRB- ,_, θ_FW 〉_FW ≥_FW ∆_NN -LRB-_-LRB- y_NN ,_, y_FW i_FW -RRB-_-RRB-
ithm_VB We_PRP propose_VBP an_DT iterative_JJ algorithm_NN to_TO solve_VB Eq_NN ._.
-LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, which_WDT is_VBZ guaranteed_VBN to_TO converge_VB to_TO a_DT global_JJ optimum_NN ._.
The_DT algorithm_NN is_VBZ closely_RB related_JJ to_TO the_DT bundle_NN method_NN -LRB-_-LRB- Hiriart-Urruty_NNP &_CC Lemarechal_NNP ,_, 1993_CD ;_: =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =--RRB-_NN ._.
The_DT optimization_NN problem_NN in_IN Eq_NN ._.
-LRB-_-LRB- 6_CD -RRB-_-RRB- maximizes_VBZ its_PRP$ objective_JJ function_NN with_IN respect_NN to_TO two_CD variables_NNS t_NN and_CC α_NN with_IN an_DT infinite_JJ number_NN of_IN -LRB-_-LRB- quadratic_JJ -RRB-_-RRB- constraints_NNS ._.
We_PRP approach_VBP the_DT optimum_NN by_IN optimizing_VBG
exploiting_VBG this_DT technique_NN ,_, which_WDT are_VBP valid_JJ for_IN both_CC infinite_JJ and_CC finite_JJ programs_NNS ._.
One_CD based_VBN on_IN a_DT batch_NN scenario_NN ,_, inspired_VBN by_IN SVMStruct_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ,_, and_CC one_CD based_VBN on_IN an_DT online_JJ setting_NN ,_, inspired_VBN by_IN BMRM\/Pegasos_NN =_JJ -_: =[_NN 15_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
3.1_CD A_NN Variant_JJ of_IN SVMStruct_NN The_DT work_NN of_IN -LRB-_-LRB- 16_CD ,_, 10_CD -RRB-_-RRB- on_IN SVMStruct-like_JJ optimization_NN methods_NNS can_MD be_VB used_VBN directly_RB to_TO solve_VB regularized_JJ risk_NN minimization_NN problems_NNS ._.
The_DT basic_JJ idea_NN is_VBZ to_TO compute_VB gradients_NNS of_IN
GS_NN often_RB fails_VBZ to_TO converge_VB on_IN such_JJ problems_NNS -LRB-_-LRB- Lukˇsan_NN and_CC Vlček_NN ,_, 1999_CD ;_: Haarala_NNP ,_, 2004_CD -RRB-_-RRB- ._.
Various_JJ subgradient-based_JJ approaches_NNS ,_, such_JJ as_IN subgradient_JJ descent_NN -LRB-_-LRB- Nedich_NNP and_CC Bertsekas_NNP ,_, 2000_CD -RRB-_-RRB- or_CC bundle_NN methods_NNS -LRB-_-LRB- =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =--RRB-_NN ,_, are_VBP therefore_RB preferred_VBN ._.
A_DT Quasi-Newton_NNP Approach_NNP to_TO Nonsmooth_NNP Convex_NNP Optimization_NNP Although_IN a_DT convex_NN function_NN might_MD not_RB be_VB differentiable_JJ everywhere_RB ,_, a_DT subgradient_NN always_RB exists_VBZ ._.
Let_VB w_NN be_VB a_DT point_NN wh_NN
urred_JJ loss_NN ._.
This_DT has_VBZ two_CD benefits_NNS :_: firstly_RB ,_, the_DT problem_NN has_VBZ no_DT local_JJ minima_NN ,_, and_CC secondly_RB ,_, the_DT optimization_NN problem_NN is_VBZ continuous_JJ and_CC piecewise_JJ differentiable_NN ,_, which_WDT allows_VBZ for_IN effective_JJ optimization_NN =_JJ -_: =[_NN 17_CD ,_, 19_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT setting_NN ,_, however_RB ,_, exhibits_VBZ a_DT significant_JJ problem_NN :_: the_DT looseness_NN of_IN the_DT convex_NN upper_JJ bounds_NNS can_MD sometimes_RB lead_VB to_TO poor_JJ accuracy_NN ._.
For_IN binary_JJ classification_NN ,_, -LRB-_-LRB- 2_LS -RRB-_-RRB- proposed_VBN to_TO switch_VB from_IN the_DT hinge_NN
is_VBZ an_DT upper_JJ bound_VBN of_IN the_DT empirical_JJ risk_NN that_IN we_PRP want_VBP to_TO minimize_VB ._.
Our_PRP$ algorithm_NN is_VBZ inspired_VBN by_IN a_DT recent_JJ variant_NN of_IN bundle_NN methods_NNS for_IN minimizing_VBG convex_NN regularized_VBD risk_NN in_IN machine_NN learning_NN problems_NNS -LRB-_-LRB- =_JJ -_: =_JJ Teo_NNP et_FW al._FW ,_, 2007_CD -_: =_JJ -_: ;_: Joachims_NNP ,_, 2006_CD -RRB-_-RRB- ._.
This_DT variant_NN has_VBZ two_CD main_JJ advantages_NNS ,_, the_DT first_JJ one_CD being_VBG its_PRP$ very_RB good_JJ convergence_NN rate_NN ,_, the_DT second_JJ one_CD being_VBG its_PRP$ relevant_JJ stopping_VBG criterion_NN ,_, namely_RB the_DT gap_NN between_IN the_DT objective_NN
hinge_NN loss_NN as_IN used_VBN in_IN support_NN vector_NN machines_NNS -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
In_IN this_DT paper_NN ,_, we_PRP focus_VBP on_IN the_DT logloss_NN ._.
This_DT ℓ1-norm_NN regularized_VBD MLE_NN problem_NN yields_VBZ a_DT sparse_JJ estimate_NN by_IN setting_VBG some_DT components_NNS of_IN w_NN to_TO exact_JJ zeros_NNS =_JJ -_: =[_NN 24_CD ,_, 11_CD -RRB-_-RRB- -_: =_JJ -_: and_CC has_VBZ efficient_JJ solvers_NNS ,_, such_JJ as_IN the_DT Orthant-Wise_NNP Limited-memory_JJ Quasi-Newton_NN -LRB-_-LRB- OWL-QN_NN -RRB-_-RRB- method_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
3_LS ._.
THE_DT STATISTICAL_JJ MODEL_NN In_IN this_DT section_NN ,_, we_PRP define_VBP the_DT task_NN of_IN entity_NN relationship_NN identification_NN
