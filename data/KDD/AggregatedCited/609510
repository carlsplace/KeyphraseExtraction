Mining_NN complex_NN models_NNS from_IN arbitrarily_RB large_JJ databases_NNS in_IN constant_JJ time_NN
In_IN this_DT paper_NN we_PRP propose_VBP a_DT scaling-up_JJ method_NN that_WDT is_VBZ applicable_JJ to_TO essentially_RB any_DT induction_NN algorithm_NN based_VBN on_IN discrete_JJ search_NN ._.
The_DT result_NN of_IN applying_VBG the_DT method_NN to_TO an_DT algorithm_NN is_VBZ that_IN its_PRP$ running_VBG time_NN becomes_VBZ independent_JJ of_IN the_DT size_NN of_IN the_DT database_NN ,_, while_IN the_DT decisions_NNS made_VBN are_VBP essentially_RB identical_JJ to_TO those_DT that_WDT would_MD be_VB made_VBN given_JJ infinite_JJ data_NNS ._.
The_DT method_NN works_VBZ within_IN pre-specified_JJ memory_NN limits_NNS and_CC ,_, as_RB long_RB as_IN the_DT data_NN is_VBZ iid_JJ ,_, only_RB requires_VBZ accessing_VBG it_PRP sequentially_RB ._.
It_PRP gives_VBZ anytime_RB results_NNS ,_, and_CC can_MD be_VB used_VBN to_TO produce_VB batch_NN ,_, stream_NN ,_, time-changing_JJ and_CC active-learning_JJ versions_NNS of_IN an_DT algorithm_NN ._.
We_PRP apply_VBP the_DT method_NN to_TO learning_VBG Bayesian_JJ networks_NNS ,_, developing_VBG an_DT algorithm_NN that_WDT is_VBZ faster_RBR than_IN previous_JJ ones_NNS by_IN orders_NNS of_IN magnitude_NN ,_, while_IN achieving_VBG essentially_RB the_DT same_JJ predictive_JJ performance_NN ._.
We_PRP observe_VBP these_DT gains_NNS on_IN a_DT series_NN of_IN large_JJ databases_NNS ``_`` generated_VBN from_IN benchmark_JJ networks_NNS ,_, on_IN the_DT KDD_NNP Cup_NNP 2000_CD e-commerce_NN data_NNS ,_, and_CC on_IN a_DT Web_NN log_NN containing_VBG 100_CD million_CD requests_NNS ._.
aining_VBG set_NN -RRB-_-RRB- ._.
We_PRP used_VBD two_CD propositional_JJ learners_NNS :_: Naive_JJ Bayes_NNS -LRB-_-LRB- Domingos_NNP &_CC Pazzani_NNP ,_, 1997_CD -RRB-_-RRB- and_CC Bayesian_JJ networks_NNS -LRB-_-LRB- Heckerman_NNP et_FW al._FW ,_, 1995_CD -RRB-_-RRB- with_IN structure_NN and_CC parameters_NNS learned_VBD using_VBG the_DT VFBN2_NN algorithm_NN -LRB-_-LRB- =_JJ -_: =_JJ Hulten_NNP &_CC Domingos_NNP ,_, 2002_CD -_: =--RRB-_NN with_IN a_DT maximum_NN of_IN four_CD parents_NNS per_IN node_NN ._.
The_DT order-2_NN attributes_NNS helped_VBD the_DT naive_JJ Bayes_NNS classifier_VBP but_CC hurt_VBP the_DT performance_NN of_IN the_DT Bayesian_JJ network_NN classifier_NN ,_, so_RB below_IN we_PRP report_VBP results_NNS using_VBG the_DT or_CC
circuits_NNS ._.
With_IN additional_JJ engineering_NN ,_, the_DT LearnAC_NN algorithm_NN could_MD be_VB adapted_VBN to_TO dynamically_RB request_VB only_RB as_IN many_JJ samples_NNS as_IN necessary_JJ to_TO be_VB confident_JJ in_IN its_PRP$ choices_NNS ._.
For_IN example_NN ,_, Hulten_NNP and_CC Domingos_NNP =_SYM -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP developed_VBN methods_NNS that_WDT scale_VBP learning_VBG algorithms_NNS to_TO datasets_NNS of_IN arbitrary_JJ size_NN ;_: the_DT same_JJ approach_NN could_MD be_VB used_VBN here_RB ,_, except_IN in_IN a_DT ``_`` pull_NN ''_'' setting_VBG where_WRB the_DT data_NN is_VBZ generated_VBN on-demand_JJ ._.
Spending_VBG a_DT
the_DT chosen_JJ clause_NN -LRB-_-LRB- s_NNS -RRB-_-RRB- are_VBP the_DT same_JJ that_WDT would_MD be_VB obtained_VBN if_IN we_PRP computed_VBD the_DT WPLL_NNP exactly_RB ._.
This_DT effectively_RB makes_VBZ the_DT runtime_NN of_IN the_DT WPLL_NN computation_NN independent_JJ of_IN the_DT number_NN of_IN predicate_JJ groundings_NNS -LRB-_-LRB- =_JJ -_: =_JJ Hulten_NNP &_CC Domingos_NNP ,_, 2002_CD -_: =--RRB-_NN ._.
At_IN the_DT end_NN of_IN the_DT algorithm_NN we_PRP do_VBP a_DT final_JJ round_NN of_IN weight_NN learning_NN without_IN subsampling_VBG ._.
•_RB We_PRP use_VBP a_DT similar_JJ strategy_NN to_TO compute_VB the_DT number_NN of_IN true_JJ groundings_NNS of_IN a_DT clause_NN ,_, required_VBN for_IN the_DT WPLL_NN and_CC i_LS
ys_RB be_VB interrupted_VBN to_TO return_VB some_DT intermediate_JJ result_NN ._.
This_DT flexibility_NN in_IN response_NN time_NN allows_VBZ anytime_RB algorithms_NNS to_TO be_VB used_VBN with_IN great_JJ success_NN in_IN real-world_JJ environments_NNS with_IN variable_JJ constraints_NNS -LRB-_-LRB- 8_CD -RRB-_-RRB- =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =-[_NN 23_CD -RRB-_-RRB- ._.
For_IN anytime_RB classification_NN ,_, one_CD well_RB established_VBN technique_NN is_VBZ the_DT anytime_RB nearest_JJS neighbor_NN classification_NN algorithm_NN -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
This_DT algorithm_NN retains_VBZ the_DT strong_JJ points_NNS of_IN the_DT nearest_JJS neighbor_NN algorit_NN
aced_VBD a_DT relational_JJ data-mining_JJ algorithm_NN ,_, RDT_NN ,_, with_IN RDBMSs_NNS and_CC Lisi_NNP and_CC Malerba_NNP -LRB-_-LRB- 92_CD -RRB-_-RRB- have_VBP shown_VBN that_IN AL-log_NN is_VBZ suitable_JJ for_IN inducing_VBG multilevel_JJ association_NN rules_NNS from_IN multiple_JJ relations_NNS ._.
Hulten_NNP et_FW al._FW =_SYM -_: =[_NN 67_CD ,_, 68_CD -RRB-_-RRB- -_: =_SYM -_: have_VBP proposed_VBN methods_NNS for_IN scaling-up_NN any_DT induction_NN algorithm_NN based_VBN on_IN discrete_JJ search_NN so_IN that_IN the_DT running_VBG time_NN becomes_VBZ independent_JJ of_IN the_DT size_NN of_IN the_DT database_NN ._.
DiMaio_NNP and_CC Shavlik_NNP -LRB-_-LRB- 35_CD ,_, 36_CD -RRB-_-RRB- have_VBP also_RB f_SYM
ng_NN into_IN account_NN the_DT numerous_JJ ingenious_JJ and_CC effective_JJ innovations_NNS in_IN recent_JJ years_NNS -LRB-_-LRB- e.g._FW -LRB-_-LRB- Chickering_NNP ,_, 1996b_CD ;_: Friedman_NNP &_CC Goldszmidt_NNP ,_, 1997_CD ;_: Xiang_NNP et_FW al._FW ,_, 1997_CD ;_: Friedman_NNP et_FW al._FW ,_, 1999_CD ;_: Elidan_NNP et_FW al._FW ,_, 2002_CD ;_: =_JJ -_: =_JJ Hulten_NNP &_CC Domingos_NNP ,_, 2002_CD -_: =--RRB-_NN -RRB-_-RRB- ,_, discussed_VBN in_IN Section_NN 4_CD ._.
Problem_NNP :_: From_IN fully_RB observed_VBN categorical_JJ data_NNS find_VBP an_DT acyclic_JJ structure_NN and_CC tabular_JJ conditional_JJ probability_NN tables_NNS -LRB-_-LRB- CPTs_NNS -RRB-_-RRB- that_WDT optimize_VBP a_DT Bayesian_NNP Network_NNP scoring_VBG criterion_NN
es_NNS -LRB-_-LRB- Domingos_NNP &_CC Pazzani_NNP ,_, 1997_CD -RRB-_-RRB- and_CC Bayesian_JJ networks_NNS -LRB-_-LRB- Heckerman_NNP et_FW al._FW ,_, 1995_CD -RRB-_-RRB- with_IN structure_NN mln_NN ._.
tex_NN ;_: 26\/01\/2006_CD ;_: 19:24_CD ;_: p._NN 20sMarkov_NN Logic_NNP Networks_NNP 21_CD and_CC parameters_NNS learned_VBD using_VBG the_DT VFBN2_NN algorithm_NN -LRB-_-LRB- =_JJ -_: =_JJ Hulten_NNP &_CC Domingos_NNP ,_, 2002_CD -_: =--RRB-_NN with_IN a_DT maximum_NN of_IN four_CD parents_NNS per_IN node_NN ._.
The_DT order-2_NN attributes_NNS helped_VBD the_DT naive_JJ Bayes_NNS classifier_VBP but_CC hurt_VBP the_DT performance_NN of_IN the_DT Bayesian_JJ network_NN classifier_NN ,_, so_RB below_IN we_PRP report_VBP results_NNS using_VBG the_DT or_CC
iven_VBN by_IN a_DT batch_NN conventional_JJ learner_NN from_IN enough_JJ examples_NNS ._.
They_PRP also_RB apply_VBP Hoeffding_NNP 's_POS inequalities_NNS to_TO build_VB a_DT scaling_NN --_: up_IN method_NN that_WDT is_VBZ applicable_JJ to_TO any_DT induction_NN algorithm_NN based_VBN on_IN discrete_JJ search_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
There_EX is_VBZ also_RB large_JJ literature_NN on_IN scaling_NN --_: up_IN algorithms_NNS -LRB-_-LRB- 7_CD ,_, 1_CD -RRB-_-RRB- ._.
6_CD ._.
CONCLUSIONS_NNS A_DT scalable_JJ classification_NN learning_VBG algorithm_NN based_VBN on_IN decision_NN rules_NNS and_CC prototypes_NNS has_VBZ been_VBN introduced_VBN in_IN this_DT paper_NN ._.
eater_NN numbers_NNS of_IN sequences_NNS ,_, and_CC thus_RB more_RBR complete_JJ samplings_NNS of_IN families_NNS ._.
Inferring_VBG graphical_JJ models_NNS from_IN such_JJ large_JJ datasets_NNS will_MD benefit_VB from_IN research_NN aimed_VBN at_IN scaling_VBG up_RP model_NN inference_NN -LRB-_-LRB- e.g._FW ,_, see_VBP =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =--RRB-_NN and_CC we_PRP propose_VBP to_TO consider_VB these_DT for_IN inferring_VBG coupled_JJ residues_NNS ._.
We_PRP would_MD also_RB like_VB to_TO ensure_VB fidelity_NN of_IN the_DT alignment_NN ,_, particularly_RB by_IN using_VBG available_JJ structural_JJ information_NN ._.
Eventually_RB ,_, we_PRP hope_VBP t_NN
constraint_NN satisfaction_NN ,_, and_CC biocomputing_NN ._.
The_DT author_NN 's_POS own_JJ interest_NN arises_VBZ from_IN work_NN that_WDT applies_VBZ sequential_JJ sampling_NN to_TO the_DT design_NN efficient_JJ approximate_JJ algorithms_NNS data_NNS mining_NN and_CC machine_NN learning_NN =_JJ -_: =[_NN 3_CD ,_, 2_CD ,_, 5_CD ,_, 9_CD ,_, 10_CD ,_, 16_CD ,_, 17_CD ,_, 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Algorithms_NNS for_IN the_DT Data_NNP Stream_NNP model_NN are_VBP naturally_RB anytime_RB algorithms_NNS ,_, although_IN the_DT strong_JJ requirement_NN we_PRP place_VBP on_IN their_PRP$ behavior_NN -LRB-_-LRB- Equation_NN -LRB-_-LRB- 1_CD -RRB-_-RRB- -RRB-_-RRB- has_VBZ never_RB been_VBN formulated_VBN explicitly_RB in_IN the_DT Data_NNP Stre_NNP
ansactions_NNS ._.
In_IN these_DT environments_NNS ,_, sequential_JJ or_CC incremental_JJ learning_NN methods_NNS become_VBP particularly_RB relevant_JJ ,_, since_IN they_PRP can_MD revise_VB existing_VBG models_NNS efficiently_RB ._.
It_PRP is_VBZ widely_RB accepted_VBN in_IN the_DT literature_NN -LRB-_-LRB- =_JJ -_: =_JJ Hulten_NNP &_CC Domingos_NNP ,_, 2002_CD -_: =--RRB-_NN that_IN incremental_JJ algorithms_NNS should_MD fulfill_VB four_CD constraints_NNS :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- build_VB a_DT model_NN using_VBG only_RB one_CD scan_VB of_IN data_NNS ,_, -LRB-_-LRB- 2_CD -RRB-_-RRB- require_VBP small_JJ and_CC constant_JJ time_NN per_IN record_NN ,_, -LRB-_-LRB- 3_CD -RRB-_-RRB- require_VBP a_DT fixed_JJ amount_NN of_IN memory_NN irres_NNS
verting_VBG conventional_JJ machine_NN learning_VBG algorithms_NNS to_TO anytime_RB algorithms_NNS ,_, including_VBG anytime_RB inductive_JJ logic_NN programming_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- ,_, the_DT anytime_RB Naïve_NNP Bayes_NNP Text_NNP Classifier_NNP -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, and_CC anytime_RB Bayesian_NNP Networks_NNP =_SYM -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Others_NNS have_VBP noted_VBN that_IN certain_JJ algorithms_NNS can_MD be_VB regarded_VBN as_RB anytime_RB algorithms_VBZ even_RB if_IN they_PRP were_VBD not_RB designed_VBN for_IN that_DT purpose_NN ,_, for_IN example_NN Roy_NNP and_CC McCallum_NNP -LRB-_-LRB- 28_CD -RRB-_-RRB- note_NN ``_`` By_IN initially_RB using_VBG a_DT fairly_RB re_JJ
hings_NNS has_VBZ happened_VBN ._.
Either_CC the_DT original_JJ decision_NN was_VBD incorrect_JJ -LRB-_-LRB- which_WDT will_MD happen_VB a_DT fraction_NN δ_NN of_IN the_DT 3_CD ._.
The_DT VFBN2_NN algorithm_NN for_IN learning_VBG the_DT structure_NN of_IN Bayesian_JJ networks_NNS from_IN massive_JJ data_NNS streams_NNS -LRB-_-LRB- =_JJ -_: =_JJ Hulten_NNP and_CC Domingos_NNP ,_, 2002_CD -_: =--RRB-_NN was_VBD developed_VBN using_VBG this_DT insight_NN ._.
9sHulten_NN ,_, Domingos_NNP ,_, and_CC Spencer_NNP time_NN -RRB-_-RRB- or_CC concept_NN drift_NN has_VBZ occurred_VBN ._.
In_IN either_DT case_NN ,_, we_PRP begin_VBP an_DT alternate_JJ search_NN starting_VBG from_IN the_DT new_JJ winners_NNS ,_, while_IN continuing_VBG to_TO
wpoint_NN ._.
Buntine_NN -LRB-_-LRB- 3_CD -RRB-_-RRB- stores_NNS in_IN memory_NN the_DT most_RBS promising_JJ networks_NNS and_CC restricts_VBZ the_DT search_NN among_IN them_PRP when_WRB new_JJ data_NNS is_VBZ available_JJ performing_VBG a_DT sort_NN of_IN beam_NN search_NN ._.
Friedman_NNP et_FW al._FW -LRB-_-LRB- 4_CD -RRB-_-RRB- and_CC Hulten_NNP et_FW al._FW =_SYM -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: use_VB two-way_JJ operators_NNS -LRB-_-LRB- i.e._FW add_VB ,_, delete_VB and_CC reverse_VB arcs_NNS -RRB-_-RRB- to_TO obtain_VB a_DT new_JJ network_NN from_IN the_DT current_JJ one_CD ._.
These_DT operators_NNS allow_VBP them_PRP to_TO reconsider_VB the_DT arcs_NNS added_VBN during_IN the_DT past_JJ learning_NN steps_NNS and_CC to_TO p_NN
At_IN first_JJ sight_NN ,_, it_PRP may_MD seem_VB unlikely_JJ that_IN all_PDT these_DT constraints_NNS can_MD be_VB satisfied_VBN simultaneously_RB ._.
However_RB ,_, we_PRP have_VBP developed_VBN a_DT general_JJ framework_NN for_IN mining_NN massive_JJ data_NN streams_NNS that_WDT satisfies_VBZ all_DT six_CD =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Within_IN this_DT framework_NN ,_, we_PRP have_VBP designed_VBN and_CC implemented_VBN massivestream_NN versions_NNS of_IN decision_NN tree_NN induction_NN -LRB-_-LRB- 1_CD ,_, 6_CD -RRB-_-RRB- ,_, Bayesian_JJ network_NN learning_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, k-means_NN clustering_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- and_CC the_DT EM_NNP algorithm_NN for_IN mixtu_NN
yesian_JJ Network_NNP is_VBZ not_RB limited_VBN we_PRP perceive_VBP it_PRP as_IN an_DT improvement_NN on_IN the_DT original_JJ Sparse_NNP Candidate_NNP algorithm_NN ._.
Sampling_NN was_VBD proposed_VBN as_IN one_CD of_IN the_DT techniques_NNS to_TO speed_VB up_RP modelling_NN in_IN massive_JJ datasets_NNS in_IN -LRB-_-LRB- =_JJ -_: =_JJ Hulten_NNP &_CC Domingos_NNP ,_, 2002_CD -_: =_JJ -_: ;_: Pelleg_NNP &_CC Moore_NNP ,_, 2002_CD -RRB-_-RRB- ._.
Though_IN an_DT interesting_JJ direction_NN it_PRP seems_VBZ to_TO be_VB orthogonal_JJ to_TO our_PRP$ approach_NN ._.
The_DT idea_NN of_IN augmenting_VBG Bayes_NNP Nets_NNPS with_IN high_JJ mutual_JJ information_NN edges_NNS is_VBZ based_VBN on_IN the_DT fact_NN that_IN such_JJ
ful_JJ information_NN as_IN possible_JJ from_IN in_IN the_DT data_NNS in_IN a_DT reasonable_JJ amount_NN of_IN time_NN ,_, while_IN not_RB fixing_VBG the_DT task_NN to_TO be_VB performed_VBN on_IN the_DT data_NNS in_IN advance_NN ._.
sThe_JJ usual_JJ approach_NN in_IN streaming_NN data_NN mining_NN applications_NNS =_JJ -_: =[_NN 1_CD ,_, 2_CD ,_, 8_CD ,_, 10_CD ,_, 12_CD ,_, 16_CD ,_, 15_CD ,_, 21_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ to_TO first_RB decide_VB what_WP data_NNS mining_NN task_NN is_VBZ to_TO be_VB performed_VBN ,_, and_CC then_RB tailor_VB the_DT processing_NN to_TO gather_VB the_DT information_NN necessary_JJ for_IN the_DT specific_JJ task_NN ._.
This_DT is_VBZ an_DT efficient_JJ approach_NN to_TO mining_NN stream_NN d_NN
ntally_RB ,_, it_PRP can_MD require_VB large_JJ amounts_NNS of_IN memory_NN ._.
Also_RB ,_, incorporating_VBG knowledge_NN obtained_VBN from_IN experts_NNS or_CC prior_JJ experience_NN is_VBZ awkward_JJ ._.
The_DT Hoeffding_NNP criterion_NN can_MD be_VB applied_VBN to_TO Bayesian_JJ network_NN learning_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: thereby_RB incrementally_RB learning_VBG the_DT network_NN structure_NN ._.
Still_RB ,_, many_JJ statistics_NNS need_VBP to_TO be_VB kept_VBN in_IN memory_NN thereby_RB slowing_VBG down_RP the_DT number_NN of_IN instances_NNS that_WDT can_MD be_VB processed_VBN per_IN second_NN ._.
Also_RB ,_, single_JJ Baye_NNP
