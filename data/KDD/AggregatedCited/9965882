Issues_NNS in_IN evaluation_NN of_IN stream_NN learning_NN algorithms_NNS
Learning_VBG from_IN data_NNS streams_NNS is_VBZ a_DT research_NN area_NN of_IN increasing_VBG importance_NN ._.
Nowadays_RB ,_, several_JJ stream_NN learning_NN algorithms_NNS have_VBP been_VBN developed_VBN ._.
Most_JJS of_IN them_PRP learn_VBP decision_NN models_NNS that_WDT continuously_RB evolve_VBP over_IN time_NN ,_, run_VBN in_IN resource-aware_JJ environments_NNS ,_, detect_VB and_CC react_VB to_TO changes_NNS in_IN the_DT environment_NN generating_NN data_NNS ._.
One_CD important_JJ issue_NN ,_, not_RB yet_RB conveniently_RB addressed_VBN ,_, is_VBZ the_DT design_NN of_IN experimental_JJ work_NN to_TO evaluate_VB and_CC compare_VB decision_NN models_NNS that_WDT evolve_VBP over_IN time_NN ._.
There_EX are_VBP no_DT golden_JJ standards_NNS for_IN assessing_VBG performance_NN in_IN non-stationary_JJ environments_NNS ._.
This_DT paper_NN proposes_VBZ a_DT general_JJ framework_NN for_IN assessing_VBG predictive_JJ stream_NN learning_NN algorithms_NNS ._.
We_PRP defend_VBP the_DT use_NN of_IN Predictive_JJ Sequential_JJ methods_NNS for_IN error_NN estimate_NN -_: the_DT prequential_JJ error_NN ._.
The_DT prequential_JJ error_NN allows_VBZ us_PRP to_TO monitor_VB the_DT evolution_NN of_IN the_DT performance_NN of_IN models_NNS that_WDT evolve_VBP over_IN time_NN ._.
Nevertheless_RB ,_, it_PRP is_VBZ known_VBN to_TO be_VB a_DT pessimistic_JJ estimator_NN in_IN comparison_NN to_TO holdout_NN estimates_NNS ._.
To_TO obtain_VB more_RBR reliable_JJ estimators_NNS we_PRP need_VBP some_DT forgetting_VBG mechanism_NN ._.
Two_CD viable_JJ alternatives_NNS are_VBP :_: sliding_VBG windows_NNS and_CC fading_JJ factors_NNS ._.
We_PRP observe_VBP that_IN the_DT prequential_JJ error_NN converges_VBZ to_TO an_DT holdout_NN estimator_NN when_WRB estimated_VBN over_IN a_DT sliding_VBG window_NN or_CC using_VBG fading_JJ factors_NNS ._.
We_PRP present_VBP illustrative_JJ examples_NNS of_IN the_DT use_NN of_IN prequential_JJ error_NN estimators_NNS ,_, using_VBG fading_JJ factors_NNS ,_, for_IN the_DT tasks_NNS of_IN :_: i_LS -RRB-_-RRB- assessing_VBG performance_NN of_IN a_DT learning_NN algorithm_NN ;_: ii_LS -RRB-_-RRB- comparing_VBG learning_NN algorithms_NNS ;_: iii_LS -RRB-_-RRB- hypothesis_NN testing_NN using_VBG McNemar_NNP test_NN ;_: and_CC iv_LS -RRB-_-RRB- change_NN detection_NN using_VBG Page-Hinkley_NNP test_NN ._.
In_IN these_DT tasks_NNS ,_, the_DT prequential_JJ error_NN estimated_VBN using_VBG fading_JJ factors_NNS provide_VBP reliable_JJ estimators_NNS ._.
In_IN comparison_NN to_TO sliding_VBG windows_NNS ,_, fading_JJ factors_NNS are_VBP faster_JJR and_CC memory-less_JJ ,_, a_DT requirement_NN for_IN streaming_NN applications_NNS ._.
This_DT paper_NN is_VBZ a_DT contribution_NN to_TO a_DT discussion_NN in_IN the_DT good-practices_NNS on_IN performance_NN assessment_NN when_WRB learning_VBG dynamic_JJ models_NNS that_WDT evolve_VBP over_IN time_NN ._.
ment_NN classifier_NN ._.
4_CD Streaming_NNP Data_NNP Evaluation_NN with_IN Unbalanced_JJ Classes_NNS In_IN data_NN stream_NN mining_NN ,_, the_DT most_RBS frequently_RB used_VBN measure_NN for_IN evaluating_VBG predictive_JJ accuracy_NN of_IN a_DT classifier_NN is_VBZ prequential_JJ accuracy_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP argue_VBP that_IN this_DT measure_NN is_VBZ only_RB appropriate_JJ when_WRB all_DT classes_NNS are_VBP balanced_JJ ,_, and_CC have_VBP -LRB-_-LRB- approximately_RB -RRB-_-RRB- the_DT same_JJ number_NN of_IN examples_NNS ._.
In_IN this_DT section_NN ,_, we_PRP propose_VBP the_DT Kappa_NNP statistic_NN as_IN a_DT more_JJR sensitiv_NN
ing_NN of_IN the_DT learning_NN and_CC the_DT evaluation_NN ._.
The_DT most_RBS efficient_JJ way_NN to_TO get_VB a_DT precise_JJ measure_NN of_IN the_DT recent_JJ accuracy_NN of_IN the_DT sub-trees_NNS is_VBZ by_IN using_VBG fading_JJ factors_NNS in_IN the_DT computation_NN of_IN the_DT sum_NN as_IN proposed_VBN in_IN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, where_WRB it_PRP has_VBZ been_VBN shown_VBN that_IN the_DT fading_JJ factor_NN mimics_VBZ the_DT sliding_VBG window_NN computation_NN of_IN any_DT statistic_NN ._.
The_DT procedure_NN is_VBZ the_DT following_NN :_: when_WRB the_DT maximum_NN number_NN of_IN options_NNS k_NN is_VBZ reached_VBN ,_, every_DT time_NN a_DT n_NN
valuation_NN methodology_NN used_VBN was_VBD Interleaved_NNP Test-Then-Train_NNP or_CC Prequential_JJ evaluation_NN -LRB-_-LRB- based_VBN on_IN 10_CD runs_NNS for_IN the_DT artificial_JJ data_NNS -RRB-_-RRB- :_: every_DT example_NN was_VBD used_VBN for_IN testing_VBG the_DT model_NN before_IN using_VBG it_PRP to_TO train_VB -LRB-_-LRB- =_JJ -_: =_JJ Gama_NNP et_FW al._FW ,_, 2009_CD -_: =--RRB-_NN ._.
This_DT interleaved_JJ test_NN followed_VBN by_IN train_NN procedure_NN was_VBD carried_VBN out_RP on_IN the_DT full_JJ training_NN set_VBN in_IN each_DT case_NN ._.
The_DT parameters_NNS of_IN the_DT artificial_JJ streams_NNS are_VBP as_IN follows_VBZ :_: â€¢_CD RBF_NN -LRB-_-LRB- x_NN ,_, v_LS -RRB-_-RRB- :_: RandomRBF_NN data_NNS stream_NN o_NN
or_CC testing_NN ,_, making_VBG maximum_JJ use_NN of_IN the_DT available_JJ data_NNS ._.
It_PRP also_RB ensures_VBZ a_DT smooth_JJ plot_NN of_IN accuracy_NN over_IN time_NN ,_, as_IN each_DT individual_JJ example_NN will_MD become_VB increasingly_RB less_RBR significant_JJ to_TO the_DT overall_JJ average_NN -LRB-_-LRB- =_JJ -_: =_JJ Gama_NNP et_FW al._FW ,_, 2009_CD -_: =--RRB-_NN ._.
1602MOA_NN :_: MASSIVE_NNP ONLINE_NNP ANALYSIS_NNP Figure_NNP 2_CD :_: MOA_NNP Graphical_NNP User_NN Interface_NNP As_IN data_NNS stream_NN classification_NN is_VBZ a_DT relatively_RB new_JJ field_NN ,_, such_JJ evaluation_NN practices_NNS are_VBP not_RB nearly_RB as_RB well_RB researched_VBD and_CC est_VBD
ues_NNS per_IN class_NN ,_, P_NN -LRB-_-LRB- xi_FW |_FW c_NN -RRB-_-RRB- ._.
Tohaverobust_JJ estimators_NNS of_IN these_DT probabilities_NNS ,_, we_PRP require_VBP a_DT minimum_JJ number_NN of_IN training_NN examples_NNS covered_VBN by_IN the_DT rule_NN 3_CD ._.
Figure_NN 1_CD plots_NNS the_DT evolution_NN of_IN the_DT prequential_JJ error_NN -LRB-_-LRB- =_JJ -_: =_JJ Gama_NNP et_FW al._FW ,_, 2009_CD -_: =--RRB-_NN for_IN the_DT LED_NNP and_CC Waveform_NNP datasets_NNS ._.
The_DT VFDRNB_NNP exhibit_VBP much_RB more_RBR powerful_JJ predicting_VBG capabilities_NNS than_IN VFDR_NNP ,_, especially_RB in_IN case_NN of_IN noisy_JJ data_NNS ,_, as_IN in_FW LED_FW dataset_NN ._.
Another_DT observation_NN is_VBZ that_IN VFDRNB_FW ex_FW
n_NN be_VB stored_VBN permanently_RB -_: they_PRP are_VBP observed_VBN and_CC then_RB forgotten_VBN ._.
Many_JJ incremental_JJ learners_NNS and_CC stream_NN mining_NN algorithms_NNS have_VBP been_VBN proposed_VBN in_IN the_DT last_JJ years_NNS ,_, accompanied_VBN by_IN methods_NNS for_IN evaluating_VBG them_PRP =_JJ -_: =[_NN 3_CD ,_, 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, modern_JJ applications_NNS ask_VB for_IN more_RBR sophisticated_JJ stream_NN learners_NNS than_IN can_MD currently_RB be_VB evaluated_VBN on_IN synthetically_RB generated_VBN data_NNS ._.
In_IN this_DT work_NN ,_, we_PRP propose_VBP a_DT generator_NN for_IN complex_JJ stream_NN data_NNS
