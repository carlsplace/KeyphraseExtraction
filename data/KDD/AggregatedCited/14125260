Large-scale_JJ matrix_NN factorization_NN with_IN distributed_VBN stochastic_JJ gradient_NN descent_NN
We_PRP provide_VBP a_DT novel_JJ algorithm_NN to_TO approximately_RB factor_VB large_JJ matrices_NNS with_IN millions_NNS of_IN rows_NNS ,_, millions_NNS of_IN columns_NNS ,_, and_CC billions_NNS of_IN nonzero_JJ elements_NNS ._.
Our_PRP$ approach_NN rests_VBZ on_IN stochastic_JJ gradient_NN descent_NN -LRB-_-LRB- SGD_NN -RRB-_-RRB- ,_, an_DT iterative_JJ stochastic_JJ optimization_NN algorithm_NN ._.
We_PRP first_RB develop_VBP a_DT novel_JJ ``_`` stratified_JJ ''_'' SGD_NN variant_NN -LRB-_-LRB- SSGD_NN -RRB-_-RRB- that_WDT applies_VBZ to_TO general_JJ loss-minimization_NN problems_NNS in_IN which_WDT the_DT loss_NN function_NN can_MD be_VB expressed_VBN as_IN a_DT weighted_JJ sum_NN of_IN ``_`` stratum_NN losses_NNS ._. ''_''
We_PRP establish_VBP sufficient_JJ conditions_NNS for_IN convergence_NN of_IN SSGD_NNP using_VBG results_NNS from_IN stochastic_JJ approximation_NN theory_NN and_CC regenerative_JJ process_NN theory_NN ._.
We_PRP then_RB specialize_VBP SSGD_NNP to_TO obtain_VB a_DT new_JJ matrix-factorization_NN algorithm_NN ,_, called_VBN DSGD_NNP ,_, that_WDT can_MD be_VB fully_RB distributed_VBN and_CC run_VBN on_IN web-scale_JJ datasets_NNS using_VBG ,_, e.g._FW ,_, MapReduce_NNP ._.
DSGD_NN can_MD handle_VB a_DT wide_JJ variety_NN of_IN matrix_NN factorizations_NNS ._.
We_PRP describe_VBP the_DT practical_JJ techniques_NNS used_VBN to_TO optimize_VB performance_NN in_IN our_PRP$ DSGD_NNP implementation_NN ._.
Experiments_NNS suggest_VBP that_IN DSGD_NN converges_VBZ significantly_RB faster_JJR and_CC has_VBZ better_JJR scalability_NN properties_NNS than_IN alternative_JJ algorithms_NNS ._.
impact_NN of_IN extreme_JJ biases_NNS and_CC very_RB large_JJ distances_NNS ._.
This_DT optimization_NN problem_NN can_MD be_VB solved_VBN efficiently_RB using_VBG stochastic_JJ gradient_NN descent_NN or_CC alternating_VBG least_JJS squares_NNS methods_NNS ,_, even_RB on_IN large_JJ data_NNS sets_VBZ =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
On_IN each_DT data_NNS set_VBD we_PRP investigated_VBD ,_, we_PRP determined_VBD the_DT parameters_NNS d_NN and_CC
lementations_NNS include_VBP Vowpal_NNP Wabbit_NNP at_IN Yahoo_NNP !_.
-LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, and_CC in_IN large-scale_JJ learning_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
IGD_NNP has_VBZ also_RB been_VBN employed_VBN for_IN specific_JJ algorithms_NNS ,_, notably_RB Gemulla_NNP et_NNP al_NNP recently_RB used_VBD it_PRP for_IN matrix_NN factorization_NN =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
What_WP distinguishes_VBZ our_PRP$ work_NN is_VBZ that_IN we_PRP have_VBP observed_VBN that_IN IGD_NN forms_VBZ the_DT basis_NN of_IN a_DT systems_NNS abstraction_NN that_WDT is_VBZ well_RB suited_VBN for_IN in-RDBMS_NN processing_NN ._.
As_IN a_DT result_NN ,_, our_PRP$ technical_JJ focus_NN is_VBZ on_IN optimizatio_NN
