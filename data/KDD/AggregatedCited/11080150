Efficient_JJ methods_NNS for_IN topic_NN model_NN inference_NN on_IN streaming_NN document_NN collections_NNS
Topic_NN models_NNS provide_VBP a_DT powerful_JJ tool_NN for_IN analyzing_VBG large_JJ text_NN collections_NNS by_IN representing_VBG high_JJ dimensional_JJ data_NNS in_IN a_DT low_JJ dimensional_JJ subspace_NN ._.
Fitting_VBG a_DT topic_NN model_NN given_VBN a_DT set_NN of_IN training_NN documents_NNS requires_VBZ approximate_JJ inference_NN techniques_NNS that_WDT are_VBP computationally_RB expensive_JJ ._.
With_IN today_NN 's_POS large-scale_JJ ,_, constantly_RB expanding_VBG document_NN collections_NNS ,_, it_PRP is_VBZ useful_JJ to_TO be_VB able_JJ to_TO infer_VB topic_NN distributions_NNS for_IN new_JJ documents_NNS without_IN retraining_VBG the_DT model_NN ._.
In_IN this_DT paper_NN ,_, we_PRP empirically_RB evaluate_VBP the_DT performance_NN of_IN several_JJ methods_NNS for_IN topic_NN inference_NN in_IN previously_RB unseen_JJ documents_NNS ,_, including_VBG methods_NNS based_VBN on_IN Gibbs_NNP sampling_NN ,_, variational_JJ inference_NN ,_, and_CC a_DT new_JJ method_NN inspired_VBN by_IN text_NN classification_NN ._.
The_DT classification-based_JJ inference_NN method_NN produces_VBZ results_NNS similar_JJ to_TO iterative_JJ inference_NN methods_NNS ,_, but_CC requires_VBZ only_RB a_DT single_JJ matrix_NN multiplication_NN ._.
In_IN addition_NN to_TO these_DT inference_NN methods_NNS ,_, we_PRP present_VBP SparseLDA_NN ,_, an_DT algorithm_NN and_CC data_NN structure_NN for_IN evaluating_VBG Gibbs_NNP sampling_NN distributions_NNS ._.
Empirical_JJ results_NNS indicate_VBP that_IN SparseLDA_NN can_MD be_VB approximately_RB 20_CD times_NNS faster_RBR than_IN traditional_JJ LDA_NN and_CC provide_VB twice_RB the_DT speedup_NN of_IN previously_RB published_VBN fast_JJ sampling_NN methods_NNS ,_, while_IN also_RB using_VBG substantially_RB less_JJR memory_NN ._.
ssifier_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, which_WDT is_VBZ a_DT multinomial_JJ generalization_NN of_IN logistic_JJ regression_NN ._.
We_PRP use_VBP the_DT LDA_NNP and_CC MaxEnt_NNP implementations_NNS in_IN the_DT MALLET_NNP toolkit_NN 2_CD ,_, with_IN a_DT slight_JJ modification_NN of_IN the_DT optimization_NN procedure_NN =_JJ -_: =[_NN 29_CD -RRB-_-RRB- -_: =_SYM -_: which_WDT enables_VBZ us_PRP to_TO train_VB a_DT MaxEnt_NNP model_NN from_IN class_NN distributions_NNS rather_RB than_IN a_DT single_JJ class_NN label_NN ._.
We_PRP refer_VBP to_TO this_DT as_IN the_DT Topic_NNP Method_NNP ._.
2_CD http:\/\/mallet.cs.umass.eduLearning_NN to_TO Tag_NNP from_IN Open_NNP Vocabu_NNP
d_NN fit_NN is_VBZ sufficient_JJ ._.
2.1_CD Collapsed_NNP Representation_NNP A_NNP direct_JJ Gibbs_NNP sampler_NN using_VBG -LRB-_-LRB- 1_LS -RRB-_-RRB- does_VBZ not_RB mix_VB sufficiently_RB quickly_RB and_CC an_DT improved_VBN strategy_NN is_VBZ to_TO integrate_VB out_RP Θ_NN and_CC ψ_NN ._.
Moreover_RB ,_, collapsed_VBD sampling_NN =_JJ -_: =[_NN 10_CD ,_, 9_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: tends_VBZ to_TO lead_VB to_TO somewhat_RB better_JJR models_NNS than_IN a_DT variational_JJ approach_NN ._.
Most_RBS importantly_RB ,_, it_PRP allows_VBZ for_IN a_DT much_RB more_RBR compact_JJ representation_NN of_IN the_DT model_NN whenever_WRB the_DT number_NN of_IN parameters_NNS is_VBZ large_JJ --_: it_PRP is_VBZ
are_VBP effective_JJ ,_, but_CC both_DT present_JJ significant_JJ computational_JJ challenges_NNS in_IN the_DT face_NN of_IN massive_JJ data_NNS sets_NNS ._.
Developing_VBG scalable_JJ approximate_JJ inference_NN methods_NNS for_IN topic_NN models_NNS is_VBZ an_DT active_JJ area_NN of_IN research_NN =_JJ -_: =[_NN 3_CD ,_, 4_CD ,_, 5_CD ,_, 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
To_TO this_DT end_NN ,_, we_PRP develop_VBP online_JJ variational_JJ inference_NN for_IN LDA_NNP ,_, an_DT approximate_JJ posterior_JJ inference_NN algorithm_NN that_WDT can_MD analyze_VB massive_JJ collections_NNS of_IN documents_NNS ._.
We_PRP first_RB review_VBP the_DT traditional_JJ variatio_NN
in_IN all_DT experiments_NNS and_CC forα_NN ,_, we_PRP adopt_VBP the_DT commonly_RB used50\/T_JJ heuristics_NNS where_WRB T_NN is_VBZ the_DT number_NN of_IN topics_NNS ._.
In_IN our_PRP$ experiments_NNS ,_, we_PRP use_VBP Collapsed_NNP Gibbs_NNP Sampling_NNP -LRB-_-LRB- 5_CD -RRB-_-RRB- with_IN speed-up_NN techniques_NNS introduced_VBN in_IN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT can_MD be_VB scaled_VBN to_TO our_PRP$ large_JJ dataset_NN ._.
4.4_CD Topic_JJ Modeling_NN In_IN this_DT section_NN ,_, we_PRP mainly_RB study_VBD two_CD questions_NNS :_: 1_LS -RRB-_-RRB- whether_IN different_JJ training_NN schemes_NNS cause_VBP the_DT model_NN to_TO learn_VB different_JJ topics_NNS from_IN the_DT
ndard_JJ calculations_NNS yield_VBP the_DT following_VBG topic_NN probability_NN for_IN resampling_VBG :_: -LRB-_-LRB- βkv_NN +_CC n_NN p_NN -LRB-_-LRB- zmn_NN =_JJ k_NN |_CD rest_NN -RRB-_-RRB- ∝_CD KV_NN -RRB-_-RRB- -LRB-_-LRB- -RRB-_-RRB- KM_NN kvmn_FW −_FW nkm_FW −_FW +_CC αk_FW nK_FW k_NN −_NN +_CC ¯_NN -LRB-_-LRB- 6_CD -RRB-_-RRB- βk_NN In_IN the_DT appendix_NN we_PRP detail_NN how_WRB to_TO addapt_VB the_DT sampler_NN of_IN =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: to_TO obtain_VB faster_JJR sampling_NN ._.
3.3_CD Topic_NNP Smoother_NNP for_IN β_NNP Optimizing_NNP over_IN y_NN is_VBZ considerably_RB hard_RB since_IN the_DT log-likelihood_NN does_VBZ not_RB decompose_VB efficiently_RB ._.
This_DT is_VBZ due_JJ to_TO the_DT dependence_NN of_IN ¯_FW βk_FW on_IN all_DT words_NNS
ire_NN immediate_JJ action_NN ._.
Tracking_VBG temporal_JJ variations_NNS in_IN social_JJ media_NNS streams_NNS is_VBZ attracting_VBG increasing_VBG interest_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
Several_JJ papers_NNS have_VBP proposed_VBN dynamic_JJ topic_NN and_CC online_NN dictionary_NN learning_NN models_NNS -LRB-_-LRB- see_VB =_JJ -_: =[_NN 3_CD ,_, 11_CD ,_, 4_CD ,_, 9_CD ,_, 14_CD ,_, 18_CD ,_, 2_CD -RRB-_-RRB- -_: =_JJ -_: and_CC references_NNS therein_RB -RRB-_-RRB- that_IN either_CC exploit_VB temporal_JJ order_NN of_IN documents_NNS in_IN offline_JJ batch_NN mode_NN -LRB-_-LRB- using_VBG variational_JJ inference_NN or_CC Gibbs_NNP sampling_NN 1techniques_NNS -RRB-_-RRB- or_CC are_VBP limited_VBN to_TO handling_VBG a_DT fixed_JJ bandwidth_NN
ts_NNS -RRB-_-RRB- ._.
There_EX has_VBZ been_VBN previous_JJ work_NN on_IN scalable_JJ inference_NN ,_, starting_VBG with_IN the_DT collapsed_JJ sampler_NN representation_NN for_IN LDA_NNP -LRB-_-LRB- Griffiths_NNP and_CC Steyvers_NNP 2004_CD -RRB-_-RRB- ,_, efficient_JJ sampling_NN algorithms_NNS that_WDT exploit_VBP sparsity_NN -LRB-_-LRB- =_JJ -_: =_JJ Yao_NNP et_FW al._FW 2009_CD -_: =--RRB-_NN ,_, distributed_VBN implementations_NNS -LRB-_-LRB- Smola_NNP and_CC Narayanamurthy_NNP 2010_CD ,_, Asuncion_NNP et_FW al._FW 2008_CD -RRB-_-RRB- ,_, and_CC Sequential_NNP Monte_NNP Carlo_NNP -LRB-_-LRB- SMC_NNP -RRB-_-RRB- estimation_NN -LRB-_-LRB- Canini_FW et_FW al._FW 2009_CD -RRB-_-RRB- ._.
The_DT problem_NN of_IN efficient_JJ inference_NN is_VBZ exacerbate_VB
d_NN by_IN the_DT number_NN of_IN potentially_RB relevant_JJ documents_NNS ,_, existing_VBG generative_JJ models_NNS can_MD be_VB deployed_VBN only_RB on_IN narrowly-selected_JJ collections_NNS ._.
Standard_JJ Bayesian_JJ inference_NN does_VBZ not_RB scale_VB easily_RB to_TO web-size_JJ data_NN =_JJ -_: =[_NN 31_CD ,_, 12_CD -RRB-_-RRB- -_: =_JJ -_: ;_: however_RB ,_, we_PRP believe_VBP that_IN the_DT scalability_NN of_IN topic_NN models_NNS such_JJ as_IN Latent_JJ Dirichlet_NNP Allocation_NNP is_VBZ also_RB limited_VBN on_IN a_DT more_RBR fundamental_JJ level_NN by_IN the_DT underlying_VBG representation_NN ._.
We_PRP address_VBP both_DT issues_NNS :_: we_PRP
the_DT topic_NN -_: -RRB-_-RRB- word_NN distributions_NNS -LRB-_-LRB- low_JJ β_NN 's_POS allow_VB more_JJR variability_NN of_IN words_NNS across_IN topics_NNS -RRB-_-RRB- ._.
The_DT typical_JJ inference_NN procedure_NN used_VBN is_VBZ the_DT collapsed_JJ Gibbs_NNP Sampler_NNP described_VBN in_IN -LRB-_-LRB- 7_CD -RRB-_-RRB- where_WRB θ_NN is_VBZ integrated_VBN out_RP ._.
=_SYM -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: describes_VBZ an_DT equivalent_JJ sampler_NN which_WDT is_VBZ significantly_RB faster_RBR through_IN careful_JJ choice_NN of_IN data_NNS structures_NNS and_CC creative_JJ rearrangement_NN of_IN the_DT conditional_JJ probabilities_NNS ._.
2.2_CD Dirichlet_NNP Multinomial_NNP Regressi_NNP
s_NN up_RB completely_RB new_JJ and_CC interesting_JJ applications_NNS for_IN machine_NN learning_NN techniques_NNS in_IN general_JJ and_CC LDA_NN in_IN particular_JJ ._.
A_DT promising_JJ approach_NN to_TO scaling_VBG LDA_NNP to_TO large_JJ data_NNS sets_NNS are_VBP online_JJ variants_NNS ,_, see_VB e.g._FW =_SYM -_: =[_NN 23_CD ,_, 2_CD ,_, 11_CD ,_, 16_CD -RRB-_-RRB- -_: =_JJ -_: and_CC references_NNS in_IN there_RB ,_, that_WDT incrementally_RB build_VBP topic_NN models_NNS when_WRB a_DT new_JJ document_NN -LRB-_-LRB- or_CC a_DT set_NN of_IN documents_NNS -RRB-_-RRB- appears_VBZ ._.
For_IN instance_NN ,_, Hoffman_NNP et_FW al._FW -LRB-_-LRB- 11_CD -RRB-_-RRB- presented_VBD an_DT online_JJ variational_JJ Bayes_NNS -LRB-_-LRB- VB_NN -RRB-_-RRB- algori_NNS
d_NN for_IN monolingual_JJ LDA_NN ._.
For_IN example_NN ,_, Smola_NNP and_CC Narayanamurthy_NNP -LRB-_-LRB- 21_CD -RRB-_-RRB- interleave_VBP the_DT topic_NN and_CC document_NN counts_NNS during_IN the_DT computation_NN of_IN the_DT conditional_JJ distribution_NN using_VBG Yao_NNP et_FW al._FW 's_POS ``_`` binning_JJ ''_'' approach_NN =_JJ -_: =[_NN 39_CD -RRB-_-RRB- -_: =_SYM -_: ._.
While_IN this_DT improves_VBZ performance_NN ,_, changing_VBG any_DT of_IN the_DT modeling_NN assumptions_NNS would_MD potentially_RB break_VB this_DT optimization_NN ._.
In_IN contrast_NN ,_, Mr._NNP LDA_NNP 's_POS philosophy_NN allows_VBZ for_IN easier_JJR development_NN of_IN extensions_NNS of_IN
e_LS highly_RB tuned_VBN specifically_RB for_IN LDA_NNP ,_, which_WDT restricts_VBZ extensions_NNS and_CC enhancements_NNS ,_, one_CD of_IN the_DT key_JJ benefits_NNS of_IN the_DT statistical_JJ approach_NN ._.
The_DT techniques_NNS to_TO improve_VB inference_NN for_IN collapsed_JJ Gibbs_NNP samplers_NNS =_JJ -_: =[_NN 27_CD -RRB-_-RRB- -_: =_SYM -_: typically_RB reduce_VB flexibility_NN ;_: the_DT factorization_NN of_IN the_DT conditional_JJ distribution_NN is_VBZ limited_VBN to_TO LDA_NNP 's_POS explicit_JJ formulation_NN ._.
Adapting_VBG such_JJ tricks_NNS beyond_IN LDA_NNP requires_VBZ repeating_VBG the_DT analysis_NN to_TO refactoriz_NN
