Exploiting_VBG unlabeled_JJ data_NNS in_IN ensemble_NN methods_NNS
An_DT adaptive_JJ semi-supervised_JJ ensemble_NN method_NN ,_, ASSEMBLE_NNP ,_, is_VBZ proposed_VBN that_IN constructs_NNS classification_NN ensembles_NNS based_VBN on_IN both_CC labeled_JJ and_CC unlabeled_JJ data_NNS ._.
ASSEMBLE_NNP alternates_VBZ between_IN assigning_VBG ``_`` pseudo-classes_NNS ''_'' to_TO the_DT unlabeled_JJ data_NNS using_VBG the_DT existing_VBG ensemble_NN and_CC constructing_VBG the_DT next_JJ base_NN classifier_NN using_VBG both_CC the_DT labeled_JJ and_CC pseudolabeled_JJ data_NNS ._.
Mathematically_RB ,_, this_DT intuitive_JJ algorithm_NN corresponds_VBZ to_TO maximizing_VBG the_DT classification_NN margin_NN in_IN hypothesis_NN space_NN as_IN measured_VBN on_IN both_CC the_DT labeled_JJ and_CC unlabeled_JJ of_IN data_NNS ._.
Unlike_IN alternative_JJ approaches_NNS ,_, ASSEMBLE_NNP does_VBZ not_RB require_VB a_DT semi-supervised_JJ learning_NN method_NN for_IN the_DT base_NN classifier_NN ._.
ASSEMBLE_NN can_MD be_VB used_VBN in_IN conjunction_NN with_IN any_DT cost-sensitive_JJ classification_NN algorithm_NN for_IN both_CC two-class_JJ and_CC multi-class_JJ problems_NNS ._.
ASSEMBLE_VB using_VBG decision_NN trees_NNS won_VBD the_DT NIPS_NNP 2001_CD Unlabeled_NNP Data_NNP Competition_NNP ._.
In_IN addition_NN ,_, strong_JJ results_NNS on_IN several_JJ benchmark_JJ datasets_NNS using_VBG both_DT decision_NN trees_NNS and_CC neural_JJ networks_NNS support_VBP the_DT proposed_VBN method_NN ._.
sed_VBD that_DT measures_VBZ the_DT ANN_NNP 's_POS confidence_NN in_IN each_DT class_NN ._.
The_DT heuristic_NN chosen_VBN for_IN this_DT paper_NN is_VBZ the_DT F-measure_NN ._.
The_DT F-measure_NN for_IN class_NN k_NN is_VBZ determined_VBN as_IN follows_VBZ :_: F_NN -_: measurek_NN =_JJ 2_CD ×_FW Recallk_FW ×_FW P_NN recisionk_NN =_JJ -_: =-LRB-_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: Recallk_NN +_CC P_NN recisionk_NN where_WRB TruePositivesk_NN Recallk_NN =_JJ -LRB-_-LRB- 3_LS -RRB-_-RRB- TruePositivesk_NN +_CC F_NN alseNegativesk_NN and_CC TruePositivesk_NN P_NN recisionk_NN =_JJ ._.
-LRB-_-LRB- 4_LS -RRB-_-RRB- TruePositivesk_NN +_CC F_NN alseP_NN ositivesk_NN Recall_VB is_VBZ a_DT measure_NN of_IN how_WRB often_RB a_DT
duce_VB reasonably_RB wellcalibrated_JJ probability_NN estimates_NNS on_IN unseen_JJ test_NN data_NNS ,_, their_PRP$ estimates_NNS on_IN training_NN data_NNS are_VBP biased_VBN ._.
Thus_RB ,_, to_TO obtain_VB the_DT ``_`` predictions_NNS ''_'' for_IN each_DT training_NN example_NN x_NN ,_, we_PRP apply_VBP stacking_VBG =_JJ -_: =[_NN 29_CD ,_, 12_CD ,_, 28_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT meta-learning_JJ scheme_NN based_VBN on_IN crossvalidation_NN ._.
In_IN other_JJ words_NNS ,_, we_PRP first_RB run_VBP the_DT simple_JJ classification_NN model_NN and_CC use_NN cross-validation_NN to_TO obtain_VB the_DT predicted_VBN job_NN categories_NNS ,_, as_IN show_NN in_IN Figure_NNP 5_CD ._.
Th_NN
dd_NN to_TO L_NN -LRB-_-LRB- ˜_CD h_NN -RRB-_-RRB- the_DT term_NN that_WDT indicates_VBZ the_DT loss_NN on_IN the_DT unlabeled_JJ examples_NNS ._.
Two_CD obvious_JJ extensions_NNS of_IN L_NN -LRB-_-LRB- ˜_CD h_NN -RRB-_-RRB- are_VBP the_DT addition_NN of_IN the_DT term_NN that_WDT depends_VBZ on_IN the_DT margin_NN w.r.t._NN unlabeled_JJ examples_NNS -LRB-_-LRB- e.g._FW see_VBP =_JJ -_: =_JJ Bennett_NNP et_FW al._FW ,_, 2002_CD -_: =_JJ -_: and_CC the_DT references_NNS therein_RB -RRB-_-RRB- and_CC the_DT addition_NN of_IN the_DT term_NN that_WDT depends_VBZ on_IN the_DT graph_NN cut_NN induced_VBN by_IN the_DT labeling_NN of_IN unlabeled_JJ examples_NNS -LRB-_-LRB- e.g._FW ,_, see_VBP Loeff_NNP et_FW al._FW ,_, 2008_CD and_CC the_DT references_NNS therein_RB -RRB-_-RRB- ._.
The_DT mod_NN
ubject_NN may_MD change_VB his_PRP$ preferential_JJ imagined_VBN temporal_JJ `_`` control_NN patterns_NNS '_POS somewhat_RB ._.
The_DT measurement_NN of_IN novel_NN -LRB-_-LRB- still_RB unlabeled_JJ -RRB-_-RRB- patterns_NNS can_MD lead_VB to_TO classifiers_NNS with_IN improved_VBN generalization_NN performance_NN -LRB-_-LRB- =_JJ -_: =_JJ Bennet_NNP ,_, Demiriz_NNP ,_, &_CC Maclin_NNP ,_, 2002_CD -_: =--RRB-_NN ._.
We_PRP expect_VBP to_TO develop_VB novel_JJ statistical_JJ criteria_NNS for_IN deciding_VBG when_WRB to_TO update_VB an_DT existing_VBG Maximum_NNP Likelihood_NNP module_NN -LRB-_-LRB- ``_`` when_WRB is_VBZ a_DT set_NN of_IN incoming_JJ measurements_NNS consistently_RB different_JJ from_IN everything_NN we_PRP
,_, no_DT work_NN was_VBD clearly_RB devoted_VBN to_TO this_DT topic_NN within_IN the_DT MCS_NN literature_NN ._.
To_TO the_DT best_JJS of_IN our_PRP$ knowledge_NN ,_, few_JJ works_NNS on_IN semi-supervised_JJ multiple_JJ classifiers_NNS have_VBP appeared_VBN in_IN the_DT machine_NN learning_NN literature_NN =_JJ -_: =[_NN 7,10,11,12,13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT paper_NN is_VBZ aimed_VBN at_IN reviewing_VBG the_DT background_NN results_NNS that_WDT can_MD be_VB exploited_VBN to_TO promote_VB research_NN on_IN semi-supervised_JJ multiple_JJ classifier_NN systems_NNS ,_, and_CC to_TO outline_VB some_DT future_JJ research_NN directions_NNS ._.
Sect_VB
Even_RB if_IN they_PRP could_MD be_VB re-trained_VBN ,_, we_PRP would_MD need_VB -LRB-_-LRB- perfectly_RB -RRB-_-RRB- labeled_VBN data_NNS to_TO evaluate_VB them_PRP ,_, which_WDT ,_, at_IN this_DT point_NN ,_, we_PRP do_VBP not_RB have_VB ._.
Although_IN ensemble_NN methods_NNS exploiting_VBG unlabeled_JJ data_NNS has_VBZ been_VBN proposed_VBN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, they_PRP are_VBP not_RB applicable_JJ to_TO our_PRP$ problem_NN ,_, because_IN they_PRP are_VBP based_VBN on_IN boosting_VBG and_CC hence_RB they_PRP re-train_VBP the_DT underlying_VBG weak_JJ classifier_NN ._.
3_LS ._.
PROBLEM_NN DEFINITION_NN We_PRP consider_VBP a_DT c-class_NN classification_NN problem_NN
of_IN labeled_JJ instances_NNS and_CC a_DT large_JJ number_NN of_IN unlabeled_JJ instances_NNS ._.
Their_PRP$ implementations_NNS are_VBP based_VBN on_IN the_DT expectation_NN maximization_NN -LRB-_-LRB- EM_NN -RRB-_-RRB- algorithm_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- as_IN a_DT base_NN semi-supervised_JJ classifier_NN ._.
Bennett_NNP et_FW al._FW =_SYM -_: =[_NN 4_CD -RRB-_-RRB- -_: =_JJ -_: present_JJ ASSEMBLE_NNP ,_, an_DT adaptive_JJ semi-supervised_JJ ensemble_NN scheme_NN that_WDT can_MD be_VB used_VBN to_TO to_TO make_VB any_DT costsensitive_JJ classifier_NN semi-supervised_JJ ._.
Li_NNP et_FW al._FW -LRB-_-LRB- 22_CD -RRB-_-RRB- recently_RB proposed_VBN CS4VM_NN -_: a_DT semi-supervised_JJ cost_NN
t_NN explore_VB this_DT open_JJ avenue_NN ._.
A_DT next_JJ step_NN in_IN this_DT work_NN is_VBZ to_TO use_VB the_DT inherent_JJ knowledge_NN from_IN unlabeled_JJ data_NNS to_TO also_RB help_VB the_DT training_NN of_IN the_DT base_NN learner_NN system_NN ._.
This_DT is_VBZ feasible_JJ by_IN the_DT use_NN of_IN Assemble_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT Semi-supervised_JJ AdaBoost_NNP extension_NN ,_, and_CC can_MD be_VB easily_RB achieved_VBN since_IN the_DT BAS_NNP framework_NN also_RB provides_VBZ a_DT way_NN to_TO deal_VB with_IN a_DT relative_JJ cost_NN function_NN that_WDT charges_VBZ more_JJR or_CC less_JJR to_TO errors_NNS on_IN different_JJ ki_NNS
istributed_VBN scheme_NN is_VBZ not_RB discussed_VBN in_IN this_DT dissertation_NN ._.
3_CD There_EX are_VBP semi-supervised_JJ learning_VBG ensembles_NNS that_WDT construct_VBP classification_NN ensembles_NNS based_VBN on_IN both_CC labeled_JJ and_CC unlabeled_JJ data_NNS ,_, e.g._FW ASSEMEBL_NN =_JJ -_: =[_NN 58_CD -RRB-_-RRB- -_: =_JJ -_: ,_, an_DT adaptive_JJ semi-supervised_JJ ensemble_NN ._.
11s2_JJ .2_NN ._.
The_DT Hypothesis_NN :_: Why_WRB Classifier_NNP Ensembles_NNP Work_NNP 12_CD fusing_NN multiple_JJ classifiers_NNS would_MD become_VB redundant_JJ or_CC out_IN of_IN date_NN were_VBD raised_VBN among_IN researchers_NNS in_IN the_DT
perceptron_NN -LRB-_-LRB- MLP_NN -RRB-_-RRB- ,_, a_DT global_JJ classifier_NN ,_, where_WRB 3NN_NN and_CC a_DT single_JJ hidden_JJ layered_JJ MLP_NN are_VBP used_VBN in_IN our_PRP$ experiments_NNS ._.
For_IN comparison_NN ,_, we_PRP report_VBP results_NNS of_IN a_DT semi-supervised_JJ boosting_VBG algorithm_NN -LRB-_-LRB- i.e._FW ,_, ASSEMBLE_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_JJ -_: or_CC Agreement_NNP Boost_NNP -LRB-_-LRB- 13_CD -RRB-_-RRB- -RRB-_-RRB- and_CC its_PRP$ regularized_VBN version_NN -LRB-_-LRB- i.e._FW ,_, Regularized_NNP Boost_NNP -RRB-_-RRB- ._.
In_IN addition_NN ,_, we_PRP also_RB provide_VBP results_NNS of_IN a_DT variant_NN of_IN Adaboost_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- trained_VBN on_IN the_DT labeled_JJ data_NNS only_RB for_IN reference_NN ._.
The_DT
-LRB-_-LRB- 18_CD -RRB-_-RRB- ._.
Nevertheless_RB ,_, for_IN the_DT proof_NN to_TO hold_VB one_NN still_RB has_VBZ to_TO use_VB the_DT strong_JJ assumption_NN of_IN view-independence_NN -LRB-_-LRB- Equation_NN 1.1_CD -RRB-_-RRB- ._.
Another_DT example_NN for_IN the_DT use_NN of_IN unlabeled_JJ examples_NNS in_IN boosting_VBG can_MD be_VB found_VBN in_IN =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Before_IN delving_NN into_IN the_DT theoretical_JJ benefits_NNS of_IN agreement_NN ,_, we_PRP present_VBP a_DT short_JJ introduction_NN to_TO boosting_VBG ._.
1.2_CD Boosting_VBG The_DT basic_JJ idea_NN behind_IN boosting_VBG is_VBZ to_TO iteratively_RB combine_VB relatively_RB simple_JJ hypoth_NN
el_NN and_CC noisy_JJ attribute_NN detection_NN based_VBN on_IN classification_NN rules_NNS -LRB-_-LRB- 16_CD -RRB-_-RRB- -LRB-_-LRB- 14_CD -RRB-_-RRB- ;_: another_DT application_NN study_NN is_VBZ learning_VBG from_IN both_CC labeled_JJ and_CC unlabeled_JJ data_NNS by_IN assigning_VBG pseudo-classes_NNS for_IN the_DT unlabeled_JJ data_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: using_VBG boosting_VBG ensembles_NNS ._.
All_PDT this_DT previous_JJ work_NN has_VBZ its_PRP$ own_JJ niche_NN concerning_VBG data_NNS quality_NN ._.
Our_PRP$ work_NN is_VBZ more_RBR general_JJ in_IN that_IN it_PRP exploits_VBZ local_JJ data_NNS constraints_NNS using_VBG Markov_NNP networks_NNS ._.
..._: ..._: ..._: sA_NN pi_NN
d_NN learning_NN algorithms_NNS is_VBZ an_DT interesting_JJ issue_NN to_TO be_VB investigated_VBN in_IN future_JJ work_NN ._.
Ensemble_NN learning_NN techniques_NNS -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, in_IN particular_JJ ,_, Boosting_NNP ,_, have_VBP already_RB been_VBN introduced_VBN into_IN semi-supervised_JJ learning_NN =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
It_PRP is_VBZ evident_JJ that_IN the_DT working_VBG style_NN of_IN tri-training_NN exhibits_VBZ a_DT new_JJ way_NN to_TO exploit_VB ensemble_NN techniques_NNS in_IN this_DT area_NN ._.
However_RB ,_, in_IN its_PRP$ current_JJ form_NN ,_, such_JJ an_DT exploitation_NN is_VBZ very_RB limited_JJ because_IN th_DT
assifiers_NNS ._.
Recent_JJ graph-based_JJ semi-supervised_JJ learning_NN algorithms_NNS work_VBP by_IN formulating_VBG the_DT assumption_NN that_WDT ''_'' nearby_JJ ''_'' points_NNS ,_, and_CC points_NNS in_IN the_DT same_JJ structure_NN should_MD have_VB similar_JJ labels_NNS -LRB-_-LRB- 19_CD ,_, 30_CD ,_, 17_CD -RRB-_-RRB- ._.
In_IN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: semi-supervised_JJ learning_NN is_VBZ combined_VBN with_IN ensemble_NN classification_NN methods_NNS ._.
An_DT approach_NN for_IN the_DT case_NN that_IN only_RB positive_JJ -LRB-_-LRB- and_CC no_DT negative_JJ -RRB-_-RRB- training_NN data_NNS plus_CC unlabeled_JJ data_NNS are_VBP available_JJ is_VBZ described_VBN i_FW
pseudo-labeled_JJ data_NNS are_VBP utilized_VBN in_IN the_DT next_JJ iteration_NN for_IN training_VBG a_DT second_JJ classifier_NN ._.
This_DT is_VBZ broadly_RB the_DT strategy_NN adopted_VBN by_IN approaches_NNS like_IN SelfNovember_NNP 21_CD ,_, 2007_CD DRAFT_NN 5straining_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, ASSEMBLE_NN =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Semi-supervised_JJ MarginBoost_NN -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
However_RB ,_, a_DT problem_NN with_IN this_DT strategy_NN is_VBZ that_IN the_DT introduction_NN of_IN examples_NNS with_IN predicted_VBN class_NN labels_NNS may_MD only_RB help_VB to_TO increase_VB the_DT classification_NN margin_NN ,_, witho_NN
ecified_VBN size_NN ._.
Much_JJ work_NN has_VBZ recently_RB been_VBN done_VBN using_VBG unlabeled_JJ data_NNS to_TO improve_VB the_DT accuracy_NN of_IN classifiers_NNS -LRB-_-LRB- Blum_NNP and_CC Mitchell_NNP ,_, 1998_CD ;_: Nigam_NNP et_FW al._FW ,_, 2000_CD ;_: Nigam_NNP and_CC Ghani_NNP ,_, 2000_CD ;_: Goldman_NNP and_CC Zhou_NNP ,_, 2000_CD ;_: =_JJ -_: =_JJ Bennett_NNP and_CC Demiriz_NNP ,_, 2002_CD -_: =--RRB-_NN ._.
The_DT ba4sic_JJ approach_NN ,_, often_RB known_VBN as_IN semi-supervised_JJ learning_NN or_CC boot-strapping_NN ,_, is_VBZ to_TO train_VB on_IN the_DT labeled_JJ data_NNS ,_, classify_VBP the_DT unlabeled_JJ data_NNS ,_, and_CC then_RB train_NN using_VBG both_CC the_DT original_JJ and_CC the_DT relabel_NN
large_JJ weights_NNS for_IN the_DT misclassification_NN data_NNS ._.
To_TO boost_VB a_DT ``_`` base_NN learner_NN ''_'' -LRB-_-LRB- or_CC ``_`` weak_JJ learner_NN ''_'' -RRB-_-RRB- to_TO deal_VB with_IN the_DT partially_RB labeled_VBN data_NNS ,_, the_DT margin_NN of_IN unlabeled_JJ data_NNS should_MD be_VB defined_VBN -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
Bennet_NNP et_FW al._FW =_SYM -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: introduced_VBN ``_`` pseudo-classes_NNS ''_'' of_IN unlabeled_JJ points_NNS ._.
Their_PRP$ method_NN ,_, ASSEMBLE_NNP ,_, has_VBZ two_CD advantages_NNS that_IN the_DT ``_`` base_NN learner_NN ''_'' can_MD be_VB any_DT supervised_JJ classifier_NN and_CC the_DT adaptive_JJ step-sizes_NNS can_MD be_VB efficiently_RB com_NN
classifier_JJR ,_, and_CC work_NN in_IN a_DT bootstrap_JJ fashion_NN ._.
Ensemble_NN methods_NNS ,_, i.e._FW Boosting_VBG algorithm_NN were_VBD applied_VBN to_TO semi-supervised_JJ learning_NN problems_NNS ,_, and_CC reported_VBD to_TO outperform_VB other_JJ methods_NNS in_IN a_DT NIST_NNP evaluation_NN =_JJ -_: =[_NN 12_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Inspired_VBN by_IN this_DT result_NN and_CC our_PRP$ previous_JJ work_NN on_IN supervised_JJ Boosting_VBG training_NN ,_, we_PRP investigated_VBD the_DT feasibility_NN of_IN ensemble_NN method_NN for_IN semi-supervised_JJ acoustic_JJ model_NN training_NN ._.
The_DT experimental_JJ result_NN
large_JJ weights_NNS for_IN the_DT misclassification_NN data_NNS ._.
To_TO boost_VB a_DT ``_`` base_NN learner_NN ''_'' -LRB-_-LRB- or_CC ``_`` weak_JJ learner_NN ''_'' -RRB-_-RRB- to_TO deal_VB with_IN the_DT partially_RB labeled_VBN data_NNS ,_, the_DT margin_NN of_IN unlabeled_JJ data_NNS should_MD be_VB defined_VBN -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
Bennet_NNP et_FW al._FW =_SYM -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: introduced_VBN ``_`` pseudo-classes_NNS ''_'' of_IN unlabeled_JJ points_NNS ._.
Their_PRP$ method_NN ,_, ASSEMBLE_NN -LRB-_-LRB- Adaptive_JJ Semi-Supervised_JJ ensEMBLE_NN -RRB-_-RRB- method_NN ,_, has_VBZ two_CD advantages_NNS that_IN the_DT ``_`` base_NN learner_NN ''_'' can_MD be_VB any_DT supervised_JJ classifier_NN and_CC the_DT
learning_NN ._.
However_RB ,_, the_DT general_JJ Boosting_NNP framework_NN is_VBZ much_RB more_RBR widely_RB applicable_JJ ._.
In_IN this_DT section_NN we_PRP present_VBP a_DT brief_JJ survey_NN of_IN several_JJ extensions_NNS and_CC generalizations_NNS ,_, although_IN many_JJ others_NNS exist_VBP ,_, e.g._FW =_JJ -_: =[_NN 76_CD ,_, 158_CD ,_, 62_CD ,_, 18_CD ,_, 31_CD ,_, 3_CD ,_, 155_CD ,_, 15_CD ,_, 44_CD ,_, 52_CD ,_, 82_CD ,_, 164_CD ,_, 66_CD ,_, 38_CD ,_, 137_CD ,_, 147_CD ,_, 16_CD ,_, 80_CD ,_, 185_CD ,_, 100_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
7.1_CD Single_JJ Class_NN A_DT classical_JJ unsupervised_JJ learning_NN task_NN is_VBZ density_NN estimation_NN ._.
Assuming_VBG that_IN the_DT unlabeled_JJ observations_NNS x1_NN ,_, ..._: ,_, xN_NN were_VBD generated_VBN independently_RB at_IN random_JJ according_VBG to_TO some_DT unknown_JJ dis_NN
ssification_NN performance_NN ._.
Many_JJ semi-supervised_JJ learning_NN algorithms_NNS have_VBP been_VBN studied_VBN in_IN the_DT literature_NN ._.
Examples_NNS are_VBP density_NN based_JJ methods_NNS -LRB-_-LRB- 1_CD ,_, 2_CD -RRB-_-RRB- ,_, graph-based_JJ algorithms_NNS -LRB-_-LRB- 3_CD --_: 6_CD -RRB-_-RRB- ,_, and_CC boosting_VBG techniques_NNS =_JJ -_: =[_NN 7_CD ,_, 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Most_JJS of_IN these_DT methods_NNS were_VBD originally_RB designed_VBN for_IN two_CD class_NN problems_NNS ._.
However_RB ,_, many_JJ real-world_JJ applications_NNS ,_, such_JJ as_IN speech_NN recognition_NN and_CC object_NN recognition_NN ,_, require_VBP multi-class_JJ categorization_NN ._.
T_NN
γi_RB ,_, i_FW =_JJ 1_CD ._. ._.
N_NN -RCB-_-RRB- =_JJ -LCB-_-LRB- mQ_NN -LRB-_-LRB- x_NN ′_NN -RRB-_-RRB- |_CD x_CC ′_FW ∈_FW XU_FW ∧_FW mQ_NN -LRB-_-LRB- x_NN ′_NN -RRB-_-RRB- -RRB-_-RRB- 0_CD -RCB-_-RRB- and_CC ∀_FW i_FW ∈_FW -LCB-_-LRB- 1_CD ,_, ..._: ,_, N_NN −_NN 1_CD -RCB-_-RRB- ,_, γi_FW -LRB-_-LRB- γi_FW +1_FW ._.
Denote_VB moreover_RB bi_NN =_JJ Pu_NN -LRB-_-LRB- BQ_NN -LRB-_-LRB- x_NN ′_NN -RRB-_-RRB- ̸_NN =_JJ y_FW ′_FW ∧_FW mQ_NN -LRB-_-LRB- x_NN ′_NN -RRB-_-RRB- =_JJ γi_NN -RRB-_-RRB- for_IN i_FW ∈_FW -LCB-_-LRB- 1_CD ,_, ..._: ,_, N_NN -RCB-_-RRB- ._.
Then_RB ,_, Ru_NN -LRB-_-LRB- GQ_NN -RRB-_-RRB- =_JJ N_NN ∑_FW i_FW =_JJ 1_CD ∀_FW θ_FW ∈_NN =_JJ -_: =[_NN 0_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Ru_FW ∧_FW θ_NN -LRB-_-LRB- BQ_NN -RRB-_-RRB- =_JJ biγi_NN +_CC 1_CD 2_CD -LRB-_-LRB- 1_CD −_NN EumQ_NN -LRB-_-LRB- x_NN ′_NN -RRB-_-RRB- -RRB-_-RRB- -LRB-_-LRB- 8_CD -RRB-_-RRB- N_NN ∑_FW i_FW =_JJ k_NN +1_CD bi_NN with_IN k_NN =_JJ max_NN -LCB-_-LRB- i_FW |_FW γi_FW ≤_FW θ_FW -RCB-_-RRB- -LRB-_-LRB- 9_CD -RRB-_-RRB- Proof_NN Equation_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- follows_VBZ the_DT definition_NN Ru_FW ∧_FW θ_NN -LRB-_-LRB- BQ_NN -RRB-_-RRB- =_JJ Pu_NN -LRB-_-LRB- BQ_NN -LRB-_-LRB- x_NN ′_NN -RRB-_-RRB- ̸_NN =_JJ y_FW ′_FW ∧_FW mQ_NN -LRB-_-LRB- x_NN ′_NN -RRB-_-RRB- -RRB-_-RRB- θ_NN -RRB-_-RRB- ._.
Equation_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- is_VBZ obtained_VBN f_SYM
to_TO semi-supervised_JJ learning_NN by_IN using_VBG a_DT semi-supervised_JJ learning_NN algorithm_NN as_IN a_DT weak_JJ classifier_NN ._.
Hertz_NNP et_FW al._FW -LRB-_-LRB- 14_CD -RRB-_-RRB- proposed_VBN to_TO include_VB unlabeled_JJ data_NNS as_IN prior_JJ for_IN the_DT weak_JJ classifiers_NNS ._.
Bennett_NNP et_FW al._FW =_SYM -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: extend_VB the_DT loss_NN function_NN LDL_NN ∪_CD DU_NN =_JJ ∑_CD e_SYM −_CD yH_NN -LRB-_-LRB- x_NN -RRB-_-RRB- +_CC C_NN ∑_CD e_SYM −_FW |_FW H_NN -LRB-_-LRB- x_NN -RRB-_-RRB- |_CD -LRB-_-LRB- 3_LS -RRB-_-RRB- x_NN ∈_NN D_NN L_NN x_NN ∈_NN D_NN U_NN in_IN order_NN to_TO take_VB the_DT unlabeled_JJ data_NNS into_IN account_NN -LRB-_-LRB- C_NN ≥_NN 0_CD is_VBZ a_DT constant_NN introduced_VBN to_TO weight_VB the_DT importance_NN between_IN the_DT labe_NN
we_PRP are_VBP directly_RB addressing_VBG the_DT semi-supervised_JJ boosting_VBG methods_NNS ,_, it_PRP should_MD be_VB noted_VBN that_IN there_EX has_VBZ been_VBN a_DT few_JJ attempts_NNS in_IN formulating_VBG the_DT semi-supervised_JJ learning_NN as_IN a_DT boosting_VBG procedure_NN ,_, started_VBN by_IN =_JJ -_: =[_NN 3_CD ,_, 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Recently_RB ,_, based_VBN on_IN the_DT idea_NN of_IN graph-based_JJ manifold_NN regularization_NN methods_NNS ,_, Mallapragada_NNP et_FW al._FW -LRB-_-LRB- 5_CD -RRB-_-RRB- and_CC Chen_NNP and_CC Wang_NNP -LRB-_-LRB- 6_CD -RRB-_-RRB- have_VBP proposed_VBN other_JJ approaches_NNS for_IN boosting_VBG models_NNS ._.
Furthermore_RB ,_, in_IN computer_NN
where_WRB the_DT training_NN set_NN used_VBN for_IN each_DT member_NN of_IN the_DT series_NN is_VBZ chosen_VBN based_VBN on_IN the_DT performance_NN of_IN the_DT earlier_JJR classifiers_NNS in_IN the_DT series_NN ._.
These_DT ensemble_NN methods_NNS are_VBP also_RB used_VBN to_TO exploit_VB unlabelled_JJ data_NN =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT work_NN of_IN Bennett_NNP et_FW al._FW in_IN fact_NN proposes_VBZ a_DT semi-supervised_JJ method_NN that_IN constructs_NNS classification_NN ensembles_NNS based_VBN on_IN both_CC labeled_JJ and_CC unlabelled_JJ data_NNS ._.
These_DT problems_NNS lie_VBP in_IN the_DT middle_NN between_IN wel_NN
