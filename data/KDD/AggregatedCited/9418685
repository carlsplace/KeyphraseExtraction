Training_VBG structural_JJ svms_NNS with_IN kernels_NNS using_VBG sampled_VBN cuts_NNS
Discriminative_JJ training_NN for_IN structured_JJ outputs_NNS has_VBZ found_VBN increasing_VBG applications_NNS in_IN areas_NNS such_JJ as_IN natural_JJ language_NN processing_NN ,_, bioinformatics_NNS ,_, information_NN retrieval_NN ,_, and_CC computer_NN vision_NN ._.
Focusing_VBG on_IN large-margin_JJ methods_NNS ,_, the_DT most_RBS general_JJ -LRB-_-LRB- in_IN terms_NNS of_IN loss_NN function_NN and_CC model_NN structure_NN -RRB-_-RRB- training_NN algorithms_NNS known_VBN to_TO date_NN are_VBP based_VBN on_IN cutting-plane_JJ approaches_NNS ._.
While_IN these_DT algorithms_NNS are_VBP very_RB efficient_JJ for_IN linear_JJ models_NNS ,_, their_PRP$ training_NN complexity_NN becomes_VBZ quadratic_JJ in_IN the_DT number_NN of_IN examples_NNS when_WRB kernels_NNS are_VBP used_VBN ._.
To_TO overcome_VB this_DT bottleneck_NN ,_, we_PRP propose_VBP new_JJ training_NN algorithms_NNS that_WDT use_VBP approximate_JJ cutting_VBG planes_NNS and_CC random_JJ sampling_NN to_TO enable_VB efficient_JJ training_NN with_IN kernels_NNS ._.
We_PRP prove_VBP that_IN these_DT algorithms_NNS have_VBP improved_VBN time_NN complexity_NN while_IN providing_VBG approximation_NN guarantees_NNS ._.
In_IN empirical_JJ evaluations_NNS ,_, our_PRP$ algorithms_NNS produced_VBD solutions_NNS with_IN training_NN and_CC test_NN error_NN rates_NNS close_RB to_TO those_DT of_IN exact_JJ solvers_NNS ._.
Even_RB on_IN binary_JJ classification_NN problems_NNS where_WRB highly_RB optimized_VBN conventional_JJ training_NN methods_NNS exist_VBP -LRB-_-LRB- e.g._FW SVM-light_JJ -RRB-_-RRB- ,_, our_PRP$ methods_NNS are_VBP about_IN an_DT order_NN of_IN magnitude_NN faster_RBR than_IN conventional_JJ training_NN methods_NNS on_IN large_JJ datasets_NNS ,_, while_IN remaining_VBG competitive_JJ in_IN speed_NN on_IN datasets_NNS of_IN medium_NN size_NN ._.
