Combining_VBG clustering_NN and_CC co-training_NN to_TO enhance_VB text_NN classification_NN using_VBG unlabelled_JJ data_NNS
In_IN this_DT paper_NN ,_, we_PRP present_VBP a_DT new_JJ co-training_JJ strategy_NN that_WDT makes_VBZ use_NN of_IN unlabelled_JJ data_NNS ._.
It_PRP trains_VBZ two_CD predictors_NNS in_IN parallel_NN ,_, with_IN each_DT predictor_NN labeling_VBG the_DT unlabelled_JJ data_NNS for_IN training_VBG the_DT other_JJ predictor_NN in_IN the_DT next_JJ round_NN ._.
Both_DT predictors_NNS are_VBP support_NN vector_NN machines_NNS ,_, one_CD trained_JJ using_VBG data_NNS from_IN the_DT original_JJ feature_NN space_NN ,_, the_DT other_JJ trained_VBN with_IN new_JJ features_NNS that_WDT are_VBP derived_VBN by_IN clustering_NN both_CC the_DT labeled_JJ and_CC unlabelled_JJ data_NNS ._.
Hence_RB ,_, unlike_IN standard_JJ co-training_NN methods_NNS ,_, our_PRP$ method_NN does_VBZ not_RB require_VB a_DT priori_FW the_DT existence_NN of_IN two_CD redundant_JJ views_NNS either_CC of_IN which_WDT can_MD be_VB used_VBN for_IN classification_NN ,_, nor_CC is_VBZ it_PRP dependent_JJ on_IN the_DT availability_NN of_IN two_CD different_JJ supervised_JJ learning_NN algorithms_NNS that_WDT complement_VBP each_DT other_JJ ._.
We_PRP evaluated_VBD our_PRP$ method_NN with_IN two_CD classifiers_NNS and_CC three_CD text_NN benchmarks_NNS :_: WebKB_NN ,_, Reuters_NNP newswire_NN articles_NNS and_CC 20_CD NewsGroups_NNS ._.
Our_PRP$ evaluation_NN shows_VBZ that_IN our_PRP$ co-training_NN technique_NN improves_VBZ text_NN classification_NN accuracy_NN especially_RB when_WRB the_DT number_NN of_IN labeled_JJ examples_NNS are_VBP very_RB few_JJ ._.
n._VB The_DT two_CD main_JJ approaches_NNS used_VBN to_TO exploit_VB unlabelled_JJ data_NNS are_VBP to_TO :_: -LRB-_-LRB- 1_LS -RRB-_-RRB- transform_VB the_DT input_NN feature_NN space_NN using_VBG information_NN in_IN the_DT unlabelled_JJ data_NNS and_CC -LRB-_-LRB- 2_LS -RRB-_-RRB- iteratively_RB label_VBP part_NN of_IN the_DT unlabelled_JJ data_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Zelikovitz_NNP and_CC Hirsh_NNP -LRB-_-LRB- 10_CD -RRB-_-RRB- apply_VBP the_DT first_JJ approach_NN where_WRB they_PRP use_VBP an_DT unlabelled_JJ corpus_NN to_TO create_VB a_DT domain_NN model_NN that_WDT incorporates_VBZ words_NNS that_WDT co-occur_VBP in_IN the_DT corpus_NN ._.
The_DT second_JJ approach_NN is_VBZ more_JJR commo_NN
a._NN Finally_RB ,_, we_PRP give_VBP the_DT conclusions_NNS in_IN section_NN 4_CD ._.
1.1_CD More_JJR Related_JJ Work_NN The_DT most_RBS closely_RB related_JJ work_NN to_TO ours_PRP is_VBZ -LRB-_-LRB- 16_CD -RRB-_-RRB- as_IN discussed_VBN above_IN ._.
Another_DT popular_JJ technique_NN using_VBG unlabeled_JJ data_NNS is_VBZ co-training_NN =_JJ -_: =[_NN 1_CD ,_, 7_CD ,_, 8_CD ,_, 15_CD ,_, 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT use_VBP two_CD complementary_JJ predictors_NNS to_TO iteratively_RB label_VB the_DT unlabeled_JJ data_NNS ._.
Clearly_RB ,_, these_DT cotraining_VBG based_VBN approaches_NNS are_VBP different_JJ from_IN our_PRP$ proposed_VBN technique_NN and_CC are_VBP not_RB specially_RB designed_VBN to_TO
ficient_JJ to_TO classify_VB the_DT examples_NNS and_CC these_DT two_CD views_NNS are_VBP conditionally_RB independent_JJ given_VBN the_DT classification_NN ._.
Co-training_NN has_VBZ been_VBN proved_VBN successful_JJ in_IN making_VBG using_VBG of_IN unlabeled_JJ data_NNS in_IN classification_NN =_JJ -_: =[_NN 12_CD ,_, 8_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT XML_NN document_NN classification_NN is_VBZ a_DT natural_JJ application_NN of_IN the_DT Co-training_JJ algorithm_NN ._.
An_DT XML_NN document_NN consists_VBZ of_IN multiple_JJ substructures_NNS -_: elements_NNS ,_, and_CC these_DT elements_NNS could_MD and_CC normally_RB does_VBZ conta_NN
s_NN used_VBD :_: i_LS -RRB-_-RRB- to_TO create_VB a_DT training_NN set_VBN from_IN the_DT unlabelled_JJ data_NNS -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, ii_LS -RRB-_-RRB- to_TO augment_VB the_DT training_NN set_VBN with_IN new_JJ documents_NNS from_IN the_DT unlabelled_JJ data_NNS -LRB-_-LRB- 18_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, iii_LS -RRB-_-RRB- to_TO augment_VB the_DT dataset_NN with_IN new_JJ features_NNS =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: ,_, -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, and_CC iv_NN -RRB-_-RRB- to_TO co-train_VB a_DT classifier_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
Finally_RB ,_, clustering_NN in_IN large-scale_JJ classification_NN problems_NNS is_VBZ another_DT major_JJ research_NN area_NN in_IN text_NN classification_NN ._.
A_DT considerable_JJ amount_NN of_IN wor_NN
labeled_VBN and_CC unlabeled_JJ data_NNS ._.
They_PRP evaluate_VBP the_DT method_NN using_VBG SVM_NN classifiers_NNS on_IN Reuters-21578_NN ,_, 20Newsgroups_NN ,_, and_CC WebKB_FW corpora_FW ,_, and_CC find_VB significant_JJ improvements_NNS in_IN their_PRP$ classification_NN performance_NN ._.
In_IN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT technique_NN presented_VBN above_IN is_VBZ combined_VBN with_IN co-training_NN ._.
The_DT algorithm_NN trains_VBZ two_CD predictors_NNS in_IN parallel_NN ,_, with_IN one_CD predictor_NN labeling_VBG the_DT unlabeled_JJ data_NNS for_IN training_VBG the_DT other_JJ in_FW thesnext_FW round_NN ._.
ied_VBD versions_NNS of_IN the_DT co-training_JJ algorithm_NN have_VBP been_VBN proposed_VBN since_IN the_DT original_JJ one_CD ._.
In_IN -LRB-_-LRB- 13_CD -RRB-_-RRB- ,_, Goldman_NNP and_CC Zhou_NNP use_VBP two_CD different_JJ supervised_JJ learning_NN algorithms_NNS to_TO label_VB data_NNS for_IN each_DT other_JJ ._.
Raskuttii_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_SYM -_: presents_VBZ a_DT new_JJ co-training_NN strategy_NN which_WDT uses_VBZ two_CD feature_NN sets_NNS ._.
One_CD classifier_NN is_VBZ trained_VBN using_VBG data_NNS from_IN original_JJ feature_NN space_NN ,_, while_IN the_DT other_JJ is_VBZ trained_VBN with_IN new_JJ features_NNS that_WDT are_VBP derived_VBN by_IN clu_NN
