Efficient_JJ progressive_JJ sampling_NN
the_DT model_NN 's_POS performance_NN at_IN each_DT epoch_NN ._.
To_TO accomplish_VB this_DT ,_, we_PRP compute_VBP x-validated_JJ accuracy_NN of_IN the_DT current_JJ model_NN on_IN the_DT available_JJ pool_NN of_IN instances_NNS ._.
Progress_NNP of_IN learning_VBG curve_NN is_VBZ estimated_VBN empirically_RB =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Here_RB we_PRP use_VBP LOESS_JJ regression_NN in_IN order_NN to_TO smooth_VB the_DT variances_NNS in_IN estimated_VBN learning_NN rates_NNS at_IN each_DT epoch_NN ._.
More_RBR succinctly_RB ,_, the_DT learning_NN rate_NN at_IN any_DT point_NN is_VBZ estimated_VBN by_IN computing_VBG the_DT slope_NN of_IN a_DT leas_NN
statistics_NNS under_IN the_DT name_NN of_IN sequential_JJ sampling_NN -LRB-_-LRB- 18_CD -RRB-_-RRB- and_CC more_RBR recently_RB in_IN the_DT database_NN community_NN -LRB-_-LRB- 14_CD ,_, 13_CD -RRB-_-RRB- ._.
In_IN the_DT KDD_NNP literature_NN ,_, related_JJ methods_NNS are_VBP described_VBN under_IN the_DT name_NN of_IN progressive_JJ sampling_NN =_JJ -_: =[_NN 11_CD ,_, 15_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Details_NNS on_IN how_WRB to_TO use_VB sampling_NN for_IN selecting_VBG the_DT stump_NN at_IN each_DT boosting_VBG iteration_NN are_VBP provided_VBN in_IN Section_NN 3_CD ._.
In_IN this_DT paper_NN we_PRP provide_VBP a_DT preliminary_JJ experimental_JJ evaluation_NN of_IN this_DT learning_NN system_NN ._.
F_NN
ing_RB ,_, but_CC it_PRP does_VBZ not_RB venture_VB intosearching_VBG among_IN all_DT possible_JJ combinations_NNS of_IN parameter_NN settings_NNS exhaustively_RB on_IN all_DT training_NN data_NNS available_JJ ._.
Rather_RB ,_, it_PRP borrows_VBZ a_DT heuristic_NN from_IN progressive_JJ sampling_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT goal_NN of_IN progressive_JJ sampling_NN is_VBZ to_TO iteratively_RB seek_VB a_DT data_NN set_NN size_NN at_IN which_WDT generalization_NN performance_NN on_IN test_NN material_NN converges_VBZ ._.
The_DT method_NN is_VBZ to_TO start_VB at_IN a_DT small_JJ sample_NN size_NN training_NN set_NN ,_, an_DT
rmance_NN at_IN a_DT reasonable_JJ time_NN ._.
Approaches_NNS to_TO tackle_VB this_DT problem_NN range_NN from_IN statistical_JJ estmiates_NNS for_IN appropriate_JJ subsample_JJ sizes_NNS -LRB-_-LRB- 15_CD ,_, 29_CD ,_, 24_CD ,_, 9_CD -RRB-_-RRB- ,_, over_IN attempts_NNS to_TO model_VB the_DT shape_NN of_IN the_DT learning_NN curve_NN =_JJ -_: =[_NN 13_CD ,_, 22_CD -RRB-_-RRB- -_: =_JJ -_: ,_, to_TO active_JJ learning_NN and_CC windowing_NN techniques_NNS that_WDT give_VBP the_DT learning_NN algorithm_NN itself_PRP control_VBP over_IN the_DT subsampling_JJ process_NN -LRB-_-LRB- 8_CD ,_, 10_CD -RRB-_-RRB- ._.
However_RB ,_, the_DT main_JJ focus_NN of_IN previous_JJ works_NNS on_IN this_DT topic_NN has_VBZ been_VBN on_IN
then_RB applying_VBG voting_NN to_TO classify_VB new_JJ examples_NNS will_MD allow_VB an_DT accurate_JJ model_NN to_TO be_VB built_VBN on_IN a_DT single_JJ computer_NN ._.
Others_NNS have_VBP suggested_VBN that_IN in_IN very_RB large_JJ data_NNS sets_VBZ many_JJ of_IN the_DT examples_NNS will_MD be_VB redundant_JJ =_JJ -_: =[_NN 7_CD ,_, 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT context_NN of_IN the_DT above_JJ work_NN ,_, the_DT use_NN of_IN subsets_NNS or_CC perhaps_RB a_DT single_JJ well-chosen_JJ subset_NN for_IN training_NN may_MD be_VB the_DT right_JJ approach_NN ._.
In_IN this_DT paper_NN ,_, we_PRP consider_VBP the_DT case_NN where_WRB either_CC all_PDT the_DT data_NNS -LRB-_-LRB- of_IN
and_CC feature_NN selection_NN ,_, that_WDT combines_VBZ classifier_NN wrapping_NN -LRB-_-LRB- using_VBG the_DT training_NN material_NN internally_RB to_TO test_VB experimental_JJ variants_NNS -RRB-_-RRB- -LRB-_-LRB- Kohavi_NNP and_CC John_NNP ,_, 1997_CD -RRB-_-RRB- with_IN progressive_JJ sampling_NN of_IN training_NN material_NN -LRB-_-LRB- =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN ._.
We_PRP start_VBP with_IN a_DT large_JJ pool_NN of_IN experiments_NNS ,_, each_DT with_IN a_DT unique_JJ combination_NN of_IN input_NN features_NNS and_CC algorithmic_JJ parameter_NN settings_NNS ._.
In_IN the_DT first_JJ step_NN ,_, each_DT attempted_VBN setting_NN is_VBZ applied_VBN to_TO a_DT small_JJ amoun_NN
sub-sampling_NN and_CC compression_NN for_IN reducing_VBG the_DT volume_NN of_IN data_NNS these_DT algorithms_NNS must_MD examine_VB ._.
While_IN serial_JJ techniques_NNS for_IN sub-sampling_NN and_CC compression_NN have_VBP been_VBN developed_VBN and_CC applied_VBN with_IN some_DT success_NN =_JJ -_: =[_NN 15_CD ,_, 24_CD ,_, 32_CD ,_, 36_CD -RRB-_-RRB- -_: =_JJ -_: ,_, a_DT variety_NN of_IN application_NN characteristics_NNS necessitate_VBP the_DT development_NN of_IN corresponding_VBG distributed_VBN formulations_NNS ._.
These_DT include_VBP :_: •_NNP Data_NNP volume_NN :_: Datasets_NNS often_RB reside_VBP at_IN geographically_RB distributed_VBN loca_NN
and_CC feature_NN selection_NN ,_, that_WDT combines_VBZ classifier_NN wrapping_NN -LRB-_-LRB- using_VBG the_DT training_NN material_NN internally_RB to_TO test_VB experimental_JJ variants_NNS -RRB-_-RRB- -LRB-_-LRB- Kohavi_NNP and_CC John_NNP ,_, 1997_CD -RRB-_-RRB- with_IN progressive_JJ sampling_NN of_IN training_NN material_NN -LRB-_-LRB- =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN ._.
We_PRP start_VBP with_IN a_DT large_JJ pool_NN of_IN experiments_NNS ,_, each_DT with_IN a_DT unique_JJ combination_NN of_IN input_NN features_NNS and_CC algorithmic_JJ parameter_NN settings_NNS ._.
In_IN the_DT first_JJ step_NN ,_, each_DT attempted_VBN setting_NN is_VBZ applied_VBN to_TO a_DT small_JJ amoun_NN
geometric_JJ sampling_NN scheme_NN is_VBZ motivated_VBN by_IN previous_JJ work_NN on_IN progressive_JJ sampling_NN ,_, which_WDT shows_VBZ that_IN given_VBN certain_JJ assumptions_NNS -LRB-_-LRB- which_WDT do_VBP not_RB hold_VB in_IN this_DT paper_NN -RRB-_-RRB- ,_, this_DT schedule_NN is_VBZ asymptotically_RB optimal_JJ =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT previous_JJ work_NN on_IN progressive_JJ sampling_NN is_VBZ described_VBN in_IN Section_NN 6_CD ._.
For_IN sampling_NN schedules_NNS S1_NN and_CC S2_NN ,_, in_IN addition_NN to_TO evaluating_VBG the_DT training_NN set_NN sizes_NNS just_RB described_VBN ,_, the_DT largest_JJS possible_JJ training_NN
ny_IN unknown_JJ properties_NNS of_IN an_DT instance_NN ._.
Other_JJ results_NNS seeking_VBG to_TO reduce_VB the_DT sample_NN complexity_NN for_IN learning_NN include_VBP decision_NN theoretic_JJ subsampling_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, on-line_JJ stopping_VBG rules_NNS -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, progressive_JJ sampling_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC active_JJ feature_NN value_NN acquisition_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
We_PRP note_VBP that_IN these_DT techniques_NNS differ_VBP from_IN our_PRP$ approach_NN because_IN we_PRP place_VBP a_DT firm_JJ prior_JJ budget_NN on_IN the_DT learner_NN 's_POS ability_NN to_TO acquire_VB information_NN ,_, while_IN these_DT ap_NN
for_IN model-based_JJ clustering_NN methods_NNS are_VBP usually_RB smooth_JJ ._.
In_IN situations_NNS where_WRB the_DT learning_NN curves_NNS are_VBP less_RBR smooth_JJ ,_, techniques_NNS that_WDT use_VBP additional_JJ samples_NNS to_TO assess_VB the_DT shape_NN of_IN the_DT learning_NN curve_NN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =_JJ Provost_NNP ,_, et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN may_MD be_VB useful_JJ and_CC the_DT use_NN of_IN an_DT abbreviated_JJ training_NN method_NN can_MD reduce_VB the_DT cost_NN of_IN doing_VBG so_RB ._.
6_CD ._.
Empirical_JJ Study_NN In_IN this_DT section_NN ,_, we_PRP describe_VBP an_DT evaluation_NN of_IN our_PRP$ learning-curve_JJ sampling_NN method_NN and_CC e_SYM
designing_VBG data_NNS mining_NN systems_NNS for_IN very_RB large_JJ data_NNS sets_NNS ._.
The_DT machine_NN learning_NN community_NN has_VBZ essentially_RB focused_VBN on_IN two_CD directions_NNS to_TO deal_VB with_IN massive_JJ data_NNS sets_NNS :_: data_NNS subsampling_VBG -LRB-_-LRB- Musick_NNP et_FW al._FW ,_, 1993_CD ;_: =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN ,_, and_CC the_DT design_NN of_IN parallel_NN or_CC distributed_VBN approaches_NNS capable_JJ of_IN handling_VBG all_PDT the_DT data_NNS -LRB-_-LRB- Chan_NNP and_CC Stolfo_NNP ,_, 1993_CD ;_: Provost_NNP and_CC Hennessy_NNP ,_, 1996_CD ;_: Hall_NNP et_FW al._FW ,_, 1999_CD ;_: Chawla_NNP et_FW al._FW ,_, 2000_CD -RRB-_-RRB- ._.
The_DT subsampling_JJ ap_NN
l_NN efficiency_NN may_MD not_RB equate_VB to_TO an_DT improvement_NN in_IN prediction_NN performance_NN ._.
A_DT critical_JJ assumption_NN underlies_VBZ many_JJ current_JJ attempts_NNS to_TO tackle_VB large_JJ data_NNS sets_NNS by_IN creating_VBG algorithms_NNS that_WDT are_VBP more_RBR efficient_JJ =_JJ -_: =[_NN 1_CD ,_, 2_CD ,_, 3_CD ,_, 4_CD ,_, 5_CD ,_, 6_CD -RRB-_-RRB- -_: =_JJ -_: :_: that_IN the_DT learning_VBG biases_NNS of_IN existing_VBG algorithms_NNS are_VBP suitable_JJ for_IN use_NN with_IN large_JJ data_NNS sets_NNS ._.
Increasing_VBG the_DT efficiency_NN of_IN an_DT algorithm_NN assumes_VBZ that_IN the_DT existing_VBG algorithm_NN only_RB requires_VBZ more_JJR time_NN ,_, rath_NN
ny_IN unknown_JJ properties_NNS of_IN an_DT instance_NN ._.
Other_JJ results_NNS seeking_VBG to_TO reduce_VB the_DT sample_NN complexity_NN for_IN learning_NN include_VBP decision_NN theoretic_JJ subsampling_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ,_, on-line_JJ stopping_VBG rules_NNS -LRB-_-LRB- 15_CD -RRB-_-RRB- ,_, progressive_JJ sampling_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC active_JJ feature_NN value_NN acquisition_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- ._.
We_PRP note_VBP that_IN these_DT techniques_NNS differ_VBP from_IN our_PRP$ approach_NN because_IN we_PRP place_VBP a_DT firm_JJ budget_NN on_IN the_DT learner_NN 's_POS ability_NN to_TO acquire_VB information_NN ,_, while_IN these_DT approach_NN
arning_VBG algorithm_NN on_IN the_DT basis_NN of_IN a_DT sequence_NN of_IN samples_NNS ._.
Some_DT authors_NNS have_VBP used_VBN samples_NNS that_WDT increase_VBP by_IN a_DT fixed_JJ amount_NN -LRB-_-LRB- John_NNP &_CC Langley_NNP ,_, 1996_CD -RRB-_-RRB- ,_, while_IN others_NNS have_VBP used_VBN progressively_RB increasing_VBG samples_NNS -LRB-_-LRB- =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =_JJ -_: ;_: Leite_NNP &_CC Brazdil_NNP ,_, 2004_CD -RRB-_-RRB- ._.
Usually_RB the_DT aim_NN is_VBZ to_TO determine_VB the_DT sample_NN size_NN in_IN which_WDT the_DT accuracy_NN does_VBZ not_RB increase_VB any_DT more_JJR ,_, called_VBD a_DT optimal_JJ sample_NN size_NN ._.
Fig._NN 1_CD shows_VBZ a_DT typical_JJ learning_NN curve_NN and_CC the_DT
in_IN no_DT significant_JJ increase_NN in_IN model_NN accuracy_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
Another_DT recent_JJ work_NN on_IN improving_VBG the_DT efficiency_NN of_IN tree_NN building_NN for_IN large_JJ data_NNS sets_NNS is_VBZ Progressive_NNP Sampling_NNP -LRB-_-LRB- PS_NNP for_IN short_JJ -RRB-_-RRB- proposed_VBN by_IN Provost_NNP et_FW al_FW =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
By_IN means_NNS of_IN a_DT learning_NN curve_NN -LRB-_-LRB- see_VB an_DT example_NN learning_NN curve_NN in_IN Figure_NNP 1_LS -RRB-_-RRB- which_WDT depicts_VBZ the_DT relationship_NN between_IN sample_NN size_NN and_CC \*_NN Email_NN :_: gubh@comp.nus.edu.sg;_NNP School_NNP of_IN Computing_NNP ,_, National_NNP University_NNP
in_IN no_DT significant_JJ increase_NN in_IN model_NN accuracy_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- ._.
Another_DT recent_JJ work_NN on_IN improving_VBG the_DT efficiency_NN of_IN tree_NN building_NN for_IN large_JJ data_NNS sets_NNS is_VBZ Progressive_NNP Sampling_NNP -LRB-_-LRB- PS_NNP for_IN short_JJ -RRB-_-RRB- proposed_VBN by_IN Provost_NNP et_FW al_FW =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
By_IN means_NNS of_IN a_DT learning_NN curve_NN -LRB-_-LRB- see_VB an_DT example_NN learning_NN curve_NN in_IN Figure_NNP 1_LS -RRB-_-RRB- which_WDT depicts_VBZ the_DT relationship_NN between_IN sample_NN size_NN and_CC ∗_NNP Email_NNP :_: gubh@comp.nus.edu.sg;_NNP School_NNP of_IN Computing_NNP ,_, National_NNP University_NNP
nerating_VBG distinct_JJ samples_NNS at_IN each_DT site_NN ._.
We_PRP then_RB merge_VBP these_DT samples_NNS to_TO finally_RB train_VB a_DT classifier_NN on_IN the_DT resulting_VBG sample_NN ._.
The_DT literature_NN in_IN data_NN mining_NN is_VBZ filled_VBN with_IN various_JJ sampling_NN algorithms_NNS -LRB-_-LRB- 3_LS -RRB-_-RRB- =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 4_LS -RRB-_-RRB- ,_, which_WDT can_MD be_VB grouped_VBN ,_, with_IN respect_NN to_TO how_WRB the_DT samples_NNS are_VBP formed_VBN ,_, to_TO form_VB three_CD distinct_JJ families_NNS :_: static_JJ ,_, dynamic_JJ and_CC active_JJ sampling_NN ._.
Static_JJ Sampling_NN refers_VBZ to_TO a_DT sampling_NN that_WDT is_VBZ performed_VBN wit_NN
a_DT heuristic_NN wrapped-based_JJ method_NN to_TO set_VB them_PRP automatically_RB for_IN all_DT experiments_NNS ._.
Wrapped_VBN progressive_JJ sampling_NN -LRB-_-LRB- wps_NNS -RRB-_-RRB- -LRB-_-LRB- 22_CD -RRB-_-RRB- combines_VBZ classifier_NN wrapping_NN -LRB-_-LRB- 23_CD -RRB-_-RRB- with_IN progressive_JJ sampling_NN of_IN training_NN material_NN =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
wps_NN starts_VBZ with_IN a_DT large_JJ pool_NN of_IN experiments_NNS ,_, each_DT with_IN one_CD systematically_RB generated_VBN recombination_NN of_IN tested_VBN algorithmic_JJ parameter_NN settings_NNS ._.
In_IN the_DT first_JJ step_NN of_IN wps_NNS ,_, each_DT attempted_VBN setting_NN is_VBZ applie_NN
and_CC feature_NN selection_NN ,_, that_WDT combines_VBZ classifier_NN wrapping_NN -LRB-_-LRB- using_VBG the_DT training_NN material_NN internally_RB to_TO test_VB experimental_JJ variants_NNS -RRB-_-RRB- -LRB-_-LRB- Kohavi_NNP and_CC John_NNP ,_, 1997_CD -RRB-_-RRB- with_IN progressive_JJ sampling_NN of_IN training_NN material_NN -LRB-_-LRB- =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN ._.
We_PRP start_VBP with_IN a_DT large_JJ pool_NN of_IN experiments_NNS ,_, each_DT with_IN a_DT unique_JJ combination_NN of_IN input_NN features_NNS and_CC algorithmic_JJ parameter_NN settings_NNS ._.
In_IN the_DT first_JJ step_NN ,_, each_DT attempted_VBN setting_NN is_VBZ applied_VBN to_TO a_DT small_JJ amoun_NN
the_DT first_JJ place_NN ._.
Geometric_JJ sampling_NN was_VBD proposed_VBN as_IN an_DT improved_JJ sampling_NN mechanism_NN ._.
It_PRP uses_VBZ the_DT following_JJ schedule_NN :_: Sgeom_NN =_JJ a_FW i_FW ·_NN n0_NN =_JJ -LCB-_-LRB- n0_NN ,_, a_DT ·_FW n0_FW ,_, a_DT 2_CD ·_FW n0_FW ,_, a_DT 3_CD ·_NN n0_NN ,_, ..._: -RCB-_-RRB- where_WRB a_DT and_CC n0_NN are_VBP constants_NNS =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN instance_NN :_: -LCB-_-LRB- 100_CD ,_, 200_CD ,_, 400_CD ,_, 800_CD ,_, ..._: -RCB-_-RRB- ._.
Although_IN a_DT ,_, which_WDT we_PRP will_MD call_VB it_PRP the_DT geometric_JJ factor_NN ,_, can_MD be_VB any_DT constant_JJ ,_, by_IN default_NN it_PRP has_VBZ been_VBN taken_VBN to_TO be_VB equal_JJ to_TO 2_CD ._.
Its_PRP$ main_JJ drawback_NN is_VBZ that_IN it_PRP is_VBZ
finite_JJ one_NN -RRB-_-RRB- as_IN the_DT reference_NN for_IN loss_NN ,_, and_CC their_PRP$ approach_NN is_VBZ based_VBN on_IN fitting_JJ a_DT learning_NN curve_NN ,_, offering_VBG no_DT guarantees_NNS that_IN the_DT criterion_NN will_MD be_VB satisfied_VBN ._.
Learning_NNP curve_NN methods_NNS -LRB-_-LRB- Meek_NNP et_FW al._FW ,_, 2002_CD ,_, =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_CD produce_NN models_NNS on_IN successively_RB larger_JJR sub-samples_NNS of_IN the_DT training_NN set_VBN until_IN model_NN quality_NN appears_VBZ to_TO asymptote_VB ._.
Progressive_JJ sampling_NN approaches_NNS attempt_VBP to_TO iteratively_RB determine_VB the_DT best_JJS number_NN of_IN ex_FW
ped_VBN by_IN detecting_VBG diminishing_VBG improvement_NN in_IN the_DT quality_NN of_IN the_DT models_NNS being_VBG built_VBN ._.
Convergence_NN detection_NN has_VBZ been_VBN studied_VBN for_IN the_DT case_NN of_IN random_JJ sampling_NN by_IN estimating_VBG the_DT slope_NN of_IN the_DT learning_NN curve_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT learning_VBG curve_NN maynotbewell_NN behaved_VBD in_IN the_DT active_JJ learning_NN case_NN making_VBG this_DT task_NN more_RBR complicated_JJ ._.
This_DT also_RB makes_VBZ the_DT more_RBR general_JJ problem_NN of_IN determining_VBG a_DT good_JJ schedule_NN for_IN adding_VBG labeled_JJ poin_NN
e_LS with_IN the_DT power_NN law_NN ._.
One_CD limitation_NN is_VBZ that_IN the_DT dynamic_JJ sampling_NN requires_VBZ the_DT learning_VBG algorithm_NN to_TO be_VB incremental_JJ ._.
Progressive_JJ Sampling_NN :_: The_DT progressive_JJ sampling_NN was_VBD first_RB proposed_VBN by_IN Provost_NNP et_FW al_FW =_JJ -_: =[_NN 45_CD -RRB-_-RRB- -_: =_SYM -_: for_IN scaling_VBG up_RP inductive_JJ algorithms_NNS ._.
Basically_RB it_PRP uses_VBZ the_DT idea_NN similar_JJ to_TO the_DT dynamic_JJ sampling_NN ,_, but_CC it_PRP does_VBZ not_RB require_VB the_DT algorithm_NN to_TO be_VB incremental_JJ ._.
It_PRP starts_VBZ learning_VBG with_IN a_DT small_JJ sample_NN ,_, and_CC p_NN
ther_JJR enlargements_NNS of_IN the_DT window_NN ._.
In_IN terms_NNS of_IN run-time_NN ,_, the_DT authors_NNS note_VBP that_IN this_DT technique_NN can_MD in_IN general_JJ only_JJ gain_NN efficiency_NN for_IN incremental_JJ algorithm_NN ._.
In_IN the_DT most_RBS recent_JJ work_NN of_IN Provost_NNP and_CC et_FW al_FW =_JJ -_: =[_NN 36_CD -RRB-_-RRB- -_: =_JJ -_: ,_, they_PRP prompted_VBD a_DT new_JJ procedure_NN named_VBN ``_`` progressive_JJ sampling_NN ''_'' ._.
The_DT basic_JJ idea_NN is_VBZ when_WRB sample_NN size_NN increases_VBZ the_DT learning_NN accuracy_NN will_MD follow_VB the_DT famous_JJ ``_`` learning_NN curve_NN ''_'' so_IN that_IN there_EX should_MD be_VB a_DT conve_NN
The_DT experimental_JJ result_NN on_IN census_NN :_: the_DT error_NN rate_NN -LRB-_-LRB- %_NN -RRB-_-RRB- ._.
Archive_NNP -LRB-_-LRB- 3_CD -RRB-_-RRB- ._.
Led_NNP -LRB-_-LRB- 10_CD %_NN -RRB-_-RRB- and_CC waveform_NN are_VBP selected_VBN from_IN UCI_NNP Machine_NNP Learning_NNP Repository_NNP -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
These_DT data_NNS sets_NNS are_VBP used_VBN in_IN the_DT paper_NN of_IN Provost_NNP et_FW al._FW =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: ._.
``_`` Led_NNP -LRB-_-LRB- 10_CD %_NN -RRB-_-RRB- ''_'' means_VBZ that_IN 10_CD %_NN of_IN instances_NNS are_VBP includes_VBZ noise_NN in_IN one_CD of_IN the_DT attribute_NN values_NNS ._.
The_DT specifications_NNS of_IN the_DT data_NNS sets_NNS are_VBP summarized_VBN in_IN Table_NNP 2_CD ._.
The_DT experimental_JJ results_NNS on_IN prediction_NN accura_NN
ly_RB ,_, adhoc_JJ sampling_NN queries_NNS that_WDT project_VBP the_DT data_NNS set_VBN along_IN space_NN and_CC time_NN have_VBP been_VBN used_VBN to_TO effectively_RB summarize_VB data_NNS for_IN tasks_NNS such_JJ as_IN anomaly_NN detection_NN and_CC network_NN monitoring_NN ._.
Progressive_JJ sampling_NN =_JJ -_: =[_NN 27_CD -RRB-_-RRB- -_: =_JJ -_: and_CC similar_JJ techniques_NNS can_MD mitigate_VB the_DT issues_NNS related_VBN to_TO the_DT quality_NN of_IN the_DT sample_NN ,_, but_CC they_PRP make_VBP it_PRP imperative_JJ that_IN sample_NN generation_NN be_VB inexpensive_JJ ._.
Despite_IN the_DT recognized_VBN importance_NN of_IN sampling_NN i_FW
he_PRP combined_VBD cost_NN of_IN training_NN -LRB-_-LRB- building_VBG the_DT model_NN -RRB-_-RRB- and_CC operating_NN -LRB-_-LRB- using_VBG the_DT model_NN -RRB-_-RRB- as_IN a_DT function_NN of_IN training_NN set_NN size_NN ._.
We_PRP can_MD then_RB optimize_VB the_DT size_NN of_IN the_DT training_NN set_VBN to_TO minimize_VB this_DT combined_JJ cost_NN -LRB-_-LRB- =_JJ -_: =_JJ Provost_NNP et_FW al._FW ,_, 1999_CD -_: =--RRB-_NN ._.
Alternatively_RB ,_, an_DT adaptive_JJ learning_NN system_NN ,_, given_VBN -LRB-_-LRB- 1_LS -RRB-_-RRB- the_DT expected_VBN number_NN of_IN classifications_NNS that_IN the_DT learned_VBN model_NN will_MD make_VB when_WRB embedded_VBN in_IN the_DT operational_JJ system_NN ,_, -LRB-_-LRB- 2_LS -RRB-_-RRB- the_DT cost_NN of_IN misclassificat_NN
geometric_JJ sampling_NN scheme_NN is_VBZ motivated_VBN by_IN previous_JJ work_NN on_IN progressive_JJ sampling_NN ,_, which_WDT shows_VBZ that_IN given_VBN certain_JJ assumptions_NNS -LRB-_-LRB- which_WDT do_VBP not_RB hold_VB in_IN this_DT paper_NN -RRB-_-RRB- ,_, this_DT schedule_NN is_VBZ asymptotically_RB optimal_JJ =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT previous_JJ work_NN on_IN progressive_JJ sampling_NN is_VBZ described_VBN in_IN Section_NN 6_CD ._.
For_IN sampling_NN schedules_NNS S1_NN and_CC S2_NN ,_, in_IN addition_NN to_TO evaluating_VBG the_DT training_NN set_NN sizes_NNS just_RB described_VBN ,_, the_DT largest_JJS possible_JJ training_NN
3_CD and_CC 4_CD ,_, the_DT batch_NN size_NN -LRB-_-LRB- i.e._FW the_DT number_NN of_IN examples_NNS labeled_VBN per_IN iteration_NN -RRB-_-RRB- was_VBD fixed_VBN for_IN each_DT dataset_NN ._.
This_DT is_VBZ typical_JJ of_IN many_JJ active_JJ learning_NN studies_NNS -LRB-_-LRB- e.g._FW -LRB-_-LRB- 1_CD ,_, 25_CD ,_, 30_CD ,_, 47_CD ,_, 49_CD ,_, 56_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Provost_NNP et_FW al._FW =_SYM -_: =[_NN 41_CD -RRB-_-RRB- -_: =_SYM -_: examine_VB methods_NNS for_IN progressive_JJ sampling_NN ,_, in_IN which_WDT successively_RB larger_JJR samples_NNS are_VBP drawn_VBN ._.
This_DT research_NN does_VBZ not_RB involve_VB active_JJ learning_NN ,_, but_CC is_VBZ instead_RB concerned_VBN with_IN learning_VBG from_IN a_DT very_RB large_JJ -LRB-_-LRB- lab_NN
ressive_JJ sampling_NN ,_, henceforth_NN referred_VBD to_TO as_IN wps_NNS -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
We_PRP briefly_RB describe_VBP wps_NNS here_RB ._.
Wps_NNP is_VBZ a_DT heuristic_NN ,_, non-exhaustive_JJ variant_NN of_IN exhaustive_JJ wrapping_NN ._.
It_PRP borrows_VBZ a_DT heuristic_NN from_IN progressive_JJ sampling_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT goal_NN of_IN progressive_JJ sampling_NN is_VBZ to_TO iteratively_RB seek_VB a_DT data_NN set_NN size_NN at_IN which_WDT generalization_NN performance_NN on_IN test_NN material_NN converges_VBZ ._.
More_RBR generally_RB ,_, wps_NN borrows_VBZ from_IN the_DT idea_NN that_IN aspects_NNS of_IN lear_NN
the_DT hope_NN of_IN capturing_VBG the_DT point_NN where_WRB most_JJS of_IN the_DT concept_NN is_VBZ learned_VBN ._.
Usually_RB ,_, the_DT rate_NN of_IN of_IN improvement_NN at_IN the_DT final_JJ stages_NNS of_IN learning_NN ,_, before_IN the_DT concept_NN is_VBZ fully_RB learned_VBN ,_, is_VBZ very_RB slow_JJ -LRB-_-LRB- see_VB eg._FW ,_, =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =--RRB-_NN with_IN several_JJ thousands_NNS of_IN instances_NNS contributing_VBG to_TO a_DT tiny_JJ improvement_NN in_IN performance_NN ,_, unnecessarily_RB inflating_VBG the_DT complexity_NN score_NN -LRB-_-LRB- See_NNP Figure_NNP 2_CD -LRB-_-LRB- a_DT -RRB-_-RRB- -RRB-_-RRB- ._.
Using_VBG a_DT log_NN scale_NN for_IN ni_NNS makes_VBZ the_DT scale_NN like_IN t_NN
ed_VBN sampling_NN or_CC cluster_NN sampling_NN -LRB-_-LRB- see_VB -LRB-_-LRB- 3_CD -RRB-_-RRB- for_IN details_NNS of_IN these_DT sampling_NN techniques_NNS -RRB-_-RRB- ._.
There_EX have_VBP also_RB been_VBN some_DT data_NNS reduction_NN methods_NNS that_WDT incorporate_VBP random_JJ sampling_NN with_IN adaptive_JJ procedures_NNS -LRB-_-LRB- 8_CD -RRB-_-RRB- -LRB-_-LRB- 10_CD -RRB-_-RRB- =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 18_CD -RRB-_-RRB- ;_: but_CC these_DT methods_NNS were_VBD intended_VBN for_IN use_NN in_IN conjunction_NN with_IN a_DT specific_JJ data_NN mining_NN technique_NN such_JJ as_IN decision_NN trees_NNS or_CC association_NN rule_NN algorithms_NNS ._.
There_EX is_VBZ a_DT weakness_NN in_IN almost_RB all_DT of_IN the_DT exis_NN
presents_VBZ an_DT improvement_NN in_IN relation_NN to_TO previous_JJ research_NN ._.
For_IN example_NN ,_, the_DT C4_NN :5_CD algorithm_NN applied_VBN by_IN Soinov_NNP et_FW al._FW -LRB-_-LRB- 10_CD -RRB-_-RRB- has_VBZ a_DT complexity_NN of_IN Oðn2_NN logðnÞÞ_NN for_IN problems_NNS with_IN continuous-valued_JJ attributes_NNS =_JJ -_: =[_NN 32_CD -RRB-_-RRB- -_: =_SYM -_: ._.
4_CD RESULTS_NNS AND_CC DISCUSSION_NN The_DT predictive_JJ performance_NN of_IN GRNCOP_NN was_VBD tested_VBN using_VBG the_DT microarray_NN data_NNS in_IN -LRB-_-LRB- 22_CD -RRB-_-RRB- ,_, which_WDT also_RB includes_VBZ data_NNS from_IN S._FW cerevisiae_FW cell_NN cultures_NNS -LRB-_-LRB- 27_CD -RRB-_-RRB- ._.
These_DT data_NNS were_VBD synchroniz_NN
