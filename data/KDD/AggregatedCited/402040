Magical_JJ thinking_NN in_IN data_NNS mining_NN :_: lessons_NNS from_IN CoIL_NN challenge_NN 2000_CD
CoIL_NN challenge_NN 2000_CD was_VBD a_DT supervised_JJ learning_NN contest_NN that_WDT attracted_VBD 43_CD entries_NNS ._.
The_DT authors_NNS of_IN 29_CD entries_NNS later_RB wrote_VBD explanations_NNS of_IN their_PRP$ work_NN ._.
This_DT paper_NN discusses_VBZ these_DT reports_NNS and_CC reaches_VBZ three_CD main_JJ conclusions_NNS ._.
First_JJ ,_, naive_JJ Bayesian_JJ classifiers_NNS remain_VBP competitive_JJ in_IN practice_NN :_: they_PRP were_VBD used_VBN by_IN both_CC the_DT winning_JJ entry_NN and_CC the_DT next_JJ best_JJS entry_NN ._.
Second_RB ,_, identifying_VBG feature_NN interactions_NNS correctly_RB is_VBZ important_JJ for_IN maximizing_VBG predictive_JJ accuracy_NN :_: this_DT was_VBD the_DT difference_NN between_IN the_DT winning_JJ classifier_NN and_CC all_DT others_NNS ._.
Third_JJ and_CC most_RBS important_JJ ,_, too_RB many_JJ researchers_NNS and_CC practitioners_NNS in_IN data_NNS mining_NN do_VBP not_RB appreciate_VB properly_RB the_DT issue_NN of_IN statistical_JJ significance_NN and_CC the_DT danger_NN of_IN overfitting_NN ._.
Given_VBN a_DT dataset_NN such_JJ as_IN the_DT one_NN for_IN the_DT CoIL_NN contest_NN ,_, it_PRP is_VBZ pointless_JJ to_TO apply_VB a_DT very_RB complicated_JJ learning_NN algorithm_NN ,_, or_CC to_TO perform_VB a_DT very_RB time-consuming_JJ model_NN search_NN ._.
In_IN either_DT ease_NN ,_, one_CD is_VBZ likely_JJ to_TO overfit_VB the_DT training_NN data_NNS and_CC to_TO fool_VB oneself_NN in_IN estimating_VBG predictive_JJ accuracy_NN and_CC in_IN discovering_VBG useful_JJ correlations_NNS ._.
t_NN receive_VBP the_DT test_NN set_NN targets_NNS nor_CC were_VBD they_PRP informed_VBD of_IN the_DT CoIL_NNP Challenge_NNP or_CC any_DT of_IN the_DT results_NNS ._.
In_IN contrast_NN ,_, the_DT second_JJ group_NN of_IN students_NNS read_VBP a_DT paper_NN written_VBN by_IN the_DT winner_NN of_IN the_DT prediction_NN task_NN -LRB-_-LRB- =_JJ -_: =_JJ Elkan_NNP ,_, 2001_CD -_: =--RRB-_NN ._.
Both_DT groups_NNS compete_VBP very_RB well_RB with_IN thes182_FW P._FW VAN_NNP DER_NNP PUTTEN_NNP AND_NNP M._NNP VAN_NNP SOMEREN_NNP Figure_NNP 1_CD ._.
Histogram_NN of_IN prediction_NN task_NN performance_NN for_IN CoIL_NNP Challenge_NNP participants_NNS and_CC two_CD reference_NN groups_NNS of_IN studen_NN
makes_VBZ the_DT naive_JJ assumption_NN more_RBR realistic_JJ ._.
In_IN fact_NN ,_, NBC_NNP can_MD be_VB even_RB very_RB competitive_JJ ,_, when_WRB trained_VBN with_IN a_DT carefully_RB designed_VBN feature_NN set_NN ._.
For_IN instance_NN ,_, the_DT CoIL_NN challenge_NN -LRB-_-LRB- 74_CD -RRB-_-RRB- was_VBD won_VBN by_IN a_DT NBC_NNP entry_NN =_JJ -_: =[_NN 41_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT outperformed_VBD more_RBR complicated_JJ models_NNS such_JJ as_IN SVMs_NNS or_CC neural_JJ networks_NNS ._.
The_DT data_NNS set_VBN of_IN the_DT competition_NN was_VBD characterized_VBN by_IN several_JJ correlated_JJ features_NNS and_CC noisy_JJ data_NNS ;_: a_DT later_JJ analysis_NN of_IN the_DT c_NN
chers_NNS fail_VBP to_TO create_VB good_JJ data_NNS mining_NN models_NNS because_IN they_PRP do_VBP not_RB place_VB enough_JJ importance_NN on_IN statistical_JJ significance_NN ,_, and_CC therefore_RB overfit_VBP the_DT model_NN to_TO the_DT specific_JJ dataset_NN that_IN they_PRP have_VBP available_JJ -LRB-_-LRB- =_JJ -_: =_JJ Elkan_NNP 2001_CD -_: =--RRB-_NN ._.
Existing_VBG methods_NNS guard_NN against_IN this_DT by_IN measuring_VBG how_WRB well_RB the_DT model_NN created_VBN on_IN part_NN of_IN the_DT data_NNS -LRB-_-LRB- the_DT training_NN set_NN -RRB-_-RRB- fits_VBZ a_DT second_JJ set_NN -LRB-_-LRB- validation_NN and\/or_CC testing_NN sets_NNS -RRB-_-RRB- ._.
Splitting_VBG the_DT data_NNS into_IN multip_NN
no_DT feature_NN noise_NN -LRB-_-LRB- 26_CD -RRB-_-RRB- ._.
Similarly_RB ,_, NBayes_NN ,_, which_WDT assumes_VBZ independence_NN among_IN all_DT data_NNS features_NNS ,_, is_VBZ suitable_JJ because_IN it_PRP has_VBZ been_VBN shown_VBN to_TO outperform_VB other_JJ much_RB more_RBR complicated_JJ machine_NN learning_NN methods_NNS =_JJ -_: =[_NN 27_CD -RRB-_-RRB- -_: =_SYM -_: ._.
SVM_NN is_VBZ chosen_VBN as_IN a_DT baseline_NN due_JJ to_TO its_PRP$ generalization_NN ability_NN and_CC popularity_NN in_IN medical_JJ classification_NN research_NN -LRB-_-LRB- 28_CD ,_, 29_CD -RRB-_-RRB- ._.
The_DT effectiveness_NN of_IN considering_VBG feature_NN dependencies_NNS under_IN BN-NC_NN would_MD be_VB w_NN
hat_NN we_PRP may_MD find_VB patterns_NNS that_WDT do_VBP not_RB exist_VB -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, or_CC greatly_RB overestimate_VB the_DT significance_NN of_IN a_DT pattern_NN because_IN of_IN a_DT failure_NN to_TO understand_VB the_DT role_NN of_IN parameter_NN searching_VBG in_IN the_DT data_NNS mining_NN process_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN ,_, as_IN we_PRP will_MD show_VB ,_, it_PRP can_MD be_VB very_RB difficult_JJ to_TO compare_VB the_DT results_NNS across_IN methods_NNS or_CC even_RB to_TO reproduce_VB the_DT results_NNS of_IN heavily_RB parameterized_VBN algorithms_NNS ._.
Permission_NN to_TO make_VB digital_JJ or_CC hard_JJ
eedings_NNS ._.
This_DT work_NN is_VBZ supported_VBN in_IN part_NN by_IN grants_NNS NSF_FW CAREER_FW IIS-0447773_NN ,_, and_CC NSF_NN DBI-0321756_NN ._.
spattern_NN because_IN of_IN a_DT failure_NN to_TO understand_VB the_DT role_NN of_IN parameter_NN searching_VBG in_IN the_DT data_NNS mining_NN process_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN ,_, as_IN we_PRP will_MD show_VB ,_, it_PRP can_MD be_VB very_RB difficult_JJ to_TO compare_VB the_DT results_NNS across_IN methods_NNS or_CC even_RB to_TO reproduce_VB the_DT results_NNS of_IN heavily_RB parameterized_VBN algorithms_NNS ._.
Data_NN mining_NN algorithm_NN should_MD ideall_VB
tree_NN induction_NN algorithms_NNS ,_, fuzzy_JJ clustering_NN and_CC rule_NN discovery_NN ,_, support_NN vector_NN machines_NNS -LRB-_-LRB- SVMs_NNS -RRB-_-RRB- ,_, logistic_JJ regression_NN ,_, boosting_VBG and_CC bagging_VBG ,_, and_CC more_JJR -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
The_DT best_JJS technique_NN for_IN prediction_NN reported_VBN in_IN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 12_CD -RRB-_-RRB- is_VBZ the_DT Naive_JJ Bayesian_JJ learning_NN ,_, provided_VBN 800_CD predictions_NNS made_VBN ,_, which_WDT gives_VBZ a_DT hit_NN rate_NN about_IN 15.2_CD %_NN ._.
Predictors_NNS based_VBN on_IN the_DT backpropagation_NN MLP_NN networks_NNS show_VBP accuracy_NN rate_NN about_IN 71_CD %_NN and_CC hit_NN r_NN
he_PRP least_JJS common_JJ ._.
entries_NNS were_VBD then_RB chosen_VBN from_IN the_DT interval_NN -LRB-_-LRB- 0_CD ,_, 1000_CD -RRB-_-RRB- ,_, which_WDT often_RB leads_VBZ to_TO cost_NN matrices_NNS in_IN which_WDT the_DT correct_JJ label_NN is_VBZ not_RB the_DT least_JJS costly_JJ one_CD ._.
Besides_IN being_VBG unreasonable_JJ -LRB-_-LRB- see_VB Elkan_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, these_DT cost_NN matrices_NNS can_MD give_VB an_DT unfair_JJ advantage_NN to_TO cost-sensitive_JJ methods_NNS over_IN cost-insensitive_JJ ones_NNS ._.
We_PRP therefore_RB set_VBD the_DT diagonal_JJ entries_NNS to_TO be_VB identically_RB zero_CD ,_, which_WDT is_VBZ consistent_JJ with_IN our_PRP$ nor_CC
d_NN to_TO the_DT processing_NN of_IN a_DT -LRB-_-LRB- typically_RB large_JJ -RRB-_-RRB- data_NNS set_VBN with_IN a_DT view_NN to_TO summarising_VBG the_DT data_NNS into_IN a_DT more_RBR usable_JJ form_NN and\/or_CC gaining_VBG insights_NNS concerning_VBG patterns_NNS in_IN the_DT data_NNS that_WDT are_VBP statistically_RB reliable_JJ =_JJ -_: =[_NN 52_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Data_NN mining_NN comprises_VBZ a_DT variety_NN of_IN tools_NNS ,_, including_VBG those_DT drawn_VBN from_IN statistics_NNS ,_, machine_NN learning_NN and_CC natural_JJ computing_NN ._.
Such_JJ approaches_NNS ,_, could_MD ,_, therefore_RB ,_, be_VB be_VB distributed_VBN amongst_IN those_DT sections_NNS c_NN
d_NN to_TO the_DT processing_NN of_IN a_DT -LRB-_-LRB- typically_RB large_JJ -RRB-_-RRB- data_NNS set_VBN with_IN a_DT view_NN to_TO summarising_VBG the_DT data_NNS into_IN a_DT more_RBR usable_JJ form_NN and\/or_CC gaining_VBG insights_NNS concerning_VBG patterns_NNS in_IN the_DT data_NNS that_WDT are_VBP statistically_RB reliable_JJ =_JJ -_: =[_NN 58_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Data_NN mining_NN comprises_VBZ a_DT variety_NN of_IN tools_NNS ,_, including_VBG those_DT drawn_VBN from_IN statistics_NNS ,_, machine_NN learning_NN and_CC natural_JJ computing_NN ._.
Such_JJ approaches_NNS ,_, could_MD ,_, therefore_RB ,_, be_VB be_VB distributed_VBN amongst_IN those_DT sections_NNS c_NN
an_DT appropriate_JJ prior_JJ -LRB-_-LRB- any_DT one_NN will_MD do_VB in_IN the_DT limit_NN -RRB-_-RRB- --_: this_DT turns_VBZ out_RP to_TO be_VB somewhat_RB problematic_JJ in_IN practice_NN when_WRB dealing_VBG with_IN continuous_JJ variables_NNS ,_, though_IN not_RB insurmountable_JJ -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
3sperformance_NN ._.
Elkan_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: ,_, for_IN example_NN ,_, describes_VBZ the_DT winning_JJ entry_NN in_IN a_DT data_NN mining_NN contest_NN based_VBN on_IN naive_JJ Bayesian_JJ learning_NN ,_, modified_VBN -LRB-_-LRB- manually_RB by_IN the_DT author_NN -RRB-_-RRB- to_TO incorporate_VB a_DT few_JJ key_JJ dependencies_NNS between_IN variables_NNS and_CC to_TO d_NN
ecause_NN of_IN the_DT incomplete_JJ of_IN knowledge_NN ,_, it_PRP is_VBZ a_DT PAC_NN learning_NN process_NN ._.
However_RB ,_, in_IN some_DT real-problems_NNS ,_, we_PRP are_VBP not_RB sure_JJ whether_IN the_DT learning_NN rules_NNS exist_VBP or_CC not_RB ._.
So_RB ,_, we_PRP have_VBP to_TO avoid_VB the_DT magical_JJ thinking_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: that_IN ,_, there_EX is_VBZ a_DT perfect_JJ rule_NN which_WDT can_MD cover_VB all_DT positive_JJ examples_NNS ._.
b_NN -RRB-_-RRB- ._.
Another_DT important_JJ problem_NN involved_VBN in_IN real-word_JJ data_NNS mining_NN ,_, that_DT is_VBZ ,_, whether_IN the_DT database_NN contains_VBZ the_DT useful_JJ or_CC appropriate_JJ
l_NN their_PRP$ entries_NNS satisfy_VBP a_DT domain_NN expert_NN ._.
Such_JJ challenges_NNS can_MD be_VB a_DT very_RB useful_JJ source_NN of_IN feedback_NN to_TO the_DT research_NN community_NN ,_, provided_VBN that_IN thorough_JJ analysis_NN of_IN results_NNS has_VBZ been_VBN performed_VBN -LRB-_-LRB- for_IN example_NN ,_, =_JJ -_: =_JJ Elkan_NNP ,_, 2001_CD -_: =_JJ -_: ,_, describes_VBZ lessons_NNS from_IN the_DT CoIL_NN Challenge_NNP 2000_CD -RRB-_-RRB- ._.
s6_NNP N._NNP LAVRAÄŒ_NNP ,_, H._NNP MOTODA_NNP ,_, AND_NNP T._NNP FAWCETT_NNP With_IN this_DT background_NN in_IN mind_NN ,_, a_DT workshop_NN on_IN Data_NNP Mining_NNP Lessons_NNP Learned_NNP -LRB-_-LRB- DMLL2002_NN -RRB-_-RRB- was_VBD organized_VBN at_IN the_DT Ninete_NN
winning_VBG entry_NN in_IN a_DT data_NN mining_NN contest_NN based_VBN on_IN naive_JJ Bayesian_JJ learning_NN ,_, modified_VBN -LRB-_-LRB- manually_RB by_IN the_DT author_NN -RRB-_-RRB- to_TO incorporate_VB a_DT few_JJ key_JJ dependencies_NNS between_IN variables_NNS and_CC to_TO discard_VB irrelevant_JJ variables_NNS =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT case_NN ,_, the_DT pre-representational_JJ problem_NN has_VBZ been_VBN solved_VBN directly_RB and_CC completely_RB by_IN the_DT human_JJ encoding_VBG the_DT task_NN ._.
A_DT more_RBR general_JJ approach_NN to_TO adaptive_JJ optimization_NN ,_, given_VBN a_DT solution_NN space_NN define_VB
