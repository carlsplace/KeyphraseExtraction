MARK_NNP :_: a_DT boosting_VBG algorithm_NN for_IN heterogeneous_JJ kernel_NN models_NNS
Support_NN Vector_NNP Machines_NNP and_CC other_JJ kernel_NN methods_NNS have_VBP proven_VBN to_TO be_VB very_RB effective_JJ for_IN nonlinear_JJ inference_NN ._.
Practical_NNP issues_NNS are_VBP how_WRB to_TO select_VB the_DT type_NN of_IN kernel_NN including_VBG any_DT parameters_NNS and_CC how_WRB to_TO deal_VB with_IN the_DT computational_JJ issues_NNS caused_VBN by_IN the_DT fact_NN that_IN the_DT kernel_NN matrix_NN grows_VBZ quadratically_RB with_IN the_DT data_NNS ._.
Inspired_VBN by_IN ensemble_NN and_CC boosting_VBG methods_NNS like_IN MART_NNP ,_, we_PRP propose_VBP the_DT Multiple_JJ Additive_JJ Regression_NN Kernels_NNS -LRB-_-LRB- MARK_NNP -RRB-_-RRB- algorithm_NN to_TO address_VB these_DT issues_NNS ._.
MARK_NNP considers_VBZ a_DT large_JJ -LRB-_-LRB- potentially_RB infinite_JJ -RRB-_-RRB- library_NN of_IN kernel_NN matrices_NNS formed_VBN by_IN different_JJ kernel_NN functions_NNS and_CC parameters_NNS ._.
Using_VBG gradient_NN boosting\/column_NN generation_NN ,_, MARK_NNP constructs_NNS columns_NNS of_IN the_DT heterogeneous_JJ kernel_NN matrix_NN -LRB-_-LRB- the_DT base_NN hypotheses_NNS -RRB-_-RRB- on_IN the_DT fly_NN and_CC then_RB adds_VBZ them_PRP into_IN the_DT kernel_NN ensemble_NN ._.
Regularization_NN methods_NNS such_JJ as_IN used_VBN in_IN SVM_NN ,_, kernel_NN ridge_NN regression_NN ,_, and_CC MART_NNP ,_, are_VBP used_VBN to_TO prevent_VB overfitting_NN ._.
We_PRP investigate_VBP how_WRB MARK_NNP is_VBZ applied_VBN to_TO heterogeneous_JJ kernel_NN ridge_NN regression_NN ._.
The_DT resulting_VBG algorithm_NN is_VBZ simple_JJ to_TO implement_VB and_CC efficient_JJ ._.
Kernel_NNP parameter_NN selection_NN is_VBZ handled_VBN within_IN MARK_NNP ._.
Sampling_NN and_CC ``_`` weak_JJ ''_'' kernels_NNS are_VBP used_VBN to_TO further_RB enhance_VB the_DT computational_JJ efficiency_NN of_IN the_DT resulting_VBG additive_JJ algorithm_NN ._.
The_DT user_NN can_MD incorporate_VB and_CC potentially_RB extract_NN domain_NN knowledge_NN by_IN restricting_VBG the_DT kernel_NN library_NN to_TO interpretable_JJ kernels_NNS ._.
MARK_NNP compares_VBZ very_RB favorably_RB with_IN SVM_NN and_CC kernel_NN ridge_NN regression_NN on_IN several_JJ benchmark_JJ datasets_NNS ._.
-LRB-_-LRB- 32_CD ,_, 85_CD -RRB-_-RRB- studied_VBD various_JJ non-linear_JJ weighting_NN schemes_NNS ,_, such_JJ as_IN using_VBG absolute_JJ values_NNS ,_, squared_VBD quantity_NN and_CC MaxMin_NNP ,_, to_TO combine_VB multiple_JJ kernels_NNS ._.
A_DT boosting_VBG algorithm_NN was_VBD proposed_VBN by_IN Bennett_NNP et_FW al._FW 136_CD =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: to_TO construct_VB a_DT heterogeneous_JJ kernel_NN from_IN a_DT large_JJ library_NN of_IN kernel_NN matrices_NNS ._.
Cristianini_NNP et_FW al._FW -LRB-_-LRB- 30_CD -RRB-_-RRB- developed_VBD a_DT kernel_NN alignment_NN theory_NN to_TO linearly_RB combine_VB multiple_JJ kernels_NNS to_TO best_RB align_VB with_IN a_DT pre_JJ -_:
ted_VBN with_IN other_JJ kernel-based_JJ methods_NNS ._.
5_CD ._.
Related_JJ Work_NN Many_JJ researchers_NNS have_VBP studied_VBN optimal_JJ kernel_NN selection_NN in_IN kernel-based_JJ classification_NN methods_NNS ,_, which_WDT is_VBZ called_VBN kernel_NN learning_NN -LRB-_-LRB- Bach_NNP et_FW al._FW ,_, 2004_CD ;_: =_JJ -_: =_JJ Bennett_NNP et_FW al._FW ,_, 2002_CD -_: =_JJ -_: ;_: Bi_FW et_FW al._FW ,_, 2004_CD ;_: Bousquet_NNP &_CC Herrmann_NNP ,_, 2003_CD ;_: Cristianini_NNP et_FW al._FW ,_, 2001_CD ;_: Crammer_NNP et_FW al._FW ,_, 2003_CD ;_: Fung_NNP et_FW al._FW ,_, 2004_CD ;_: Lanckriet_NNP et_FW al._FW ,_, 2004b_CD ;_: Lanckriet_NNP et_FW al._FW ,_, 2004a_CD ;_: Ong_NNP et_FW al._FW ,_, 2005_CD ;_: Xiong_NNP et_FW al._FW ,_, 2005_CD -RRB-_-RRB-
he_PRP mixture-of-kernel_JJ approach_NN is_VBZ f_LS -LRB-_-LRB- x_NN -RRB-_-RRB- =_JJ p_NN j_FW α_FW p_NN j_NN Kp_NN -LRB-_-LRB- x_NN ,_, xj_NN -RRB-_-RRB- ._.
-LRB-_-LRB- 3_LS -RRB-_-RRB- 2_CD where_WRB α_NN p_NN j_NN gives_VBZ the_DT weight_NN for_IN the_DT j-th_NN column_NN in_IN the_DT pth_NN kernel_NN matrix_NN ._.
Similar_JJ strategies_NNS have_VBP appeared_VBN in_IN some_DT works_NNS such_JJ as_IN =_JJ -_: =[_NN 3_CD ,_, 19_CD -RRB-_-RRB- -_: =_SYM -_: to_TO improve_VB generalization_NN and_CC reduce_VB training_NN and_CC prediction_NN computational_JJ costs_NNS ._.
MARK_NNP -LRB-_-LRB- 3_CD -RRB-_-RRB- optimized_VBD a_DT heterogeneous_JJ kernel_NN using_VBG a_DT gradient_NN descent_NN algorithm_NN in_IN function_NN space_NN ._.
GSVC_NN and_CC POKER_NN -LRB-_-LRB- 19_CD ,_, 1_CD
robabilities_NNS -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, cross-validation_NN error_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, and_CC class_NN separability_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
Instead_RB of_IN adapting_VBG only_RB the_DT kernel_NN parameters_NNS ,_, recent_JJ developments_NNS also_RB adapt_VBP the_DT form_NN of_IN the_DT kernel_NN itself_PRP -LRB-_-LRB- 1_LS -RRB-_-RRB- ,_, -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: --_: -LRB-_-LRB- 16_CD -RRB-_-RRB- ._.
In_IN a_DT transductive_JJ setting_NN ,_, as_IN all_DT information_NN on_IN the_DT feature_NN space_NN is_VBZ encoded_VBN in_IN the_DT kernel_NN matrix_NN -LRB-_-LRB- with_IN entries_NNS for_IN both_CC the_DT training_NN and_CC test_NN patterns_NNS -RRB-_-RRB- ,_, one_PRP can_MD bypass_VB the_DT learning_NN of_IN kernel_NN
that_IN the_DT use_NN of_IN a_DT block_NN diagonal_JJ kernel_NN matrix_NN is_VBZ not_RB an_DT option_NN in_IN the_DT current_JJ 2_CD Several_JJ studies_NNS focused_VBD on_IN finding_VBG sparse_JJ solutions_NNS of_IN Equation_NN 5.8_CD or_CC optimization_NN problems_NNS similar_JJ to_TO Equation_NN 5.8_CD -LRB-_-LRB- =_JJ -_: =_JJ Bennett_NNP et_FW al._FW ,_, 2002_CD -_: =_JJ -_: ,_, Girosi_NNP ,_, 1998_CD ,_, Smola_NNP and_CC Schölkopf_NNP ,_, 2000_CD ,_, Zhu_NNP and_CC Hastie_NNP ,_, 2001_CD -RRB-_-RRB- ._.
65ssetting_NN ,_, since_IN it_PRP would_MD prohibit_VB generalizing_VBG across_IN label_NN sequences_NNS that_WDT differ_VBP in_IN as_RB little_JJ as_IN a_DT single_JJ micro-label_NN ._.
The_DT kernel_NN f_SYM
his_PRP$ approach_NN is_VBZ f_LS -LRB-_-LRB- x_NN -RRB-_-RRB- =_JJ Kp_NN -LRB-_-LRB- x_NN ,_, xj_NN -RRB-_-RRB- ._.
-LRB-_-LRB- 3_LS -RRB-_-RRB- p_NN p_NN j_NN αp_FW j_FW Previous_JJ kernel_NN methods_NNS have_VBP employed_VBN similar_JJ strategies_NNS to_TO improve_VB generalization_NN and_CC reduce_VB training_NN and_CC prediction_NN computational_JJ costs_NNS ._.
MARKING_VBG =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: optimized_VBD a_DT heterogeneous_JJ kernel_NN using_VBG a_DT gradient_NN descent_NN algorithm_NN in_IN function_NN space_NN ._.
GSVC_NN and_CC POKER_NN -LRB-_-LRB- 14_CD ,_, 13_CD -RRB-_-RRB- grew_VBD mixtures_NNS of_IN kernelsfunctions_NNS from_IN a_DT family_NN of_IN RBF_NN kernels_NNS ,_, but_CC they_PRP were_VBD designed_VBN t_NN
n_NN a_DT semi-supervised_JJ setting_NN -LRB-_-LRB- see_VB -LRB-_-LRB- Argyriou_NNP et_FW al._FW ,_, 2006b_CD ,_, Dai_NNP and_CC Yeung_NNP ,_, 2007_CD ,_, Sindhwani_NNP et_FW al._FW ,_, 2005_CD ,_, Tsuda_NNP et_FW al._FW ,_, 2005_CD ,_, Zhu_NNP et_FW al._FW ,_, 2005_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Several_JJ recently_RB proposed_VBN approaches_NNS -LRB-_-LRB- Bach_NNP et_FW al._FW ,_, 2004_CD ,_, =_JJ -_: =_JJ Bennett_NNP et_FW al._FW ,_, 2002_CD -_: =_JJ -_: ,_, Bi_FW et_FW al._FW ,_, 2004_CD ,_, Crammer_NNP et_FW al._FW ,_, 2003_CD ,_, Girolami_NNP and_CC Rogers_NNP ,_, 2005_CD ,_, Girolami_NNP and_CC Zhong_NNP ,_, 2007_CD ,_, Kondor_NNP and_CC Jebara_NNP ,_, 2007_CD ,_, Lanckriet_NNP et_FW al._FW ,_, 2004_CD ,_, Lin_NNP and_CC Zhang_NNP ,_, 2003_CD ,_, Micchelli_NNP and_CC Pontil_NNP ,_, 2005_CD ,_, Ong_NNP et_NNP a_DT
nel_NN decomposition_NN presented_VBN in_IN Section_NN 2_CD to_TO other_JJ methods_NNS that_WDT use_VBP the_DT same_JJ kernel_NN and\/or_CC decomposition_NN :_: -LRB-_-LRB- a_LS -RRB-_-RRB- the_DT greedy_JJ strategy_NN of_IN selecting_VBG basis_NN kernels_NNS one_CD after_IN the_DT other_JJ ,_, a_DT procedure_NN similar_JJ to_TO =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC -LRB-_-LRB- b_LS -RRB-_-RRB- the_DT regular_JJ polynomial_JJ kernel_NN regularization_NN with_IN the_DT full_JJ kernel_NN -LRB-_-LRB- i.e._FW ,_, the_DT sum_NN of_IN all_DT basis_NN kernels_NNS -RRB-_-RRB- ._.
In_IN Figure_NNP 2_CD ,_, we_PRP compare_VBP the_DT two_CD approaches_NNS on_IN 40_CD replications_NNS in_IN the_DT following_JJ two_CD sit_VBP
