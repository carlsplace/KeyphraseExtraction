Knowledge_NN transfer_NN via_IN multiple_JJ model_NN local_JJ structure_NN mapping_NN
The_DT effectiveness_NN of_IN knowledge_NN transfer_NN using_VBG classification_NN algorithms_NNS depends_VBZ on_IN the_DT difference_NN between_IN the_DT distribution_NN that_WDT generates_VBZ the_DT training_NN examples_NNS and_CC the_DT one_NN from_IN which_WDT test_NN examples_NNS are_VBP to_TO be_VB drawn_VBN ._.
The_DT task_NN can_MD be_VB especially_RB difficult_JJ when_WRB the_DT training_NN examples_NNS are_VBP from_IN one_CD or_CC several_JJ domains_NNS different_JJ from_IN the_DT test_NN domain_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT locally_RB weighted_JJ ensemble_NN framework_NN to_TO combine_VB multiple_JJ models_NNS for_IN transfer_NN learning_NN ,_, where_WRB the_DT weights_NNS are_VBP dynamically_RB assigned_VBN according_VBG to_TO a_DT model_NN 's_POS predictive_JJ power_NN on_IN each_DT test_NN example_NN ._.
It_PRP can_MD integrate_VB the_DT advantages_NNS of_IN various_JJ learning_VBG algorithms_NNS and_CC the_DT labeled_JJ information_NN from_IN multiple_JJ training_NN domains_NNS into_IN one_CD unified_JJ classification_NN model_NN ,_, which_WDT can_MD then_RB be_VB applied_VBN on_IN a_DT different_JJ domain_NN ._.
Importantly_RB ,_, different_JJ from_IN many_JJ previously_RB proposed_VBN methods_NNS ,_, none_NN of_IN the_DT base_NN learning_NN method_NN is_VBZ required_VBN to_TO be_VB specifically_RB designed_VBN for_IN transfer_NN learning_NN ._.
We_PRP show_VBP the_DT optimality_NN of_IN a_DT locally_RB weighted_JJ ensemble_NN framework_NN as_IN a_DT general_JJ approach_NN to_TO combine_VB multiple_JJ models_NNS for_IN domain_NN transfer_NN ._.
We_PRP then_RB propose_VBP an_DT implementation_NN of_IN the_DT local_JJ weight_NN assignments_NNS by_IN mapping_VBG the_DT structures_NNS of_IN a_DT model_NN onto_IN the_DT structures_NNS of_IN the_DT test_NN domain_NN ,_, and_CC then_RB weighting_NN each_DT model_NN locally_RB according_VBG to_TO its_PRP$ consistency_NN with_IN the_DT neighborhood_NN structure_NN around_IN the_DT test_NN example_NN ._.
Experimental_JJ results_NNS on_IN text_NN classification_NN ,_, spam_NN filtering_VBG and_CC intrusion_NN detection_NN data_NNS sets_NNS demonstrate_VBP significant_JJ improvements_NNS in_IN classification_NN accuracy_NN gained_VBN by_IN the_DT framework_NN ._.
On_IN a_DT transfer_NN learning_VBG task_NN of_IN newsgroup_NN message_NN categorization_NN ,_, the_DT proposed_VBN locally_RB weighted_JJ ensemble_NN framework_NN achieves_VBZ 97_CD %_NN accuracy_NN when_WRB the_DT best_JJS single_JJ model_NN predicts_VBZ correctly_RB only_RB on_IN 73_CD %_NN of_IN the_DT test_NN examples_NNS ._.
In_IN summary_NN ,_, the_DT improvement_NN in_IN accuracy_NN is_VBZ over_IN 10_CD %_NN and_CC up_IN to_TO 30_CD %_NN across_IN different_JJ problems_NNS ._.
s_VBZ the_DT boosting_VBG weight_NN formula_NN as_IN the_DT re-weighting_JJ scheme_NN ._.
Some_DT other_JJ methods_NNS base_VBP on_IN dimension_NN reduction_NN ,_, which_WDT usually_RB map_VBP data_NNS to_TO a_DT new_JJ representation_NN facilitating_VBG domain_NN transfer_NN -LRB-_-LRB- -LRB-_-LRB- 10_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Recently_RB ,_, =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: proposes_VBZ a_DT locally_RB weighted_JJ ensemble_NN framework_NN to_TO combine_VB multiple_JJ models_NNS for_IN transfer_NN learning_NN by_IN dynamically_RB assigning_VBG weights_NNS of_IN a_DT model_NN according_VBG to_TO a_DT model_NN 's_POS predictive_JJ power_NN on_IN each_DT test_NN exampl_NN
sumes_NNS that_IN conditional_JJ probabilities_NNS rt_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- andrs_NNS -LRB-_-LRB- y_FW |_FW x_LS -RRB-_-RRB- aresimilar_NN in_IN regions_NNS of_IN the_DT latent_JJ space_NN where_WRB marginal_JJ distribution_NN qt_NN -LRB-_-LRB- x_NN -RRB-_-RRB- and_CC qs_NN -LRB-_-LRB- x_NN -RRB-_-RRB- of_IN corresponding_JJ examples_NNS are_VBP close_JJ ._.
Other_JJ works_NNS ,_, such_JJ as_IN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, assumes_VBZ q_NN -LRB-_-LRB- x_NN -RRB-_-RRB- is_VBZ related_JJ to_TO r_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- ._.
They_PRP both_DT implicitly_RB assume_VB that_IN marginal_JJ distribution_NN and_CC conditional_JJ probability_NN are_VBP directly_RB related_JJ ._.
In_IN summary_NN ,_, either_CC of_IN the_DT following_NN is_VBZ assumed_VBN to_TO be_VB true_JJ
s_NN to_TO raw_JJ data_NNS ,_, but_CC instead_RB use_VBP prediction_NN results_NNS from_IN multiple_JJ models_NNS as_IN input_NN ._.
Some_DT other_JJ types_NNS of_IN information_NN combination_NN have_VBP also_RB drawn_VBN researchers_NNS '_POS attention_NN ,_, such_JJ as_IN transfer_NN learning_NN ensemble_NN =_JJ -_: =[_NN 10_CD ,_, 19_CD -RRB-_-RRB- -_: =_JJ -_: ,_, webpages_NNS classification_NN based_VBN on_IN content_NN and_CC link_NN information_NN -LRB-_-LRB- 28_CD -RRB-_-RRB- ,_, label_NN inference_NN from_IN two_CD unlabeled_JJ data_NNS sources_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, and_CC ensemble_NN of_IN relational_JJ classifiers_NNS -LRB-_-LRB- 23_CD -RRB-_-RRB- ._.
However_RB ,_, all_PDT these_DT methods_NNS only_RB
aches_NNS for_IN transfer_NN learning_NN have_VBP been_VBN proposed_VBN ._.
Given_VBN multiple_JJ weak_JJ classifiers_NNS that_WDT are_VBP trained_VBN in_IN different_JJ domains_NNS ,_, these_DT are_VBP combined_VBN with_IN weights_NNS based_VBN on_IN relatedness_NN to_TO the_DT target_NN domains_NNS -LRB-_-LRB- 17_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: ._.
An_DT AdaBoost_NNP algorithm_NN was_VBD modified_VBN so_RB as_IN to_TO be_VB better_JJR fit_NN for_IN transfer_NN learning_NN by_IN less_JJR weighting_NN of_IN useless_JJ source_NN data_NNS -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
While_IN boosting_VBG adjusts_VBZ the_DT weights_NNS of_IN data_NNS or_CC classifiers_NNS adaptively_RB ,_, ba_NN
s_NN to_TO task_NN parameters_NNS ._.
This_DT method_NN not_RB only_RB models_NNS the_DT relationships_NNS between_IN tasks_NNS explicitly_RB ,_, but_CC also_RB gives_VBZ an_DT algorithm_NN for_IN the_DT informed_JJ use_NN of_IN several_JJ source_NN tasks_NNS in_IN transfer_NN learning_NN ._.
Gao_NNP et_FW al._FW =_SYM -_: =[_NN 32_CD -RRB-_-RRB- -_: =_SYM -_: propose_VBP that_IN task_NN similarity_NN can_MD be_VB a_DT local_JJ measure_NN rather_RB than_IN a_DT global_JJ measure_NN ._.
They_PRP estimate_VBP similarities_NNS in_IN the_DT neighborhood_NN of_IN each_DT test_NN example_NN individually_RB ._.
These_DT local_JJ consistency_NN estimates_VBZ b_NN
transfer_NN method_NN of_IN Wu_NNP and_CC Dietterich_NNP -LRB-_-LRB- 8_CD -RRB-_-RRB- ,_, which_WDT uses_VBZ auxiliary_JJ data_NNS in_IN SVMs_NNS to_TO both_DT constrain_VB the_DT learning_NN and_CC identify_VB support_NN vectors_NNS ._.
Ensembles_NNS have_VBP been_VBN used_VBN previously_RB for_IN transfer_NN by_IN Gao_NNP et_FW al._FW =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, who_WP developed_VBD a_DT locally_RB weighted_JJ ensemble_NN that_IN weights_NNS each_DT member_NN classifier_NN differently_RB depending_VBG on_IN the_DT data_NNS region_NN ._.
Like_IN Gao_NNP et_FW al._FW 's_POS method_NN ,_, TransferBoost_NNP can_MD utilize_VB base_NN algorithms_NNS not_RB inhere_VBP
ature_NN ,_, which_WDT makes_VBZ it_PRP more_RBR flexible_JJ compared_VBN with_IN parametric_JJ models_NNS ._.
The_DT proposed_VBN transfer_NN learning_NN framework_NN is_VBZ fundamentally_RB different_JJ from_IN existing_VBG graph-based_JJ methods_NNS ._.
For_IN example_NN ,_, the_DT authors_NNS of_IN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT locally_RB weighted_JJ ensemble_NN framework_NN to_TO combine_VB multiple_JJ models_NNS for_IN transfer_NN learning_NN ,_, where_WRB the_DT weights_NNS of_IN different_JJ models_NNS are_VBP approximated_VBN using_VBG a_DT graph-based_JJ approach_NN ;_: the_DT authors_NNS of_IN -LRB-_-LRB- 1_CD
adaption_NN -RRB-_-RRB- has_VBZ been_VBN a_DT very_RB active_JJ research_NN area_NN in_IN recent_JJ years_NNS ._.
Representative_JJ work_NN includes_VBZ multi-label_JJ text_NN classification_NN -LRB-_-LRB- 30_CD ,_, 13_CD -RRB-_-RRB- ,_, crossdomain_NN sentiment_NN prediction_NN -LRB-_-LRB- 4_CD ,_, 6_CD ,_, 15_CD -RRB-_-RRB- ,_, intrusion_NN detection_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_JJ -_: ,_, verb_NN argument_NN classification_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, cross-lingual_JJ classification_NN -LRB-_-LRB- 25_CD -RRB-_-RRB- ,_, and_CC cross-domain_JJ relation_NN extraction_NN -LRB-_-LRB- 29_CD -RRB-_-RRB- ._.
In_IN all_DT of_IN these_DT scenarios_NNS ,_, the_DT features_NNS are_VBP given_VBN as_IN the_DT input_NN of_IN their_PRP$ algorithms_NNS -LRB-_-LRB- e_SYM
from_IN different_JJ models_NNS Mi_NNP ,_, because_IN a_DT good_JJ Mi_NN with_IN low_JJ test_NN error_NN implies_VBZ that_IN the_DT training_NN and_CC testing_NN data_NNS share_VBP similar_JJ data_NNS distributions_NNS ._.
Thus_RB ,_, we_PRP employ_VBP the_DT Locally_RB Weighted_JJ Ensemble_NN -LRB-_-LRB- LWE_NN -RRB-_-RRB- scheme_NN =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: to_TO dynamically_RB estimate_VB combination_NN weights_NNS for_IN every_DT test_NN example_NN by_IN checking_VBG the_DT consistency_NN of_IN data_NNS structures_NNS between_IN the_DT model_NN predictions_NNS -LRB-_-LRB- near_IN the_DT test_NN example_NN -RRB-_-RRB- and_CC the_DT real_JJ data_NNS distribution_NN
nce_NN to_TO our_PRP$ work_NN are_VBP domain_JJ adaptation_NN techniques_NNS specifically_RB developed_VBD for_IN text_NN and_CC sentiment_NN classification_NN -LRB-_-LRB- e.g._FW ,_, Blitzer_NNP ,_, McDonald_NNP ,_, &_CC Pereira_NNP ,_, 2006_CD ;_: Finn_NNP &_CC Kushmerick_NNP ,_, 2006_CD ;_: Blitzer_NNP et_FW al._FW ,_, 2007_CD ;_: =_JJ -_: =_JJ Gao_NN ,_, Fan_NN ,_, Jiang_NNP ,_, &_CC Han_NNP ,_, 2008_CD -_: =_JJ -_: ;_: Ling_NNP ,_, Dai_NNP ,_, Xue_NNP ,_, Yang_NNP ,_, &_CC Yu_NNP ,_, 2008_CD ;_: Tan_NNP ,_, Cheng_NNP ,_, Wang_NNP ,_, &_CC Xu_NNP ,_, 2009_CD -RRB-_-RRB- ._.
It_PRP is_VBZ worth_JJ noting_VBG that_IN our_PRP$ domain_NN adaptation_NN setting_NN is_VBZ different_JJ from_IN the_DT traditional_JJ setting_NN ._.
Traditionally_RB ,_, sophisticated_JJ classif_NN
other_JJ methods_NNS that_WDT learn_VBP model_NN parameters_NNS to_TO reduce_VB marginal_JJ probability_NN differences_NNS -LRB-_-LRB- 10_CD ,_, 11_CD -RRB-_-RRB- ._.
Similarly_RB ,_, several_JJ algorithms_NNS have_VBP been_VBN developed_VBN in_IN the_DT past_NN to_TO combine_VB knowledge_NN from_IN multiple_JJ sources_NNS =_JJ -_: =[_NN 12_CD ,_, 13_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Most_JJS of_IN these_DT methods_NNS measure_VBP the_DT distribution_NN difference_NN between_IN each_DT source_NN and_CC target_NN domain_NN data_NNS ,_, independently_RB ,_, based_VBN on_IN marginal_JJ or_CC conditional_JJ probability_NN differences_NNS and_CC combine_VB the_DT hypothes_NNS
we_PRP introduce_VBP five_CD traditional_JJ classifiers_NNS ,_, including_VBG Naive_JJ Bayes_NNS -LRB-_-LRB- NB_NN -RRB-_-RRB- ,_, SVM_NN ,_, C4_NN .5_NN ,_, K-NN_NN and_CC NNge_NN -LRB-_-LRB- Ng_NN -RRB-_-RRB- ,_, and_CC three_CD state-of-theart_JJ transfer_NN learning_NN methods_NNS :_: TrAdaBoost_NNP -LRB-_-LRB- TA_NNP -RRB-_-RRB- -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, LatentMap_NN -LRB-_-LRB- LM_NN -RRB-_-RRB- -LRB-_-LRB- 13_CD -RRB-_-RRB- and_CC LWE_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Amongthem_NNP ,_, TrAdaBoostisbasedoninstancesweighting_NNP ,_, LatentMapTable_NNP 2_CD :_: Dataset_NNP for_IN Algorithm_NNP and_CC Parameters_NNP Selection_NN Data_NNP Set_NNP |_NNP S_NNP |_FW |_FW T_NN |_CD Description_NN Red-White_NN -LRB-_-LRB- RW_NN -RRB-_-RRB- 1599_CD 4998_CD physicochemical_JJ White-Red_JJ -LRB-_-LRB- WR_NN -RRB-_-RRB- 49_CD
s_VBZ another_DT widely_RB used_VBN data_NNS set_VBN for_IN evaluating_VBG learning_NN algorithms_NNS ._.
It_PRP contains_VBZ five_CD top_JJ categories_NNS and_CC many_JJ subcategories_NNS ._.
For_IN easy_JJ comparison_NN ,_, we_PRP use_VBP a_DT preprocessed_JJ version_NN adopted_VBN in_IN previous_JJ work_NN -LRB-_-LRB- =_JJ -_: =_JJ Gao_NNP et_FW al._FW 2008_CD -_: =_JJ -_: ;_: Zhuang_NNP et_FW al._FW 2010_CD -RRB-_-RRB- ,_, which_WDT contains_VBZ three_CD cross-domain_JJ data_NNS sets_VBZ orgs_NNS vs_CC people_NNS ,_, orgs_NNS vs_CC place_NN and_CC people_NNS vs_CC place_NN ._.
We_PRP use_VBP Accuracy_NNP as_IN the_DT evaluation_NN criteria_NNS ,_, as_IN it_PRP is_VBZ widely_RB adopted_VBN in_IN the_DT literat_NN
n._NN With_IN the_DT new_JJ feature_NN representation_NN ,_, the_DT performance_NN of_IN the_DT target_NN task_NN is_VBZ expected_VBN to_TO improve_VB significantly_RB ._.
A_DT third_JJ case_NN can_MD be_VB referred_VBN to_TO as_IN parameter-transfer_JJ approach_NN -LRB-_-LRB- 45_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 46_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 47_CD -RRB-_-RRB- ,_, -LRB-_-LRB- 48_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 49_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT assumes_VBZ that_IN the_DT source_NN tasks_NNS and_CC the_DT target_NN tasks_NNS share_VBP some_DT parameters_NNS or_CC prior_JJ distributions_NNS of_IN the_DT hyperparameters_NNS of_IN the_DT models_NNS ._.
The_DT transferred_VBN knowledge_NN is_VBZ encoded_VBN into_IN the_DT shared_JJ param_NN
._.
found_VBN in_IN a_DT subspace_NN -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- -RRB-_-RRB- or_CC a_DT projected_VBN feature_NN space_NN where_WRB the_DT different_JJ tasks_NNS are_VBP similar_JJ to_TO each_DT other_JJ -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 2_CD -RRB-_-RRB- -RRB-_-RRB- ._.
There_EX are_VBP also_RB some_DT other_JJ solutions_NNS like_IN model-combination_NN based_VBN -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, transfer_NN across_IN similar_JJ learning_NN parameters_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 13_CD -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC so_RB on_RB ._.
Different_JJ from_IN these_DT works_NNS ,_, we_PRP mainly_RB study_VBD the_DT problem_NN to_TO transfer_VB knowledge_NN across_IN tasks_NNS having_VBG different_JJ class_NN labels_NNS ._.
One_CD im_NN
