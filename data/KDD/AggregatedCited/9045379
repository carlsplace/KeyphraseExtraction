Structured_VBN metric_JJ learning_NN for_IN high_JJ dimensional_JJ problems_NNS
The_DT success_NN of_IN popular_JJ algorithms_NNS such_JJ as_IN k-means_NN clustering_NN or_CC nearest_JJS neighbor_NN searches_NNS depend_VBP on_IN the_DT assumption_NN that_IN the_DT underlying_VBG distance_NN functions_NNS reflect_VBP domain-specific_JJ notions_NNS of_IN similarity_NN for_IN the_DT problem_NN at_IN hand_NN ._.
The_DT distance_NN metric_JJ learning_NN problem_NN seeks_VBZ to_TO optimize_VB a_DT distance_NN function_NN subject_JJ to_TO constraints_NNS that_WDT arise_VBP from_IN fully-supervised_JJ or_CC semisupervised_JJ information_NN ._.
Several_JJ recent_JJ algorithms_NNS have_VBP been_VBN proposed_VBN to_TO learn_VB such_JJ distance_NN functions_NNS in_IN low_JJ dimensional_JJ settings_NNS ._.
One_CD major_JJ shortcoming_NN of_IN these_DT methods_NNS is_VBZ their_PRP$ failure_NN to_TO scale_VB to_TO high_JJ dimensional_JJ problems_NNS that_WDT are_VBP becoming_VBG increasingly_RB ubiquitous_JJ in_IN modern_JJ data_NNS mining_NN applications_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP present_VBP metric_JJ learning_NN algorithms_NNS that_WDT scale_VBP linearly_RB with_IN dimensionality_NN ,_, permitting_VBG efficient_JJ optimization_NN ,_, storage_NN ,_, and_CC evaluation_NN of_IN the_DT learned_VBN metric_NN ._.
This_DT is_VBZ achieved_VBN through_IN our_PRP$ main_JJ technical_JJ contribution_NN which_WDT provides_VBZ a_DT framework_NN based_VBN on_IN the_DT log-determinant_JJ matrix_NN divergence_NN which_WDT enables_VBZ efficient_JJ optimization_NN of_IN structured_JJ ,_, low-parameter_JJ Mahalanobis_NNP distances_NNS ._.
Experimentally_RB ,_, we_PRP evaluate_VBP our_PRP$ methods_NNS across_IN a_DT variety_NN of_IN high_JJ dimensional_JJ domains_NNS ,_, including_VBG text_NN ,_, statistical_JJ software_NN analysis_NN ,_, and_CC collaborative_JJ filtering_VBG ,_, showing_VBG that_IN our_PRP$ methods_NNS scale_VBP to_TO data_NNS sets_NNS with_IN tens_NNS of_IN thousands_NNS or_CC more_JJR features_NNS ._.
We_PRP show_VBP that_IN our_PRP$ learned_VBN metric_NN can_MD achieve_VB excellent_JJ quality_NN with_IN respect_NN to_TO various_JJ criteria_NNS ._.
For_IN example_NN ,_, in_IN the_DT context_NN of_IN metric_JJ learning_NN for_IN nearest_JJS neighbor_NN classification_NN ,_, we_PRP show_VBP that_IN our_PRP$ methods_NNS achieve_VBP 24_CD %_NN higher_JJR accuracy_NN over_IN the_DT baseline_NN distance_NN ._.
Additionally_RB ,_, our_PRP$ methods_NNS yield_VBP very_RB good_JJ precision_NN while_IN providing_VBG recall_NN measures_NNS up_IN to_TO 20_CD %_NN higher_JJR than_IN other_JJ baseline_NN methods_NNS such_JJ as_IN latent_JJ semantic_JJ analysis_NN ._.
eliver_JJR satisfactory_JJ results_NNS ,_, findinga_JJ good_JJ distance_NN metric_NN for_IN the_DT problem_NN at_IN hand_NN often_RB plays_VBZ a_DT very_RB crucial_JJ role_NN ._.
As_IN such_JJ ,_, metric_JJ learning_NN -LRB-_-LRB- 25_CD -RRB-_-RRB- has_VBZ received_VBN much_JJ attention_NN in_IN the_DT research_NN community_NN =_JJ -_: =[_NN 12_CD ,_, 25_CD ,_, 5_CD ,_, 22_CD ,_, 8_CD ,_, 26_CD ,_, 6_CD ,_, 7_CD ,_, 27_CD ,_, 29_CD ,_, 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Many_JJ metric_JJ learning_NN methods_NNS have_VBP been_VBN proposed_VBN ._.
From_IN the_DT perspective_NN of_IN the_DT underlying_VBG learning_NN paradigm_NN ,_, these_DT methods_NNS can_MD be_VB grouped_VBN into_IN three_CD categories_NNS ,_, namely_RB ,_, supervised_JJ metric_JJ learning_NN ,_, uns_NNS
ngs_NNS of_IN the_DT Fifteenth_NNP Conference_NNP on_IN Computational_NNP Natural_NNP Language_NNP Learning_NNP ,_, pages_NNS 247_CD --_: 256_CD ,_, Portland_NNP ,_, Oregon_NNP ,_, USA_NNP ,_, 23_CD --_: 24_CD June_NNP 2011_CD ._.
c_NN ○_CD 2011_CD Association_NNP for_IN Computational_NNP Linguisticsalso_NNP been_VBN proposed_VBN -LRB-_-LRB- =_JJ -_: =_JJ Davis_NNP and_CC Dhillon_NNP ,_, 2008_CD -_: =--RRB-_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT new_JJ projection_NN learning_NN framework_NN ,_, Similarity_NNP Learning_NNP via_IN Siamese_NNP Neural_NNP Network_NNP -LRB-_-LRB- S2Net_NN -RRB-_-RRB- ,_, to_TO discriminatively_RB learn_VB the_DT concept_NN vector_NN representations_NNS of_IN input_NN text_NN obj_NN
ngs_NNS of_IN the_DT Fifteenth_NNP Conference_NNP on_IN Computational_NNP Natural_NNP Language_NNP Learning_NNP ,_, pages_NNS 247_CD --_: 256_CD ,_, Portland_NNP ,_, Oregon_NNP ,_, USA_NNP ,_, 23_CD --_: 24_CD June_NNP 2011_CD ._.
c_NN ○_CD 2011_CD Association_NNP for_IN Computational_NNP Linguisticsalso_NNP been_VBN proposed_VBN -LRB-_-LRB- =_JJ -_: =_JJ Davis_NNP and_CC Dhillon_NNP ,_, 2008_CD -_: =--RRB-_NN ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT new_JJ projection_NN learning_NN framework_NN ,_, Similarity_NNP Learning_NNP via_IN Siamese_NNP Neural_NNP Network_NNP -LRB-_-LRB- S2Net_NN -RRB-_-RRB- ,_, to_TO discriminatively_RB learn_VB the_DT concept_NN vector_NN representations_NNS of_IN input_NN text_NN obj_NN
measure_NN relational_JJ similarity_NN is_VBZ two-fold_JJ ._.
First_JJ ,_, Mahalanobis_JJ distance_NN can_MD be_VB learned_VBN from_IN a_DT few_JJ data_NNS points_NNS ,_, and_CC efficient_JJ algorithms_NNS that_WDT can_MD scale_VB well_RB to_TO high-dimensional_JJ feature_NN spaces_NNS are_VBP known_VBN =_JJ -_: =[_NN 13_CD ,_, 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Second_RB ,_, unlike_IN Euclidean_JJ distance_NN ,_, Mahalanobis_NNP distance_NN does_VBZ not_RB assume_VB that_IN features_NNS are_VBP independent_JJ ._.
This_DT is_VBZ particularly_RB important_JJ for_IN relational_JJ similarity_NN measures_NNS because_IN semantic_JJ relations_NNS ar_IN
cted_VBN ._.
For_IN the_DT case_NN when_WRB d_NN is_VBZ not_RB significantly_RB larger_JJR than_IN n_NN and_CC feature_NN space_NN vectors_NNS Φ_NN are_VBP available_JJ explicitly_RB ,_, the_DT basis_NN R_NN can_MD be_VB selected_VBN by_IN using_VBG one_CD of_IN the_DT following_JJ heuristics_NNS -LRB-_-LRB- see_VB Section_NNP 5_CD ,_, =_JJ -_: =_JJ Davis_NNP and_CC Dhillon_NNP ,_, 2008_CD -_: =_SYM -_: for_IN more_JJR details_NNS -RRB-_-RRB- :_: •_NN Using_VBG the_DT top_JJ k_NN singular_JJ vectors_NNS of_IN Φ_NN ._.
•_NNP Clustering_NNP the_DT columns_NNS of_IN Φ_NN and_CC using_VBG the_DT mean_NN vectors_NNS as_IN the_DT basis_NN R._NNP •_NNP For_IN the_DT fully-supervised_JJ case_NN ,_, if_IN the_DT number_NN of_IN classes_NNS -LRB-_-LRB- c_NN -RRB-_-RRB- is_VBZ g_NN
