Learning_VBG the_DT kernel_NN matrix_NN in_IN discriminant_JJ analysis_NN via_IN quadratically_RB constrained_VBN quadratic_JJ programming_NN
The_DT kernel_NN function_NN plays_VBZ a_DT central_JJ role_NN in_IN kernel_NN methods_NNS ._.
In_IN this_DT paper_NN ,_, we_PRP consider_VBP the_DT automated_JJ learning_NN of_IN the_DT kernel_NN matrix_NN over_IN a_DT convex_JJ combination_NN of_IN pre-specified_JJ kernel_NN matrices_NNS in_IN Regularized_NNP Kernel_NNP Discriminant_NNP Analysis_NNP -LRB-_-LRB- RKDA_NNP -RRB-_-RRB- ,_, which_WDT performs_VBZ lineardiscriminant_JJ analysis_NN in_IN the_DT feature_NN space_NN via_IN the_DT kernel_NN trick_NN ._.
Previous_JJ studies_NNS have_VBP shown_VBN that_IN this_DT kernel_NN learning_NN problem_NN can_MD be_VB formulated_VBN as_IN a_DT semidefinite_NN program_NN -LRB-_-LRB- SDP_NN -RRB-_-RRB- ,_, which_WDT is_VBZ however_RB computationally_RB expensive_JJ ,_, even_RB with_IN the_DT recent_JJ advances_NNS in_IN interior_JJ point_NN methods_NNS ._.
Based_VBN on_IN the_DT equivalence_JJ relationship_NN between_IN RKDA_NNP and_CC least_JJS square_JJ problems_NNS in_IN the_DT binary-class_JJ case_NN ,_, we_PRP propose_VBP a_DT Quadratically_RB Constrained_VBN Quadratic_JJ Programming_NN -LRB-_-LRB- QCQP_NN -RRB-_-RRB- formulation_NN for_IN the_DT kernel_NN learning_NN problem_NN ,_, which_WDT can_MD be_VB solved_VBN more_RBR efficiently_RB than_IN SDP_NNP ._.
While_IN most_JJS existing_VBG work_NN on_IN kernel_NN learning_NN deal_NN with_IN binary-class_JJ problems_NNS only_RB ,_, we_PRP show_VBP that_IN our_PRP$ QCQP_NN formulation_NN can_MD be_VB extended_VBN naturally_RB to_TO the_DT multi-class_JJ case_NN ._.
Experimental_JJ results_NNS on_IN both_CC binary-class_JJ and_CC multi-class_JJ benchmarkdata_NN sets_NNS show_VBP the_DT efficacy_NN of_IN the_DT proposed_VBN QCQP_NN formulations_NNS ._.
nels_NNS equally_RB ._.
Varma_NNP and_CC Babu_NNP -LRB-_-LRB- 17_CD -RRB-_-RRB- proposed_VBD a_DT general_JJ multiple_JJ kernel_NN learning_VBG algorithm_NN that_IN not_RB only_RB considers_VBZ the_DT linear_JJ combination_NN of_IN base_NN kernels_NNS but_CC also_RB employs_VBZ more_RBR prior_JJ knowledge_NN ._.
Ye_FW et_FW al._FW =_SYM -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT QCQP_NN formulation_NN for_IN kernel_NN learning_VBG to_TO address_VB both_DT bi-class_JJ problems_NNS and_CC multi-class_JJ problems_NNS ._.
Multiple_JJ kernel_NN learning_NN techniques_NNS have_VBP been_VBN applied_VBN to_TO a_DT variety_NN of_IN domains_NNS ._.
Fu_FW et_FW al._FW -LRB-_-LRB- 6_CD
complexity_NN and_CC thus_RB it_PRP can_MD not_RB be_VB applied_VBN to_TO large-scale_JJ problems_NNS ._.
To_TO improve_VB the_DT efficiency_NN of_IN this_DT formulation_NN ,_, a_DT quadratically_RB constrained_VBN quadratic_JJ program_NN -LRB-_-LRB- QCQP_NN -RRB-_-RRB- -LRB-_-LRB- 7_CD -RRB-_-RRB- formulation_NN was_VBD proposed_VBN in_IN =_JJ -_: =[_NN 70_CD -RRB-_-RRB- -_: =_JJ -_: and_CC it_PRP is_VBZ more_RBR scalable_JJ than_IN the_DT SDP_NN formulations_NNS ._.
s16_NN 1.7_CD OTHER_NNP LDA_NNP EXTENSIONS_NNP Sparsity_NNP has_VBZ recently_RB received_VBN much_JJ attention_NN for_IN extending_VBG existing_VBG algorithms_NNS to_TO induce_VB sparse_JJ solutions_NNS -LRB-_-LRB- 15_CD ,_, 35_CD ,_, 80_CD -RRB-_-RRB- ._.
ing_IN a_DT single_JJ fixed_JJ kernel_NN ,_, recent_JJ developments_NNS in_IN the_DT SVM_NN and_CC other_JJ kernel_NN methods_NNS have_VBP shown_VBN encouraging_JJ results_NNS in_IN constructing_VBG the_DT kernel_NN from_IN a_DT number_NN of_IN homogeneous_JJ or_CC even_RB heterogeneous_JJ kernels_NNS =_JJ -_: =[_NN 1_CD ,_, 10_CD ,_, 13_CD ,_, 14_CD ,_, 18_CD ,_, 33_CD ,_, 23_CD ,_, 24_CD ,_, 29_CD -RRB-_-RRB- -_: =_SYM -_: ._.
This_DT provides_VBZ extra_JJ flexibility_NN and_CC also_RB allows_VBZ domain_NN knowledge_NN from_IN possibly_RB different_JJ information_NN sources_NNS to_TO be_VB incorporated_VBN to_TO the_DT base_NN kernels_NNS ._.
However_RB ,_, previous_JJ works_NNS in_IN this_DT so-called_JJ multipl_NN
