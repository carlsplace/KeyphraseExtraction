Fast_JJ collapsed_JJ gibbs_NNS sampling_NN for_IN latent_JJ dirichlet_NN allocation_NN
In_IN this_DT paper_NN we_PRP introduce_VBP a_DT novel_JJ collapsed_JJ Gibbs_NNP sampling_NN method_NN for_IN the_DT widely_RB used_VBN latent_JJ Dirichlet_NNP allocation_NN -LRB-_-LRB- LDA_NN -RRB-_-RRB- model_NN ._.
Our_PRP$ new_JJ method_NN results_VBZ in_IN significant_JJ speedups_NNS on_IN real_JJ world_NN text_NN corpora_NN ._.
Conventional_JJ Gibbs_NNP sampling_NN schemes_NNS for_IN LDA_NNP require_VB O_NN -LRB-_-LRB- K_NN -RRB-_-RRB- operations_NNS per_IN sample_NN where_WRB K_NN is_VBZ the_DT number_NN of_IN topics_NNS in_IN the_DT model_NN ._.
Our_PRP$ proposed_VBN method_NN draws_VBZ equivalent_JJ samples_NNS but_CC requires_VBZ on_IN average_NN significantly_RB less_RBR then_RB K_NN operations_NNS per_IN sample_NN ._.
On_IN real-word_JJ corpora_NN FastLDA_NN can_MD be_VB as_RB much_JJ as_IN 8_CD times_NNS faster_RBR than_IN the_DT standard_JJ collapsed_JJ Gibbs_NNP sampler_NN for_IN LDA_NNP ._.
No_DT approximations_NNS are_VBP necessary_JJ ,_, and_CC we_PRP show_VBP that_IN our_PRP$ fast_JJ sampling_NN scheme_NN produces_VBZ exactly_RB the_DT same_JJ results_NNS as_IN the_DT standard_JJ -LRB-_-LRB- but_CC slower_JJR -RRB-_-RRB- sampling_NN scheme_NN ._.
Experiments_NNS on_IN four_CD real_JJ world_NN data_NNS sets_NNS demonstrate_VBP speedups_NNS for_IN a_DT wide_JJ range_NN of_IN collection_NN sizes_NNS ._.
For_IN the_DT PubMed_NNP collection_NN of_IN over_IN 8_CD million_CD documents_NNS with_IN a_DT required_JJ computation_NN time_NN of_IN 6_CD CPU_NNP months_NNS for_IN LDA_NNP ,_, our_PRP$ speedup_NN of_IN 5.7_CD can_MD save_VB 5_CD CPU_NNP months_NNS of_IN computation_NN ._.
a_DT in_IN applications_NNS such_JJ as_IN Web_NN search_NN ,_, speeding_VBG up_RP the_DT inference_NN procedure_NN in_IN topic_NN models_NNS is_VBZ in_IN great_JJ need_NN ._.
One_CD kind_NN of_IN the_DT accelerated_JJ procedures_NNS slices_VBZ up_RP the_DT sampling_NN probability_NN in_IN different_JJ ways_NNS =_JJ -_: =[_NN 48_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN any_DT particular_JJ word_NN and_CC document_NN ,_, the_DT distributions_NNS we_PRP want_VBP to_TO draw_VB sample_NN from_IN are_VBP often_RB skewed_VBN such_JJ that_IN most_JJS of_IN the_DT probability_NN 99mass_NN is_VBZ concentrated_VBN on_IN a_DT few_JJ topics_NNS ,_, which_WDT leads_VBZ to_TO a_DT possi_NN
A_DT model_NN learning_NN times_NNS by_IN reducing_VBG the_DT total_JJ computational_JJ cost_NN :_: --_: Gomes_NNP ,_, Welling_NNP and_CC Perona_NNP -LRB-_-LRB- 10_CD -RRB-_-RRB- presented_VBD an_DT enhancement_NN of_IN the_DT VEM_NN algorithm_NN using_VBG a_DT bounded_JJ amount_NN of_IN memory_NN ._.
--_: Porteous_NNP and_CC et_FW al._FW =_SYM -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBD a_DT method_NN to_TO accelerate_VB the_DT computation_NN of_IN -LRB-_-LRB- Eq_NN .1_NN -RRB-_-RRB- ._.
The_DT acceleration_NN is_VBZ achieved_VBN by_IN no_DT approximations_NNS but_CC using_VBG the_DT property_NN that_IN the_DT probability_NN vectors_NNS ,_, θd_NN ,_, are_VBP sparse_JJ in_IN most_JJS cases_NNS ._.
3_CD PLDA_NN
riables_NNS in_IN the_DT model_NN ,_, in_IN effect_NN reducing_VBG the_DT state_NN space_NN of_IN the_DT Markov_NNP chain_NN ._.
Collapsed_VBN sampling_NN has_VBZ been_VBN previously_RB demonstrated_VBN to_TO be_VB effective_JJ for_IN LDA_NNP and_CC its_PRP$ variants_NNS -LRB-_-LRB- Griffiths_NNP &_CC Steyvers_NNP ,_, 2004_CD ;_: =_JJ -_: =_JJ Porteous_NNP ,_, Newman_NNP ,_, Ihler_NNP ,_, Asuncion_NNP ,_, Smyth_NNP ,_, &_CC Welling_NNP ,_, 2008_CD -_: =_JJ -_: ;_: Titov_NNP &_CC McDonald_NNP ,_, 2008_CD -RRB-_-RRB- ._.
It_PRP is_VBZ typically_RB preferred_VBN over_IN the_DT explicit_JJ Gibbs_NNP sampling_NN of_IN all_PDT the_DT hidden_JJ variables_NNS ,_, because_IN of_IN the_DT smaller_JJR search_NN space_NN and_CC generally_RB shorter_JJR mixing_VBG time_NN ._.
Our_PRP$ sampler_NN an_DT
ables_NNS in_IN the_DT model_NN ,_, in_IN effect_NN reducing_VBG the_DT state_NN space_NN of_IN the_DT Markov_NNP chain_NN ._.
Collapsed_VBN sampling_NN has_VBZ been_VBN previously_RB demonstrated_VBN to_TO be_VB effective_JJ for_IN LDA_NNP and_CC its_PRP$ variants_NNS -LRB-_-LRB- Griffiths_NNP and_CC Steyvers_NNP ,_, 2004_CD ;_: =_JJ -_: =_JJ Porteous_NNP et_FW al._FW ,_, 2008_CD -_: =_JJ -_: ;_: Titov_NNP and_CC McDonald_NNP ,_, 2008_CD -RRB-_-RRB- ._.
Our_PRP$ sampler_NN integrates_VBZ over_IN all_DT but_CC three_CD sets_NNS 7_CD Multiple_JJ permutations_NNS can_MD contribute_VB to_TO the_DT probability_NN of_IN a_DT single_JJ document_NN 's_POS topic_NN assignments_NNS zd_VBP ,_, if_IN there_EX are_VBP topics_NNS t_NN
e_LS is_VBZ a_DT wealth_NN of_IN literature_NN on_IN approximate_JJ inference_NN algorithms_NNS for_IN topic_NN models_NNS ,_, such_JJ Gibbs_NNP sampling_NN and_CC variational_JJ inference_NN -LRB-_-LRB- Blei_NNP et_FW al._FW ,_, 2003_CD ;_: Griffiths_NNP &_CC Steyvers_NNP ,_, 2004_CD ;_: Mukherjee_NNP &_CC Blei_NNP ,_, 2009_CD ;_: =_JJ -_: =_JJ Porteous_NNP et_FW al._FW ,_, 2008_CD -_: =_JJ -_: ;_: Teh_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- ,_, little_JJ is_VBZ known_VBN about_IN the_DT complexity_NN of_IN exact_JJ inference_NN ._.
In_IN this_DT paper_NN ,_, we_PRP consider_VBP the_DT computational_JJ complexity_NN of_IN inference_NN in_IN topic_NN models_NNS ,_, beginning_VBG with_IN one_CD of_IN the_DT simplest_JJS a_DT
where_WRB N_NN ¬_FW nd_FW wk_NN ,_, N_NN ¬_FW nd_FW k_NN ,_, and_CC N_NN ¬_FW nd_FW kd_NN are_VBP expected_JJ counts_NNS derived_VBN from_IN q_NN -LRB-_-LRB- z_SYM -RRB-_-RRB- ._.
For_IN more_JJR details_NNS ,_, see_VBP Asuncion_NNP et_FW al._FW -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
While_IN other_JJ algorithms_NNS such_JJ as_IN fast_RB collapsed_VBN Gibbs_NNP sampling_NN can_MD also_RB be_VB used_VBN =_JJ -_: =[_NN 40_CD -RRB-_-RRB- -_: =_JJ -_: ,_, CVB0_NN is_VBZ very_RB fast_RB --_: later_RB in_IN the_DT paper_NN we_PRP will_MD show_VB timing_NN results_NNS when_WRB applying_VBG CVB0_NN to_TO software_NN artifacts_NNS ._.
Furthermore_RB ,_, topic_NN modeling_NN can_MD easily_RB scale_VB to_TO hundreds_NNS of_IN thousands_NNS of_IN artifacts_NNS ,_, espec_NN
ampling_VBG performance_NN with_IN a_DT small_JJ memory_NN footprint_NN ._.
SparseLDA_NN is_VBZ approximately_RB 20_CD times_NNS faster_RBR than_IN highly_RB optimized_VBN traditional_JJ LDA_NN and_CC twice_RB the_DT speedup_NN of_IN previously_RB published_VBN fast_JJ sampling_NN methods_NNS =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2_CD ._.
BACKGROUND_NN A_DT statistical_JJ topic_NN model_NN represents_VBZ the_DT words_NNS in_IN documents_NNS in_IN a_DT collection_NN W_NN as_IN mixtures_NNS of_IN T_NN ``_`` topics_NNS ,_, ''_'' whichare_JJ multinomials_NNS over_IN a_DT vocabulary_NN of_IN size_NN V_NN ._.
Each_DT document_NN d_NN is_VBZ associa_NN
r_NN the_DT LDA_NNP model_NN are_VBP the_DT number_NN of_IN topics_NNS K_NNP and_CC Dirichlet_NNP priors_NNS α_NN and_CC β_NN ._.
We_PRP experimented_VBD with_IN topics_NNS K_NN :_: -LRB-_-LRB- 600_CD −_CD 1400_CD -RRB-_-RRB- ,_, again_RB with_IN step_NN size_NN 200_CD ._.
We_PRP fixed_VBD β_NN to_TO 0.01_CD and_CC tested_VBN two_CD values_NNS for_IN α_NN :_: 2_CD 50_CD K_NN -LRB-_-LRB- =_JJ -_: =_JJ Porteous_NNP et_FW al._FW ,_, 2008_CD -_: =--RRB-_NN and_CC K_NN -LRB-_-LRB- Griffiths_NNP and_CC Steyvers_NNP ,_, 2004_CD -RRB-_-RRB- ._.
We_PRP used_VBD Gibbs_NNP sampling_NN on_IN the_DT ``_`` document_NN collection_NN ''_'' obtained_VBN from_IN the_DT input_NN matrix_NN and_CC estimated_VBD the_DT sense_NN distributions_NNS as_IN described_VBN in_IN Section_NN 4_CD ._.
We_PRP ran_VBD the_DT c_NN
ime_NN and_CC less_JJR time_NN is_VBZ spent_VBN in_IN synchronizing_VBG and_CC exchanging_VBG data_NNS between_IN processors_NNS ._.
All_DT tests_NNS were_VBD conducted_VBN on_IN a_DT 16-core_JJ AMD_NNP Opteron_NNP 2.7_CD GHz_FW server_FW computer_NN with_IN 64_CD gigabytes_NNS of_IN RAM_NNP ._.
Remark_NN :_: FastLDA_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ a_DT modified_VBN version_NN of_IN the_DT Collapsed_NNP Gibbs_NNP sampler_NN for_IN LDA_NNP ,_, which_WDT improves_VBZ running_VBG time_NN significantly_RB by_IN avoiding_VBG computing_VBG all_DT elements_NNS of_IN the_DT sampling_NN distribution_NN -LRB-_-LRB- 2_CD -RRB-_-RRB- ._.
However_RB ,_, in_IN this_DT project_NN w_NN
a_DT wealth_NN of_IN literature_NN on_IN approximate_JJ inference_NN algorithms_NNS for_IN topic_NN models_NNS ,_, such_JJ Gibbs_NNP sampling_NN and_CC variational_JJ inference_NN -LRB-_-LRB- Blei_NNP et_FW al._FW ,_, 2003_CD ;_: Griffiths_NNP and_CC Steyvers_NNP ,_, 2004_CD ;_: Mukherjee_NNP and_CC Blei_NNP ,_, 2009_CD ;_: =_JJ -_: =_JJ Porteous_NNP et_FW al._FW ,_, 2008_CD -_: =_JJ -_: ;_: Teh_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- ,_, little_JJ is_VBZ known_VBN about_IN the_DT computational_JJ complexity_NN of_IN exact_JJ inference_NN ._.
Furthermore_RB ,_, the_DT existing_VBG inference_NN algorithms_NNS ,_, although_IN well-motivated_JJ ,_, do_VBP not_RB provide_VB guarantees_NNS of_IN optima_NN
optimised_VBN computing_NN kernels_NNS ._.
In_IN addition_NN to_TO targeting_VBG computing_NN platforms_NNS ,_, improvements_NNS may_MD be_VB gained_VBN from_IN heuristics_NNS like_IN the_DT statistically_RB motivated_JJ acceleration_NN of_IN multinomial_JJ samplers_NNS proposed_VBN in_IN =_JJ -_: =[_NN 31_CD -RRB-_-RRB- -_: =_JJ -_: ,_, especially_RB for_IN the_DT large_JJ sampling_NN spaces_NNS of_IN dependent_JJ latent_JJ variables_NNS ._.
2_CD http:\/\/www.cs.toronto.edu\/˜roweis\/data.html_NN ._.
3_CD http:\/\/www.daviddlewis.com\/resources\/testcollections\/reuters21578\/.15_NN Refere_NN
