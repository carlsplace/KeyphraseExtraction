Towards_IN scalable_JJ support_NN vector_NN machines_NNS using_VBG squashing_VBG
ed_VBN to_TO construct_VB an_DT approximate_JJ SVM_NN by_IN approximating_VBG the_DT Gram_NN matrix_NN with_IN a_DT smaller_JJR matrix_NN using_VBG either_CC low_JJ rank_NN representation_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- or_CC sampling_NN -LRB-_-LRB- 1,29_CD -RRB-_-RRB- ._.
Assuming_VBG a_DT linear_JJ kernel_NN is_VBZ used_VBN ,_, Pavlov_NNP et_FW al._FW =_SYM -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBN to_TO squash_VB the_DT original_JJ training_NN dataset_NN into_IN a_DT limited_JJ number_NN of_IN representatives_NNS and_CC construct_VB an_DT approximate_JJ SVM_NN using_VBG these_DT representatives_NNS ._.
Although_IN these_DT algorithms_NNS ,_, together_RB with_IN many_JJ o_NN
support_NN vectors_NNS after_IN some_DT straightforward_JJ extensions_NNS to_TO the_DT current_JJ work_NN ._.
Enhancing_NN the_DT SVM_NN training_NN process_NN with_IN clustering_NN or_CC similar_JJ techniques_NNS has_VBZ been_VBN examined_VBN with_IN several_JJ variations_NNS in_IN -LRB-_-LRB- 34_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 29_CD -RRB-_-RRB- ._.
Based_VBN on_IN a_DT hierarchical_JJ micro-clustering_JJ algorithm_NN ,_, Yu_FW et_FW al._FW -LRB-_-LRB- 34_CD -RRB-_-RRB- proposed_VBD a_DT scalable_JJ algorithm_NN to_TO train_VB support_NN vector_NN machines_NNS with_IN a_DT linear_JJ kernel_NN ._.
However_RB ,_, their_PRP$ algorithm_NN currently_RB wo_MD
can_MD be_VB combined_VBN with_IN fast_JJ QP_NN approaches_NNS like_IN SMO_FW etc._FW for_IN fast_JJ training_NN and_CC prediction_NN ._.
However_RB ,_, most_JJS work_NN -LRB-_-LRB- 38_CD -RRB-_-RRB- -LRB-_-LRB- 3_CD -RRB-_-RRB- -LRB-_-LRB- 46_CD -RRB-_-RRB- in_IN data_NNS squashing_VBG +_CC SVM_NNP requires_VBZ clustering_VBG the_DT data_NNS and\/or_CC linear_JJ kernels_NNS -LRB-_-LRB- 38_CD -RRB-_-RRB- -LRB-_-LRB- 46_CD -RRB-_-RRB- =_JJ -_: =[_NN 30_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Clustering_NN usually_RB needs_VBZ O_NN -LRB-_-LRB- m2_NN -RRB-_-RRB- computations_NNS and_CC high-order_JJ kernels_NNS ,_, like_IN the_DT RBF_NN kernel_NN ,_, are_VBP widely_RB used_VBN and_CC essential_JJ to_TO many_JJ successful_JJ applications_NNS ._.
Therefore_RB ,_, a_DT fast_JJ squashing_NN method_NN and_CC experi_NNS
support_NN vectors_NNS after_IN some_DT straightforward_JJ extensions_NNS to_TO the_DT current_JJ work_NN ._.
Enhancing_NN the_DT SVM_NN training_NN process_NN with_IN clustering_NN or_CC similar_JJ techniques_NNS has_VBZ been_VBN examined_VBN with_IN several_JJ variations_NNS in_IN -LRB-_-LRB- 34_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 29_CD -RRB-_-RRB- ._.
Based_VBN on_IN a_DT hierarchical_JJ micro-clustering_JJ algorithm_NN ,_, Yu_FW et_FW al._FW -LRB-_-LRB- 34_CD -RRB-_-RRB- proposed_VBD a_DT scalable_JJ algorithm_NN to_TO train_VB support_NN vector_NN machines_NNS with_IN a_DT linear_JJ kernel_NN ._.
However_RB ,_, their_PRP$ algorithm_NN currently_RB wo_MD
m_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- has_VBZ at_IN least_JJS quadratic_JJ complexity_NN in_IN the_DT number_NN of_IN data_NNS points_NNS per_IN iteration_NN ._.
There_EX has_VBZ been_VBN work_NN on_IN speeding_VBG up_RP SVM_NN training_NN includes_VBZ various_JJ forms_NNS of_IN data_NN sampling_NN ,_, boosting_VBG -LRB-_-LRB- 7_CD -RRB-_-RRB- and_CC squashing_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_SYM -_: ._.
However_RB ,_, in_IN our_PRP$ experience_NN handling_VBG multiclass_JJ problems_NNS is_VBZ still_RB an_DT issue_NN while_IN the_DT preprocessing_VBG steps_NNS above_IN can_MD easily_RB result_VB in_IN either_CC inaccurate_JJ classifier_NN -LRB-_-LRB- e.g._FW ,_, sampling_NN -RRB-_-RRB- or_CC are_VBP too_RB computatio_JJ
ed_VBN to_TO construct_VB an_DT approximate_JJ SVM_NN by_IN approximating_VBG the_DT Gram_NN matrix_NN with_IN a_DT smaller_JJR matrix_NN using_VBG either_CC low_JJ rank_NN representation_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- or_CC sampling_NN -LRB-_-LRB- 1,29_CD -RRB-_-RRB- ._.
Assuming_VBG a_DT linear_JJ kernel_NN is_VBZ used_VBN ,_, Pavlov_NNP et_FW al._FW =_SYM -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: proposed_VBN to_TO squash_VB the_DT original_JJ training_NN dataset_NN into_IN a_DT limited_JJ number_NN of_IN representatives_NNS and_CC construct_VB an_DT approximate_JJ SVM_NN using_VBG these_DT representatives_NNS ._.
Although_IN these_DT algorithms_NNS ,_, together_RB with_IN many_JJ o_NN
er_NN case_NN ,_, the_DT cluster_NN centers_NNS are_VBP used_VBN as_IN training_NN patterns_NNS ._.
The_DT idea_NN of_IN preclustering_VBG data_NNS has_VBZ been_VBN extended_VBN in_IN an_DT interesting_JJ direction_NN by_IN applying_VBG the_DT concept_NN of_IN squashing_VBG to_TO training_VBG a_DT linear_JJ SVM_NN -LRB-_-LRB- =_JJ -_: =_JJ Pavlov_NNP ,_, Chudova_NNP ,_, and_CC Smyth_NNP ,_, 200_CD -_: =_JJ -0_CD -RRB-_-RRB- ._.
For_IN training_NN ,_, the_DT SMO_NNP algorithm_NN is_VBZ used_VBN ._.
Clustering_NN is_VBZ performed_VBN using_VBG a_DT metric_NN derived_VBN from_IN the_DT likelihood_NN prole_NN of_IN the_DT data_NNS ._.
First_RB ,_, a_DT small_JJ percentage_NN of_IN the_DT original_JJ training_NN data_NNS set_VBN are_VBP rando_NN
proach_NN developed_VBD in_IN this_DT paper_NN ._.
Instead_RB of_IN performing_VBG random_JJ sampling_NN on_IN the_DT training_NN set_NN ,_, some_DT ``_`` intelligent_JJ ''_'' sampling_NN techniques_NNS have_VBP been_VBN proposed_VBN to_TO perform_VB down-sampling_JJ -LRB-_-LRB- Smola_NNP &_CC Schölkopf_NNP ,_, 2000_CD ;_: =_JJ -_: =_JJ Pavlov_NNP et_FW al._FW ,_, 2000_CD -_: =_JJ -_: ;_: Tresp_NNP ,_, 2001_CD -RRB-_-RRB- ._.
Recently_RB ,_, -LRB-_-LRB- Yu_NNP et_FW al._FW ,_, 2003_CD -RRB-_-RRB- uses_VBZ a_DT hierarchical_JJ microclustering_NN technique_NN to_TO capture_VB the_DT training_NN instances_NNS that_WDT are_VBP close_JJ to_TO the_DT decision_NN boundary_NN ._.
However_RB ,_, since_IN the_DT hierarchical_JJ mic_NN
icular_JJ data_NNS set_VBN ._.
Here_RB ,_, we_PRP are_VBP going_VBG to_TO instead_RB examine_VB how_WRB boosting_VBG may_MD be_VB used_VBN to_TO deal_VB with_IN data_NNS sets_NNS that_WDT have_VBP a_DT large_JJ number_NN of_IN examples_NNS and\/or_CC learning_VBG algorithms_NNS that_WDT are_VBP very_RB time-consuming_JJ ._.
In_IN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Pavlov_NNP et_NNP ._.
al._FW examined_VBN several_JJ methods_NNS for_IN speeding_VBG up_RP the_DT learning_NN process_NN for_IN support_NN vector_NN machines_NNS ._.
One_CD of_IN those_DT methods_NNS used_VBD boosting_VBG on_IN a_DT small_JJ subset_NN of_IN the_DT data_NNS and_CC reported_VBD good_JJ accuracie_NN
ll_NN examples_NNS within_IN the_DT margin_NN is_VBZ computationally_RB expensive_JJ ._.
Following_VBG the_DT idea_NN of_IN the_DT likelihood-based_JJ squashing_NN -LRB-_-LRB- 17_CD -RRB-_-RRB- -LRB-_-LRB- 21_CD -RRB-_-RRB- ,_, a_DT likelihood_NN squashing_NN method_NN was_VBD developed_VBN for_IN a_DT SVM_NN by_IN Pavlov_NN and_CC Chudova_NN =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT likelihood_NN squashing_NN method_NN assumes_VBZ a_DT probability_NN model_NN as_IN the_DT classifier_NN ._.
Examples_NNS with_IN similar_JJ probability_NN p_NN -LRB-_-LRB- xi_NN ,_, yi_FW |_FW θ_FW -RRB-_-RRB- are_VBP grouped_VBN together_RB and_CC taken_VBN as_IN a_DT weighted_JJ exemplar_NN ._.
Pavlov_NNP and_CC Chudova_NNP
support_NN vectors_NNS after_IN some_DT straightforward_JJ extensions_NNS to_TO the_DT current_JJ work_NN ._.
Enhancing_NN the_DT SVM_NN training_NN process_NN with_IN clustering_NN or_CC similar_JJ techniques_NNS has_VBZ been_VBN examined_VBN with_IN several_JJ variations_NNS in_IN -LRB-_-LRB- 34_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 26_CD -RRB-_-RRB- -_: =_JJ -_: and_CC -LRB-_-LRB- 29_CD -RRB-_-RRB- ._.
Based_VBN on_IN a_DT hierarchical_JJ micro-clustering_JJ algorithm_NN ,_, Yu_FW et_FW al._FW -LRB-_-LRB- 34_CD -RRB-_-RRB- proposed_VBD a_DT scalable_JJ algorithm_NN to_TO train_VB support_NN vector_NN machines_NNS with_IN a_DT linear_JJ kernel_NN ._.
However_RB ,_, their_PRP$ algorithm_NN currently_RB wo_MD
