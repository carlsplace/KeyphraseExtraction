Finding_VBG advertising_NN keywords_NNS on_IN web_NN pages_NNS
A_DT large_JJ and_CC growing_VBG number_NN of_IN web_NN pages_NNS display_VBP contextual_JJ advertising_NN based_VBN on_IN keywords_NNS automatically_RB extracted_VBN from_IN the_DT text_NN of_IN the_DT page_NN ,_, and_CC this_DT is_VBZ a_DT substantial_JJ source_NN of_IN revenue_NN supporting_VBG the_DT web_NN today_NN ._.
Despite_IN the_DT importance_NN of_IN this_DT area_NN ,_, little_JJ formal_JJ ,_, published_VBN research_NN exists_VBZ ._.
We_PRP describe_VBP a_DT system_NN that_WDT learns_VBZ how_WRB to_TO extract_VB keywords_NNS from_IN web_NN pages_NNS for_IN advertisement_NN targeting_NN ._.
The_DT system_NN uses_VBZ a_DT number_NN of_IN features_NNS ,_, such_JJ as_IN term_NN frequency_NN of_IN each_DT potential_JJ keyword_JJ ,_, inverse_JJ document_NN frequency_NN ,_, presence_NN in_FW meta-data_FW ,_, and_CC how_WRB often_RB the_DT term_NN occurs_VBZ in_IN search_NN query_NN logs_NNS ._.
The_DT system_NN is_VBZ trained_VBN with_IN a_DT set_NN of_IN example_NN pages_NNS that_WDT have_VBP been_VBN hand-labeled_JJ with_IN ``_`` relevant_JJ ''_'' keywords_NNS ._.
Based_VBN on_IN this_DT training_NN ,_, it_PRP can_MD then_RB extract_VB new_JJ keywords_NNS from_IN previously_RB unseen_JJ pages_NNS ._.
Accuracy_NN is_VBZ substantially_RB better_JJR than_IN several_JJ baseline_NN systems_NNS ._.
tudied_VBN areas_NNS like_IN information_NN extraction_NN ,_, named-entity_NN recognition_NN and_CC phrase_NN labeling_NN :_: they_PRP all_DT attempt_VBP to_TO find_VB important_JJ phrases_NNS in_IN documents_NNS ._.
State-of-the-art_JJ information_NN extraction_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 5_CD ,_, 20_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, named-entity_JJ recognition_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 21_CD -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC phrase_NN labeling_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 3_CD -RRB-_-RRB- -RRB-_-RRB- typically_RB decompose_VBP phrases_NNS into_IN individual_JJ words_NNS ,_, rather_RB than_IN examining_VBG them_PRP monolithically_RB ._.
It_PRP thus_RB seemed_VBD w_NN
e_LS components_NNS -LRB-_-LRB- Begininning_NNP ,_, Inside_NNP ,_, etc._NN -RRB-_-RRB- into_IN probabilities_NNS of_IN relevance_NN of_IN whole_JJ phrases_NNS ._.
Typically_RB ,_, in_IN phrase_NN labeling_NN problems_NNS like_IN information_NN extraction_NN ,_, this_DT conversion_NN is_VBZ done_VBN with_IN the_DT Viterbi_NN =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =_JJ -_: algorithm_NN ,_, to_TO find_VB the_DT most_RBS probable_JJ assignment_NN of_IN the_DT word_NN label_NN sequence_NN of_IN each_DT sentence_NN -LRB-_-LRB- 5_CD -RRB-_-RRB- ._.
We_PRP rejected_VBD this_DT method_NN for_IN two_CD reasons_NNS ._.
First_RB ,_, because_IN in_IN our_PRP$ training_NN set_NN ,_, only_RB a_DT few_JJ words_NNS per_IN docu_NN
keyword_JJ extraction_NN would_MD be_VB an_DT interesting_JJ area_NN of_IN future_JJ research_NN ,_, although_IN our_PRP$ intuition_NN is_VBZ that_IN these_DT features_NNS are_VBP less_RBR likely_JJ to_TO be_VB useful_JJ in_IN this_DT case_NN ._.
4.4_CD Impedance_NN Coupling_NN Ribeiro-Neto_NNP et_FW al._FW =_SYM -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: describe_VBP an_DT Impedance_NN Coupling_NN technique_NN for_IN content-targeted_JJ advertising_NN ._.
Their_PRP$ work_NN is_VBZ perhaps_RB the_DT most_RBS extensive_JJ previously_RB published_VBN work_NN specifically_RB on_IN content-targeted_JJ advertising_NN ._.
However_RB ,_, Ri_NN
evaluation_NN metrics_NNS ,_, where_WRB the_DT numbers_NNS are_VBP 0.239_CD and_CC 0.128_CD respectively_RB for_IN all_DT documents_NNS ._.
Turney_NNP showed_VBD GenEx_NNP 's_POS superiority_NN by_IN comparing_VBG it_PRP with_IN an_DT earlier_JJR keyphrase_NN extraction_NN system_NN trained_VBN by_IN C4_NN .5_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: with_IN several_JJ complicated_JJ features_NNS -LRB-_-LRB- 22_CD -RRB-_-RRB- ._.
4.2_CD KEA_NN and_CC Variations_NNS Concurrently_RB with_IN the_DT development_NN of_IN GenEx_NNP ,_, Frank_NNP et_FW al._FW developed_VBD the_DT KEA_NNP keyphrase_NN extraction_NN algorithm_NN using_VBG a_DT simple_JJ machine_NN learnin_NN
s_NNS append_VBP a_DT special_JJ ``_`` always_RB on_IN ''_'' feature_NN -LRB-_-LRB- i.e._FW a_DT value_NN of_IN 1_CD -RRB-_-RRB- to_TO the_DT x_NN vector_NN ,_, that_WDT serves_VBZ as_IN a_DT bias_NN term_NN ._.
In_IN order_NN to_TO prevent_VB overfitting_NN ,_, we_PRP also_RB apply_VBP a_DT Gaussian_JJ prior_JJ with_IN variance_NN 0.3_CD for_IN smoothing_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
2.3.2_NN Features_VBZ We_PRP experimented_VBD with_IN various_JJ features_NNS that_WDT are_VBP potentially_RB useful_JJ ._.
Some_DT of_IN these_DT features_NNS are_VBP binary_JJ ,_, taking_VBG only_RB the_DT values_NNS 0_CD or_CC 1_CD ,_, such_JJ as_IN whether_IN the_DT phrase_NN appears_VBZ in_IN the_DT title_NN ._.
O_NN
words_NNS in_IN the_DT whole_JJ document_NN -RRB-_-RRB- ,_, and_CC keyphrase-frequency_NN ,_, which_WDT is_VBZ the_DT number_NN of_IN times_NNS the_DT candidate_NN phrase_NN occurs_VBZ in_IN other_JJ documents_NNS ._.
The_DT classifier_NN is_VBZ trained_VBN using_VBG the_DT naive_JJ Bayes_NNS learning_VBG algorithm_NN =_JJ -_: =[_NN 14_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Frank_NNP et_FW al._FW compared_VBN KEA_NN with_IN GenEx_NN ,_, and_CC showed_VBD that_IN KEA_NNP is_VBZ slightly_RB better_JJR in_IN general_JJ ,_, but_CC the_DT difference_NN is_VBZ not_RB statistically_RB significant_JJ -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
sKEA_NN 's_POS performance_NN was_VBD improved_VBN later_RB by_IN adding_VBG Web_NN r_NN
direct_JJ comparison_NN to_TO KEA_NN was_VBD not_RB reported_VBN ._.
4.3_CD Information_NN Extraction_NN Analogous_JJ to_TO keyword_VB extraction_NN ,_, information_NN extraction_NN is_VBZ also_RB a_DT problem_NN that_WDT aims_VBZ to_TO extract_VB or_CC label_VB phrases_NNS given_VBN a_DT document_NN =_JJ -_: =[_NN 8_CD ,_, 2_CD ,_, 19_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Unlike_IN keyword_JJ extraction_NN ,_, information_NN extraction_NN tasks_NNS are_VBP usually_RB associated_VBN with_IN predefined_JJ semantic_JJ templates_NNS ._.
The_DT goal_NN of_IN the_DT extraction_NN tasks_NNS is_VBZ to_TO find_VB certain_JJ phrases_NNS in_IN the_DT documents_NNS to_TO fil_VB
ompared_VBN KEA_NN with_IN GenEx_NN ,_, and_CC showed_VBD that_IN KEA_NNP is_VBZ slightly_RB better_JJR in_IN general_JJ ,_, but_CC the_DT difference_NN is_VBZ not_RB statistically_RB significant_JJ -LRB-_-LRB- 7_CD -RRB-_-RRB- ._.
sKEA_NN 's_POS performance_NN was_VBD improved_VBN later_RB by_IN adding_VBG Web_NN related_JJ features_NNS =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN short_JJ ,_, the_DT number_NN of_IN documents_NNS returned_VBN by_IN a_DT search_NN engine_NN using_VBG the_DT keyphrase_NN as_IN query_NN terms_NNS is_VBZ used_VBN as_IN additional_JJ information_NN ._.
This_DT feature_NN is_VBZ particularly_RB useful_JJ when_WRB training_NN and_CC testing_NN data_NNS
short_JJ noun_NN phrases_NNS ._.
Therefore_RB ,_, having_VBG this_DT information_NN available_JJ as_IN a_DT feature_NN would_MD potentially_RB be_VB useful_JJ ._.
We_PRP thus_RB applied_VBD a_DT state-of-the-art_JJ chunker_NN to_TO detect_VB the_DT base_NN noun_NN phrases_NNS in_IN each_DT document_NN =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =_SYM -_: 2_CD ._.
2.2_CD Candidate_NNP Selector_NNP Our_NNP system_NN considers_VBZ each_DT word_NN or_CC phrase_NN -LRB-_-LRB- consecutive_JJ words_NNS -RRB-_-RRB- up_IN to_TO length_NN 5_CD that_WDT appears_VBZ in_IN the_DT document_NN as_IN a_DT candidate_NN keyword_NN ._.
This_DT includes_VBZ all_DT keywords_NNS that_WDT appear_VBP in_IN t_NN
n_NN our_PRP$ case_NN ,_, the_DT training_NN instances_NNS consist_VBP of_IN every_DT possible_JJ candidate_NN phrases_NNS selected_VBN from_IN the_DT training_NN documents_NNS ,_, with_IN Y_NN =_JJ 1_CD if_IN they_PRP were_VBD labeled_VBN relevant_JJ ,_, and_CC 0_CD otherwise_RB ._.
We_PRP use_VBP the_DT SCGIS_NN method_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: as_IN the_DT actual_JJ training_NN method_NN ._.
However_RB ,_, because_IN there_EX is_VBZ a_DT unique_JJ best_RBS logistic_JJ regression_NN model_NN for_IN any_DT training_NN set_NN ,_, and_CC the_DT space_NN is_VBZ convex_NN ,_, the_DT actual_JJ choice_NN of_IN training_NN algorithm_NN makes_VBZ relativel_NN
tity_NN recognition_NN and_CC phrase_NN labeling_NN :_: they_PRP all_DT attempt_VBP to_TO find_VB important_JJ phrases_NNS in_IN documents_NNS ._.
State-of-the-art_JJ information_NN extraction_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 5_CD ,_, 20_CD -RRB-_-RRB- -RRB-_-RRB- ,_, named-entity_JJ recognition_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 21_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, and_CC phrase_NN labeling_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 3_CD -RRB-_-RRB- -RRB-_-RRB- typically_RB decompose_VBP phrases_NNS into_IN individual_JJ words_NNS ,_, rather_RB than_IN examining_VBG them_PRP monolithically_RB ._.
It_PRP thus_RB seemed_VBD worthwhile_JJ to_TO examine_VB similar_JJ techniques_NNS for_IN key_NN
r_NN more_RBR challenging_JJ :_: optimizations_NNS are_VBP possible_JJ ,_, but_CC would_MD need_VB to_TO be_VB an_DT area_NN of_IN additional_JJ research_NN ._.
In_IN contrast_NN ,_, our_PRP$ methods_NNS are_VBP directly_RB applicable_JJ today_NN ._.
4.5_CD News_NNP Query_NNP Extraction_NNP Henzinger_NNP et_FW al._FW =_SYM -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: explored_VBD the_DT domain_NN of_IN keyword_JJ extraction_NN from_IN a_DT news_NN source_NN ,_, to_TO automatically_RB drive_VB queries_NNS ._.
In_IN particular_JJ ,_, they_PRP extracted_VBD query_NN terms_NNS from_IN the_DT closed_JJ captioning_NN of_IN television_NN news_NN stories_NNS ,_, to_TO drive_VB
tudied_VBN areas_NNS like_IN information_NN extraction_NN ,_, named-entity_NN recognition_NN and_CC phrase_NN labeling_NN :_: they_PRP all_DT attempt_VBP to_TO find_VB important_JJ phrases_NNS in_IN documents_NNS ._.
State-of-the-art_JJ information_NN extraction_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 5_CD ,_, 20_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, named-entity_JJ recognition_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 21_CD -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC phrase_NN labeling_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 3_CD -RRB-_-RRB- -RRB-_-RRB- typically_RB decompose_VBP phrases_NNS into_IN individual_JJ words_NNS ,_, rather_RB than_IN examining_VBG them_PRP monolithically_RB ._.
It_PRP thus_RB seemed_VBD w_NN
rom_NN different_JJ domains_NNS ._.
In_IN both_CC intra_NN and_CC inter_NN domain_NN evaluations_NNS ,_, the_DT relative_JJ performance_NN gain_NN in_IN precision_NN is_VBZ about_RB 10_CD %_NN ._.
Kelleher_NNP and_CC Luz_NNP also_RB reported_VBD an_DT enhanced_VBN version_NN of_IN KEA_NN for_IN web_NN documents_NNS =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_SYM -_: ._.
They_PRP exploited_VBD the_DT link_NN information_NN of_IN a_DT web_NN document_NN by_IN adding_VBG a_DT ``_`` semantic_JJ ratio_NN ''_'' feature_NN ,_, which_WDT is_VBZ the_DT frequency_NN of_IN the_DT candidate_NN phrase_NN P_NN in_IN the_DT original_JJ document_NN D_NN ,_, divided_VBN by_IN the_DT frequency_NN of_IN P_NN
were_VBD used_VBN to_TO help_VB select_JJ phrase_NN candidates_NNS ._.
In_IN addition_NN ,_, whether_IN the_DT candidate_NN phrase_NN has_VBZ certain_JJ POS_NN tags_NNS is_VBZ used_VBN as_IN features_NNS ._.
Along_IN with_IN the_DT three_CD features_NNS in_IN the_DT KEA_NN system_NN ,_, Hulth_NNP applied_VBD bagging_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: as_IN the_DT learning_VBG algorithm_NN to_TO train_VB the_DT system_NN ._.
The_DT experimental_JJ results_NNS showed_VBD different_JJ degrees_NNS of_IN improvements_NNS compared_VBN with_IN systems_NNS that_WDT did_VBD not_RB use_VB linguistic_JJ information_NN ._.
However_RB ,_, direct_JJ compari_NNS
splitter_NN to_TO separate_JJ text_NN in_IN the_DT same_JJ block_NN into_IN various_JJ sentences_NNS ._.
To_TO evaluate_VB whether_IN linguistic_JJ information_NN can_MD help_VB keyword_VB extraction_NN ,_, we_PRP also_RB apply_VBP a_DT state-of-the-art_JJ part-ofspeech_NN -LRB-_-LRB- POS_NN -RRB-_-RRB- tagger_NN =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC record_VB the_DT pos_NN tag_NN of_IN each_DT word_NN ._.
In_IN addition_NN ,_, we_PRP have_VBP observed_VBN that_IN most_JJS words_NNS or_CC phrases_NNS that_WDT are_VBP relevant_JJ are_VBP short_JJ noun_NN phrases_NNS ._.
Therefore_RB ,_, having_VBG this_DT information_NN available_JJ as_IN a_DT feature_NN woul_NN
that_DT page_NN ._.
We_PRP compared_VBD these_DT approaches_NNS to_TO several_JJ different_JJ baseline_NN approaches_NNS ,_, including_VBG a_DT traditional_JJ TF_NN Ã—_CD IDF_NN model_NN ;_: a_DT model_NN using_VBG TF_NN and_CC IDF_NN features_NNS but_CC with_IN learned_VBN weights_NNS ;_: and_CC the_DT KEA_NN system_NN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
KEA_NNP is_VBZ also_RB a_DT machine_NN learning_NN system_NN ,_, but_CC with_IN a_DT simpler_JJR learning_NN mechanism_NN and_CC fewer_JJR features_NNS ._.
As_IN we_PRP will_MD show_VB ,_, our_PRP$ system_NN is_VBZ substantially_RB better_JJR than_IN any_DT of_IN these_DT baseline_NN systems_NNS ._.
We_PRP also_RB compa_VBP
le_DT learning_NN method_NN -LRB-_-LRB- Naive_JJ Bayes_NNS -RRB-_-RRB- ,_, or_CC both_DT ._.
In_IN this_DT section_NN ,_, we_PRP describe_VBP this_DT previous_JJ research_NN in_IN more_JJR detail_NN ._.
4.1_CD GenEx_NN One_CD of_IN the_DT best_JJS known_JJ programs_NNS for_IN keyword_JJ extraction_NN is_VBZ Turney_NNP 's_POS GenEx_NN system_NN =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
GenEx_NN is_VBZ a_DT rule-based_JJ keyphrase_NN extraction_NN system_NN with_IN 12_CD parameters_NNS tuned_VBN using_VBG a_DT Genetic_NNP Algorithm_NNP ._.
GenEx_NN has_VBZ been_VBN trained_VBN and_CC evaluated_VBN on_IN a_DT collection_NN of_IN 652_CD documents_NNS of_IN three_CD different_JJ types_NNS :_:
direct_JJ comparison_NN to_TO KEA_NN was_VBD not_RB reported_VBN ._.
4.3_CD Information_NN Extraction_NN Analogous_JJ to_TO keyword_VB extraction_NN ,_, information_NN extraction_NN is_VBZ also_RB a_DT problem_NN that_WDT aims_VBZ to_TO extract_VB or_CC label_VB phrases_NNS given_VBN a_DT document_NN =_JJ -_: =[_NN 8_CD ,_, 2_CD ,_, 19_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Unlike_IN keyword_JJ extraction_NN ,_, information_NN extraction_NN tasks_NNS are_VBP usually_RB associated_VBN with_IN predefined_JJ semantic_JJ templates_NNS ._.
The_DT goal_NN of_IN the_DT extraction_NN tasks_NNS is_VBZ to_TO find_VB certain_JJ phrases_NNS in_IN the_DT documents_NNS to_TO fil_VB
direct_JJ comparison_NN to_TO KEA_NN was_VBD not_RB reported_VBN ._.
4.3_CD Information_NN Extraction_NN Analogous_JJ to_TO keyword_VB extraction_NN ,_, information_NN extraction_NN is_VBZ also_RB a_DT problem_NN that_WDT aims_VBZ to_TO extract_VB or_CC label_VB phrases_NNS given_VBN a_DT document_NN =_JJ -_: =[_NN 8_CD ,_, 2_CD ,_, 19_CD ,_, 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Unlike_IN keyword_JJ extraction_NN ,_, information_NN extraction_NN tasks_NNS are_VBP usually_RB associated_VBN with_IN predefined_JJ semantic_JJ templates_NNS ._.
The_DT goal_NN of_IN the_DT extraction_NN tasks_NNS is_VBZ to_TO find_VB certain_JJ phrases_NNS in_IN the_DT documents_NNS to_TO fil_VB
a_DT page_NN could_MD result_VB in_IN simulating_VBG clicks_NNS on_IN ,_, e.g._FW ,_, unsubscribe_JJ links_NNS ,_, leading_VBG to_TO unexpected_JJ or_CC undesirable_JJ results_NNS ._.
The_DT use_NN of_IN linguistic_JJ information_NN for_IN keyword_JJ extraction_NN was_VBD first_RB studied_VBN by_IN Hulth_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT work_NN ,_, noun_NN phrases_NNS and_CC predefined_VBN part-of-speech_JJ tag_NN patterns_NNS were_VBD used_VBN to_TO help_VB select_JJ phrase_NN candidates_NNS ._.
In_IN addition_NN ,_, whether_IN the_DT candidate_NN phrase_NN has_VBZ certain_JJ POS_NN tags_NNS is_VBZ used_VBN as_IN features_NNS ._.
closely_RB related_JJ to_TO the_DT previously_RB mentioned_VBN work_NN of_IN Ribeiro-Neto_NNP et_FW al._FW ,_, who_WP found_VBD analogous_JJ comparisons_NNS between_IN documents_NNS and_CC advertisements_NNS helpful_JJ ._.
4.6_CD Email_NNP Query_NNP Extraction_NNP Goodman_NNP and_CC Carvalho_NNP =_SYM -_: =[_NN 10_CD -RRB-_-RRB- -_: =_SYM -_: previously_RB applied_VBN similar_JJ techniques_NNS to_TO the_DT ones_NNS described_VBN here_RB for_IN query_NN extraction_NN for_IN email_NN ._.
Their_PRP$ goal_NN was_VBD somewhat_RB different_JJ than_IN our_PRP$ goal_NN here_RB ,_, namely_RB to_TO find_VB good_JJ search_NN queries_NNS ,_, to_TO drive_VB tra_NN
all_DT attempt_NN to_TO find_VB important_JJ phrases_NNS in_IN documents_NNS ._.
State-of-the-art_JJ information_NN extraction_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 5_CD ,_, 20_CD -RRB-_-RRB- -RRB-_-RRB- ,_, named-entity_JJ recognition_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, -LRB-_-LRB- 21_CD -RRB-_-RRB- -RRB-_-RRB- ,_, and_CC phrase_NN labeling_NN systems_NNS -LRB-_-LRB- e.g._FW ,_, =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =--RRB-_NN typically_RB decompose_VBP phrases_NNS into_IN individual_JJ words_NNS ,_, rather_RB than_IN examining_VBG them_PRP monolithically_RB ._.
It_PRP thus_RB seemed_VBD worthwhile_JJ to_TO examine_VB similar_JJ techniques_NNS for_IN keyword_JJ extraction_NN ._.
Decomposing_VBG a_DT phrase_NN int_NN
