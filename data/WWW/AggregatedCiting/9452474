Mining_NNP multilingual_JJ topics_NNS from_IN wikipedia_NN
In_IN this_DT paper_NN ,_, we_PRP try_VBP to_TO leverage_NN a_DT large-scale_JJ and_CC multilingual_JJ knowledge_NN base_NN ,_, Wikipedia_NNP ,_, to_TO help_VB effectively_RB analyze_VB and_CC organize_VB Web_NN information_NN written_VBN in_IN different_JJ languages_NNS ._.
Based_VBN on_IN the_DT observation_NN that_IN one_CD Wikipedia_NNP concept_NN may_MD be_VB described_VBN by_IN articles_NNS in_IN different_JJ languages_NNS ,_, we_PRP adapt_VBP existing_VBG topic_NN modeling_NN algorithm_NN for_IN mining_NN multilingual_JJ topics_NNS from_IN this_DT knowledge_NN base_NN ._.
The_DT extracted_VBN `_`` universal_JJ '_'' topics_NNS have_VBP multiple_JJ types_NNS of_IN representations_NNS ,_, with_IN each_DT type_NN corresponding_VBG to_TO one_CD language_NN ._.
Accordingly_RB ,_, new_JJ documents_NNS of_IN different_JJ languages_NNS can_MD be_VB represented_VBN in_IN a_DT space_NN using_VBG a_DT group_NN of_IN universal_JJ topics_NNS ,_, which_WDT makes_VBZ various_JJ multilingual_JJ Web_NN applications_NNS feasible_JJ ._.
for_IN experiments_NNS ._.
4.1_CD Cross-lingual_JJ Text_NN Classification_NN Cross-lingual_JJ text_NN classification_NN -LRB-_-LRB- CLTC_NN -RRB-_-RRB- addresses_VBZ the_DT problem_NN of_IN using_VBG texts_NNS labeled_VBN in_IN one_CD language_NN to_TO help_VB classify_VB texts_NNS in_IN another_DT language_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =-[_CD 5_CD -RRB-_-RRB- ._.
We_PRP have_VBP built_VBN two_CD CLTC_NNP tasks_NNS :_: 1_LS -RRB-_-RRB- classify_VB Chinese_JJ pages_NNS by_IN using_VBG the_DT training_NN data_NNS in_IN English_NNP -LRB-_-LRB- Ento-Ch_NNP -RRB-_-RRB- ;_: 2_LS -RRB-_-RRB- classify_VB English_JJ pages_NNS by_IN using_VBG the_DT training_NN data_NNS in_IN Chinese_JJ -LRB-_-LRB- Ch-to-En_NN -RRB-_-RRB- ._.
Support_NN Vector_NNP
ferent_JJ languages_NNS ,_, share_NN identical_JJ topic_NN distribution_NN ._.
Figure_NN 1_CD presents_VBZ the_DT graphical_JJ model_NN of_IN ML-LDA_NN ._.
Figure_NN 1_CD ._.
Graphical_JJ model_NN representation_NN of_IN ML-LDA_NN ._.
The_DT notations_NNS are_VBP similar_JJ to_TO those_DT in_IN LDA_NNP -LRB-_-LRB- 1_LS -RRB-_-RRB- =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Here_RB Lj_NNP denotes_VBZ one_CD language_NN and_CC denotes_VBZ the_DT word_NN distribution_NN for_IN topic_NN k_NN in_FW ϕ_FW k_NN ,_, L_NN j_NN r_NN Language_NN Lj_NN ._.
We_PRP modify_VBP Gibbs_NNP Sampling_NNP -LRB-_-LRB- 2_LS -RRB-_-RRB- method_NN for_IN the_DT estimation_NN of_IN ML-LDA_NN ._.
Here_RB in_IN one_CD concept-unit_NN ,_, doc_NN
different_JJ languages_NNS ,_, share_NN identical_JJ topic_NN distribution_NN ._.
Figure_NN 1_CD presents_VBZ the_DT graphical_JJ model_NN of_IN ML-LDA_NN ._.
Figure_NN 1_CD ._.
Graphical_JJ model_NN representation_NN of_IN ML-LDA_NN ._.
The_DT notations_NNS are_VBP similar_JJ to_TO those_DT in_IN LDA_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =-[_NN 2_CD -RRB-_-RRB- ._.
Here_RB Lj_NNP denotes_VBZ one_CD language_NN and_CC denotes_VBZ the_DT word_NN distribution_NN for_IN topic_NN k_NN in_FW ϕ_FW k_NN ,_, L_NN j_NN r_NN Language_NN Lj_NN ._.
We_PRP modify_VBP Gibbs_NNP Sampling_NNP -LRB-_-LRB- 2_LS -RRB-_-RRB- method_NN for_IN the_DT estimation_NN of_IN ML-LDA_NN ._.
Here_RB in_IN one_CD concept-unit_NN ,_,
experiments_NNS ._.
4.1_CD Cross-lingual_JJ Text_NN Classification_NN Cross-lingual_JJ text_NN classification_NN -LRB-_-LRB- CLTC_NN -RRB-_-RRB- addresses_VBZ the_DT problem_NN of_IN using_VBG texts_NNS labeled_VBN in_IN one_CD language_NN to_TO help_VB classify_VB texts_NNS in_IN another_DT language_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP have_VBP built_VBN two_CD CLTC_NNP tasks_NNS :_: 1_LS -RRB-_-RRB- classify_VB Chinese_JJ pages_NNS by_IN using_VBG the_DT training_NN data_NNS in_IN English_NNP -LRB-_-LRB- Ento-Ch_NNP -RRB-_-RRB- ;_: 2_LS -RRB-_-RRB- classify_VB English_JJ pages_NNS by_IN using_VBG the_DT training_NN data_NNS in_IN Chinese_JJ -LRB-_-LRB- Ch-to-En_NN -RRB-_-RRB- ._.
Support_NN Vector_NNP Ma_NNP
