Mr._NNP LDA_NNP :_: a_DT flexible_JJ large_JJ scale_NN topic_NN modeling_NN package_NN using_VBG variational_JJ inference_NN in_IN MapReduce_NNP
Latent_JJ Dirichlet_NNP Allocation_NNP -LRB-_-LRB- LDA_NNP -RRB-_-RRB- is_VBZ a_DT popular_JJ topic_NN modeling_NN technique_NN for_IN exploring_VBG document_NN collections_NNS ._.
Because_IN of_IN the_DT increasing_VBG prevalence_NN of_IN large_JJ datasets_NNS ,_, there_EX is_VBZ a_DT need_NN to_TO improve_VB the_DT scalability_NN of_IN inference_NN for_IN LDA_NNP ._.
In_IN this_DT paper_NN ,_, we_PRP introduce_VBP a_DT novel_JJ and_CC flexible_JJ large_JJ scale_NN topic_NN modeling_NN package_NN in_IN MapReduce_NNP -LRB-_-LRB- Mr._NNP LDA_NNP -RRB-_-RRB- ._.
As_IN opposed_VBN to_TO other_JJ techniques_NNS which_WDT use_VBP Gibbs_NNP sampling_NN ,_, our_PRP$ proposed_VBN framework_NN uses_VBZ variational_JJ inference_NN ,_, which_WDT easily_RB fits_VBZ into_IN a_DT distributed_VBN environment_NN ._.
More_RBR importantly_RB ,_, this_DT variational_JJ implementation_NN ,_, unlike_IN highly_RB tuned_VBN and_CC specialized_VBN implementations_NNS based_VBN on_IN Gibbs_NNP sampling_NN ,_, is_VBZ easily_RB extensible_JJ ._.
We_PRP demonstrate_VBP two_CD extensions_NNS of_IN the_DT models_NNS possible_JJ with_IN this_DT scalable_JJ framework_NN :_: informed_VBN priors_NNS to_TO guide_VB topic_NN discovery_NN and_CC extracting_VBG topics_NNS from_IN a_DT multilingual_JJ corpus_NN ._.
We_PRP compare_VBP the_DT scalability_NN of_IN Mr._NNP LDA_NNP against_IN Mahout_NNP ,_, an_DT existing_VBG large_JJ scale_NN topic_NN modeling_NN package_NN ._.
Mr._NNP LDA_NNP out-performs_NNS Mahout_NNP both_CC in_IN execution_NN speed_NN and_CC held-out_JJ likelihood_NN ._.
le_DT document_NN ._.
For_IN the_DT sake_NN of_IN brevity_NN ,_, we_PRP omit_VBP that_IN step_NN here_RB ;_: in_IN later_JJ iterations_NNS ,_, global_JJ parameters_NNS are_VBP stored_VBN in_IN distributed_VBN cache_NN --_: a_DT synchronized_JJ read-only_JJ memory_NN that_WDT is_VBZ shared_VBN among_IN all_DT mappers_NNS =_JJ -_: =[_NN 35_CD -RRB-_-RRB- -_: =_SYM -_: --_: and_CC retrieved_VBD prior_RB to_TO mapper_VB execution_NN in_IN a_DT configuration_NN step_NN ._.
A_DT document_NN is_VBZ represented_VBN as_IN a_DT term_NN frequency_NN sequence_NN w_NN =_JJ ‖_FW w1_FW ,_, w2_NN ,_, ..._: ,_, wV_NN ‖_NN ,_, where_WRB wi_NN is_VBZ the_DT corresponding_JJ term_NN frequency_NN in_IN d_NN
ed_VBN form_NN to_TO compute_VB the_DT expectation_NN with_IN of_IN the_DT topic_NN with_IN respect_NN to_TO the_DT variational_JJ distribution_NN ._.
To_TO improve_VB the_DT normalization_NN step_NN ,_, we_PRP compute_VBP the_DT sum_NN of_IN the_DT λ_NN variational_JJ parameters_NNS in_IN the_DT reducer_NN =_JJ -_: =[_NN 38_CD ,_, 39_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC then_RB emit_VB this_DT sum_NN before_IN we_PRP emit_VBP the_DT other_JJ λ_NN terms_NNS ._.
Although_IN this_DT requires_VBZ O_NN -LRB-_-LRB- V_NN -RRB-_-RRB- additional_JJ memory_NN ,_, it_PRP is_VBZ strictly_RB less_JJR than_IN the_DT memory_NN required_VBN by_IN mappers_NNS ,_, so_IN it_PRP in_IN practice_NN improves_VBZ performan_NN
e_LS highly_RB tuned_VBN specifically_RB for_IN LDA_NNP ,_, which_WDT restricts_VBZ extensions_NNS and_CC enhancements_NNS ,_, one_CD of_IN the_DT key_JJ benefits_NNS of_IN the_DT statistical_JJ approach_NN ._.
The_DT techniques_NNS to_TO improve_VB inference_NN for_IN collapsed_JJ Gibbs_NNP samplers_NNS =_JJ -_: =[_NN 27_CD -RRB-_-RRB- -_: =_SYM -_: typically_RB reduce_VB flexibility_NN ;_: the_DT factorization_NN of_IN the_DT conditional_JJ distribution_NN is_VBZ limited_VBN to_TO LDA_NNP 's_POS explicit_JJ formulation_NN ._.
Adapting_VBG such_JJ tricks_NNS beyond_IN LDA_NNP requires_VBZ repeating_VBG the_DT analysis_NN to_TO refactoriz_NN
el_IN computing_VBG environment_NN ._.
Variational_JJ inference_NN is_VBZ also_RB attractive_JJ for_IN its_PRP$ ability_NN to_TO handle_VB online_NN updates_NNS ._.
Mr._NNP LDA_NNP could_MD be_VB extended_VBN to_TO more_RBR efficiently_RB handle_VB online_NN batches_NNS in_IN streaming_NN inference_NN =_JJ -_: =[_NN 49_CD -RRB-_-RRB- -_: =_JJ -_: ,_, allowing_VBG for_IN even_RB larger_JJR document_NN collections_NNS to_TO be_VB quickly_RB analyzed_VBN and_CC understood_VBN ._.
7_CD ._.
ACKNOWLEDGMENTS_NNS The_DT authors_NNS would_MD like_VB to_TO thank_VB Dr._NNP Jimmy_NNP Lin_NNP for_IN valuable_JJ comments_NNS throughout_IN this_DT project_NN an_DT
apReduce_NN is_VBZ one_CD of_IN the_DT mainstays_NNS of_IN industrial_JJ data_NNS processing_NN and_CC has_VBZ also_RB been_VBN gaining_VBG traction_NN for_IN problems_NNS of_IN interest_NN to_TO the_DT academic_JJ community_NN such_JJ as_IN machine_NN translation_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, language_NN modeling_NN =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC grammar_NN induction_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT parallelized_JJ LDA_NN algorithm_NN in_IN the_DT MapReduce_NNP programming_NN framework_NN -LRB-_-LRB- Mr._NNP LDA_NNP -RRB-_-RRB- ._.
1_CD Mr._NNP LDA_NNP relies_VBZ on_IN variational_JJ inference_NN ,_, as_IN opposed_VBN to_TO the_DT prev_NN
usly_RB unseen_JJ links_NNS between_IN documents_NNS and_CC trends_NNS over_IN time_NN ._.
Although_IN our_PRP$ focus_NN is_VBZ on_IN text_NN data_NNS ,_, LDA_NNP is_VBZ widely_RB used_VBN in_IN computer_NN vision_NN -LRB-_-LRB- 2_CD ,_, 3_CD -RRB-_-RRB- ,_, computational_JJ biology_NN -LRB-_-LRB- 4_CD ,_, 5_CD -RRB-_-RRB- ,_, and_CC computational_JJ linguistics_NNS =_JJ -_: =[_NN 6_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN to_TO being_VBG noisy_JJ ,_, data_NNS from_IN the_DT web_NN are_VBP big_JJ ._.
The_DT MapReduce_NNP framework_NN for_IN large-scale_JJ data_NNS processing_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- is_VBZ simple_JJ to_TO learn_VB but_CC flexible_JJ enough_RB to_TO be_VB broadly_RB applicable_JJ ._.
Designed_VBN at_IN Google_NNP
ed_VBN form_NN to_TO compute_VB the_DT expectation_NN with_IN of_IN the_DT topic_NN with_IN respect_NN to_TO the_DT variational_JJ distribution_NN ._.
To_TO improve_VB the_DT normalization_NN step_NN ,_, we_PRP compute_VBP the_DT sum_NN of_IN the_DT λ_NN variational_JJ parameters_NNS in_IN the_DT reducer_NN =_JJ -_: =[_NN 38_CD ,_, 39_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC then_RB emit_VB this_DT sum_NN before_IN we_PRP emit_VBP the_DT other_JJ λ_NN terms_NNS ._.
Although_IN this_DT requires_VBZ O_NN -LRB-_-LRB- V_NN -RRB-_-RRB- additional_JJ memory_NN ,_, it_PRP is_VBZ strictly_RB less_JJR than_IN the_DT memory_NN required_VBN by_IN mappers_NNS ,_, so_IN it_PRP in_IN practice_NN improves_VBZ performan_NN
nd_IN a_DT distribution_NN over_IN the_DT latent_JJ variables_NNS that_WDT is_VBZ close_JJ to_TO the_DT posterior_NN of_IN interest_NN -LRB-_-LRB- 28_CD ,_, 29_CD -RRB-_-RRB- ._.
Variational_JJ methods_NNS provide_VBP effective_JJ approximations_NNS in_IN topic_NN models_NNS and_CC nonparametric_JJ Bayesian_JJ models_NNS =_JJ -_: =[_NN 30_CD ,_, 31_CD ,_, 32_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP believe_VBP that_IN it_PRP is_VBZ well-suited_JJ to_TO MapReduce_NNP ._.
Variational_JJ methods_NNS enjoy_VBP clear_JJ convergence_NN criterion_NN ,_, tend_VBP to_TO be_VB faster_RBR than_IN MCMC_NNP in_IN high-dimensional_JJ problems_NNS ,_, and_CC provide_VB particular_JJ advantages_NNS ove_VBP
variational_JJ inference_NN ._.
Variational_JJ methods_NNS ,_, based_VBN on_IN techniques_NNS from_IN statistical_JJ physics_NNS ,_, use_NN optimization_NN to_TO find_VB a_DT distribution_NN over_IN the_DT latent_JJ variables_NNS that_WDT is_VBZ close_JJ to_TO the_DT posterior_NN of_IN interest_NN =_JJ -_: =[_NN 28_CD ,_, 29_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Variational_JJ methods_NNS provide_VBP effective_JJ approximations_NNS in_IN topic_NN models_NNS and_CC nonparametric_JJ Bayesian_JJ models_NNS -LRB-_-LRB- 30_CD ,_, 31_CD ,_, 32_CD -RRB-_-RRB- ._.
We_PRP believe_VBP that_IN it_PRP is_VBZ well-suited_JJ to_TO MapReduce_NNP ._.
Variational_JJ methods_NNS enjoy_VBP clear_JJ c_NN
parison_NN of_IN Mr._NNP LDA_NNP against_IN Mahout_NNP LDA_NNP ,_, another_DT large_JJ scale_NN topic_NN modeling_NN implementation_NN based_VBN on_IN variational_JJ inference_NN ._.
We_PRP report_VBP results_NNS on_IN three_CD datasets_NNS :_: •_FW TREC_FW document_NN collection_NN -LRB-_-LRB- disks_NNS 4_CD and_CC 5_CD =_JJ -_: =[_NN 43_CD -RRB-_-RRB- -_: =--RRB-_NN ,_, newswire_NN documents_NNS from_IN the_DT Financial_NNP Times_NNP and_CC LA_NNP Times_NNP ._.
It_PRP contains_VBZ more_JJR than_IN 300k_CD distinct_JJ types_NNS over_IN half_PDT a_DT million_CD documents_NNS ._.
We_PRP remove_VBP types_NNS appearing_VBG fewer_JJR than_IN 20_CD times_NNS ,_, reducing_VBG the_DT vocabu_NN
algorithms_NNS that_WDT are_VBP unsupervised_JJ and_CC scalable_JJ ._.
In_IN this_DT paper_NN ,_, we_PRP present_VBP Mr._NNP LDA_NNP ,_, which_WDT fulfills_VBZ both_DT of_IN these_DT requirements_NNS ._.
Beyond_NNP text_NN ,_, LDA_NN is_VBZ continually_RB being_VBG applied_VBN to_TO new_JJ fields_NNS such_JJ as_IN music_NN =_JJ -_: =[_NN 45_CD -RRB-_-RRB- -_: =_JJ -_: and_CC source_NN code_NN -LRB-_-LRB- 46_CD -RRB-_-RRB- ._.
All_DT of_IN these_DT domains_NNS struggle_VBP with_IN the_DT scale_NN of_IN data_NNS ,_, and_CC Mr._NNP LDA_NNP could_MD help_VB them_PRP better_RBR cope_VB with_IN large_JJ data_NNS ._.
Mr._NNP LDA_NNP represents_VBZ a_DT viable_JJ alternative_NN to_TO the_DT existing_VBG scalable_JJ m_NN
17_CD -RRB-_-RRB- use_NN message_NN passing_VBG to_TO ensure_VB that_IN different_JJ slices_NNS maintain_VBP consistent_JJ counts_NNS ._.
Smola_NNP and_CC Narayanamurthy_NNP -LRB-_-LRB- 18_CD -RRB-_-RRB- use_VBP a_DT distributed_VBN memory_NN system_NN to_TO achieve_VB consistent_JJ counts_NNS in_IN LDA_NNP ,_, and_CC Ahmed_NNP et_FW al._FW =_SYM -_: =[_NN 26_CD -RRB-_-RRB- -_: =_SYM -_: extend_VB the_DT approach_NN more_RBR generally_RB to_TO latent_JJ variable_JJ models_NNS ._.
Gibbs_NNP sampling_NN approaches_VBZ to_TO scaling_VBG thus_RB face_VBP a_DT difficult_JJ dilemma_NN :_: completely_RB synchronize_VB counts_NNS ,_, which_WDT can_MD compromise_VB scaling_NN ,_, or_CC allow_VB
puter_NN vision_NN -LRB-_-LRB- 2_CD ,_, 3_CD -RRB-_-RRB- ,_, computational_JJ biology_NN -LRB-_-LRB- 4_CD ,_, 5_CD -RRB-_-RRB- ,_, and_CC computational_JJ linguistics_NNS -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- ._.
In_IN addition_NN to_TO being_VBG noisy_JJ ,_, data_NNS from_IN the_DT web_NN are_VBP big_JJ ._.
The_DT MapReduce_NNP framework_NN for_IN large-scale_JJ data_NNS processing_NN =_JJ -_: =[_NN 8_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ simple_JJ to_TO learn_VB but_CC flexible_JJ enough_RB to_TO be_VB broadly_RB applicable_JJ ._.
Designed_VBN at_IN Google_NNP and_CC open-sourced_JJ by_IN Yahoo_NNP ,_, Hadoop_NNP MapReduce_NNP is_VBZ one_CD of_IN the_DT mainstays_NNS of_IN industrial_JJ data_NNS processing_NN and_CC has_VBZ also_RB been_VBN
ely_RB used_VBN approximate_JJ inference_NN techniques_NNS for_IN such_JJ models_NNS is_VBZ Markov_NNP chain_NN Monte_NNP Carlo_NNP -LRB-_-LRB- MCMC_NNP -RRB-_-RRB- sampling_NN ,_, where_WRB one_CD samples_NNS from_IN a_DT Markov_NNP chain_NN whose_WP$ stationary_JJ distribution_NN is_VBZ the_DT posterior_NN of_IN interest_NN =_JJ -_: =[_NN 20_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Gibbs_NNP sampling_NN ,_, where_WRB the_DT 1_CD Download_NN the_DT code_NN at_IN http:\/\/mrlda.cc_NN ._.
879WWW_NN 2012_CD --_: Session_NN :_: Information_NNP Extraction_NNP April_NNP 16_CD --_: 20_CD ,_, 2012_CD ,_, Lyon_NNP ,_, France_NNP Likelihood_NNP Asymmetric_NNP Hyperparameter_NNP Informed_NNP Framewo_NNP
s_NN ,_, LDA_NNP also_RB associates_VBZ documents_NNS with_IN these_DT topics_NNS ,_, revealing_VBG previously_RB unseen_JJ links_NNS between_IN documents_NNS and_CC trends_NNS over_IN time_NN ._.
Although_IN our_PRP$ focus_NN is_VBZ on_IN text_NN data_NNS ,_, LDA_NNP is_VBZ widely_RB used_VBN in_IN computer_NN vision_NN =_JJ -_: =[_NN 2_CD ,_, 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, computational_JJ biology_NN -LRB-_-LRB- 4_CD ,_, 5_CD -RRB-_-RRB- ,_, and_CC computational_JJ linguistics_NNS -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- ._.
In_IN addition_NN to_TO being_VBG noisy_JJ ,_, data_NNS from_IN the_DT web_NN are_VBP big_JJ ._.
The_DT MapReduce_NNP framework_NN for_IN large-scale_JJ data_NNS processing_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- is_VBZ simple_JJ to_TO le_FW
es_NNS among_IN variables_NNS to_TO enforce_VB consistency_NN with_IN the_DT computational_JJ constraints_NNS of_IN MapReduce_NNP ._.
Developing_VBG automatic_JJ ways_NNS to_TO enforce_VB those_DT computational_JJ constraints_NNS and_CC then_RB automatically_RB derive_VB inference_NN =_JJ -_: =[_NN 48_CD -RRB-_-RRB- -_: =_JJ -_: would_MD allow_VB for_IN a_DT greater_JJR variety_NN of_IN statistical_JJ models_NNS to_TO be_VB learned_VBN efficiently_RB in_IN a_DT parallel_JJ computing_NN environment_NN ._.
Variational_JJ inference_NN is_VBZ also_RB attractive_JJ for_IN its_PRP$ ability_NN to_TO handle_VB online_NN update_VBP
variational_JJ inference_NN ._.
Variational_JJ methods_NNS ,_, based_VBN on_IN techniques_NNS from_IN statistical_JJ physics_NNS ,_, use_NN optimization_NN to_TO find_VB a_DT distribution_NN over_IN the_DT latent_JJ variables_NNS that_WDT is_VBZ close_JJ to_TO the_DT posterior_NN of_IN interest_NN =_JJ -_: =[_NN 28_CD ,_, 29_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Variational_JJ methods_NNS provide_VBP effective_JJ approximations_NNS in_IN topic_NN models_NNS and_CC nonparametric_JJ Bayesian_JJ models_NNS -LRB-_-LRB- 30_CD ,_, 31_CD ,_, 32_CD -RRB-_-RRB- ._.
We_PRP believe_VBP that_IN it_PRP is_VBZ well-suited_JJ to_TO MapReduce_NNP ._.
Variational_JJ methods_NNS enjoy_VBP clear_JJ c_NN
It_PRP contains_VBZ more_JJR than_IN 300k_CD distinct_JJ types_NNS over_IN half_PDT a_DT million_CD documents_NNS ._.
We_PRP remove_VBP types_NNS appearing_VBG fewer_JJR than_IN 20_CD times_NNS ,_, reducing_VBG the_DT vocabulary_NN size_NN to_TO approximately_RB 60k_JJ ._.
•_NNP The_NNP BlogAuthorship_NNP corpus_NN =_JJ -_: =[_NN 44_CD -RRB-_-RRB- -_: =_JJ -_: ,_, which_WDT contains_VBZ about_IN 10_CD million_CD blog_NN posts_NNS from_IN American_JJ users_NNS ._.
In_IN contrast_NN to_TO the_DT newswire-heavy_JJ TREC_NN corpus_NN ,_, the_DT BlogAuthorship_NN corpus_NN is_VBZ more_RBR personal_JJ and_CC informal_JJ ._.
Again_RB ,_, terms_NNS in_IN fewer_JJR than_IN 20_CD
hreaded_JJ implementations_NNS ._.
-LRB-_-LRB- ∼_SYM -_: not_RB available_JJ from_IN available_JJ documentation_NN ._. -RRB-_-RRB-
Markov_NNP chain_NN is_VBZ defined_VBN by_IN the_DT conditional_JJ distribution_NN of_IN each_DT latent_JJ variable_NN ,_, has_VBZ found_VBN widespread_JJ use_NN in_IN Bayesian_JJ models_NNS =_JJ -_: =[_NN 20_CD ,_, 22_CD ,_, 23_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
MCMC_NN is_VBZ a_DT powerful_JJ methodology_NN ,_, but_CC it_PRP has_VBZ drawbacks_NNS ._.
Convergence_NN of_IN the_DT sampler_NN to_TO its_PRP$ stationary_JJ distribution_NN is_VBZ difficult_JJ to_TO diagnose_VB ,_, and_CC sampling_NN algorithms_NNS can_MD be_VB slow_JJ to_TO converge_VB in_IN high_JJ dime_NN
ts_NNS with_IN these_DT topics_NNS ,_, revealing_VBG previously_RB unseen_JJ links_NNS between_IN documents_NNS and_CC trends_NNS over_IN time_NN ._.
Although_IN our_PRP$ focus_NN is_VBZ on_IN text_NN data_NNS ,_, LDA_NNP is_VBZ widely_RB used_VBN in_IN computer_NN vision_NN -LRB-_-LRB- 2_CD ,_, 3_CD -RRB-_-RRB- ,_, computational_JJ biology_NN =_JJ -_: =[_NN 4_CD ,_, 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC computational_JJ linguistics_NNS -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- ._.
In_IN addition_NN to_TO being_VBG noisy_JJ ,_, data_NNS from_IN the_DT web_NN are_VBP big_JJ ._.
The_DT MapReduce_NNP framework_NN for_IN large-scale_JJ data_NNS processing_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- is_VBZ simple_JJ to_TO learn_VB but_CC flexible_JJ enough_RB to_TO be_VB
ts_NNS with_IN these_DT topics_NNS ,_, revealing_VBG previously_RB unseen_JJ links_NNS between_IN documents_NNS and_CC trends_NNS over_IN time_NN ._.
Although_IN our_PRP$ focus_NN is_VBZ on_IN text_NN data_NNS ,_, LDA_NNP is_VBZ widely_RB used_VBN in_IN computer_NN vision_NN -LRB-_-LRB- 2_CD ,_, 3_CD -RRB-_-RRB- ,_, computational_JJ biology_NN =_JJ -_: =[_NN 4_CD ,_, 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC computational_JJ linguistics_NNS -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- ._.
In_IN addition_NN to_TO being_VBG noisy_JJ ,_, data_NNS from_IN the_DT web_NN are_VBP big_JJ ._.
The_DT MapReduce_NNP framework_NN for_IN large-scale_JJ data_NNS processing_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- is_VBZ simple_JJ to_TO learn_VB but_CC flexible_JJ enough_RB to_TO be_VB
hreaded_JJ implementations_NNS ._.
-LRB-_-LRB- ∼_SYM -_: not_RB available_JJ from_IN available_JJ documentation_NN ._. -RRB-_-RRB-
Markov_NNP chain_NN is_VBZ defined_VBN by_IN the_DT conditional_JJ distribution_NN of_IN each_DT latent_JJ variable_NN ,_, has_VBZ found_VBN widespread_JJ use_NN in_IN Bayesian_JJ models_NNS =_JJ -_: =[_NN 20_CD ,_, 22_CD ,_, 23_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
MCMC_NN is_VBZ a_DT powerful_JJ methodology_NN ,_, but_CC it_PRP has_VBZ drawbacks_NNS ._.
Convergence_NN of_IN the_DT sampler_NN to_TO its_PRP$ stationary_JJ distribution_NN is_VBZ difficult_JJ to_TO diagnose_VB ,_, and_CC sampling_NN algorithms_NNS can_MD be_VB slow_JJ to_TO converge_VB in_IN high_JJ dime_NN
nd_IN a_DT distribution_NN over_IN the_DT latent_JJ variables_NNS that_WDT is_VBZ close_JJ to_TO the_DT posterior_NN of_IN interest_NN -LRB-_-LRB- 28_CD ,_, 29_CD -RRB-_-RRB- ._.
Variational_JJ methods_NNS provide_VBP effective_JJ approximations_NNS in_IN topic_NN models_NNS and_CC nonparametric_JJ Bayesian_JJ models_NNS =_JJ -_: =[_NN 30_CD ,_, 31_CD ,_, 32_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP believe_VBP that_IN it_PRP is_VBZ well-suited_JJ to_TO MapReduce_NNP ._.
Variational_JJ methods_NNS enjoy_VBP clear_JJ convergence_NN criterion_NN ,_, tend_VBP to_TO be_VB faster_RBR than_IN MCMC_NNP in_IN high-dimensional_JJ problems_NNS ,_, and_CC provide_VB particular_JJ advantages_NNS ove_VBP
hat_NN scale_NN out_RB collapsed_VBD Gibbs_NNP sampling_NN for_IN LDA_NNP ,_, the_DT major_JJ challenge_NN is_VBZ keeping_VBG these_DT second_JJ counts_NNS for_IN collapsed_JJ Gibbs_NNP sampling_NN consistent_JJ when_WRB there_EX is_VBZ not_RB a_DT shared_JJ memory_NN environment_NN ._.
Newman_NNP et_FW al._FW =_SYM -_: =[_NN 25_CD -RRB-_-RRB- -_: =_SYM -_: consider_VB a_DT variety_NN of_IN methods_NNS to_TO achieve_VB consistent_JJ counts_NNS :_: creating_VBG hierarchical_JJ models_NNS to_TO view_VB each_DT slice_NN as_IN independent_JJ or_CC simply_RB syncing_VBG counts_NNS in_IN a_DT batch_NN update_VB ._.
Yan_NNP et_FW al._FW -LRB-_-LRB- 14_CD -RRB-_-RRB- first_RB cleverly_RB p_NN
hreaded_JJ implementations_NNS ._.
-LRB-_-LRB- ∼_SYM -_: not_RB available_JJ from_IN available_JJ documentation_NN ._. -RRB-_-RRB-
Markov_NNP chain_NN is_VBZ defined_VBN by_IN the_DT conditional_JJ distribution_NN of_IN each_DT latent_JJ variable_NN ,_, has_VBZ found_VBN widespread_JJ use_NN in_IN Bayesian_JJ models_NNS =_JJ -_: =[_NN 20_CD ,_, 22_CD ,_, 23_CD ,_, 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
MCMC_NN is_VBZ a_DT powerful_JJ methodology_NN ,_, but_CC it_PRP has_VBZ drawbacks_NNS ._.
Convergence_NN of_IN the_DT sampler_NN to_TO its_PRP$ stationary_JJ distribution_NN is_VBZ difficult_JJ to_TO diagnose_VB ,_, and_CC sampling_NN algorithms_NNS can_MD be_VB slow_JJ to_TO converge_VB in_IN high_JJ dime_NN
LDA_NN ._.
While_IN we_PRP only_RB discuss_VBP two_CD extensions_NNS here_RB ,_, 884WWW_NN 2012_CD --_: Session_NN :_: Information_NNP Extraction_NNP April_NNP 16_CD --_: 20_CD ,_, 2012_CD ,_, Lyon_NNP ,_, France_NNP other_JJ extensions_NNS are_VBP possible_JJ ._.
For_IN example_NN ,_, implementing_VBG supervised_JJ LDA_NN =_JJ -_: =[_NN 41_CD -RRB-_-RRB- -_: =_SYM -_: only_RB requires_VBZ changing_VBG the_DT computation_NN of_IN φ_NN and_CC a_DT regression_NN ;_: the_DT rest_NN of_IN the_DT model_NN is_VBZ unchanged_JJ ._.
Implementing_VBG syntactic_JJ topic_NN models_NNS -LRB-_-LRB- 42_CD -RRB-_-RRB- requires_VBZ changing_VBG the_DT mapper_NN to_TO incorporate_VB syntactic_JJ depend_VB
nd_IN a_DT distribution_NN over_IN the_DT latent_JJ variables_NNS that_WDT is_VBZ close_JJ to_TO the_DT posterior_NN of_IN interest_NN -LRB-_-LRB- 28_CD ,_, 29_CD -RRB-_-RRB- ._.
Variational_JJ methods_NNS provide_VBP effective_JJ approximations_NNS in_IN topic_NN models_NNS and_CC nonparametric_JJ Bayesian_JJ models_NNS =_JJ -_: =[_NN 30_CD ,_, 31_CD ,_, 32_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP believe_VBP that_IN it_PRP is_VBZ well-suited_JJ to_TO MapReduce_NNP ._.
Variational_JJ methods_NNS enjoy_VBP clear_JJ convergence_NN criterion_NN ,_, tend_VBP to_TO be_VB faster_RBR than_IN MCMC_NNP in_IN high-dimensional_JJ problems_NNS ,_, and_CC provide_VB particular_JJ advantages_NNS ove_VBP
der_NN class_NN of_IN models_NNS than_IN could_MD be_VB built_VBN with_IN Gibbs_NNP samplers_NNS alone_RB ._.
Mr._NNP LDA_NNP ,_, however_RB ,_, would_MD benefit_VB from_IN many_JJ of_IN the_DT efficient_JJ ,_, scalable_JJ data_NNS structures_NNS that_WDT improved_VBD other_JJ scalable_JJ statistical_JJ models_NNS =_JJ -_: =[_NN 47_CD -RRB-_-RRB- -_: =_JJ -_: ;_: incorporating_VBG these_DT insights_NNS would_MD further_RB improve_VB performance_NN and_CC scalability_NN ._.
While_IN we_PRP focused_VBD on_IN LDA_NNP ,_, the_DT approaches_NNS used_VBN here_RB are_VBP applicable_JJ to_TO many_JJ other_JJ models_NNS ._.
Variational_JJ inference_NN is_VBZ an_DT att_NN
unsupervised_JJ and_CC scalable_JJ ._.
In_IN this_DT paper_NN ,_, we_PRP present_VBP Mr._NNP LDA_NNP ,_, which_WDT fulfills_VBZ both_DT of_IN these_DT requirements_NNS ._.
Beyond_NNP text_NN ,_, LDA_NN is_VBZ continually_RB being_VBG applied_VBN to_TO new_JJ fields_NNS such_JJ as_IN music_NN -LRB-_-LRB- 45_CD -RRB-_-RRB- and_CC source_NN code_NN =_JJ -_: =[_NN 46_CD -RRB-_-RRB- -_: =_SYM -_: ._.
All_DT of_IN these_DT domains_NNS struggle_VBP with_IN the_DT scale_NN of_IN data_NNS ,_, and_CC Mr._NNP LDA_NNP could_MD help_VB them_PRP better_RBR cope_VB with_IN large_JJ data_NNS ._.
Mr._NNP LDA_NNP represents_VBZ a_DT viable_JJ alternative_NN to_TO the_DT existing_VBG scalable_JJ mechanisms_NNS for_IN inferen_NN
usly_RB unseen_JJ links_NNS between_IN documents_NNS and_CC trends_NNS over_IN time_NN ._.
Although_IN our_PRP$ focus_NN is_VBZ on_IN text_NN data_NNS ,_, LDA_NNP is_VBZ widely_RB used_VBN in_IN computer_NN vision_NN -LRB-_-LRB- 2_CD ,_, 3_CD -RRB-_-RRB- ,_, computational_JJ biology_NN -LRB-_-LRB- 4_CD ,_, 5_CD -RRB-_-RRB- ,_, and_CC computational_JJ linguistics_NNS =_JJ -_: =[_NN 6_CD ,_, 7_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN addition_NN to_TO being_VBG noisy_JJ ,_, data_NNS from_IN the_DT web_NN are_VBP big_JJ ._.
The_DT MapReduce_NNP framework_NN for_IN large-scale_JJ data_NNS processing_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- is_VBZ simple_JJ to_TO learn_VB but_CC flexible_JJ enough_RB to_TO be_VB broadly_RB applicable_JJ ._.
Designed_VBN at_IN Google_NNP
s_NN independence_NN can_MD be_VB engineered_VBN to_TO allow_VB parallelization_NN of_IN independent_JJ components_NNS across_IN multiple_JJ computers_NNS ._.
Maximizing_VBG the_DT global_JJ parameters_NNS in_IN MapReduce_NNP can_MD be_VB handled_VBN in_IN a_DT manner_NN analogous_JJ to_TO EM_NN =_JJ -_: =[_NN 33_CD -RRB-_-RRB- -_: =_JJ -_: ;_: the_DT expected_VBN counts_NNS -LRB-_-LRB- of_IN the_DT variational_JJ distribution_NN -RRB-_-RRB- generated_VBN in_IN many_JJ parallel_JJ jobs_NNS are_VBP efficiently_RB aggregated_VBN and_CC used_VBN to_TO recompute_VB the_DT top-level_JJ parameters_NNS ._.
2.3_CD Related_NNP Work_NNP Nallapati_NNP ,_, Cohen_NNP and_CC
cs_IN K_NNP even_RB for_IN a_DT large_JJ document_NN collection_NN ._.
As_IN a_DT result_NN ,_, we_PRP can_MD safely_RB assume_VB the_DT dimensionality_NN of_IN α_NN ,_, H_NN ,_, and_CC g_NN are_VBP reasonably_RB low_JJ ,_, and_CC additional_JJ gains_NNS come_VBP from_IN the_DT diagonal_JJ structure_NN of_IN the_DT Hessian_JJ =_JJ -_: =[_NN 37_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Hence_RB ,_, the_DT updating_VBG of_IN α_NN is_VBZ efficient_JJ and_CC will_MD not_RB create_VB a_DT bottleneck_NN in_IN the_DT driver_NN ._.
3.5_CD Likelihood_NNP Computation_NNP The_DT driver_NN monitors_VBZ the_DT ELBO_NN to_TO determine_VB whether_IN inference_NN has_VBZ converged_VBN ._.
If_IN not_RB ,_, it_PRP
nt_NN collections_NNS is_VBZ topic_NN modeling_NN ,_, which_WDT discovers_VBZ the_DT themes_NNS that_WDT permeate_VBP a_DT corpus_NN ._.
Topic_NN modeling_NN is_VBZ exemplified_VBN by_IN Latent_JJ Dirichlet_NNP Allocation_NNP -LRB-_-LRB- LDA_NNP -RRB-_-RRB- ,_, a_DT generative_JJ model_NN for_IN documentcentric_JJ corpora_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
It_PRP is_VBZ appealing_JJ for_IN noisy_JJ data_NNS because_IN it_PRP requires_VBZ no_DT annotation_NN and_CC discovers_VBZ ,_, without_IN any_DT supervision_NN ,_, the_DT thematic_JJ trends_NNS in_IN a_DT corpus_NN ._.
In_IN addition_NN to_TO discovering_VBG which_WDT topics_NNS exist_VBP in_IN a_DT Copyright_NN
rced_VBN by_IN Yahoo_NNP ,_, Hadoop_NNP MapReduce_NNP is_VBZ one_CD of_IN the_DT mainstays_NNS of_IN industrial_JJ data_NNS processing_NN and_CC has_VBZ also_RB been_VBN gaining_VBG traction_NN for_IN problems_NNS of_IN interest_NN to_TO the_DT academic_JJ community_NN such_JJ as_IN machine_NN translation_NN =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_JJ -_: ,_, language_NN modeling_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, and_CC grammar_NN induction_NN -LRB-_-LRB- 11_CD -RRB-_-RRB- ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT parallelized_JJ LDA_NN algorithm_NN in_IN the_DT MapReduce_NNP programming_NN framework_NN -LRB-_-LRB- Mr._NNP LDA_NNP -RRB-_-RRB- ._.
1_CD Mr._NNP LDA_NNP relies_VBZ on_IN variational_JJ inference_NN
sions_NNS are_VBP possible_JJ ._.
For_IN example_NN ,_, implementing_VBG supervised_JJ LDA_NN -LRB-_-LRB- 41_CD -RRB-_-RRB- only_RB requires_VBZ changing_VBG the_DT computation_NN of_IN φ_NN and_CC a_DT regression_NN ;_: the_DT rest_NN of_IN the_DT model_NN is_VBZ unchanged_JJ ._.
Implementing_VBG syntactic_JJ topic_NN models_NNS =_JJ -_: =[_NN 42_CD -RRB-_-RRB- -_: =_SYM -_: requires_VBZ changing_VBG the_DT mapper_NN to_TO incorporate_VB syntactic_JJ dependencies_NNS ._.
5_CD ._.
EXPERIMENTS_NNS We_PRP implemented_VBD Mr._NNP LDA_NNP using_VBG Java_NNP with_IN Hadoop_FW 0.20.1_FW and_CC ran_VBD all_DT experiments_NNS on_IN a_DT cluster_NN containing_VBG 16_CD physical_JJ nod_NN
ely_RB used_VBN approximate_JJ inference_NN techniques_NNS for_IN such_JJ models_NNS is_VBZ Markov_NNP chain_NN Monte_NNP Carlo_NNP -LRB-_-LRB- MCMC_NNP -RRB-_-RRB- sampling_NN ,_, where_WRB one_CD samples_NNS from_IN a_DT Markov_NNP chain_NN whose_WP$ stationary_JJ distribution_NN is_VBZ the_DT posterior_NN of_IN interest_NN =_JJ -_: =[_NN 20_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Gibbs_NNP sampling_NN ,_, where_WRB the_DT 1_CD Download_NN the_DT code_NN at_IN http:\/\/mrlda.cc_NN ._.
879WWW_NN 2012_CD --_: Session_NN :_: Information_NNP Extraction_NNP April_NNP 16_CD --_: 20_CD ,_, 2012_CD ,_, Lyon_NNP ,_, France_NNP Likelihood_NNP Asymmetric_NNP Hyperparameter_NNP Informed_NNP Framewo_NNP
ions_NNS of_IN LDA_NNP to_TO demonstrate_VB the_DT flexibility_NN of_IN the_DT proposed_VBN framework_NN ._.
These_DT are_VBP an_DT informed_VBN prior_RB to_TO guide_VB topic_NN discovery_NN and_CC a_DT new_JJ inference_NN technique_NN for_IN discovering_VBG topics_NNS in_IN multilingual_JJ corpora_NN =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Next_RB ,_, we_PRP evaluate_VBP Mr._NNP LDA_NNP 's_POS ability_NN to_TO scale_VB in_IN Section_NNP 5_CD before_IN concluding_VBG with_IN Section_NN 6_CD ._.
2_CD ._.
SCALING_FW OUT_FW LDA_NN In_IN practice_NN ,_, probabilistic_JJ models_NNS work_VBP by_IN maximizing_VBG the_DT loglikelihood_NN of_IN observed_VBN data_NNS
lementations_NNS such_JJ as_IN supplying_VBG per-document_JJ topic_NN distributions_NNS and_CC optimizing_VBG hyperparameters_NNS -LRB-_-LRB- for_IN an_DT explanation_NN of_IN why_WRB this_DT is_VBZ essential_JJ for_IN model_NN quality_NN ,_, see_VBP Wallach_NNP et_FW al._FW 's_POS ``_`` Why_WRB Priors_NNP Matter_NNP ''_'' =_SYM -_: =[_NN 34_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Without_IN perdocument_NN topic_NN distributions_NNS ,_, many_JJ of_IN the_DT downstream_JJ applications_NNS of_IN LDA_NN -LRB-_-LRB- e.g._FW document_NN clustering_NN -RRB-_-RRB- become_VBP more_RBR difficult_JJ ._.
Table_NNP 1_CD provides_VBZ a_DT general_JJ overview_NN and_CC comparison_NN of_IN features_NNS
tays_NNS of_IN industrial_JJ data_NNS processing_NN and_CC has_VBZ also_RB been_VBN gaining_VBG traction_NN for_IN problems_NNS of_IN interest_NN to_TO the_DT academic_JJ community_NN such_JJ as_IN machine_NN translation_NN -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, language_NN modeling_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- ,_, and_CC grammar_NN induction_NN =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN this_DT paper_NN ,_, we_PRP propose_VBP a_DT parallelized_JJ LDA_NN algorithm_NN in_IN the_DT MapReduce_NNP programming_NN framework_NN -LRB-_-LRB- Mr._NNP LDA_NNP -RRB-_-RRB- ._.
1_CD Mr._NNP LDA_NNP relies_VBZ on_IN variational_JJ inference_NN ,_, as_IN opposed_VBN to_TO the_DT prevailing_JJ trend_NN of_IN using_VBG Gibbs_NNP
s_NN ,_, LDA_NNP also_RB associates_VBZ documents_NNS with_IN these_DT topics_NNS ,_, revealing_VBG previously_RB unseen_JJ links_NNS between_IN documents_NNS and_CC trends_NNS over_IN time_NN ._.
Although_IN our_PRP$ focus_NN is_VBZ on_IN text_NN data_NNS ,_, LDA_NNP is_VBZ widely_RB used_VBN in_IN computer_NN vision_NN =_JJ -_: =[_NN 2_CD ,_, 3_CD -RRB-_-RRB- -_: =_JJ -_: ,_, computational_JJ biology_NN -LRB-_-LRB- 4_CD ,_, 5_CD -RRB-_-RRB- ,_, and_CC computational_JJ linguistics_NNS -LRB-_-LRB- 6_CD ,_, 7_CD -RRB-_-RRB- ._.
In_IN addition_NN to_TO being_VBG noisy_JJ ,_, data_NNS from_IN the_DT web_NN are_VBP big_JJ ._.
The_DT MapReduce_NNP framework_NN for_IN large-scale_JJ data_NNS processing_NN -LRB-_-LRB- 8_CD -RRB-_-RRB- is_VBZ simple_JJ to_TO le_FW
important_JJ from_IN the_DT perspective_NN of_IN equalizing_VBG differences_NNS between_IN inference_NN techniques_NNS ;_: as_RB long_RB as_IN hyperparameters_NNS are_VBP optimized_VBN ,_, there_EX is_VBZ little_JJ difference_NN between_IN the_DT output_NN of_IN inference_NN techniques_NNS =_JJ -_: =[_NN 36_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT driver_NN program_NN marshals_VBZ the_DT entire_JJ inference_NN process_NN ._.
On_IN the_DT first_JJ iteration_NN ,_, the_DT driver_NN is_VBZ responsible_JJ for_IN initializing_VBG all_PDT the_DT model_NN parameters_NNS -LRB-_-LRB- K_NN ,_, V_NN ,_, C_NN ,_, η_NN ,_, α_NN -RRB-_-RRB- ;_: the_DT number_NN of_IN topics_NNS K_NN is_VBZ user_NN
