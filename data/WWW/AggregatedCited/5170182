Performance_NNP of_IN compressed_VBN inverted_JJ list_NN caching_NN in_IN search_NN engines_NNS
Due_JJ to_TO the_DT rapid_JJ growth_NN in_IN the_DT size_NN of_IN the_DT web_NN ,_, web_NN search_NN engines_NNS are_VBP facing_VBG enormous_JJ performance_NN challenges_NNS ._.
The_DT larger_JJR engines_NNS in_IN particular_JJ have_VBP to_TO be_VB able_JJ to_TO process_VB tens_NNS of_IN thousands_NNS of_IN queries_NNS per_IN second_NN on_IN tens_NNS of_IN billions_NNS of_IN documents_NNS ,_, making_VBG query_JJ throughput_NN a_DT critical_JJ issue_NN ._.
To_TO satisfy_VB this_DT heavy_JJ workload_NN ,_, search_NN engines_NNS use_VBP a_DT variety_NN of_IN performance_NN optimizations_NNS including_VBG index_NN compression_NN ,_, caching_NN ,_, and_CC early_JJ termination_NN ._.
We_PRP focus_VBP on_IN two_CD techniques_NNS ,_, inverted_JJ index_NN compression_NN and_CC index_NN caching_NN ,_, which_WDT play_VBP a_DT crucial_JJ rule_NN in_IN web_NN search_NN engines_NNS as_RB well_RB as_IN other_JJ high-performance_JJ information_NN retrieval_NN systems_NNS ._.
We_PRP perform_VBP a_DT comparison_NN and_CC evaluation_NN of_IN several_JJ inverted_JJ list_NN compression_NN algorithms_NNS ,_, including_VBG new_JJ variants_NNS of_IN existing_VBG algorithms_NNS that_WDT have_VBP not_RB been_VBN studied_VBN before_RB ._.
We_PRP then_RB evaluate_VBP different_JJ inverted_JJ list_NN caching_NN policies_NNS on_IN large_JJ query_NN traces_NNS ,_, and_CC finally_RB study_VB the_DT possible_JJ performance_NN benefits_NNS of_IN combining_VBG compression_NN and_CC caching_NN ._.
The_DT overall_JJ goal_NN of_IN this_DT paper_NN is_VBZ to_TO provide_VB an_DT updated_VBN discussion_NN and_CC evaluation_NN of_IN these_DT two_CD techniques_NNS ,_, and_CC to_TO show_VB how_WRB to_TO select_VB the_DT best_JJS set_NN of_IN approaches_NNS and_CC settings_NNS depending_VBG on_IN parameter_NN such_JJ as_IN disk_NN speed_NN and_CC main_JJ memory_NN cache_NN size_NN ._.
512_CD ,_, we_PRP can_MD store_VB them_PRP as_IN 3_CD 9-bit_JJ values_NNS -LRB-_-LRB- leaving_VBG one_CD data_NN bit_NN unused_JJ -RRB-_-RRB- ._.
Simple16_NN is_VBZ a_DT variation_NN of_IN Simple9_NN that_WDT uses_VBZ 16_CD instead_RB of_IN 9_CD cases_NNS and_CC thus_RB achieves_VBZ a_DT slightly_RB better_JJR use_NN of_IN each_DT 32-bit_JJ word_NN =_JJ -_: =[_NN 29_CD -RRB-_-RRB- -_: =_SYM -_: ._.
PForDelta_NN is_VBZ a_DT recent_JJ technique_NN proposed_VBN in_IN -LRB-_-LRB- 13_CD ,_, 31_CD -RRB-_-RRB- for_IN compression_NN in_IN database_NN and_CC IR_NN systems_NNS ._.
The_DT basic_JJ idea_NN is_VBZ to_TO split_VB a_DT list_NN into_IN chunks_NNS of_IN some_DT fixed_JJ size_NN ,_, and_CC then_RB select_VB a_DT value_NN b_NN such_JJ that_IN
IR_NNP was_VBD that_IN they_PRP easily_RB support_VBP integrating_VBG new_JJ fields_NNS required_VBN to_TO support_VB ranked_VBN queries_NNS ._.
The_DT switch_NN from_IN bitmap_NN indexes_NNS to_TO inverted_JJ indexes_NNS lead_VBP to_TO a_DT flood_NN of_IN research_NN on_IN efficient_JJ inverted_JJ indexes_NNS =_JJ -_: =[_NN 25_CD ,_, 30_CD ,_, 6_CD ,_, 13_CD ,_, 24_CD ,_, 3_CD ,_, 31_CD ,_, 29_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC inverted_JJ indexes_NNS are_VBP now_RB the_DT preferred_JJ indexing_NN method_NN in_IN search_NN engines_NNS -LRB-_-LRB- 30_CD -RRB-_-RRB- ._.
In_IN this_DT paper_NN ,_, we_PRP are_VBP asking_VBG -LRB-_-LRB- and_CC answering_NN -RRB-_-RRB- the_DT question_NN :_: What_WP are_VBP the_DT trade-offs_NNS of_IN using_VBG inverted_JJ indexes_NNS in_IN DSS_NN
n_NN main_JJ memory_NN ._.
In_IN the_DT following_NN ,_, we_PRP review_VBP the_DT current_JJ methods_NNS that_IN ,_, from_IN our_PRP$ point_NN of_IN view_NN ,_, best_JJS address_NN these_DT objectives_NNS ,_, and_CC discuss_VBP their_PRP$ limitations_NNS ._.
In_IN particular_JJ ,_, we_PRP evaluate_VBP index_NN compression_NN =_JJ -_: =[_NN 15_CD ,_, 16_CD -RRB-_-RRB- -_: =_JJ -_: ,_, an_DT approach_NN that_WDT has_VBZ never_RB been_VBN considered_VBN in_IN large-scale_JJ image_NN search_NN ._.
3.1_CD ._.
Binary_JJ BOF_NN A_NN straightforward_JJ way_NN of_IN compacting_VBG a_DT BOF_NN vector_NN is_VBZ to_TO use_VB a_DT binary_JJ BOF_NN representation_NN ,_, i.e._FW ,_, to_TO discard_VB the_DT i_FW
uencies_NNS are_VBP often_RB stored_VBN separately_RB ,_, and_CC thus_RB we_PRP compress_VBP sequences_NNS of_IN d-gaps_NN and_CC frequencies_NNS ._.
A_DT large_JJ number_NN of_IN inverted_JJ index_NN compression_NN techniques_NNS have_VBP been_VBN proposed_VBN ;_: see_VB -LRB-_-LRB- 32_CD -RRB-_-RRB- for_IN an_DT overview_NN and_CC =_JJ -_: =[_NN 30_CD -RRB-_-RRB- -_: =_SYM -_: for_IN a_DT recent_JJ experimental_JJ evaluation_NN of_IN state-of-the-art_JJ methods_NNS ._.
In_IN our_PRP$ work_NN here_RB ,_, we_PRP utilize_VBP two_CD techniques_NNS that_WDT have_VBP been_VBN previously_RB applied_VBN to_TO versioned_RB document_VB collections_NNS -LRB-_-LRB- 11_CD -RRB-_-RRB- ,_, Interpolative_JJ C_NN
effect_NN on_IN query_NN processing_NN throughput_NN ._.
In_IN addition_NN to_TO the_DT direct_JJ reduction_NN in_IN memory_NN and_CC disk_NN space_NN ,_, more_RBR compact_JJ indexes_NNS lead_VBP to_TO savings_NNS in_IN data_NN transfers_NNS and_CC increase_VB the_DT hit_NN rate_NN of_IN memory_NN caches_NNS =_JJ -_: =[_NN 22_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Therefore_RB ,_, a_DT large_JJ body_NN of_IN work_NN has_VBZ focused_VBN on_IN index_NN compaction_NN and_CC compression_NN methods_NNS ._.
The_DT structure_NN described_VBN above_IN leaves_NNS two_CD main_JJ degrees_NNS of_IN freedom_NN for_IN compression_NN optimization_NN ._.
The_DT first_JJ is_VBZ t_NN
docIDs_NN and_CC frequencies_NNS but_CC do_VBP not_RB consider_VB other_JJ data_NNS such_JJ as_IN positions_NNS or_CC contexts_NNS ._.
Many_JJ techniques_NNS for_IN inverted_JJ index_NN compression_NN have_VBP been_VBN studied_VBN in_IN the_DT literature_NN ;_: see_VB -LRB-_-LRB- 26_CD ,_, 29_CD -RRB-_-RRB- for_IN a_DT survey_NN and_CC =_JJ -_: =[_NN 1_CD ,_, 2_CD ,_, 3_CD ,_, 30_CD ,_, 27_CD ,_, 14_CD -RRB-_-RRB- -_: =_SYM -_: for_IN very_RB recent_JJ work_NN ._.
Most_JJS techniques_NNS first_RB replace_VB each_DT docID_NN -LRB-_-LRB- except_IN the_DT first_JJ in_IN a_DT list_NN -RRB-_-RRB- by_IN the_DT difference_NN between_IN it_PRP and_CC the_DT preceding_VBG docID_NN ,_, called_VBN d-gap_NN ,_, and_CC then_RB encode_VBP the_DT d-gap_NN using_VBG some_DT in_IN
ance_NN ._.
2_CD ._.
BACKGROUND_NN AND_CC RELATED_NNS WORK_VBP For_IN a_DT basic_JJ overview_NN of_IN IR_FW query_FW processing_NN ,_, see_VB -LRB-_-LRB- 24_CD -RRB-_-RRB- ._.
For_IN recent_JJ work_NN on_IN performance_NN optimizations_NNS such_JJ as_IN index_NN compression_NN ,_, caching_NN ,_, and_CC early_JJ termination_NN ,_, see_VBP =_JJ -_: =[_NN 2_CD ,_, 23_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP assume_VBP that_IN we_PRP are_VBP given_VBN a_DT collection_NN of_IN N_NN documents_NNS -LRB-_-LRB- web_NN pages_NNS covered_VBN by_IN the_DT search_NN engine_NN -RRB-_-RRB- ,_, where_WRB each_DT document_NN is_VBZ uniquely_RB identified_VBN by_IN a_DT document_NN ID_NN -LRB-_-LRB- docID_NN -RRB-_-RRB- between_IN 0_CD and_CC N_NN −_NN 1_CD ._.
The_DT collect_VBP
counts_NNS of_IN how_WRB many_JJ times_NNS each_DT cache_NN item_NN was_VBD hit_VBN in_IN the_DT past_NN and_CC evicts_VBZ the_DT least_JJS frequently_RB used_VBN item_NN when_WRB needed_VBN ._.
Several_JJ variants_NNS of_IN lfu_NN have_VBP been_VBN proposed_VBN to_TO deal_VB with_IN its_PRP$ different_JJ shortcomings_NNS =_JJ -_: =[_NN 1_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
For_IN example_NN ,_, it_PRP may_MD happen_VB in_IN lfu_NN that_IN certain_JJ items_NNS occur_VBP in_IN a_DT burst_NN to_TO accumulate_VB such_JJ high_JJ frequency_NN counts_VBZ that_IN they_PRP never_RB get_VBP evicted_VBN from_IN the_DT cache_NN ._.
Window-lfu_JJ deals_NNS with_IN this_DT by_IN counting_VBG the_DT
trieval_NN ._.
Query_NN processing_NN in_IN search_NN engines_NNS has_VBZ been_VBN extensively_RB studied_VBN ;_: for_IN a_DT basic_JJ overview_NN ,_, see_VB -LRB-_-LRB- 37_CD ,_, 6_CD -RRB-_-RRB- ._.
For_IN recent_JJ research_NN on_IN performance_NN optimization_NN such_JJ as_IN index_NN compression_NN and_CC pruning_NN ,_, see_VBP =_JJ -_: =[_NN 26_CD ,_, 36_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP assume_VBP we_PRP are_VBP given_VBN a_DT collection_NN of_IN N_NN documents_NNS ,_, where_WRB each_DT document_NN is_VBZ uniquely_RB identified_VBN by_IN a_DT document_NN ID_NN -LRB-_-LRB- docID_NN -RRB-_-RRB- between_IN 0_CD and_CC N_NN −_NN 1_CD ._.
The_DT collection_NN is_VBZ indexed_VBN by_IN an_DT inverted_JJ index_NN structure_NN ,_,
or_CC current_JJ search_NN systems_NNS ,_, which_WDT usually_RB cache_NN at_IN least_JJS some_DT of_IN the_DT inverted_JJ lists_NNS in_IN main_JJ memory_NN even_RB when_WRB the_DT complete_JJ index_NN does_VBZ not_RB fit_VB ._.
Caching_NN is_VBZ known_VBN to_TO provide_VB significant_JJ performance_NN boosts_VBZ =_JJ -_: =[_NN 24_CD -RRB-_-RRB- -_: =_SYM -_: ._.
To_TO determine_VB the_DT impact_NN of_IN caching_NN ,_, we_PRP implemented_VBD a_DT simple_JJ caching_NN scheme_NN that_WDT selects_VBZ a_DT static_JJ subset_NN of_IN lists_NNS to_TO be_VB kept_VBN in_IN memory_NN ._.
Lists_NNS are_VBP selected_VBN based_VBN on_IN their_PRP$ frequency_NN in_IN a_DT large_JJ query_NN tr_NN
and_CC text_NN information_NN retrieval_NN systems_NNS are_VBP based_VBN on_IN inverted_JJ indexes_NNS ,_, and_CC substantial_JJ efforts_NNS have_VBP been_VBN invested_VBN in_IN optimizing_VBG the_DT construction_NN ,_, size_NN ,_, and_CC access_NN speed_NN of_IN these_DT structures_NNS ;_: see_VB ,_, e.g._FW ,_, =_JJ -_: =[_NN 28_CD ,_, 26_CD -RRB-_-RRB- -_: =_SYM -_: ._.
An_DT inverted_JJ index_NN contains_VBZ one_CD inverted_JJ list_NN for_IN each_DT distinct_JJ term_NN -LRB-_-LRB- word_NN -RRB-_-RRB- in_IN the_DT document_NN collection_NN ._.
Each_DT list_NN consists_VBZ of_IN postings_NNS describing_VBG occurrences_NNS of_IN the_DT term_NN in_IN the_DT collection_NN ._.
We_PRP assume_VBP
trieval_NN ._.
Query_NN processing_NN in_IN search_NN engines_NNS has_VBZ been_VBN extensively_RB studied_VBN ;_: for_IN a_DT basic_JJ overview_NN ,_, see_VB -LRB-_-LRB- 37_CD ,_, 6_CD -RRB-_-RRB- ._.
For_IN recent_JJ research_NN on_IN performance_NN optimization_NN such_JJ as_IN index_NN compression_NN and_CC pruning_NN ,_, see_VBP =_JJ -_: =[_NN 26_CD ,_, 36_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP assume_VBP we_PRP are_VBP given_VBN a_DT collection_NN of_IN N_NN documents_NNS ,_, where_WRB each_DT document_NN is_VBZ uniquely_RB identified_VBN by_IN a_DT document_NN ID_NN -LRB-_-LRB- docID_NN -RRB-_-RRB- between_IN 0_CD and_CC N_NN −_NN 1_CD ._.
The_DT collection_NN is_VBZ indexed_VBN by_IN an_DT inverted_JJ index_NN structure_NN ,_,
converted_VBN into_IN document_NN gaps_NNS ,_, or_CC differences_NNS between_IN consecutive_JJ document_NN ids_NNS ._.
These_DT gaps_NNS are_VBP then_RB compressed_VBN with_IN integer_NN coding_NN techniques_NNS such_JJ as_IN γ_NN or_CC Rice_NNP codes_NNS ,_, or_CC more_RBR recently_RB ,_, PForDelta_NN -LRB-_-LRB- 44_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 45_CD -RRB-_-RRB- -_: =_SYM -_: ._.
It_PRP would_MD be_VB tricky_JJ for_IN gap-based_JJ compression_NN -LRB-_-LRB- also_RB commonly_RB known_VBN as_IN delta_NN compression_NN -RRB-_-RRB- to_TO support_VB backwards_RB traversal_JJ ._.
Prefix-free_JJ codes_NNS -LRB-_-LRB- γ_NN and_CC Rice_NNP codes_NNS -RRB-_-RRB- are_VBP meant_VBN to_TO be_VB decoded_VBN only_RB in_IN the_DT forwa_NN
h_NN of_IN documents_NNS at_IN a_DT time_NN in_IN a_DT structure_NN where_WRB the_DT lists_NNS are_VBP compressed_VBN using_VBG VByte_NN -LRB-_-LRB- 30_CD -RRB-_-RRB- ._.
The_DT batches_NNS are_VBP combined_VBN in_IN the_DT end_NN to_TO form_VB the_DT complete_JJ index_NN ,_, where_WRB the_DT lists_NNS are_VBP compressed_VBN using_VBG PForDelta_NN =_JJ -_: =[_NN 38_CD ,_, 36_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Next_RB ,_, we_PRP describe_VBP how_WRB queries_NNS are_VBP processed_VBN ._.
3_LS ._.
THE_DT HEAPUNION_JJ OPERATOR_NN In_IN this_DT section_NN ,_, we_PRP describe_VBP how_WRB our_PRP$ solution_NN supports_VBZ efficient_JJ query_NN processing_NN that_IN scales_NNS to_TO a_DT large_JJ number_NN of_IN author_NN lists_NNS
al._FW -LRB-_-LRB- 2_CD -RRB-_-RRB- investigate_VBP the_DT impact_NN of_IN results_NNS caching_VBG and_CC static_JJ caching_NN of_IN posting_VBG lists_NNS in_IN the_DT context_NN of_IN Web_NN search_NN engine_NN architecture_NN ._.
The_DT impact_NN of_IN compression_NN on_IN caching_NN efficiency_NN is_VBZ addressed_VBN in_IN =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Finally_RB ,_, Long_NNP and_CC Suel_NNP -LRB-_-LRB- 17_CD -RRB-_-RRB- introduce_VB a_DT 3-level_JJ caching_NN architecture_NN that_WDT includes_VBZ on-disk_JJ caching_NN of_IN the_DT posting_VBG lists_NNS for_IN popular_JJ term_NN combinations_NNS ._.
Index_NN pruning_NN reduces_VBZ significantly_RB the_DT fractio_NN
ance_NN ._.
2_CD ._.
BACKGROUND_NN AND_CC RELATED_NNS WORK_VBP For_IN a_DT basic_JJ overview_NN of_IN IR_FW query_FW processing_NN ,_, see_VB -LRB-_-LRB- 24_CD -RRB-_-RRB- ._.
For_IN recent_JJ work_NN on_IN performance_NN optimizations_NNS such_JJ as_IN index_NN compression_NN ,_, caching_NN ,_, and_CC early_JJ termination_NN ,_, see_VBP =_JJ -_: =[_NN 2_CD ,_, 23_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP assume_VBP that_IN we_PRP are_VBP given_VBN a_DT collection_NN of_IN N_NN documents_NNS -LRB-_-LRB- web_NN pages_NNS covered_VBN by_IN the_DT search_NN engine_NN -RRB-_-RRB- ,_, where_WRB each_DT document_NN is_VBZ uniquely_RB identified_VBN by_IN a_DT document_NN ID_NN -LRB-_-LRB- docID_NN -RRB-_-RRB- between_IN 0_CD and_CC N_NN −_NN 1_CD ._.
The_DT collect_VBP
