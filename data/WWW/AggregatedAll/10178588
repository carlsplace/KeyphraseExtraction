Latent_JJ space_NN domain_NN transfer_NN between_IN high_JJ dimensional_JJ overlapping_VBG distributions_NNS
Transferring_VBG knowledge_NN from_IN one_CD domain_NN to_TO another_DT is_VBZ challenging_VBG due_JJ to_TO a_DT number_NN of_IN reasons_NNS ._.
Since_IN both_CC conditional_JJ and_CC marginal_JJ distribution_NN of_IN the_DT training_NN data_NNS and_CC test_NN data_NNS are_VBP non-identical_JJ ,_, model_NN trained_VBN in_IN one_CD domain_NN ,_, when_WRB directly_RB applied_VBN to_TO a_DT different_JJ domain_NN ,_, is_VBZ usually_RB low_JJ in_IN accuracy_NN ._.
For_IN many_JJ applications_NNS with_IN large_JJ feature_NN sets_NNS ,_, such_JJ as_IN text_NN document_NN ,_, sequence_NN data_NNS ,_, medical_JJ data_NNS ,_, image_NN data_NNS of_IN different_JJ resolutions_NNS ,_, etc._FW two_CD domains_NNS usually_RB do_VBP not_RB contain_VB exactly_RB the_DT same_JJ features_NNS ,_, thus_RB introducing_VBG large_JJ numbers_NNS of_IN ``_`` missing_VBG values_NNS ''_'' when_WRB considered_VBN over_IN the_DT union_NN of_IN features_NNS from_IN both_DT domains_NNS ._.
In_IN other_JJ words_NNS ,_, its_PRP$ marginal_JJ distributions_NNS are_VBP at_IN most_JJS overlapping_VBG ._.
In_IN the_DT same_JJ time_NN ,_, these_DT problems_NNS are_VBP usually_RB high_JJ dimensional_JJ ,_, such_JJ as_IN ,_, several_JJ thousands_NNS of_IN features_NNS ._.
Thus_RB ,_, the_DT combination_NN of_IN high_JJ dimensionality_NN and_CC missing_VBG values_NNS make_VBP the_DT relationship_NN in_IN conditional_JJ probabilities_NNS between_IN two_CD domains_NNS hard_JJ to_TO measure_NN and_CC model_NN ._.
To_TO address_VB these_DT challenges_NNS ,_, we_PRP propose_VBP a_DT framework_NN that_WDT first_RB brings_VBZ the_DT marginal_JJ distributions_NNS of_IN two_CD domains_NNS closer_RB by_IN ``_`` filling_VBG up_RP ''_'' those_DT missing_VBG values_NNS of_IN disjoint_NN features_NNS ._.
Afterwards_RB ,_, it_PRP looks_VBZ for_IN those_DT comparable_JJ sub-structures_NNS in_IN the_DT ``_`` latent-space_NN ''_'' as_IN mapped_VBN from_IN the_DT expanded_VBN feature_NN vector_NN ,_, where_WRB both_CC marginal_JJ and_CC conditional_JJ distribution_NN are_VBP similar_JJ ._.
With_IN these_DT sub-structures_NNS in_IN latent_JJ space_NN ,_, the_DT proposed_VBN approach_NN then_RB find_VBP common_JJ concepts_NNS that_WDT are_VBP transferable_JJ across_IN domains_NNS with_IN high_JJ probability_NN ._.
During_IN prediction_NN ,_, unlabeled_JJ instances_NNS are_VBP treated_VBN as_IN ``_`` queries_NNS ''_'' ,_, the_DT mostly_RB related_JJ labeled_JJ instances_NNS from_IN out-domain_NN are_VBP retrieved_VBN ,_, and_CC the_DT classification_NN is_VBZ made_VBN by_IN weighted_JJ voting_NN using_VBG retrieved_VBN out-domain_JJ examples_NNS ._.
We_PRP formally_RB show_VBP that_IN importing_VBG feature_NN values_NNS across_IN domains_NNS and_CC latent_JJ semantic_JJ index_NN can_MD jointly_RB make_VB the_DT distributions_NNS of_IN two_CD related_JJ domains_NNS easier_JJR to_TO measure_VB than_IN in_IN original_JJ feature_NN space_NN ,_, the_DT nearest_JJS neighbor_NN method_NN employed_VBN to_TO retrieve_VB related_VBN out_RP domain_NN examples_NNS is_VBZ bounded_VBN in_IN error_NN when_WRB predicting_VBG in-domain_JJ examples_NNS ._.
Software_NNP and_CC datasets_NNS are_VBP available_JJ for_IN download_NN ._.
aims_NNS at_IN transferring_VBG knowledge_NN across_IN domains_NNS or_CC tasks_NNS ._.
Besides_IN sentiment_NN classification_NN ,_, domain_NN adaptation_NN techniques_NNS have_VBP been_VBN widely_RB applied_VBN to_TO other_JJ Web_NN applications_NNS ,_, such_JJ as_IN text_NN classification_NN =_JJ -_: =[_NN 11_CD ,_, 8_CD ,_, 33_CD ,_, 10_CD -RRB-_-RRB- -_: =_JJ -_: ,_, part_NN of_IN speech_NN tagging_NN -LRB-_-LRB- 2_CD ,_, 7_CD ,_, 20_CD ,_, 12_CD -RRB-_-RRB- ,_, named-entity_JJ recognition_NN and_CC shallow_JJ parsing_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
Most_JJS existing_VBG domain_NN adaptation_NN methods_NNS can_MD be_VB classified_VBN into_IN two_CD categories_NNS :_: feature-representation_JJ adapta_NN
aims_NNS at_IN transferring_VBG knowledge_NN across_IN domains_NNS or_CC tasks_NNS ._.
Besides_IN sentiment_NN classification_NN ,_, domain_NN adaptation_NN techniques_NNS have_VBP been_VBN widely_RB applied_VBN to_TO other_JJ Web_NN applications_NNS ,_, such_JJ as_IN text_NN classification_NN =_JJ -_: =[_NN 11_CD ,_, 8_CD ,_, 33_CD ,_, 10_CD -RRB-_-RRB- -_: =_JJ -_: ,_, part_NN of_IN speech_NN tagging_NN -LRB-_-LRB- 2_CD ,_, 7_CD ,_, 20_CD ,_, 12_CD -RRB-_-RRB- ,_, named-entity_JJ recognition_NN and_CC shallow_JJ parsing_NN -LRB-_-LRB- 12_CD -RRB-_-RRB- ._.
Most_JJS existing_VBG domain_NN adaptation_NN methods_NNS can_MD be_VB classified_VBN into_IN two_CD categories_NNS :_: feature-representation_JJ adapta_NN
t_NN criteria_NNS ,_, we_PRP introduce_VBP five_CD traditional_JJ classifiers_NNS ,_, including_VBG Naive_JJ Bayes_NNS -LRB-_-LRB- NB_NN -RRB-_-RRB- ,_, SVM_NN ,_, C4_NN .5_NN ,_, K-NN_NN and_CC NNge_NN -LRB-_-LRB- Ng_NN -RRB-_-RRB- ,_, and_CC three_CD state-of-theart_JJ transfer_NN learning_NN methods_NNS :_: TrAdaBoost_NNP -LRB-_-LRB- TA_NNP -RRB-_-RRB- -LRB-_-LRB- 12_CD -RRB-_-RRB- ,_, LatentMap_NN -LRB-_-LRB- LM_NN -RRB-_-RRB- =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: and_CC LWE_NN -LRB-_-LRB- 14_CD -RRB-_-RRB- ._.
Amongthem_NNP ,_, TrAdaBoostisbasedoninstancesweighting_NNP ,_, LatentMapTable_NNP 2_CD :_: Dataset_NNP for_IN Algorithm_NNP and_CC Parameters_NNP Selection_NN Data_NNP Set_NNP |_NNP S_NNP |_FW |_FW T_NN |_CD Description_NN Red-White_NN -LRB-_-LRB- RW_NN -RRB-_-RRB- 1599_CD 4998_CD physicochemical_JJ Whit_NN
n_NN and_CC similar_JJ structure_NN discovery_NN ._.
SVD_NNP first_RB maps_VBZ the_DT high_JJ dimensional_JJ out-domain_NN and_CC in-domain_JJ data_NNS to_TO a_DT latent_JJ space_NN with_IN lower_JJR dimension_NN ._.
In_IN this_DT space_NN ,_, data_NNS giving_VBG the_DT same_JJ concept_NN will_MD lie_VB nearby_RB =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- see_VB Section_NNP 3.3_CD -RRB-_-RRB- ,_, i.e._FW the_DT closer_JJR two_CD instances_NNS are_VBP ,_, the_DT more_RBR likely_JJ they_PRP are_VBP having_VBG the_DT same_JJ label_NN ,_, thus_RB p_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- across_IN domains_NNS will_MD be_VB similar_JJ within_IN cluster_NN where_WRB instances_NNS are_VBP coherently_RB nearby_JJ ._.
ans_NN clustering_NN 's_POS objective_JJ function_NN ,_, namely_RB ,_, the_DT low_JJ dimensional_JJ representation_NN of_IN data_NNS are_VBP cluster_NN membership_NN indicators_NNS from_IN which_WDT clusters_NNS structure_NN can_MD be_VB reconstructed_VBN -LRB-_-LRB- 6_CD -RRB-_-RRB- ._.
By_IN cluster_NN assumption_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, points_NNS in_IN the_DT same_JJ cluster_NN has_VBZ similar_JJ conditional_JJ distributions_NNS regardless_RB of_IN whether_IN the_DT points_NNS are_VBP from_IN the_DT same_JJ domain_NN or_CC not_RB ,_, thus_RB we_PRP have_VBP identify_VB transferable_JJ sub-structure_NN in_IN latent_JJ space_NN ._.
S_NN
on_IN rule_NN minimizes_VBZ the_DT Bayes_NNP risk_NN R_NNP ∗_NNP ,_, defined_VBN as_IN R_NN ∗_NN =_JJ E_NN -LRB-_-LRB- r_NN ∗_FW ∫_FW -RRB-_-RRB- =_JJ r_NN ∗_FW f_FW -LRB-_-LRB- x_NN -RRB-_-RRB- dx_IN where_WRB f_LS -LRB-_-LRB- x_NN -RRB-_-RRB- =_JJ ∑_NN 1_CD i_LS =_JJ 0_CD p_NN -LRB-_-LRB- y_NN =_JJ i_LS -RRB-_-RRB- p_NN -LRB-_-LRB- x_NN |_CD y_NN =_JJ i_LS -RRB-_-RRB- ._.
We_PRP have_VBP the_DT following_JJ theorem_NN that_WDT is_VBZ the_DT counterpart_NN of_IN the_DT analysis_NN of_IN k-nn_NN in_IN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: in_IN the_DT transfer_NN learning_NN setting_NN ._.
Theorem_NNP 3.3_CD Let_NNP p_NN -LRB-_-LRB- ·_FW |_FW y_NN =_JJ i_LS -RRB-_-RRB- ,_, i_FW =_JJ 0,1_CD be_VB such_JJ that_IN with_IN probability_NN one_CD ,_, x_NN is_VBZ either_CC 1_LS -RRB-_-RRB- a_DT continuity_NN point_NN of_IN p_NN -LRB-_-LRB- ·_FW |_FW y_NN =_JJ i_LS -RRB-_-RRB- ,_, or_CC 2_LS -RRB-_-RRB- a_DT point_NN of_IN non-zero_JJ probability_NN measure_NN
tion_NN such_JJ that_IN the_DT predictions_NNS are_VBP expected_VBN to_TO have_VB deviation_NN from_IN the_DT actual_JJ values_NNS within_IN a_DT given_VBN small_JJ tolerance_NN ɛ_NN for_IN all_PDT the_DT training_NN data_NNS ._.
For_IN implementation_NN ,_, ɛ-Support_NN Vector_NNP Regression_NN -LRB-_-LRB- ɛ-SVR_NN -RRB-_-RRB- =_JJ -_: =[_NN 14_CD ,_, 15_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ employed_VBN to_TO fill_VB up_RP missing_VBG values_NNS ,_, it_PRP can_MD be_VB formulated_VBN as_IN :_: minimize_VB s.t._RB 1_CD 2_CD ‖_CD w_NN ‖_NN 2_CD +_CC λ_NN n_NN ∑_FW i_FW =_JJ 1_CD -LRB-_-LRB- ξi_NN +_CC ξ_NN ∗_CD i_LS -RRB-_-RRB- y_FW li_FW +_CC −_FW 〈_FW w_NN ,_, hi_UH 〉_FW −_FW b_NN ≤_CD ɛ_NN +_CC ξi_FW 〈_FW w_NN ,_, hi_UH 〉_NN +_CC b_NN −_CD y_NN li_NN +_CC ≤_NN ɛ_NN +_CC ξ_FW ∗_FW i_FW ξi_FW ,_, ξ_FW ∗_FW i_FW ≥_FW 0_CD where_WRB hi_UH is_VBZ
ctively_RB ._.
Iterative_JJ algorithms_NNS for_IN computing_VBG the_DT first_JJ k_NN −_NN 1_CD eigenvectors_NNS -LRB-_-LRB- and_CC the_DT corresponding_JJ eigenvalues_NNS -RRB-_-RRB- exist_VBP such_JJ as_IN Lanczos_NN method_NN ._.
Recently_RB ,_, randomized_VBN SVD_NNP are_VBP proposed_VBN ,_, such_JJ as_IN the_DT method_NN in_IN =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, it_PRP sample_NN columns_NNS of_IN a_DT large_JJ matrix_NN according_VBG to_TO a_DT suitable_JJ probability_NN distribution_NN then_RB a_DT smaller_JJR matrix_NN is_VBZ constructed_VBN on_IN which_WDT SVD_NNP is_VBZ applied_VBN ._.
This_DT method_NN provides_VBZ a_DT good_JJ approximation_NN of_IN the_DT ex_FW
Mlight_NNP is_VBZ used_VBN as_IN SVM_NN classifier_NN ._.
Procedural_JJ parameters_NNS are_VBP kept_VBN as_IN default_NN for_IN all_PDT the_DT classifiers_NNS ._.
To_TO predict_VB missing_VBG values_NNS ,_, we_PRP use_VBP SVR_NN as_IN our_PRP$ predictor_NN and_CC the_DT implementation_NN is_VBZ provided_VBN by_IN libSVM_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT traditional_JJ learning_NN algorithms_NNS assume_VBP that_IN the_DT training_NN and_CC test_NN data_NNS are_VBP governed_VBN by_IN an_DT identical_JJ distribution_NN p_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- =_JJ p_NN -LRB-_-LRB- x_NN -RRB-_-RRB- p_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- ._.
For_IN this_DT reason_NN ,_, we_PRP provide_VBP these_DT learning_VBG methods_NNS with_IN the_DT
-RRB-_-RRB- 5_CD ._.
RELATED_NNS WORK_VBP One_CD main_JJ challenge_NN of_IN transfer_NN learning_NN is_VBZ how_WRB to_TO resolve_VB and_CC ,_, in_IN the_DT same_JJ time_NN ,_, take_VB advantage_NN of_IN the_DT difference_NN between_IN two_CD domains_NNS ._.
Some_DT are_VBP based_VBN on_IN instance_NN weighting_NN strategy_NN -LRB-_-LRB- =_JJ -_: =[_NN 2_CD ,_, 5_CD ,_, 8_CD ,_, 12_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
For_IN example_NN ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- adopts_VBZ the_DT boosting_VBG weight_NN formula_NN as_IN the_DT re-weighting_JJ scheme_NN ._.
Some_DT other_JJ methods_NNS base_VBP on_IN dimension_NN reduction_NN ,_, which_WDT usually_RB map_VBP data_NNS to_TO a_DT new_JJ representation_NN facilitating_VBG domain_NN tr_NN
regression_NN is_VBZ to_TO minimize_VB the_DT discrepency_NN between_IN the_DT marginal_JJ distributions_NNS of_IN two_CD domains_NNS ._.
Admittedly_RB ,_, H_NN ℓ_NN and_CC H_NN u_NN come_VBN from_IN different_JJ distributioins_NNS ,_, however_RB generalization_NN error_NN bound_VBN is_VBZ given_VBN in_IN =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: when_WRB training_NN and_CC test_NN data_NNS are_VBP from_IN different_JJ distribution_NN ,_, and_CC this_DT justifies_VBZ our_PRP$ regression_NN strategy_NN ._.
2.2_CD Dimensionality_NN Reduction_NN The_DT marginal_JJ distributions_NNS p_NN -LRB-_-LRB- x_NN -RRB-_-RRB- of_IN two_CD domains_NNS are_VBP made_VBN easier_JJR to_TO
algorithms_NNS ,_, including_VBG naive_JJ Bayes_NNS -LRB-_-LRB- NB_NN -RRB-_-RRB- ,_, Logistic_JJ regression_NN -LRB-_-LRB- LR_NN -RRB-_-RRB- ,_, decision_NN trees_NNS -LRB-_-LRB- C4_NN .5_CD -RRB-_-RRB- and_CC SVM_NN ._.
For_IN the_DT implementation_NN of_IN naive_JJ Bayes_NNS ,_, Logistic_JJ regression_NN and_CC decision_NN trees_NNS ,_, we_PRP use_VBP the_DT Weka_NNP package_NN =_JJ -_: =[_NN 16_CD -RRB-_-RRB- -_: =_SYM -_: ._.
SVMlight_NN is_VBZ used_VBN as_IN SVM_NN classifier_NN ._.
Procedural_JJ parameters_NNS are_VBP kept_VBN as_IN default_NN for_IN all_PDT the_DT classifiers_NNS ._.
To_TO predict_VB missing_VBG values_NNS ,_, we_PRP use_VBP SVR_NN as_IN our_PRP$ predictor_NN and_CC the_DT implementation_NN is_VBZ provided_VBN by_IN li_NN
-RRB-_-RRB- 5_CD ._.
RELATED_NNS WORK_VBP One_CD main_JJ challenge_NN of_IN transfer_NN learning_NN is_VBZ how_WRB to_TO resolve_VB and_CC ,_, in_IN the_DT same_JJ time_NN ,_, take_VB advantage_NN of_IN the_DT difference_NN between_IN two_CD domains_NNS ._.
Some_DT are_VBP based_VBN on_IN instance_NN weighting_NN strategy_NN -LRB-_-LRB- =_JJ -_: =[_NN 2_CD ,_, 5_CD ,_, 8_CD ,_, 12_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
For_IN example_NN ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- adopts_VBZ the_DT boosting_VBG weight_NN formula_NN as_IN the_DT re-weighting_JJ scheme_NN ._.
Some_DT other_JJ methods_NNS base_VBP on_IN dimension_NN reduction_NN ,_, which_WDT usually_RB map_VBP data_NNS to_TO a_DT new_JJ representation_NN facilitating_VBG domain_NN tr_NN
-RRB-_-RRB- 5_CD ._.
RELATED_NNS WORK_VBP One_CD main_JJ challenge_NN of_IN transfer_NN learning_NN is_VBZ how_WRB to_TO resolve_VB and_CC ,_, in_IN the_DT same_JJ time_NN ,_, take_VB advantage_NN of_IN the_DT difference_NN between_IN two_CD domains_NNS ._.
Some_DT are_VBP based_VBN on_IN instance_NN weighting_NN strategy_NN -LRB-_-LRB- =_JJ -_: =[_NN 2_CD ,_, 5_CD ,_, 8_CD ,_, 12_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
For_IN example_NN ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- adopts_VBZ the_DT boosting_VBG weight_NN formula_NN as_IN the_DT re-weighting_JJ scheme_NN ._.
Some_DT other_JJ methods_NNS base_VBP on_IN dimension_NN reduction_NN ,_, which_WDT usually_RB map_VBP data_NNS to_TO a_DT new_JJ representation_NN facilitating_VBG domain_NN tr_NN
tion_NN such_JJ that_IN the_DT predictions_NNS are_VBP expected_VBN to_TO have_VB deviation_NN from_IN the_DT actual_JJ values_NNS within_IN a_DT given_VBN small_JJ tolerance_NN ɛ_NN for_IN all_PDT the_DT training_NN data_NNS ._.
For_IN implementation_NN ,_, ɛ-Support_NN Vector_NNP Regression_NN -LRB-_-LRB- ɛ-SVR_NN -RRB-_-RRB- =_JJ -_: =[_NN 14_CD ,_, 15_CD -RRB-_-RRB- -_: =_JJ -_: is_VBZ employed_VBN to_TO fill_VB up_RP missing_VBG values_NNS ,_, it_PRP can_MD be_VB formulated_VBN as_IN :_: minimize_VB s.t._RB 1_CD 2_CD ‖_CD w_NN ‖_NN 2_CD +_CC λ_NN n_NN ∑_FW i_FW =_JJ 1_CD -LRB-_-LRB- ξi_NN +_CC ξ_NN ∗_CD i_LS -RRB-_-RRB- y_FW li_FW +_CC −_FW 〈_FW w_NN ,_, hi_UH 〉_FW −_FW b_NN ≤_CD ɛ_NN +_CC ξi_FW 〈_FW w_NN ,_, hi_UH 〉_NN +_CC b_NN −_CD y_NN li_NN +_CC ≤_NN ɛ_NN +_CC ξ_FW ∗_FW i_FW ξi_FW ,_, ξ_FW ∗_FW i_FW ≥_FW 0_CD where_WRB hi_UH is_VBZ
oposes_VBZ a_DT locally_RB weighted_JJ ensemble_NN framework_NN to_TO combine_VB multiple_JJ models_NNS for_IN transfer_NN learning_NN by_IN dynamically_RB assigning_VBG weights_NNS of_IN a_DT model_NN according_VBG to_TO a_DT model_NN 's_POS predictive_JJ power_NN on_IN each_DT test_NN example_NN ._.
=_SYM -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: Table_NNP 3_CD :_: Comparison_NN of_IN Performance_NNP Methods_NNS SRAA_NN 20_CD News_NNP Groups_NNS Reuters_NNP Re_NNP vs_CC Si_NNP Au_NN vs_CC Av_NN C_NN vs_CC R_NN C_NN vs_CC S_NN C_NN vs_CC T_NN R_NN vs_CC S_NN R_NN vs_CC T_NN S_NN vs_CC T_NN O_NN vs_CC Pe_NN O_NN vs_CC Pl_NN Pe_NN vs_CC Pl_NN NB_NN 0.6838_CD 0.6889_CD 0.8098_CD 0.7042_CD 0.89_CD 0.7113_CD
-RRB-_-RRB- 5_CD ._.
RELATED_NNS WORK_VBP One_CD main_JJ challenge_NN of_IN transfer_NN learning_NN is_VBZ how_WRB to_TO resolve_VB and_CC ,_, in_IN the_DT same_JJ time_NN ,_, take_VB advantage_NN of_IN the_DT difference_NN between_IN two_CD domains_NNS ._.
Some_DT are_VBP based_VBN on_IN instance_NN weighting_NN strategy_NN -LRB-_-LRB- =_JJ -_: =[_NN 2_CD ,_, 5_CD ,_, 8_CD ,_, 12_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
For_IN example_NN ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- adopts_VBZ the_DT boosting_VBG weight_NN formula_NN as_IN the_DT re-weighting_JJ scheme_NN ._.
Some_DT other_JJ methods_NNS base_VBP on_IN dimension_NN reduction_NN ,_, which_WDT usually_RB map_VBP data_NNS to_TO a_DT new_JJ representation_NN facilitating_VBG domain_NN tr_NN
xample_RB ,_, -LRB-_-LRB- 5_CD -RRB-_-RRB- adopts_VBZ the_DT boosting_VBG weight_NN formula_NN as_IN the_DT re-weighting_JJ scheme_NN ._.
Some_DT other_JJ methods_NNS base_VBP on_IN dimension_NN reduction_NN ,_, which_WDT usually_RB map_VBP data_NNS to_TO a_DT new_JJ representation_NN facilitating_VBG domain_NN transfer_NN -LRB-_-LRB- =_JJ -_: =[_NN 10_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Recently_RB ,_, -LRB-_-LRB- 9_CD -RRB-_-RRB- proposes_VBZ a_DT locally_RB weighted_JJ ensemble_NN framework_NN to_TO combine_VB multiple_JJ models_NNS for_IN transfer_NN learning_NN by_IN dynamically_RB assigning_VBG weights_NNS of_IN a_DT model_NN according_VBG to_TO a_DT model_NN 's_POS predictive_JJ power_NN on_IN
the_DT distance_NN of_IN marginal_JJ distributions_NNS of_IN two_CD domains_NNS can_MD be_VB bounded_VBN in_IN latent_JJ space_NN ._.
Bound_VBN the_DT conditional_JJ distributions_NNS in_IN latent_JJ space_NN In_IN this_DT section_NN ,_, we_PRP show_VBP that_IN under_IN the_DT clustering_NN assumption_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT proposed_VBN retrieval_NN strategy_NN is_VBZ optimal_JJ in_IN making_VBG the_DT conditional_JJ distributions_NNS p0_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- and_CC p1_NN -LRB-_-LRB- y_NN |_CD x_NN -RRB-_-RRB- similar_JJ ._.
Then_RB in_IN the_DT next_JJ section_NN ,_, we_PRP derive_VBP the_DT Bayes_NNP risk_NN of_IN this_DT retrieval_NN process_NN ._.
Followin_NNP
s_VBZ the_DT boosting_VBG weight_NN formula_NN as_IN the_DT re-weighting_JJ scheme_NN ._.
Some_DT other_JJ methods_NNS base_VBP on_IN dimension_NN reduction_NN ,_, which_WDT usually_RB map_VBP data_NNS to_TO a_DT new_JJ representation_NN facilitating_VBG domain_NN transfer_NN -LRB-_-LRB- -LRB-_-LRB- 10_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Recently_RB ,_, =_JJ -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: proposes_VBZ a_DT locally_RB weighted_JJ ensemble_NN framework_NN to_TO combine_VB multiple_JJ models_NNS for_IN transfer_NN learning_NN by_IN dynamically_RB assigning_VBG weights_NNS of_IN a_DT model_NN according_VBG to_TO a_DT model_NN 's_POS predictive_JJ power_NN on_IN each_DT test_NN exampl_NN
