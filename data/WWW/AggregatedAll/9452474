Mining_NNP multilingual_JJ topics_NNS from_IN wikipedia_NN
In_IN this_DT paper_NN ,_, we_PRP try_VBP to_TO leverage_NN a_DT large-scale_JJ and_CC multilingual_JJ knowledge_NN base_NN ,_, Wikipedia_NNP ,_, to_TO help_VB effectively_RB analyze_VB and_CC organize_VB Web_NN information_NN written_VBN in_IN different_JJ languages_NNS ._.
Based_VBN on_IN the_DT observation_NN that_IN one_CD Wikipedia_NNP concept_NN may_MD be_VB described_VBN by_IN articles_NNS in_IN different_JJ languages_NNS ,_, we_PRP adapt_VBP existing_VBG topic_NN modeling_NN algorithm_NN for_IN mining_NN multilingual_JJ topics_NNS from_IN this_DT knowledge_NN base_NN ._.
The_DT extracted_VBN `_`` universal_JJ '_'' topics_NNS have_VBP multiple_JJ types_NNS of_IN representations_NNS ,_, with_IN each_DT type_NN corresponding_VBG to_TO one_CD language_NN ._.
Accordingly_RB ,_, new_JJ documents_NNS of_IN different_JJ languages_NNS can_MD be_VB represented_VBN in_IN a_DT space_NN using_VBG a_DT group_NN of_IN universal_JJ topics_NNS ,_, which_WDT makes_VBZ various_JJ multilingual_JJ Web_NN applications_NNS feasible_JJ ._.
ferent_JJ languages_NNS ._.
In_IN CorrLDA_NN -LRB-_-LRB- 10_CD -RRB-_-RRB- authors_NNS propose_VBP an_DT asymmetric_JJ model_NN to_TO match_VB words_NNS and_CC pictures_NNS ,_, even_RB in_IN this_DT model_NN both_CC the_DT image_NN and_CC its_PRP$ corresponding_JJ words_NNS are_VBP generated_VBN simultaneously_RB ._.
Recently_RB =_JJ -_: =[_NN 11_CD -RRB-_-RRB- -_: =_SYM -_: propose_VBP an_DT extension_NN of_IN LDA_NNP to_TO mine_VB multilingual_JJ topics_NNS from_IN Wikipedia_NNP articles_NNS by_IN forcing_VBG aligned_VBN articles_NNS to_TO share_VB at_IN least_JJS one_CD topical_JJ distribution_NN ._.
All_DT these_DT approaches_NNS critically_RB require_VBP alignme_NN
ngual_JJ topic_NN models_NNS assume_VBP documents_NNS in_IN multiple_JJ languages_NNS are_VBP aligned_VBN either_CC at_IN the_DT document_NN level_NN ,_, sentence_NN level_NN or_CC by_IN time_NN stamps_NNS -LRB-_-LRB- Mimno_NNP et_FW al._FW ,_, 2009_CD ;_: Zhao_NNP and_CC Xing_NNP ,_, 2006_CD ;_: Kim_NNP and_CC Khudanpur_NNP ,_, 2004_CD ;_: =_JJ -_: =_JJ Ni_FW et_FW al._FW ,_, 2009_CD -_: =_JJ -_: ;_: Wang_NNP et_FW al._FW ,_, 2007_CD -RRB-_-RRB- ._.
However_RB ,_, in_IN many_JJ applications_NNS ,_, we_PRP need_VBP to_TO mine_VB topics_NNS from_IN unaligned_JJ text_NN corpus_NN ._.
For_IN example_NN ,_, mining_NN topics_NNS from_IN search_NN results_VBZ in_IN different_JJ languages_NNS can_MD facilitate_VB summarizati_NNS
rnational_JJ users_NNS independent_JJ of_IN the_DT languages_NNS they_PRP speak_VBP ._.
For_IN example_NN ,_, Scholl_NN et_FW al_FW -LRB-_-LRB- 19_CD -RRB-_-RRB- proposed_VBD a_DT method_NN to_TO search_VB the_DT Web_NN by_IN genres_NNS such_JJ as_IN blog_NN and_CC forum_NN in_IN a_DT language_NN independent_JJ manner_NN ._.
Ni_FW et_FW al_FW =_JJ -_: =[_NN 18_CD -RRB-_-RRB- -_: =_SYM -_: explored_VBN ways_NNS to_TO analyze_VB and_CC organize_VB Web_NN information_NN written_VBN in_IN different_JJ languages_NNS by_IN mining_VBG multilingual_JJ topics_NNS from_IN Wikipedia_NNP ._.
Tanaka-Ishii_NNP and_CC Nakagawa_NNP -LRB-_-LRB- 20_CD -RRB-_-RRB- developed_VBD a_DT tool_NN for_IN language_NN learner_NN
,_, where_WRB we_PRP can_MD encode_VB the_DT strength_NN of_IN the_DT links_NNS between_IN each_DT pair_NN of_IN words_NNS ._.
Previous_JJ work_NN on_IN multilingual_JJ topic_NN models_NNS requires_VBZ parallelism_NN at_IN either_CC the_DT sentence_NN level_NN -LRB-_-LRB- -LRB-_-LRB- 20_CD -RRB-_-RRB- -RRB-_-RRB- or_CC document_NN level_NN -LRB-_-LRB- -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, =_JJ -_: =[_NN 15_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
More_RBR recent_JJ work_NN -LRB-_-LRB- 13_CD -RRB-_-RRB- relaxes_VBZ that_IN ,_, but_CC still_RB requires_VBZ that_IN a_DT significant_JJ fraction_NN -LRB-_-LRB- at_IN least_JJS 25_CD %_NN -RRB-_-RRB- of_IN the_DT documents_NNS are_VBP paired_VBN up_RP ._.
Multilingual_JJ topic_NN alignment_NN without_IN parallelism_NN was_VBD recently_RB propo_VBN
sion_NN on_IN a_DT K-dimensional_JJ vector_NN ._.
By_IN not_RB assuming_VBG parallel_JJ text_NN ,_, this_DT approach_NN can_MD be_VB applied_VBN to_TO a_DT broad_JJ class_NN of_IN corpora_NN ._.
Other_JJ multilingual_JJ topic_NN models_NNS require_VBP parallel_JJ text_NN ,_, either_CC at_IN the_DT document_NN -LRB-_-LRB- =_JJ -_: =_JJ Ni_FW et_FW al._FW ,_, 2009_CD -_: =_JJ -_: ;_: Mimno_NNP et_FW al._FW ,_, 2009_CD -RRB-_-RRB- or_CC word-level_JJ -LRB-_-LRB- Kim_NNP and_CC Khudanpur_NNP ,_, 2004_CD ;_: Zhao_NNP and_CC Xing_NNP ,_, 2006_CD -RRB-_-RRB- ._.
Similarly_RB ,_, other_JJ multilingual_JJ sentiment_NN approaches_NNS also_RB require_VBP parallel_JJ text_NN ,_, often_RB supplied_VBN via_IN automatic_JJ translat_NN
ined_VBN for_IN both_CC source_NN and_CC target_NN corpora_NN ._.
The_DT BiLDA_NN model_NN we_PRP use_VBP is_VBZ a_DT natural_JJ extension_NN of_IN the_DT standard_JJ LDA_NN model_NN and_CC ,_, along_IN with_IN the_DT definition_NN of_IN per-topic_JJ word_NN distributions_NNS ,_, has_VBZ been_VBN presented_VBN in_IN -LRB-_-LRB- =_JJ -_: =_JJ Ni_FW et_FW al._FW ,_, 2009_CD -_: =_JJ -_: ;_: De_NNP Smet_NNP and_CC Moens_NNP ,_, 2009_CD ;_: Mimno_NNP et_FW al._FW ,_, 2009_CD -RRB-_-RRB- ._.
BiLDA_NN takes_VBZ advantage_NN of_IN the_DT document_NN alignment_NN by_IN using_VBG a_DT single_JJ variable_NN that_WDT contains_VBZ the_DT topic_NN distribution_NN θ_NN ._.
This_DT variable_NN is_VBZ language-independent_JJ
uitable_JJ for_IN topically_RB similar_JJ document_NN tuples_NNS -LRB-_-LRB- where_WRB documents_NNS are_VBP not_RB direct_JJ translations_NNS of_IN one_CD another_DT -RRB-_-RRB- in_IN more_JJR than_IN two_CD languages_NNS ._.
A_DT recent_JJ extended_JJ abstract_JJ ,_, developed_VBD concurrently_RB by_IN Ni_FW et_FW al._FW -LRB-_-LRB- =_JJ -_: =_JJ Ni_FW et_FW al._FW ,_, 2009_CD -_: =--RRB-_NN ,_, discusses_VBZ a_DT multilingual_JJ topic_NN model_NN similar_JJ to_TO the_DT one_CD presented_VBN here_RB ._.
However_RB ,_, they_PRP evaluate_VBP their_PRP$ model_NN on_IN only_RB two_CD languages_NNS -LRB-_-LRB- English_NNP and_CC Chinese_NNP -RRB-_-RRB- ,_, and_CC do_VBP not_RB use_VB the_DT model_NN to_TO detect_VB difference_NN
for_IN experiments_NNS ._.
4.1_CD Cross-lingual_JJ Text_NN Classification_NN Cross-lingual_JJ text_NN classification_NN -LRB-_-LRB- CLTC_NN -RRB-_-RRB- addresses_VBZ the_DT problem_NN of_IN using_VBG texts_NNS labeled_VBN in_IN one_CD language_NN to_TO help_VB classify_VB texts_NNS in_IN another_DT language_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =-[_CD 5_CD -RRB-_-RRB- ._.
We_PRP have_VBP built_VBN two_CD CLTC_NNP tasks_NNS :_: 1_LS -RRB-_-RRB- classify_VB Chinese_JJ pages_NNS by_IN using_VBG the_DT training_NN data_NNS in_IN English_NNP -LRB-_-LRB- Ento-Ch_NNP -RRB-_-RRB- ;_: 2_LS -RRB-_-RRB- classify_VB English_JJ pages_NNS by_IN using_VBG the_DT training_NN data_NNS in_IN Chinese_JJ -LRB-_-LRB- Ch-to-En_NN -RRB-_-RRB- ._.
Support_NN Vector_NNP
ferent_JJ languages_NNS ,_, share_NN identical_JJ topic_NN distribution_NN ._.
Figure_NN 1_CD presents_VBZ the_DT graphical_JJ model_NN of_IN ML-LDA_NN ._.
Figure_NN 1_CD ._.
Graphical_JJ model_NN representation_NN of_IN ML-LDA_NN ._.
The_DT notations_NNS are_VBP similar_JJ to_TO those_DT in_IN LDA_NNP -LRB-_-LRB- 1_LS -RRB-_-RRB- =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Here_RB Lj_NNP denotes_VBZ one_CD language_NN and_CC denotes_VBZ the_DT word_NN distribution_NN for_IN topic_NN k_NN in_FW ϕ_FW k_NN ,_, L_NN j_NN r_NN Language_NN Lj_NN ._.
We_PRP modify_VBP Gibbs_NNP Sampling_NNP -LRB-_-LRB- 2_LS -RRB-_-RRB- method_NN for_IN the_DT estimation_NN of_IN ML-LDA_NN ._.
Here_RB in_IN one_CD concept-unit_NN ,_, doc_NN
different_JJ languages_NNS ,_, share_NN identical_JJ topic_NN distribution_NN ._.
Figure_NN 1_CD presents_VBZ the_DT graphical_JJ model_NN of_IN ML-LDA_NN ._.
Figure_NN 1_CD ._.
Graphical_JJ model_NN representation_NN of_IN ML-LDA_NN ._.
The_DT notations_NNS are_VBP similar_JJ to_TO those_DT in_IN LDA_NN =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =-[_NN 2_CD -RRB-_-RRB- ._.
Here_RB Lj_NNP denotes_VBZ one_CD language_NN and_CC denotes_VBZ the_DT word_NN distribution_NN for_IN topic_NN k_NN in_FW ϕ_FW k_NN ,_, L_NN j_NN r_NN Language_NN Lj_NN ._.
We_PRP modify_VBP Gibbs_NNP Sampling_NNP -LRB-_-LRB- 2_LS -RRB-_-RRB- method_NN for_IN the_DT estimation_NN of_IN ML-LDA_NN ._.
Here_RB in_IN one_CD concept-unit_NN ,_,
experiments_NNS ._.
4.1_CD Cross-lingual_JJ Text_NN Classification_NN Cross-lingual_JJ text_NN classification_NN -LRB-_-LRB- CLTC_NN -RRB-_-RRB- addresses_VBZ the_DT problem_NN of_IN using_VBG texts_NNS labeled_VBN in_IN one_CD language_NN to_TO help_VB classify_VB texts_NNS in_IN another_DT language_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
We_PRP have_VBP built_VBN two_CD CLTC_NNP tasks_NNS :_: 1_LS -RRB-_-RRB- classify_VB Chinese_JJ pages_NNS by_IN using_VBG the_DT training_NN data_NNS in_IN English_NNP -LRB-_-LRB- Ento-Ch_NNP -RRB-_-RRB- ;_: 2_LS -RRB-_-RRB- classify_VB English_JJ pages_NNS by_IN using_VBG the_DT training_NN data_NNS in_IN Chinese_JJ -LRB-_-LRB- Ch-to-En_NN -RRB-_-RRB- ._.
Support_NN Vector_NNP Ma_NNP
