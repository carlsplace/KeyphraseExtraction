Dynamic_NNP cost-per-action_JJ mechanisms_NNS and_CC applications_NNS to_TO online_JJ advertising_NN
We_PRP study_VBD the_DT Cost-Per-Action_NNP or_CC Cost-Per-Acquisition_NNP -LRB-_-LRB- CPA_NNP -RRB-_-RRB- charging_VBG scheme_NN in_IN online_NN advertising_NN ._.
In_IN this_DT scheme_NN ,_, instead_RB of_IN paying_VBG per_IN click_VBP ,_, the_DT advertisers_NNS pay_VBP only_RB when_WRB a_DT user_NN takes_VBZ a_DT specific_JJ action_NN -LRB-_-LRB- e.g._FW fills_VBZ out_RP a_DT form_NN -RRB-_-RRB- or_CC completes_VBZ a_DT transaction_NN on_IN their_PRP$ websites_NNS ._.
We_PRP focus_VBP on_IN designing_VBG efficient_JJ and_CC incentive_NN compatible_JJ mechanisms_NNS that_WDT use_VBP this_DT charging_VBG scheme_NN ._.
We_PRP describe_VBP a_DT mechanism_NN based_VBN on_IN a_DT sampling-based_JJ learning_NN algorithm_NN that_IN under_IN suitable_JJ assumptions_NNS is_VBZ asymptotically_RB individually_RB rational_JJ ,_, asymptotically_RB Bayesian_JJ incentive_NN compatible_JJ and_CC asymptotically_RB ex-ante_JJ efficient_JJ ._.
In_IN particular_JJ ,_, we_PRP demonstrate_VBP our_PRP$ mechanism_NN for_IN the_DT case_NN where_WRB the_DT utility_NN functions_NNS of_IN the_DT advertisers_NNS are_VBP independent_JJ and_CC identically-distributed_JJ random_JJ variables_NNS as_RB well_RB as_IN the_DT case_NN where_WRB they_PRP evolve_VBP like_IN independent_JJ reflected_VBN Brownian_JJ motions_NNS ._.
tting_NN and_CC prove_VB matching_JJ upper_JJ and_CC lower_JJR bounds_NNS on_IN the_DT regret_NN ,_, which_WDT in_IN their_PRP$ case_NN is_VBZ the_DT difference_NN in_IN the_DT social_JJ welfare_NN achieved_VBN by_IN the_DT mechanism_NN and_CC the_DT optimum_JJ social_JJ welfare_NN ._.
Nazerzadeh_NNP et_NNP ._.
al._FW =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: consider_VB a_DT similar_JJ problem_NN ,_, where_WRB the_DT goal_NN is_VBZ to_TO design_VB a_DT truthful_JJ pay-per-acquisition_NN auction_NN --_: the_DT key_JJ difference_NN being_VBG that_IN the_DT bidders_NNS report_VBP whether_IN an_DT acquisition_NN happened_VBD or_CC not_RB ._.
Their_PRP$ auction_NN
from_IN large_JJ scale_NN data_NNS ,_, can_MD provide_VB effective_JJ solutions_NNS to_TO the_DT above_JJ problems_NNS in_IN different_JJ types_NNS of_IN online_NN advertising_NN ,_, including_VBG sponsored_VBN search_NN ,_, contextual_JJ advertising_NN ,_, behavior_NN targeting_NN and_CC so_RB on_IN =_JJ -_: =[_NN 6_CD ;_: 12_CD ;_: 13_CD ;_: 3_CD ;_: 14_CD ;_: 9_CD ;_: 4_LS -RRB-_-RRB- -_: =_SYM -_: ._.
∗_FW Workshop_FW report_NN on_IN ADKDD_NN 2008_CD :_: the_DT 2nd_JJ International_NNP Workshop_NNP on_IN Data_NNP Mining_NNP and_CC Audience_NNP Intelligence_NNP for_IN Advertising_NNP ,_, held_VBN in_IN conjunction_NN with_IN KDD_NN 2008_CD ,_, The_DT 14th_JJ ACM_NNP SIGKDD_NNP International_NNP Confere_NNP
t_NN the_DT learning_VBG black_JJ box_NN is_VBZ optimal_JJ on_IN each_DT bandit_NN problem_NN ._.
The_DT large_JJ literature_NN on_IN multi-armed_JJ bandit_NN optimization_NN is_VBZ relevant_JJ here_RB -LRB-_-LRB- 10_CD ,_, 4_CD ,_, 3_CD ,_, 7_CD -RRB-_-RRB- ,_, including_VBG some_DT directly_RB applied_VBN to_TO sponsored_VBN search_NN =_JJ -_: =[_NN 8_CD ,_, 11_CD ,_, 14_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Thus_RB far_RB we_PRP have_VBP described_VBN a_DT standard_JJ bandits_NN framework_NN ;_: we_PRP now_RB introduce_VBP outsourcing_NN ._.
We_PRP assume_VBP that_IN at_IN every_DT observation_NN of_IN any_DT keyphrase_FW i_FW ,_, the_DT agent_NN can_MD choose_VB to_TO forgo_VB the_DT opportunity_NN to_TO learn_VB
t_NN the_DT learning_VBG black_JJ box_NN is_VBZ optimal_JJ on_IN each_DT bandit_NN problem_NN ._.
The_DT large_JJ literature_NN on_IN multi-armed_JJ bandit_NN optimization_NN is_VBZ relevant_JJ here_RB -LRB-_-LRB- 10_CD ,_, 4_CD ,_, 3_CD ,_, 7_CD -RRB-_-RRB- ,_, including_VBG some_DT directly_RB applied_VBN to_TO sponsored_VBN search_NN =_JJ -_: =[_NN 8_CD ,_, 11_CD ,_, 14_CD ,_, 6_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Thus_RB far_RB we_PRP have_VBP described_VBN a_DT standard_JJ bandits_NN framework_NN ;_: we_PRP now_RB introduce_VBP outsourcing_NN ._.
We_PRP assume_VBP that_IN at_IN every_DT observation_NN of_IN any_DT keyphrase_FW i_FW ,_, the_DT agent_NN can_MD choose_VB to_TO forgo_VB the_DT opportunity_NN to_TO learn_VB
directly_RB ._.
The_DT PPA_NN scheme_NN is_VBZ a_DT relatively_RB new_JJ model_NN ,_, and_CC we_PRP are_VBP aware_JJ of_IN only_RB two_CD studies_NNS that_WDT deal_VBP with_IN it_PRP explicitly_RB ._.
-LRB-_-LRB- 8_CD -RRB-_-RRB- adopts_VBZ the_DT model_NN used_VBN for_IN the_DT PPC_NNP context_NN ,_, with_IN clicks_NNS replaced_VBN by_IN actions_NNS ._.
=_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: designs_VBZ a_DT mechanism_NN for_IN repeated_VBN single-item_JJ auction_NN with_IN stochastically_RB distributed_VBN agents_NNS '_POS valuations_NNS ,_, in_IN which_WDT only_RB the_DT winning_JJ agent_NN reports_VBZ his_PRP$ utility_NN ._.
While_IN their_PRP$ mechanism_NN has_VBZ many_JJ desirable_JJ
where_WRB learning_VBG algorithms_NNS satisfying_VBG these_DT sufficient_JJ conditions_NNS exist_VBP ._.
The_DT mechanism_NN randomly_RB alternates_VBZ between_IN two_CD actions_NNS :_: exploration_NN and_CC exploitation_NN ._.
At_IN time_NN t_NN ,_, with_IN probability_NN η_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, η_NN :_: N_NN →_NN =_JJ -_: =[_NN 0_CD ,_, 1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT mechanism_NN explores_VBZ i.e._FW it_PRP allocates_VBZ the_DT item_NN for_IN free_JJ to_TO an_DT agent_NN chosen_VBN uniformly_RB at_IN random_JJ ._.
With_IN the_DT remaining_VBG probability_NN ,_, the_DT mechanism_NN exploits_NNS ._.
During_IN exploitation_NN ,_, the_DT item_NN is_VBZ allocated_VBN
re_RB remain_VB constant_JJ overtime_NN ._.
In_IN contrast_NN ,_, we_PRP study_VBD conditions_NNS which_WDT guarantee_VBP incentive_NN compatibility_NN and_CC efficiency_NN ,_, while_IN the_DT utility_NN of_IN -LRB-_-LRB- all_DT -RRB-_-RRB- agents_NNS may_MD evolve_VB over_IN time_NN ._.
Recently_RB ,_, Babaioff_NNP et_FW al._FW =_SYM -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC Devanur_NNP and_CC Kakade_NNP -LRB-_-LRB- 13_CD -RRB-_-RRB- give_VBP a_DT characterization_NN of_IN incentive_NN compatible_JJ multi-armed_JJ bandit_NN mechanisms_NNS ._.
They_PRP show_VBP that_IN if_IN one_CD requires_VBZ truthfulness_NN as_IN a_DT weakly_RB dominant_JJ strategy_NN then_RB the_DT the_DT expl_NN
nism_NN that_WDT satisfy_VBP desirable_JJ economic_JJ properties_NNS under_IN a_DT limited_JJ set_NN of_IN assumptions_NNS ._.
An_DT interesting_JJ line_NN of_IN research_NN that_WDT is_VBZ gaining_VBG a_DT lot_NN of_IN attention_NN is_VBZ to_TO characterize_VB such_JJ mechanisms_NNS ._.
Pavan_NNP et_FW al._FW =_SYM -_: =[_NN 25_CD -RRB-_-RRB- -_: =_JJ -_: study_NN incentive_NN compatible_JJ mechanisms_NNS in_IN a_DT Bayesian_JJ setting_NN ._.
On_IN the_DT flip_JJ side_NN ,_, Babaioff_NNP et_FW al._FW -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, and_CC Devanur_NNP and_CC Kakade_NNP -LRB-_-LRB- 13_CD -RRB-_-RRB- study_NN the_DT problem_NN in_IN a_DT prior-free_JJ setting_NN ,_, but_CC for_IN a_DT rather_RB strong_JJ noti_NNS
ting_JJ results_NNS on_IN using_VBG machine_NN learning_NN techniques_NNS in_IN mechanism_NN design_NN ._.
We_PRP only_RB briefly_RB survey_VB the_DT main_JJ techniques_NNS and_CC ideas_NNS and_CC compare_VB them_PRP with_IN the_DT approach_NN of_IN this_DT paper_NN ._.
Most_JJS of_IN these_DT works_NNS ,_, like_IN =_JJ -_: =[_NN 5_CD ,_, 8_CD ,_, 11_CD ,_, 18_CD -RRB-_-RRB- -_: =_JJ -_: ,_, consider_VB one-shot_JJ games_NNS or_CC repeated_VBN auctions_NNS in_IN which_WDT the_DT agents_NNS leave_VBP the_DT environment_NN after_IN they_PRP received_VBD an_DT item_NN ._.
In_IN our_PRP$ setting_NN we_PRP may_MD allocates_VB items_NNS to_TO an_DT agent_NN several_JJ times_NNS and_CC hence_RB ,_, we_PRP need_VBP
ion_NN hold_NN as_RB long_RB as_IN the_DT expected_VBN value_NN of_IN the_DT error_NN of_IN these_DT estimates_NNS at_IN time_NN t_NN is_VBZ o_NN -LRB-_-LRB- t_NN 1_CD 6_CD -RRB-_-RRB- ._.
We_PRP begin_VBP analyzing_VBG the_DT mechanism_NN by_IN stating_VBG some_DT wellknown_JJ properties_NNS of_IN reflected_VBN Brownian_JJ motions_NNS -LRB-_-LRB- see_VB =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Proposition_NN 6_CD ._.
Let_NNP -LRB-_-LRB- Wt_NNP ,_, t_NN ≥_NN 0_CD -RRB-_-RRB- be_VB a_DT reflected_VBN Brownian_JJ motion_NN with_IN mean_JJ zero_NN and_CC variance_NN σ_NN 2_CD ;_: the_DT reflection_NN barrier_NN is_VBZ 0_CD ._.
Assume_VB the_DT value_NN of_IN Wt_NN at_IN time_NN t_NN is_VBZ equal_JJ to_TO y_NN :_: E_NN -LRB-_-LRB- y_NN -RRB-_-RRB- =_JJ θ_NN -LRB-_-LRB- √_NN tσ_NN 2_CD -RRB-_-RRB- -LRB-_-LRB- 12_CD -RRB-_-RRB-
ting_JJ results_NNS on_IN using_VBG machine_NN learning_NN techniques_NNS in_IN mechanism_NN design_NN ._.
We_PRP only_RB briefly_RB survey_VB the_DT main_JJ techniques_NNS and_CC ideas_NNS and_CC compare_VB them_PRP with_IN the_DT approach_NN of_IN this_DT paper_NN ._.
Most_JJS of_IN these_DT works_NNS ,_, like_IN =_JJ -_: =[_NN 5_CD ,_, 8_CD ,_, 11_CD ,_, 18_CD -RRB-_-RRB- -_: =_JJ -_: ,_, consider_VB one-shot_JJ games_NNS or_CC repeated_VBN auctions_NNS in_IN which_WDT the_DT agents_NNS leave_VBP the_DT environment_NN after_IN they_PRP received_VBD an_DT item_NN ._.
In_IN our_PRP$ setting_NN we_PRP may_MD allocates_VB items_NNS to_TO an_DT agent_NN several_JJ times_NNS and_CC hence_RB ,_, we_PRP need_VBP
on_IN the_DT ad_NN in_IN slot_NN j2_NN being_VBG clicked_VBN ._.
During_IN the_DT exploit_VBP phase_NN ,_, M_NN allocates_VBZ the_DT slots_NNS to_TO the_DT advertisers_NNS with_IN the_DT highest_JJS expected_VBN utility_NN ,_, and_CC the_DT prices_NNS are_VBP determined_VBN according_VBG to_TO Holmstrom_NNP 's_POS lemma_NN -LRB-_-LRB- =_JJ -_: =[_NN 20_CD -RRB-_-RRB- -_: =_JJ -_: ,_, see_VB also_RB -LRB-_-LRB- 1_LS -RRB-_-RRB- -RRB-_-RRB- The_DT estimates_NNS of_IN the_DT utilities_NNS are_VBP updated_VBN based_VBN on_IN the_DT reports_NNS ,_, using_VBG the_DT conditional_JJ distribution_NN ._.
Delayed_VBN Reports_NNP ._.
In_IN some_DT applications_NNS ,_, the_DT value_NN of_IN receiving_VBG the_DT item_NN is_VBZ realized_VBN
-LRB-_-LRB- bµit_NN -LRB-_-LRB- T_NN -RRB-_-RRB- =_JJ PT_NN k_NN =_JJ 1_CD xikrik_NN -RRB-_-RRB- \/_: niT_NN ,_, niT_NN -RRB-_-RRB- 0_CD 0_CD ,_, niT_NN =_JJ 0_CD Call_NN the_DT mechanism_NN based_VBN on_IN this_DT learning_VBG algorithm_NN Mɛ_NN -LRB-_-LRB- iid_NN -RRB-_-RRB- ._.
Lemma_NN 4_CD ._.
If_IN all_DT agents_NNS are_VBP truthful_JJ ,_, then_RB ,_, under_IN Mɛ_NN -LRB-_-LRB- iid_NN -RRB-_-RRB- 1_CD E_NN -LRB-_-LRB- ∆_CD t_NN -RRB-_-RRB- =_JJ O_NN -LRB-_-LRB- √_FW t1_FW −_FW ɛ_FW -RRB-_-RRB- ._.
3_CD See_NNP =_SYM -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: for_IN a_DT similar_JJ algorithm_NN ._.
The_DT proof_NN of_IN this_DT lemma_NN is_VBZ given_VBN in_IN appendix_NN A._NN We_PRP show_VBP that_IN Mɛ_NN -LRB-_-LRB- iid_NN -RRB-_-RRB- ,_, for_IN ε_FW ≤_FW 1_CD ,_, satisfies_VBZ all_PDT the_DT desired_VBN 3_CD properties_NNS we_PRP discussed_VBD in_IN the_DT previous_JJ section_NN ._.
Moreover_RB ,_, it_PRP sa_FW
eber_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- consider_VBP the_DT infinite_JJ horizon_NN version_NN of_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- and_CC propose_VBP a_DT class_NN of_IN incentive_NN compatible_JJ mechanisms_NNS based_VBN on_IN the_DT Gittins_NNP index_NN -LRB-_-LRB- see_VB -LRB-_-LRB- 12_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Taking_VBG a_DT different_JJ approach_NN ,_, Bergemann_NNP and_CC Välimäki_NNP =_SYM -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: and_CC Cavallo_NNP et_FW al._FW -LRB-_-LRB- 9_CD -RRB-_-RRB- propose_VBP an_DT incentive_NN compatible_JJ generalization_NN of_IN the_DT Vickrey-Clark-Groves_NNP mechanism_NN based_VBN on_IN the_DT marginal_JJ contribution_NN of_IN each_DT agent_NN for_IN this_DT environment_NN ._.
All_DT these_DT mechanisms_NNS
ently_RB used_VBN CPC_NNP mechanisms_NNS ._.
We_PRP will_MD use_VB techniques_NNS in_IN learning_NN and_CC mechanism_NN design_NN to_TO obtain_VB this_DT result_NN ._.
In_IN the_DT next_JJ section_NN ,_, we_PRP will_MD formally_RB describe_VB our_PRP$ model_NN in_IN mechanism_NN design_NN terminology_NN -LRB-_-LRB- see_VB =_JJ -_: =[_NN 22_CD -RRB-_-RRB- -_: =_SYM -_: ._. -RRB-_-RRB-
We_PRP will_MD refer_VB to_TO advertisers_NNS as_IN agents_NNS and_CC to_TO the_DT impression_NN of_IN an_DT ad_NN as_IN an_DT item_NN ._.
For_IN simplicity_NN of_IN exposition_NN only_RB ,_, we_PRP assume_VBP only_RB one_CD advertisement_NN slot_NN per_IN page_NN ._.
In_IN section_NN 6_CD we_PRP outline_VBP how_WRB to_TO e_SYM
nfinite_JJ horizon_NN version_NN of_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- and_CC propose_VBP a_DT class_NN of_IN incentive_NN compatible_JJ mechanisms_NNS based_VBN on_IN the_DT Gittins_NNP index_NN -LRB-_-LRB- see_VB -LRB-_-LRB- 12_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Taking_VBG a_DT different_JJ approach_NN ,_, Bergemann_NNP and_CC Välimäki_NNP -LRB-_-LRB- 6_CD -RRB-_-RRB- and_CC Cavallo_NNP et_FW al._FW =_SYM -_: =[_NN 9_CD -RRB-_-RRB- -_: =_SYM -_: propose_VBP an_DT incentive_NN compatible_JJ generalization_NN of_IN the_DT Vickrey-Clark-Groves_NNP mechanism_NN based_VBN on_IN the_DT marginal_JJ contribution_NN of_IN each_DT agent_NN for_IN this_DT environment_NN ._.
All_DT these_DT mechanisms_NNS need_VBP the_DT exact_JJ solu-t_NN
ot_RB apply_VB when_WRB the_DT evolution_NN of_IN the_DT utilities_NNS of_IN the_DT agents_NNS is_VBZ not_RB stationary_JJ over_IN time_NN ._.
This_DT violates_VBZ the_DT last_JJ of_IN our_PRP$ desiderata_NNS ._.
For_IN a_DT comprehensive_JJ survey_NN in_IN dynamic_JJ mechanism_NN design_NN literature_NN see_VBP =_JJ -_: =[_NN 23_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN the_DT context_NN of_IN sponsored_VBN search_NN ,_, attention_NN has_VBZ focused_VBN on_IN ways_NNS of_IN estimating_NN click_VBP through_IN rates_NNS ._.
Gonen_NNP and_CC Pavlov_NNP -LRB-_-LRB- 13_CD -RRB-_-RRB- give_VBP a_DT mechanism_NN which_WDT learns_VBZ the_DT click-through_JJ rates_NNS via_IN sampling_NN and_CC show_NN
rata_JJ ._.
For_IN a_DT comprehensive_JJ survey_NN in_IN dynamic_JJ mechanism_NN design_NN literature_NN see_VBP -LRB-_-LRB- 23_CD -RRB-_-RRB- ._.
In_IN the_DT context_NN of_IN sponsored_VBN search_NN ,_, attention_NN has_VBZ focused_VBN on_IN ways_NNS of_IN estimating_NN click_VBP through_IN rates_NNS ._.
Gonen_NNP and_CC Pavlov_NNP =_SYM -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: give_VB a_DT mechanism_NN which_WDT learns_VBZ the_DT click-through_JJ rates_NNS via_IN sampling_NN and_CC show_VBP that_IN truthful_JJ bidding_NN is_VBZ ,_, with_IN high_JJ probability_NN ,_, a_DT -LRB-_-LRB- weakly_JJ -RRB-_-RRB- dominant_JJ strategy_NN in_IN this_DT mechanism_NN ._.
Along_IN this_DT line_NN ,_, Wortman_NNP e_SYM
a_DT mechanism_NN which_WDT learns_VBZ the_DT click-through_JJ rates_NNS via_IN sampling_NN and_CC show_VBP that_IN truthful_JJ bidding_NN is_VBZ ,_, with_IN high_JJ probability_NN ,_, a_DT -LRB-_-LRB- weakly_JJ -RRB-_-RRB- dominant_JJ strategy_NN in_IN this_DT mechanism_NN ._.
Along_IN this_DT line_NN ,_, Wortman_NNP et_FW al._FW =_SYM -_: =[_NN 26_CD -RRB-_-RRB- -_: =_SYM -_: introduced_VBD an_DT exploration_NN scheme_NN for_IN learning_VBG advertisers_NNS '_POS click-through_JJ rates_NNS in_IN sponsored_VBN search_NN which_WDT maintains_VBZ the_DT equilibrium_NN of_IN the_DT system_NN ._.
In_IN these_DT works_NNS ,_, unlike_IN ours_PRP ,_, the_DT distribution_NN of_IN the_DT
of_IN the_DT ad_NN or_CC by_IN a_DT rival_NN who_WP wishes_VBZ to_TO increase_VB the_DT cost_NN of_IN advertising_NN for_IN the_DT advertiser_NN ._.
Click_VB fraud_NN is_VBZ considered_VBN by_IN many_JJ experts_NNS to_TO be_VB the_DT biggest_JJS challenge_NN facing_VBG the_DT online_JJ advertising_NN industry_NN =_JJ -_: =[_NN 14_CD ,_, 10_CD ,_, 24_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
CPA_NN schemes_NNS are_VBP less_RBR vulnerable_JJ because_IN generating_VBG a_DT fraudulent_JJ action_NN is_VBZ typically_RB more_RBR costly_JJ than_IN generating_VBG a_DT fraudulent_JJ click_VB ._.
For_IN example_NN ,_, an_DT advertiser_NN can_MD define_VB the_DT action_NN as_IN a_DT sale_NN and_CC pay_NN
of_IN the_DT ad_NN or_CC by_IN a_DT rival_NN who_WP wishes_VBZ to_TO increase_VB the_DT cost_NN of_IN advertising_NN for_IN the_DT advertiser_NN ._.
Click_VB fraud_NN is_VBZ considered_VBN by_IN many_JJ experts_NNS to_TO be_VB the_DT biggest_JJS challenge_NN facing_VBG the_DT online_JJ advertising_NN industry_NN =_JJ -_: =[_NN 14_CD ,_, 10_CD ,_, 24_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
CPA_NN schemes_NNS are_VBP less_RBR vulnerable_JJ because_IN generating_VBG a_DT fraudulent_JJ action_NN is_VBZ typically_RB more_RBR costly_JJ than_IN generating_VBG a_DT fraudulent_JJ click_VB ._.
For_IN example_NN ,_, an_DT advertiser_NN can_MD define_VB the_DT action_NN as_IN a_DT sale_NN and_CC pay_NN
n_NN each_DT period_NN is_VBZ Bayesian_JJ incentive_NN compatible_JJ ._.
Bapna_NNP and_CC Weber_NNP -LRB-_-LRB- 4_CD -RRB-_-RRB- consider_VBP the_DT infinite_JJ horizon_NN version_NN of_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- and_CC propose_VBP a_DT class_NN of_IN incentive_NN compatible_JJ mechanisms_NNS based_VBN on_IN the_DT Gittins_NNP index_NN -LRB-_-LRB- see_VB =_JJ -_: =[_NN 12_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Taking_VBG a_DT different_JJ approach_NN ,_, Bergemann_NNP and_CC Välimäki_NNP -LRB-_-LRB- 6_CD -RRB-_-RRB- and_CC Cavallo_NNP et_FW al._FW -LRB-_-LRB- 9_CD -RRB-_-RRB- propose_VBP an_DT incentive_NN compatible_JJ generalization_NN of_IN the_DT Vickrey-Clark-Groves_NNP mechanism_NN based_VBN on_IN the_DT marginal_JJ contributio_NN
s_NNS are_VBP changing_VBG arbitrarily_RB ._.
However_RB ,_, the_DT efficiency_NN -LRB-_-LRB- and_CC therefore_RB the_DT revenue_NN -RRB-_-RRB- of_IN these_DT algorithms_NNS is_VBZ comparable_JJ to_TO the_DT mechanisms_NNS that_WDT allocates_VBZ the_DT item_NN to_TO the_DT single_JJ best_JJS agent_NN -LRB-_-LRB- expert_NN -RRB-_-RRB- -LRB-_-LRB- e.g._FW see_VBP =_JJ -_: =[_NN 17_CD -RRB-_-RRB- -_: =--RRB-_NN ._.
Our_PRP$ goal_NN is_VBZ more_RBR ambitious_JJ :_: our_PRP$ efficiency_NN is_VBZ close_RB the_DT most_RBS efficient_JJ allocation_NN which_WDT might_MD allocate_VB the_DT item_NN to_TO different_JJ agents_NNS at_IN different_JJ times_NNS ._.
On_IN the_DT other_JJ hand_NN ,_, we_PRP focus_VBP on_IN utility_NN values_NNS
o_NN small_JJ compared_VBN to_TO a_DT second_JJ price_NN auction_NN ._.
4_LS ._.
The_DT correctness_NN of_IN the_DT mechanism_NN does_VBZ not_RB depend_VB on_IN an_DT a-priori_NN knowledge_NN of_IN the_DT distribution_NN of_IN uit_NN 's_POS ._.
This_DT feature_NN is_VBZ motivated_VBN by_IN the_DT Wilson_NNP doctrine_NN =_JJ -_: =[_NN 25_CD -RRB-_-RRB- -_: =_SYM -_: 2_CD ._.
The_DT precise_JJ manner_NN in_IN which_WDT these_DT properties_NNS are_VBP formalized_VBN is_VBZ described_VBN in_IN section_NN 2_CD ._.
We_PRP will_MD build_VB our_PRP$ mechanisms_NNS on_IN a_DT sampling-based_JJ learning_NN algorithm_NN ._.
The_DT learning_NN algorithm_NN is_VBZ used_VBN to_TO estim_VB
n_NN of_IN the_DT environment_NN considered_VBN here_RB ,_, Athey_NNP and_CC Segal_NNP -LRB-_-LRB- 2_LS -RRB-_-RRB- construct_NN an_DT efficient_JJ ,_, budget_NN balanced_JJ ,_, mechanism_NN where_WRB truthful_JJ revelation_NN in_IN each_DT period_NN is_VBZ Bayesian_JJ incentive_NN compatible_JJ ._.
Bapna_NNP and_CC Weber_NNP =_SYM -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: consider_VB the_DT infinite_JJ horizon_NN version_NN of_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- and_CC propose_VBP a_DT class_NN of_IN incentive_NN compatible_JJ mechanisms_NNS based_VBN on_IN the_DT Gittins_NNP index_NN -LRB-_-LRB- see_VB -LRB-_-LRB- 12_CD -RRB-_-RRB- -RRB-_-RRB- ._.
Taking_VBG a_DT different_JJ approach_NN ,_, Bergemann_NNP and_CC Välimäki_NNP -LRB-_-LRB- 6_CD -RRB-_-RRB- and_CC
ting_JJ results_NNS on_IN using_VBG machine_NN learning_NN techniques_NNS in_IN mechanism_NN design_NN ._.
We_PRP only_RB briefly_RB survey_VB the_DT main_JJ techniques_NNS and_CC ideas_NNS and_CC compare_VB them_PRP with_IN the_DT approach_NN of_IN this_DT paper_NN ._.
Most_JJS of_IN these_DT works_NNS ,_, like_IN =_JJ -_: =[_NN 5_CD ,_, 8_CD ,_, 11_CD ,_, 18_CD -RRB-_-RRB- -_: =_JJ -_: ,_, consider_VB one-shot_JJ games_NNS or_CC repeated_VBN auctions_NNS in_IN which_WDT the_DT agents_NNS leave_VBP the_DT environment_NN after_IN they_PRP received_VBD an_DT item_NN ._.
In_IN our_PRP$ setting_NN we_PRP may_MD allocates_VB items_NNS to_TO an_DT agent_NN several_JJ times_NNS and_CC hence_RB ,_, we_PRP need_VBP
h_NN rates_NNS in_IN sponsored_VBN search_NN which_WDT maintains_VBZ the_DT equilibrium_NN of_IN the_DT system_NN ._.
In_IN these_DT works_NNS ,_, unlike_IN ours_PRP ,_, the_DT distribution_NN of_IN the_DT utilities_NNS of_IN agents_NNS are_VBP assumed_VBN to_TO be_VB fixed_VBN over_IN time_NN ._.
Immorlica_NNP et_FW al._FW =_SYM -_: =[_NN 15_CD -RRB-_-RRB- -_: =_JJ -_: ,_, and_CC later_JJ Mahdian_NN and_CC Tomak_NN -LRB-_-LRB- 19_CD -RRB-_-RRB- ,_, examine_VBP the_DT vulnerability_NN of_IN various_JJ procedures_NNS for_IN estimating_NN click_VBP through_IN ,_, and_CC identify_VBP a_DT class_NN of_IN click_VB through_IN learning_VBG algorithms_NNS in_IN which_WDT fraudulent_JJ clicks_NNS c_NN
of_IN the_DT ad_NN or_CC by_IN a_DT rival_NN who_WP wishes_VBZ to_TO increase_VB the_DT cost_NN of_IN advertising_NN for_IN the_DT advertiser_NN ._.
Click_VB fraud_NN is_VBZ considered_VBN by_IN many_JJ experts_NNS to_TO be_VB the_DT biggest_JJS challenge_NN facing_VBG the_DT online_JJ advertising_NN industry_NN =_JJ -_: =[_NN 14_CD ,_, 10_CD ,_, 24_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
CPA_NN schemes_NNS are_VBP less_RBR vulnerable_JJ because_IN generating_VBG a_DT fraudulent_JJ action_NN is_VBZ typically_RB more_RBR costly_JJ than_IN generating_VBG a_DT fraudulent_JJ click_VB ._.
For_IN example_NN ,_, an_DT advertiser_NN can_MD define_VB the_DT action_NN as_IN a_DT sale_NN and_CC pay_NN
t_NN different_JJ times_NNS ._.
On_IN the_DT other_JJ hand_NN ,_, we_PRP focus_VBP on_IN utility_NN values_NNS that_WDT change_VBP smoothly_RB -LRB-_-LRB- e.g._FW like_IN a_DT Brownian_JJ motion_NN -RRB-_-RRB- ._.
In_IN a_DT finitely_RB repeated_VBN version_NN of_IN the_DT environment_NN considered_VBN here_RB ,_, Athey_NNP and_CC Segal_NNP =_SYM -_: =[_NN 2_CD -RRB-_-RRB- -_: =_JJ -_: construct_NN an_DT efficient_JJ ,_, budget_NN balanced_JJ ,_, mechanism_NN where_WRB truthful_JJ revelation_NN in_IN each_DT period_NN is_VBZ Bayesian_JJ incentive_NN compatible_JJ ._.
Bapna_NNP and_CC Weber_NNP -LRB-_-LRB- 4_CD -RRB-_-RRB- consider_VBP the_DT infinite_JJ horizon_NN version_NN of_IN -LRB-_-LRB- 2_CD -RRB-_-RRB- and_CC prop_VB
of_IN the_DT ad_NN or_CC by_IN a_DT rival_NN who_WP wishes_VBZ to_TO increase_VB the_DT cost_NN of_IN advertising_NN for_IN the_DT advertiser_NN ._.
Click_VB fraud_NN is_VBZ considered_VBN by_IN many_JJ experts_NNS to_TO be_VB the_DT biggest_JJS challenge_NN facing_VBG the_DT online_JJ advertising_NN industry_NN =_JJ -_: =[_NN 14_CD ,_, 10_CD ,_, 24_CD ,_, 21_CD -RRB-_-RRB- -_: =_SYM -_: ._.
CPA_NN schemes_NNS are_VBP less_RBR vulnerable_JJ because_IN generating_VBG a_DT fraudulent_JJ action_NN is_VBZ typically_RB more_RBR costly_JJ than_IN generating_VBG a_DT fraudulent_JJ click_VB ._.
For_IN example_NN ,_, an_DT advertiser_NN can_MD define_VB the_DT action_NN as_IN a_DT sale_NN and_CC pay_NN
way_NN ._.
CPA_NN models_NNS can_MD be_VB the_DT ideal_JJ charging_NN scheme_NN ,_, especially_RB for_IN small_JJ and_CC risk_VB averse_JJ advertisers_NNS ._.
We_PRP will_MD briefly_RB describe_VB a_DT few_JJ advantages_NNS of_IN this_DT charging_VBG scheme_NN over_IN CPC_NNP and_CC refer_VB the_DT reader_NN to_TO =_JJ -_: =[_NN 19_CD -RRB-_-RRB- -_: =_SYM -_: for_IN a_DT more_RBR detailed_JJ discussion_NN ._.
One_CD of_IN the_DT drawbacks_NNS of_IN the_DT CPC_NNP scheme_NN is_VBZ that_IN it_PRP requires_VBZ the_DT advertisers_NNS to_TO submit_VB their_PRP$ bids_NNS before_IN observing_VBG the_DT profits_NNS generated_VBN by_IN the_DT users_NNS clicking_VBG on_IN their_PRP$ a_DT
In_IN contrast_NN ,_, we_PRP study_VBD conditions_NNS which_WDT guarantee_VBP incentive_NN compatibility_NN and_CC efficiency_NN ,_, while_IN the_DT utility_NN of_IN -LRB-_-LRB- all_DT -RRB-_-RRB- agents_NNS may_MD evolve_VB over_IN time_NN ._.
Recently_RB ,_, Babaioff_NNP et_FW al._FW -LRB-_-LRB- 5_CD -RRB-_-RRB- ,_, and_CC Devanur_NN and_CC Kakade_NN =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =_JJ -_: give_VB a_DT characterization_NN of_IN incentive_NN compatible_JJ multi-armed_JJ bandit_NN mechanisms_NNS ._.
They_PRP show_VBP that_IN if_IN one_CD requires_VBZ truthfulness_NN as_IN a_DT weakly_RB dominant_JJ strategy_NN then_RB the_DT the_DT exploration_NN and_CC exploitation_NN phas_NNS
nvironments_NNS where_WRB learning_VBG algorithms_NNS satisfying_VBG these_DT sufficient_JJ conditions_NNS exist_VBP ._.
The_DT mechanism_NN consists_VBZ of_IN two_CD phases_NNS :_: explore_VB and_CC exploit_VB ._.
During_IN the_DT explore_VBP phase_NN ,_, with_IN probability_NN η_NN -LRB-_-LRB- t_NN -RRB-_-RRB- ,_, η_NN :_: N_NN →_NN =_JJ -_: =[_NN 0,1_CD -RRB-_-RRB- -_: =_JJ -_: ,_, the_DT item_NN is_VBZ allocated_VBN for_IN free_JJ to_TO a_DT randomly_RB chosen_VBN agent_NN ._.
During_IN the_DT exploit_VBP phase_NN ,_, the_DT mechanism_NN allocates_VBZ the_DT item_NN to_TO the_DT agent_NN with_IN the_DT highest_JJS estimated_VBN expected_JJ utility_NN ._.
Afterwards_RB ,_, the_DT agent_NN
