Combining_VBG anchor_NN text_NN categorization_NN and_CC graph_NN analysis_NN for_IN paid_VBN link_NN detection_NN
In_IN order_NN to_TO artificially_RB boost_VB the_DT rank_NN of_IN commercial_JJ pages_NNS in_IN search_NN engine_NN results_NNS ,_, search_NN engine_NN optimizers_NNS pay_VBP for_IN links_NNS to_TO these_DT pages_NNS on_IN other_JJ websites_NNS ._.
Identifying_VBG paid_VBN links_NNS is_VBZ important_JJ for_IN a_DT web_NN search_NN engine_NN to_TO produce_VB highly_RB relevant_JJ results_NNS ._.
In_IN this_DT paper_NN we_PRP introduce_VBP a_DT novel_JJ method_NN of_IN identifying_VBG such_JJ links_NNS ._.
We_PRP start_VBP with_IN training_NN a_DT classifier_NN of_IN anchor_NN text_NN topics_NNS and_CC analyzing_VBG web_NN pages_NNS for_IN diversity_NN of_IN their_PRP$ outgoing_JJ commercial_JJ links_NNS ._.
Then_RB we_PRP use_VBP this_DT information_NN and_CC analyze_VB link_NN graph_NN of_IN the_DT Russian_JJ Web_NN to_TO find_VB pages_NNS that_WDT sell_VBP links_NNS and_CC sites_NNS that_WDT buy_VBP links_NNS and_CC to_TO identify_VB the_DT paid_VBN links_NNS ._.
Testing_VBG on_IN manually_RB marked_JJ samples_NNS showed_VBD high_JJ efficiency_NN of_IN the_DT algorithm_NN ._.
ork_NN already_RB done_VBN by_IN optimizers_NNS ._.
2.3_CD SEO-out_NN and_CC SEO-in_NN Classifiers_NNS For_IN further_JJ analysis_NN we_PRP used_VBD a_DT BHITS-like_JJ algorithm_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
HITS_NNS and_CC its_PRP$ various_JJ modifications_NNS were_VBD already_RB used_VBN to_TO find_VB spam_NN links_NNS -LRB-_-LRB- 5_CD -RRB-_-RRB- =_JJ -_: =[_NN 6_CD -RRB-_-RRB- -_: =_JJ -_: ,_, now_RB we_PRP use_VBP it_PRP to_TO find_VB paid_VBN links_NNS ._.
We_PRP used_VBD a_DT bipartite_JJ link_NN graph_NN -LRB-_-LRB- the_DT source_NN pages_NNS to_TO the_DT left_NN and_CC the_DT target_NN hosts_NNS to_TO the_DT right_NN -RRB-_-RRB- with_IN all_DT known_JJ spam_NN pages_NNS and_CC links_NNS from_IN link_NN farms_NNS ,_, etc._FW removed_VBN ._.
W_NN
The_DT work_NN consists_VBZ of_IN two_CD parts_NNS ._.
The_DT first_JJ part_NN is_VBZ text_NN processing_NN and_CC topic_NN classification_NN ,_, the_DT second_JJ one_CD is_VBZ creating_VBG of_IN a_DT multi-topic_JJ page_NN seed_NN set_NN and_CC the_DT link_NN graph_NN mark_NN up_IN using_VBG a_DT modified_VBN HITS_NN =_SYM -_: =[_NN 1_CD -RRB-_-RRB- -_: =_JJ -_: algorithm_NN hubs_NNS corresponding_VBG to_TO link_VB selling_VBG sites_NNS and_CC authorities_NNS -_: to_TO link-promoted_JJ sites_NNS ._.
However_RB ,_, the_DT main_JJ purpose_NN of_IN the_DT algorithm_NN is_VBZ to_TO detect_VB every_DT paid_VBN rather_RB than_IN link_VB selling_VBG pages_NNS or_CC paid_VBN
he_PRP work_VB already_RB done_VBN by_IN optimizers_NNS ._.
2.3_CD SEO-out_NN and_CC SEO-in_NN Classifiers_NNS For_IN further_JJ analysis_NN we_PRP used_VBD a_DT BHITS-like_JJ algorithm_NN -LRB-_-LRB- 4_CD -RRB-_-RRB- ._.
HITS_NNS and_CC its_PRP$ various_JJ modifications_NNS were_VBD already_RB used_VBN to_TO find_VB spam_NN links_NN =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_JJ -_: -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, now_RB we_PRP use_VBP it_PRP to_TO find_VB paid_VBN links_NNS ._.
We_PRP used_VBD a_DT bipartite_JJ link_NN graph_NN -LRB-_-LRB- the_DT source_NN pages_NNS to_TO the_DT left_NN and_CC the_DT target_NN hosts_NNS to_TO the_DT right_NN -RRB-_-RRB- with_IN all_DT known_JJ spam_NN pages_NNS and_CC links_NNS from_IN link_NN farms_NNS ,_, etc._FW remove_VB
f_LS these_DT texts_NNS a_DT new_JJ lexicon_NN with_IN about_RB 200_CD 000_CD words_NNS and_CC 800_CD 000_CD word_NN pairs_NNS was_VBD created_VBN ._.
The_DT large_JJ number_NN of_IN terms_NNS lets_VBZ us_PRP create_VB a_DT new_JJ efficient_JJ topic_NN classifier_NN based_VBN on_IN the_DT 1_CD st_NN order_NN Markov_NNP chain_NN =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: ._.
The_DT lexicon_NN was_VBD manually_RB adjusted_VBN according_VBG to_TO the_DT outrage_NN mistakes_NNS analysis_NN ._.
Thus_RB building_VBG such_PDT a_DT huge_JJ lexicon_NN needs_VBZ rather_RB little_JJ human_JJ effort_NN ._.
In_IN fact_NN we_PRP use_VBP automatically_RB the_DT work_NN already_RB done_VBN b_NN
ing_NN ''_'' a_DT text_NN fragment_NN is_VBZ will_MD be_VB called_VBN SEO-text_JJ score_NN ._.
From_IN a_DT popular_JJ SEO_NN industry_NN rating_NN site_NN we_PRP took_VBD a_DT seed_NN set_NN of_IN the_DT SEO_NNP queries_NNS ._.
On_IN its_PRP$ basis_NN we_PRP created_VBD an_DT initial_JJ SEO_NN text_NN classifier_NN similar_JJ to_TO =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: in_IN which_WDT only_RB 2_CD topics_NNS ,_, SEO_NN and_CC non-SEO_NN ,_, were_VBD used_VBN ._.
Using_VBG the_DT iteration_NN method_NN similar_JJ to_TO the_DT one_NN we_PRP describe_VBP in_IN 2.2_CD we_PRP got_VBD a_DT large_JJ list_NN of_IN word_NN unigrams_NNS -LRB-_-LRB- 300_CD 000_CD -RRB-_-RRB- and_CC bigrams_NNS -LRB-_-LRB- 1_CD 500_CD 000_CD -RRB-_-RRB- typical_JJ for_IN
huge_JJ lexicon_NN needs_VBZ rather_RB little_JJ human_JJ effort_NN ._.
In_IN fact_NN we_PRP use_VBP automatically_RB the_DT work_NN already_RB done_VBN by_IN optimizers_NNS ._.
2.3_CD SEO-out_NN and_CC SEO-in_NN Classifiers_NNS For_IN further_JJ analysis_NN we_PRP used_VBD a_DT BHITS-like_JJ algorithm_NN =_JJ -_: =[_NN 4_CD -RRB-_-RRB- -_: =_SYM -_: ._.
HITS_NNS and_CC its_PRP$ various_JJ modifications_NNS were_VBD already_RB used_VBN to_TO find_VB spam_NN links_NNS -LRB-_-LRB- 5_CD -RRB-_-RRB- -LRB-_-LRB- 6_CD -RRB-_-RRB- ,_, now_RB we_PRP use_VBP it_PRP to_TO find_VB paid_VBN links_NNS ._.
We_PRP used_VBD a_DT bipartite_JJ link_NN graph_NN -LRB-_-LRB- the_DT source_NN pages_NNS to_TO the_DT left_NN and_CC the_DT target_NN hosts_VBZ t_NN
