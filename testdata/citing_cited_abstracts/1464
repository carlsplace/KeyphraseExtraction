Learning Planning Operators in Real-World, Partially Observable Environments We are interested in the development of activities in  situated, embodied agents such as mobile robots. Central  to our theory of development is means-ends analysis  planning, and as such, we must rely on operator  models that can express the eects of a robot's action  in a dynamic, partially-observable environment. This  paper presents a two-step process which employs clustering  and decision tree induction to perform unsupervised  learning of operator models from simple interactions  between an agent and its environment. We report  our ndings with an implementation of this system on  a Pioneer-1 mobile robot.
