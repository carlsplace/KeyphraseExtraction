BLEU: a Method for Automatic Evaluation of Machine Translation Human evaluations of machine translation  are extensive but expensive. Human evaluations  can take months to finish and involve  human labor that can not be reused.
