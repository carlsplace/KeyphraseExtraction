PEGASUS_NNS :_: A_DT policy_NN search_NN method_NN for_IN large_JJ MDPs_NNS and_CC POMDPs_NNS We_PRP propose_VBP a_DT new_JJ approach_NN to_TO the_DT problem_NN of_IN searching_VBG a_DT space_NN of_IN policies_NNS for_IN a_DT Markov_NNP decision_NN process_NN -LRB-_-LRB- MDP_NN -RRB-_-RRB- or_CC a_DT partially_RB observable_JJ Markov_NNP decision_NN process_NN -LRB-_-LRB- POMDP_NN -RRB-_-RRB- ,_, given_VBN a_DT model_NN ._.
Our_PRP$ approach_NN is_VBZ based_VBN on_IN the_DT following_JJ observation_NN :_: Any_DT -LRB-_-LRB- PO_NN -RRB-_-RRB- MDP_NN can_MD be_VB transformed_VBN into_IN an_DT ``_`` equivalent_JJ ''_'' POMDP_NN in_IN which_WDT all_DT state_NN transitions_NNS -LRB-_-LRB- given_VBN the_DT current_JJ state_NN and_CC action_NN -RRB-_-RRB- are_VBP deterministic_JJ ._.
This_DT reduces_VBZ the_DT general_JJ problem_NN of_IN policy_NN search_NN to_TO one_CD in_IN which_WDT we_PRP need_VBP only_RB consider_VB POMDPs_NNS with_IN deterministic_JJ transitions_NNS ._.
We_PRP give_VBP a_DT natural_JJ way_NN of_IN estimating_VBG the_DT value_NN of_IN all_DT policies_NNS in_IN these_DT transformed_VBN POMDPs_NNS ._.
Policy_NN search_NN is_VBZ then_RB simply_RB performed_VBN by_IN searching_VBG for_IN a_DT policy_NN with_IN high_JJ estimated_VBN value_NN ._.
We_PRP also_RB establish_VBP conditions_NNS under_IN which_WDT our_PRP$ value_NN estimates_NNS will_MD be_VB good_JJ ,_, recovering_VBG theoretical_JJ results_NNS similar_JJ to_TO those_DT of_IN Kearns_NNP ,_, Mansour_NNP and_CC Ng_NNP -LRB-_-LRB- 7_CD -RRB-_-RRB- ,_, but_CC with_IN ``_`` sample_NN complexity_NN ''_'' bounds_NNS that_WDT have_VBP only_RB a_DT polynomial_JJ rather_RB than_IN exponential_JJ dependence_NN on_IN the_DT horizon_NN time_NN ._.
Our_PRP$ method_NN appl_NN ..._:
