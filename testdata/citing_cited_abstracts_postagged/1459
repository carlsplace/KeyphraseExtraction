Instance-Based_NNP Utile_NNP Distinctions_NNPS for_IN Reinforcement_NNP Learning_NNP with_IN Hidden_NNP State_NNP We_PRP present_VBP Utile_NNP Suffix_NNP Memory_NN ,_, a_DT reinforcement_NN learning_NN algorithm_NN that_WDT uses_VBZ short-term_JJ memory_NN to_TO overcome_VB the_DT state_NN aliasing_NN that_WDT results_VBZ from_IN hidden_JJ state_NN ._.
By_IN combining_VBG the_DT advantages_NNS of_IN previous_JJ work_NN in_IN instance-based_JJ -LRB-_-LRB- or_CC ``_`` memorybased_JJ ''_'' -RRB-_-RRB- learning_NN and_CC previous_JJ work_NN with_IN statistical_JJ tests_NNS for_IN separating_VBG noise_NN from_IN task_NN structure_NN ,_, the_DT method_NN learns_VBZ quickly_RB ,_, creates_VBZ only_RB as_RB much_JJ memory_NN as_IN needed_VBN for_IN the_DT task_NN at_IN hand_NN ,_, and_CC handles_VBZ noise_NN well_RB ._.
Utile_FW Suffix_FW Memory_NN uses_VBZ a_DT tree-structured_JJ representation_NN ,_, and_CC is_VBZ related_JJ to_TO work_VB on_IN Prediction_NNP Suffix_NNP Trees_NNP -LRB-_-LRB- Ron_NNP et_FW al._FW ,_, 1994_CD -RRB-_-RRB- ,_, Parti-game_NN -LRB-_-LRB- Moore_NNP ,_, 1993_CD -RRB-_-RRB- ,_, G-algorithm_NN -LRB-_-LRB- Chapman_NNP and_CC Kaelbling_NNP ,_, 1991_CD -RRB-_-RRB- ,_, and_CC Variable_NNP Resolution_NNP Dynamic_NNP Programming_NNP -LRB-_-LRB- Moore_NNP ,_, 1991_CD -RRB-_-RRB- ._.
1_CD INTRODUCTION_NN The_DT sensory_JJ systems_NNS of_IN embedded_JJ agents_NNS are_VBP inherently_RB limited_JJ ._.
When_WRB a_DT reinforcement_NN learning_NN agent_NN 's_POS sensory_JJ limitations_NNS hide_VBP features_NNS of_IN the_DT environment_NN from_IN the_DT agent_NN ,_, we_PRP say_VBP that_IN the_DT agent_NN suffers_VBZ from_IN hidden_JJ state_NN ._.
There_EX are_VBP many_JJ reasons_NNS why_WRB important_JJ features_NNS can_MD be_VB hidden_VBN ..._:
