Text_NN Classification_NN from_IN Labeled_JJ and_CC Unlabeled_JJ Documents_NNS using_VBG EM_NN ._.
This_DT paper_NN shows_VBZ that_IN the_DT accuracy_NN of_IN learned_VBN text_NN classifiers_NNS can_MD be_VB improved_VBN by_IN augmenting_VBG a_DT small_JJ number_NN of_IN labeled_JJ training_NN documents_NNS with_IN a_DT large_JJ pool_NN of_IN unlabeled_JJ documents_NNS ._.
This_DT is_VBZ important_JJ because_IN in_IN many_JJ text_NN classification_NN problems_NNS obtaining_VBG training_NN labels_NNS is_VBZ expensive_JJ ,_, while_IN large_JJ quantities_NNS of_IN unlabeled_JJ documents_NNS are_VBP readily_RB available_JJ ._.
We_PRP introduce_VBP an_DT algorithm_NN for_IN learning_VBG from_IN labeled_JJ and_CC unlabeled_JJ documents_NNS based_VBN on_IN the_DT combination_NN of_IN Expectation-Maximization_NN -LRB-_-LRB- EM_NN -RRB-_-RRB- and_CC a_DT naive_JJ Bayes_NNP classifier_NN ._.
The_DT algorithm_NN first_RB trains_VBZ a_DT classifier_NN using_VBG the_DT available_JJ labeled_JJ documents_NNS ,_, and_CC probabilistically_RB labels_VBZ the_DT unlabeled_JJ documents_NNS ._.
It_PRP then_RB trains_VBZ a_DT new_JJ classifier_NN using_VBG the_DT labels_NNS for_IN all_PDT the_DT documents_NNS ,_, and_CC iterates_NNS to_TO convergence_NN ._.
This_DT basic_JJ EM_NN procedure_NN works_VBZ well_RB when_WRB the_DT data_NNS conform_VBP to_TO the_DT generative_JJ assumptions_NNS of_IN the_DT model_NN ._.
However_RB these_DT assumptions_NNS are_VBP often_RB violated_VBN in_IN practice_NN ,_, and_CC poor_JJ performance_NN can_MD result_VB ._.
We_PRP present_VBP two_CD extensions_NNS to_TO the_DT algorithm_NN that_WDT improve_VBP ..._:
