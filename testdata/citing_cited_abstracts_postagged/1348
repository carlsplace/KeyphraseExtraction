Finding_VBG structure_NN in_IN time_NN Time_NNP underlies_VBZ many_JJ interesting_JJ human_JJ behaviors_NNS ._.
Thus_RB ,_, the_DT question_NN of_IN how_WRB to_TO represent_VB time_NN in_IN connectionist_NN models_NNS is_VBZ very_RB important_JJ ._.
One_CD approach_NN is_VBZ to_TO represent_VB time_NN implicitly_RB by_IN its_PRP$ effects_NNS on_IN processing_NN rather_RB than_IN explicitly_RB -LRB-_-LRB- as_IN in_IN a_DT spatial_JJ representation_NN -RRB-_-RRB- ._.
The_DT current_JJ report_NN develops_VBZ a_DT proposal_NN along_IN these_DT lines_NNS first_RB described_VBN by_IN Jordan_NNP -LRB-_-LRB- 1986_CD -RRB-_-RRB- which_WDT involves_VBZ the_DT use_NN of_IN recurrent_JJ links_NNS in_IN order_NN to_TO provide_VB networks_NNS with_IN a_DT dynamic_JJ memory_NN ._.
In_IN this_DT approach_NN ,_, hidden_JJ unit_NN patterns_NNS are_VBP fed_VBN back_RB to_TO themselves_PRP ;_: the_DT internal_JJ representations_NNS which_WDT develop_VBP thus_RB reflect_VBP task_NN demands_NNS in_IN the_DT context_NN of_IN prior_JJ internal_JJ states_NNS ._.
A_DT set_NN of_IN simulations_NNS is_VBZ reported_VBN which_WDT range_VBP from_IN relatively_RB simple_JJ problems_NNS -LRB-_-LRB- temporal_JJ version_NN of_IN XOR_NN -RRB-_-RRB- to_TO discovering_VBG syntactic\/semantic_JJ features_NNS for_IN words_NNS ._.
The_DT networks_NNS are_VBP able_JJ to_TO learn_VB interesting_JJ internal_JJ representations_NNS which_WDT incorporate_VBP task_NN demands_NNS with_IN memory_NN demands_NNS ;_: indeed_RB ,_, in_IN this_DT approach_NN the_DT notion_NN of_IN memory_NN is_VBZ inextricably_RB bound_VBN up_RP with_IN task_NN processing_NN ._.
These_DT representations_NNS reveal_VBP a_DT rich_JJ structure_NN ,_, which_WDT allows_VBZ them_PRP to_TO be_VB highly_RB context-dependent_JJ while_IN also_RB expressing_VBG generalizations_NNS across_IN classes_NNS of_IN items_NNS ._.
These_DT representations_NNS suggest_VBP a_DT method_NN for_IN representing_VBG lexical_JJ categories_NNS and_CC the_DT type\/token_NN distinction_NN ._.
