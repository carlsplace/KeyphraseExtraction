Learning_VBG finite-state_JJ controllers_NNS for_IN partially_RB observable_JJ environments_NNS Reactive_JJ -LRB-_-LRB- memoryless_JJ -RRB-_-RRB- policies_NNS are_VBP sufficient_JJ in_IN completely_RB observable_JJ Markov_NNP decision_NN processes_NNS -LRB-_-LRB- MDPs_NNS -RRB-_-RRB- ,_, but_CC some_DT kind_NN of_IN memory_NN is_VBZ usually_RB necessary_JJ for_IN optimal_JJ control_NN of_IN a_DT partially_RB observable_JJ MDP_NN ._.
Policies_NNS with_IN finite_JJ memory_NN can_MD be_VB represented_VBN as_IN finite-state_JJ automata_NN ._.
In_IN this_DT paper_NN ,_, we_PRP extend_VBP Baird_NNP and_CC Moore_NNP 's_POS VAPS_NNP algorithm_NN to_TO the_DT problem_NN of_IN learning_VBG general_JJ finite-state_JJ automata_NN ._.
Because_IN it_PRP performs_VBZ stochastic_JJ gradient_NN descent_NN ,_, this_DT algorithm_NN can_MD be_VB shown_VBN to_TO converge_VB to_TO a_DT locally_RB optimal_JJ finitestate_NN controller_NN ._.
We_PRP provide_VBP the_DT details_NNS of_IN the_DT algorithm_NN and_CC then_RB consider_VB the_DT question_NN of_IN under_IN what_WP conditions_VBZ stochastic_JJ gradient_NN descent_NN will_MD outperform_VB exact_JJ gradient_NN descent_NN ._.
We_PRP conclude_VBP with_IN empirical_JJ results_NNS comparing_VBG the_DT performance_NN of_IN stochastic_JJ and_CC exact_JJ gradient_NN descent_NN ,_, and_CC showing_VBG the_DT ability_NN of_IN our_PRP$ algorithm_NN to_TO extract_VB the_DT useful_JJ information_NN contained_VBN in_IN the_DT sequence_NN of_IN past_JJ observations_NNS to_TO compensate_VB for_IN the_DT lack_NN of_IN observability_NN at_IN each_DT time-step_NN ._.
1_CD
