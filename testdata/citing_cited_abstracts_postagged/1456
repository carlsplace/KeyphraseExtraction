Approximate_NNP Planning_NNP in_IN Large_JJ POMDPs_NNS via_IN Reusable_NNP Trajectories_NNPS We_PRP consider_VBP the_DT problem_NN of_IN choosing_VBG a_DT near-best_JJ strategy_NN from_IN a_DT restricted_JJ class_NN of_IN strategies_NNS in_IN a_DT partially_RB observable_JJ Markov_NNP decision_NN process_NN -LRB-_-LRB- POMDP_NN -RRB-_-RRB- ._.
We_PRP assume_VBP we_PRP are_VBP given_VBN the_DT ability_NN to_TO simulate_VB the_DT behavior_NN of_IN the_DT POMDP_NN ,_, and_CC we_PRP provide_VBP methods_NNS for_IN generating_VBG simulated_JJ experience_NN su_FW cient_FW to_TO accurately_RB approximate_JJ the_DT expected_VBN return_NN of_IN any_DT strategy_NN in_IN the_DT class_NN ._.
We_PRP prove_VBP upper_JJ bounds_NNS on_IN the_DT amount_NN of_IN simulated_JJ experience_NN our_PRP$ methods_NNS must_MD generate_VB in_IN order_NN to_TO achieve_VB such_JJ uniform_JJ approximation_NN ._.
These_DT bounds_NNS have_VBP no_DT dependence_NN on_IN the_DT size_NN or_CC complexity_NN of_IN the_DT underlying_JJ POMDP_NN ,_, but_CC depend_VBP only_RB on_IN the_DT complexity_NN of_IN the_DT restricted_JJ strategy_NN class_NN ._.
The_DT main_JJ challenge_NN is_VBZ in_IN generating_VBG trajectories_NNS in_IN the_DT POMDP_NN that_WDT can_MD be_VB reused_VBN ,_, in_IN the_DT sense_NN that_IN they_PRP simultaneously_RB provide_VBP estimates_NNS of_IN the_DT return_NN of_IN many_JJ strategies_NNS in_IN the_DT class_NN ._.
Our_PRP$ measure_NN of_IN strategy_NN class_NN complexity_NN generalizes_VBZ the_DT classical_JJ notion_NN of_IN VC_NN dimension_NN ,_, and_CC our_PRP$ methods_NNS develop_VBP connections_NNS between_IN problems_NNS of_IN current_JJ interest_NN in_IN reinforcement_NN learning_NN and_CC well-studied_JJ issues_NNS in_IN the_DT theory_NN of_IN supervised_JJ learning_NN ._.
We_PRP also_RB discuss_VBP a_DT number_NN of_IN practical_JJ planning_NN algorithms_NNS for_IN POMDPs_NNS that_WDT arise_VBP from_IN our_PRP$ reusable_JJ trajectories_NNS ._.
